WEBVTT

1
00:00:00.730 --> 00:00:01.391
In this video,

2
00:00:01.391 --> 00:00:06.100
we'll look at loss functions that a we can use for training or throwing
quarters.

3
00:00:08.020 --> 00:00:10.080
So we mentioned in the previous video that,
uh,

4
00:00:10.110 --> 00:00:14.290
notone quarter is really just a feed forward neural network where at the output,

5
00:00:14.320 --> 00:00:15.430
instead of predicting,

6
00:00:15.431 --> 00:00:20.431
say a class label will be predicting or reconstructing the original input x.

7
00:00:20.530 --> 00:00:25.450
So we'll call it the input x hat.
And uh,
uh,

8
00:00:25.451 --> 00:00:29.680
and so now what we need to train an auto encoder is to define a loss function

9
00:00:29.681 --> 00:00:33.940
that will compare x with x hats somehow,
uh,
to measure how well,

10
00:00:33.941 --> 00:00:37.330
how good the reconstruction is.
And then using that lost function,

11
00:00:37.331 --> 00:00:40.480
we can train the auto encoder to minimize it on average,

12
00:00:40.481 --> 00:00:44.620
on a training set by doing some grading dissent on that last function.

13
00:00:45.400 --> 00:00:48.070
So let's look at some loss functions for different types of data.

14
00:00:50.330 --> 00:00:51.163
<v 1>Okay.</v>

15
00:00:51.450 --> 00:00:54.450
<v 0>So imagine we have binary inputs.</v>

16
00:00:54.480 --> 00:00:59.480
That is the input vector x is a l a r vectors that contain elements that are

17
00:01:01.560 --> 00:01:03.030
either zero or one.

18
00:01:04.110 --> 00:01:07.770
The most popular loss function is this lost function here.

19
00:01:07.800 --> 00:01:12.570
It's often referred to as the cross entropy,
uh,
mentioned,
uh,
in a minute.

20
00:01:12.630 --> 00:01:17.000
Why we call it the cross entropy will just look at the uh,

21
00:01:17.040 --> 00:01:17.701
equation.

22
00:01:17.701 --> 00:01:22.701
So it's going to be minus the sum over all the elements in the input vector of

23
00:01:25.370 --> 00:01:26.011
ex gay.

24
00:01:26.011 --> 00:01:31.011
So that's the key element of the input vector times log of the reconstruction of

25
00:01:31.051 --> 00:01:32.640
the key input element,

26
00:01:33.150 --> 00:01:38.150
plus one minus x gay of log of one minus x minus x hat came.

27
00:01:39.510 --> 00:01:44.460
Um,
so let's look at this a little bit more closely.
For instance,
imagine that,

28
00:01:44.530 --> 00:01:47.280
uh,
for some given K ECC was equal to one,

29
00:01:48.120 --> 00:01:50.610
then this would be one minus one.

30
00:01:50.611 --> 00:01:54.750
So this whole term here would be zero.
So we can just ignore it.

31
00:01:55.860 --> 00:01:59.760
And so this would be one.
So really in this case,

32
00:01:59.761 --> 00:02:03.750
what I'd be minimizing a is the loss here,

33
00:02:03.751 --> 00:02:06.190
which would be minus,
we have a minus here.
So it's the,

34
00:02:06.390 --> 00:02:09.390
I could remove the minus and just put it right in front here.

35
00:02:09.930 --> 00:02:14.220
So it'd be minimizing minus the log of the,
uh,
output.

36
00:02:15.210 --> 00:02:18.780
And so if I'm in demise,
this is the same as maximizing log of the output.

37
00:02:18.960 --> 00:02:22.650
So I'm trying to push the output as close as possible to one.

38
00:02:22.710 --> 00:02:26.250
So remember that for binary inputs,
we'll use a sigmoidal,

39
00:02:27.000 --> 00:02:29.630
a activation function at the output.
So the,
the,

40
00:02:29.700 --> 00:02:33.480
the biggest x had cake and be as one.
So if Xb is one,

41
00:02:33.481 --> 00:02:35.250
we'll try to push the output towards one.

42
00:02:37.830 --> 00:02:41.520
If Ska was zero,
then this term would be zero,

43
00:02:41.940 --> 00:02:44.850
this would be zero.
So this whole term would be one.

44
00:02:44.851 --> 00:02:49.380
So we just have this and using the same reasoning that we'd find that in that

45
00:02:49.381 --> 00:02:53.760
case we'll try to push x had k towards zero.
That is,

46
00:02:53.761 --> 00:02:57.210
we'll try to push it closer to the actual value of x.
K.

47
00:02:57.990 --> 00:03:01.540
So you can see that this particular construction function is actually well

48
00:03:01.541 --> 00:03:02.320
behaved.
It's,

49
00:03:02.320 --> 00:03:07.320
it's a actually minimized if ex gay is equal to x,

50
00:03:08.351 --> 00:03:12.880
had gay for our case,
it would actually be,
uh,
in this case,

51
00:03:12.881 --> 00:03:17.350
if x is,
uh,
uh,
is either zero one,
it would be zero.

52
00:03:17.380 --> 00:03:21.630
If we had this,
if we had the perfect reconstruction,
uh,

53
00:03:21.670 --> 00:03:24.880
we call it the cross entropy because,

54
00:03:25.060 --> 00:03:28.000
so if you don't know what cross entropy is,

55
00:03:28.030 --> 00:03:32.710
if I have a probability distribution p of x and another one Q of x,

56
00:03:33.130 --> 00:03:37.570
uh,
then the cross entropy we did in the two distribution would be minus the sum

57
00:03:38.020 --> 00:03:41.460
over all possible values of that x,
uh,

58
00:03:41.660 --> 00:03:46.660
a random variable of the probability of that.

59
00:03:46.780 --> 00:03:51.160
A particular value of x times the log of the pro,

60
00:03:51.190 --> 00:03:56.080
the log probability,
uh,
Q of x for the other distribution.

61
00:03:57.040 --> 00:03:59.920
Now,
if x was binary here,

62
00:03:59.921 --> 00:04:04.780
so if x was taking all these year or one values,
then there's some would be a,

63
00:04:04.781 --> 00:04:09.781
some over the probability of x being zero and then the property of x being one.

64
00:04:10.480 --> 00:04:14.830
And so these is how you would get these two terms here.
Uh,

65
00:04:14.831 --> 00:04:17.560
so this,
uh,
so in this case,

66
00:04:17.950 --> 00:04:22.950
p of x would be a s or p of x equal to one would be equivalent to our ex gay and

67
00:04:26.261 --> 00:04:31.261
then a Q of x being equal to one would be equivalent to x hat.

68
00:04:32.620 --> 00:04:34.660
Kay.
Uh,

69
00:04:34.720 --> 00:04:39.720
so we see that we get the case where a p of x equal one that would be xk here

70
00:04:41.951 --> 00:04:45.520
and this would be x hat here.
And then p of x equals zero.

71
00:04:45.550 --> 00:04:48.880
Well because it's a binary variable,
it's Bernoulli,

72
00:04:49.180 --> 00:04:52.270
then it will be more minus the probability of being equal to one.

73
00:04:52.271 --> 00:04:56.410
So that's one minus x gay log of one minus probably being pulled to one

74
00:04:56.411 --> 00:05:00.130
according to Q and Q is accept gay.
That's the problem.

75
00:05:00.220 --> 00:05:04.150
The probability and that distribution of x,
k of x being equal to one.

76
00:05:04.151 --> 00:05:06.790
So we have here,
one minus x had cake.

77
00:05:07.540 --> 00:05:12.420
So essentially we get this form here.
Uh,

78
00:05:12.610 --> 00:05:17.470
if we consider the cross entropy between Bernoulli random variables and then we

79
00:05:17.471 --> 00:05:17.711
have a,

80
00:05:17.711 --> 00:05:22.711
some of Bertolli Cross entropy's because we're summing over all the input

81
00:05:23.020 --> 00:05:26.530
elements.
So all the dimensions in my input factor,
okay,

82
00:05:26.531 --> 00:05:29.560
so that's why we call it the uh,
cross entropy loss.

83
00:05:31.100 --> 00:05:31.780
Um,

84
00:05:31.780 --> 00:05:36.780
if we had inputs that were not exactly zero one but between zero and one,

85
00:05:36.970 --> 00:05:39.790
we could still use this last function here.

86
00:05:39.791 --> 00:05:44.260
It would still be well defined in the sense that the minimum will still be,

87
00:05:44.590 --> 00:05:48.140
uh,
when we reach a ex,

88
00:05:48.730 --> 00:05:53.020
when we reach x gay equals two x hat case,

89
00:05:53.021 --> 00:05:56.710
so that we'll still minimize this.
Even if ex gay can be between zero and one,

90
00:05:57.170 --> 00:06:00.880
however you will not be zero.
So if a ex gay,
uh,

91
00:06:00.890 --> 00:06:05.480
say is equal to 0.4 and then it's x hat k is equal to your point forward,

92
00:06:05.481 --> 00:06:10.160
then the cross entropy for that input dimension is not zero.
So,

93
00:06:10.210 --> 00:06:14.660
uh,
you shouldn't be surprised if your inputs are not exactly zero one that you

94
00:06:14.661 --> 00:06:18.920
don't reach a,
a perfect cross entropy,
a perfect cross entropy loss.

95
00:06:20.210 --> 00:06:24.110
But another alternative people who have real value with inputs is to just use

96
00:06:24.260 --> 00:06:26.900
the square difference.
So essentially,
uh,

97
00:06:27.350 --> 00:06:32.350
a half times the sum over all the input dimensions of my reconstruction minus

98
00:06:33.170 --> 00:06:38.000
the actual input squared.
So this is the sum of squared differences.

99
00:06:38.210 --> 00:06:38.780
Uh,

100
00:06:38.780 --> 00:06:43.130
we can actually think of it also as the squared Euclidean distance between the

101
00:06:43.131 --> 00:06:45.450
vector x hat and the Vac,

102
00:06:45.500 --> 00:06:50.500
the input vector x and a actually often when we're optimizing this,

103
00:06:50.900 --> 00:06:54.920
uh,
and if the inputs are really real value in the sense that it can take value

104
00:06:54.921 --> 00:06:59.150
between minus infinity and plus infinity,
then in this case often will leave.

105
00:06:59.250 --> 00:07:02.780
We'll use a linner activation function at the output.

106
00:07:03.200 --> 00:07:08.200
So x hat would actually be equal to a hat of,

107
00:07:08.930 --> 00:07:13.240
uh,
ex.
Uh,
so if you go back into our previous rotation,
uh,

108
00:07:13.340 --> 00:07:16.610
Ahab was the pre activation and the output layer.
So we just,

109
00:07:16.960 --> 00:07:20.850
if we have real valid inputs,
we would normally use a linear activation function.

110
00:07:20.851 --> 00:07:23.360
So in other words,
no activation functions at the output layer.

111
00:07:23.690 --> 00:07:27.050
So that's another alternative if we have real value the inputs.

112
00:07:30.050 --> 00:07:34.100
It turns out then that if we use either of these two loss functions,

113
00:07:34.101 --> 00:07:37.580
so the cross entropy with a sigmoid,
uh,

114
00:07:37.670 --> 00:07:42.590
activation function or the square difference using a linear activation function,

115
00:07:43.160 --> 00:07:44.540
then in both cases,

116
00:07:44.570 --> 00:07:48.860
the gradient with respect to the pre activation at the output layer of the loss

117
00:07:49.100 --> 00:07:54.100
is just going to be the difference between the reconstruction and the input.

118
00:07:55.790 --> 00:07:58.990
So here I added the t because,
uh,
you know,

119
00:07:59.020 --> 00:08:02.450
you imagine you'd be computing this grain and four different inputs.

120
00:08:02.451 --> 00:08:05.840
So I'm indexing now the input here and I'm also putting the index for the

121
00:08:05.841 --> 00:08:09.230
reconstruction to say that it's,
this is the reconstruction of that input.

122
00:08:10.610 --> 00:08:12.560
So we can do the math to verify this,

123
00:08:12.561 --> 00:08:15.560
but it's so turns out that it has a very simple form.

124
00:08:15.920 --> 00:08:19.710
And then to get the grain and with respect to the parameters and perform great

125
00:08:19.711 --> 00:08:20.544
gradient descent,

126
00:08:20.690 --> 00:08:25.000
we can just use regular backpropagation like we've seen it before because you

127
00:08:25.010 --> 00:08:25.071
know,

128
00:08:25.071 --> 00:08:29.260
tone color is really just a feed forward neural network and just back propagate

129
00:08:29.500 --> 00:08:33.710
the gradients towards all the parameters.
One thing that's important to notice,

130
00:08:33.711 --> 00:08:35.930
however,
is that if we use tide waits,

131
00:08:35.960 --> 00:08:39.530
that is if you use for the reconstruction weights,
w star,

132
00:08:39.770 --> 00:08:44.630
the transpose of the encoder weights the weights into the hidden layer.

133
00:08:45.110 --> 00:08:49.820
Um,
then in this case we have to realize that the gradient with respect to the

134
00:08:49.821 --> 00:08:53.510
matrix w is really the sum of two Graydon's.

135
00:08:53.720 --> 00:08:58.020
It's going to be the sum of the group or the transpose,
uh,
sorry,
sorry.

136
00:08:58.050 --> 00:09:01.950
It's going to do that.
Some of the gradient of uh,

137
00:09:01.970 --> 00:09:06.970
w star transposed and it's going plus the gradient with respect to a w uh,

138
00:09:10.230 --> 00:09:14.880
uh,
or respected the gradient in the encoder,

139
00:09:14.940 --> 00:09:18.000
uh,
the in quarter part of the network.
Okay.

140
00:09:18.060 --> 00:09:22.890
And that's because these parameters double you are present Indian quarter and

141
00:09:22.891 --> 00:09:23.401
the decoder.

142
00:09:23.401 --> 00:09:27.810
So they get gradients both from just a decoder part and the in quarter part.

143
00:09:28.110 --> 00:09:32.150
Okay.
So that's something important to realize when you,
if you implement a,
uh,

144
00:09:32.170 --> 00:09:33.570
not long corner with tight weights.

145
00:09:36.510 --> 00:09:40.230
And I'll just a quick word about how can we just adapt an auto in quarter four

146
00:09:40.231 --> 00:09:44.850
different types of inputs.
If we have inputs that are,
uh,
not real value,

147
00:09:44.851 --> 00:09:49.851
they might be integer value in my correspondence to a multinomial observations

148
00:09:50.551 --> 00:09:54.470
or other types of observations,
perhaps more structured.
Uh,

149
00:09:54.540 --> 00:09:57.450
well here's a recipe for doing that.
Uh,

150
00:09:57.451 --> 00:10:01.440
so what you could do is first you define the joint distribution over the input

151
00:10:01.441 --> 00:10:03.320
space,
uh,
choose,
uh,

152
00:10:03.360 --> 00:10:06.600
so choose a distribution over the input space that will depend on some

153
00:10:06.601 --> 00:10:09.120
parameters of that distribution,
which I'm calling new here.

154
00:10:09.240 --> 00:10:12.090
So it's going to be the vector of parameters of that distribution.

155
00:10:12.870 --> 00:10:17.870
And then you choose some Parametrix relationship between mew and the encoder,

156
00:10:18.840 --> 00:10:23.220
the a hidden layer of the computer by the quarter.
So in our case,

157
00:10:23.221 --> 00:10:27.240
we usually have something like a linear transformation and possibly a nonlinear

158
00:10:27.241 --> 00:10:30.570
cavity.
And then the last function we'll use,

159
00:10:30.571 --> 00:10:35.400
we'll just be minus the log of the probability of x given mew.

160
00:10:35.401 --> 00:10:39.840
But where mew here depends on h of X.
Okay.

161
00:10:40.410 --> 00:10:44.490
And so we'll just use that as the form for the last function.

162
00:10:45.510 --> 00:10:49.590
In fact,
we can recover the sum of squared distance.

163
00:10:50.210 --> 00:10:54.180
Uh,
if we,
uh,
use for P of x,

164
00:10:54.181 --> 00:10:59.100
given you a Gaussian distribution where the mean is new and the covariance

165
00:10:59.101 --> 00:11:01.920
matrix is just the identity matrix.

166
00:11:02.370 --> 00:11:06.510
So if we have p of x that corresponds to that,
uh,

167
00:11:06.540 --> 00:11:08.220
and then if we choose that can you,

168
00:11:08.221 --> 00:11:12.900
is going to be just a linear transformation of the input,
uh,
sorry,
of the,

169
00:11:12.901 --> 00:11:17.901
a latent representation h of x that we recover what I described before as the

170
00:11:18.061 --> 00:11:22.410
last function would be using for an auto encoder with real value the inputs.

171
00:11:22.830 --> 00:11:24.390
So I'll let you do that exercise,

172
00:11:24.391 --> 00:11:28.830
but essentially you see that minus log here is going to counsel cancel out with

173
00:11:28.831 --> 00:11:31.110
the exponential and the minus here.

174
00:11:31.111 --> 00:11:34.260
So we'd be left with half and this part,

175
00:11:34.290 --> 00:11:36.870
and this is just a constant so we don't have to consider it.

176
00:11:37.080 --> 00:11:41.340
So then we'd fall back on the square difference.
Last function.
All right,

177
00:11:41.341 --> 00:11:43.350
so that's all for the loss function.

178
00:11:43.351 --> 00:11:47.310
We can use it now we have different alternatives and even the recipe to

179
00:11:47.311 --> 00:11:50.310
generalize to,
uh,
many types of data.

