WEBVTT

1
00:00:00.550 --> 00:00:00.710
Okay.

2
00:00:00.710 --> 00:00:01.221
<v 1>In this video,</v>

3
00:00:01.221 --> 00:00:05.840
we'll look at a surprising result that states that under certain conditions a

4
00:00:05.841 --> 00:00:10.070
linear autoencoder,
it's actually as powerful as in nonlinear roto in quarter.

5
00:00:12.010 --> 00:00:12.843
<v 0>Okay?</v>

6
00:00:13.180 --> 00:00:16.150
<v 1>So I know Tom Quarter,
as we said before,</v>

7
00:00:16.151 --> 00:00:21.151
is a part of a neural network that first encodes the input into a hidden there

8
00:00:22.240 --> 00:00:25.480
and then the codes it to obtain a reconstruction we call x hat.

9
00:00:26.290 --> 00:00:27.310
And uh,

10
00:00:27.370 --> 00:00:31.390
in the previous videos then I was suggesting we use in the,

11
00:00:32.070 --> 00:00:35.400
uh,
Indian quarter in nonlinearity,
uh,

12
00:00:35.420 --> 00:00:40.300
applied on the linear transformation of the input for feed forward neural

13
00:00:40.301 --> 00:00:41.350
network for classification.

14
00:00:41.351 --> 00:00:46.240
We also justify using a nonlinearity because it would yield a nonlinear

15
00:00:46.241 --> 00:00:49.150
classifier,
which would be more powerful than a linear classifier.

16
00:00:49.870 --> 00:00:53.380
So that's considered a question.
Um,
for auto encoders,

17
00:00:53.381 --> 00:00:58.381
do we actually gain something by having a nonlinearity or would removing it?

18
00:00:58.691 --> 00:01:03.691
So not having this or this here and just having a linear transformation for the

19
00:01:04.870 --> 00:01:07.600
June quarter,
what,
uh,
how powerful would that be?

20
00:01:08.010 --> 00:01:12.530
How good would this auto encoder be at reconstructing it?
It's imperative.

21
00:01:12.580 --> 00:01:13.900
We trained it on some data.

22
00:01:17.520 --> 00:01:18.040
<v 0>Okay.</v>

23
00:01:18.040 --> 00:01:21.640
<v 1>So what we'll actually show is a pretty surprising result.</v>

24
00:01:21.670 --> 00:01:26.670
It turns out that linear autoencoder is actually optimal under certain

25
00:01:27.221 --> 00:01:28.060
conditions.
That is,

26
00:01:28.061 --> 00:01:33.061
it will give you a reconstruction error that for a given number of hidden units

27
00:01:33.580 --> 00:01:36.240
is a small as it can be.
And,
uh,

28
00:01:36.250 --> 00:01:41.250
this result applies if we consider the square difference error and it applies

29
00:01:43.211 --> 00:01:46.750
to,
we assume that the decoder is linear.
So,
uh,

30
00:01:46.780 --> 00:01:50.710
for the squared difference error,
uh,
or the square difference loss,

31
00:01:50.711 --> 00:01:52.600
I suggested we use a Lennar decoder.

32
00:01:52.660 --> 00:01:57.040
So in that case we can actually show that a linear autumn quarter,

33
00:01:57.310 --> 00:01:58.680
uh,
will be optimal.

34
00:01:58.681 --> 00:02:01.720
We will give you the best training or it can for a given number of hidden units.

35
00:02:02.380 --> 00:02:06.580
So we'll,
we'll show us sketch up that,
uh,
uh,
actual proof.

36
00:02:07.300 --> 00:02:10.840
And for that we'll need this particular result here,
which I'm describing.

37
00:02:11.380 --> 00:02:16.380
So if we have any matrix a and which we can write into a singular value

38
00:02:17.591 --> 00:02:20.350
decomposition.
So that is,
we can write a,

39
00:02:20.351 --> 00:02:25.300
as the product of a u n a r u matrix,
which,
uh,
has,

40
00:02:25.360 --> 00:02:30.100
uh,
it's columns,
which is Ortho normal.
So it's columns are,
uh,

41
00:02:30.700 --> 00:02:34.420
uh,
our Ortho normal vectors,
a sigma,

42
00:02:34.421 --> 00:02:37.690
which is a diagonal matrix.
And on the diagonal of the Matrix,

43
00:02:37.691 --> 00:02:41.980
you have the singular values of the matrix and then be transposed would be

44
00:02:41.981 --> 00:02:45.850
another,
uh,
Ortho,
Ortho normal matrix.

45
00:02:47.170 --> 00:02:49.120
So this is what is being stated here.

46
00:02:49.870 --> 00:02:54.610
So imagine this matrix a and imagine that we write it's a single value,

47
00:02:54.611 --> 00:02:59.350
the composition like this here.
And now consider the,

48
00:02:59.440 --> 00:03:04.330
uh,
a particular case where we'd be self selecting only the key,

49
00:03:04.331 --> 00:03:05.900
largest singular value.

50
00:03:05.901 --> 00:03:09.490
So imagine the diagonal of the Sigma Matrix.

51
00:03:09.491 --> 00:03:14.260
We'd actually had ordered the singular values in,
uh,
uh,
uh,

52
00:03:14.290 --> 00:03:17.770
in decreasing order.
So the first one on the top,
uh,

53
00:03:17.830 --> 00:03:22.100
top left of the Matrix would be the largest singular value and then a decrease

54
00:03:22.101 --> 00:03:24.760
as we go along the diagonal.
And a,

55
00:03:24.761 --> 00:03:28.240
so imagine then that we just select this sub matrix by selling thing,

56
00:03:28.250 --> 00:03:31.330
the k first,
uh,
rows and columns.
And then we'd do,

57
00:03:31.690 --> 00:03:35.380
we'd select the left and right,
uh,
uh,
Ortho,

58
00:03:35.381 --> 00:03:39.550
normal vectors in a u and v.
So that's what I'm noting here.

59
00:03:40.990 --> 00:03:43.750
Then we can actually show that,
uh,

60
00:03:43.810 --> 00:03:48.810
imagined for this matrix a we wanted to find in other Matrix B A,

61
00:03:49.211 --> 00:03:51.790
which is up rank k,
so it's a,

62
00:03:51.791 --> 00:03:56.740
and k would be presumably smaller than the rank of uh,
of a.

63
00:03:57.460 --> 00:04:01.780
So imagine we want to find this sort of approximation B of a,
but,

64
00:04:01.970 --> 00:04:05.170
and it's an approximation that's less complicated in the sense that it's a

65
00:04:05.171 --> 00:04:09.520
matrix for the lower rank and want it and want to find the approximation that

66
00:04:09.521 --> 00:04:12.460
minimizes the for beenus norm between a and B,

67
00:04:12.461 --> 00:04:15.060
sort of the difference between a and B that makes,
uh,

68
00:04:15.200 --> 00:04:19.150
the approximation B that is closest to a according to the for Venus norm.

69
00:04:19.900 --> 00:04:22.840
So that's what is stated here.
Amen.

70
00:04:23.020 --> 00:04:27.730
So the Matrix B with the constraint that [inaudible] needs to be a rank k,
uh,

71
00:04:27.760 --> 00:04:31.890
of this expression here.
Well,
it turns out that,
uh,

72
00:04:31.930 --> 00:04:36.310
the solution to that optimization problems,
so the Matrix be star,
uh,

73
00:04:36.460 --> 00:04:41.460
that optimizes different being a storm here is actually just a matrix who's

74
00:04:41.501 --> 00:04:45.370
singular value decomposition is compose is made of.

75
00:04:45.760 --> 00:04:48.010
Um,
or sorry,

76
00:04:48.011 --> 00:04:53.011
the matrix speed trend star that can be written as the product of the k,

77
00:04:54.550 --> 00:04:58.450
uh,
the selected first k columns of you,

78
00:04:58.660 --> 00:05:00.550
the a sub matrix.

79
00:05:00.800 --> 00:05:05.260
And I select the k first rows and columns of a sigma.

80
00:05:05.261 --> 00:05:10.030
And then,
uh,
we transpose where V we've also selected selected the k,

81
00:05:10.090 --> 00:05:14.860
uh,
um,
the key,
uh,
first,
uh,
Ortho,
normal vectors,

82
00:05:14.890 --> 00:05:17.800
capers,
columns.
So in other words,

83
00:05:17.801 --> 00:05:20.110
by arranging the matrix like this,

84
00:05:20.111 --> 00:05:25.111
then it's actually easy to get an approximation of it that is as best as

85
00:05:25.541 --> 00:05:27.190
possible for some giving ranks.

86
00:05:27.370 --> 00:05:31.930
We just select the part of that expression that corresponds to the k largest,

87
00:05:32.170 --> 00:05:37.120
a singular values.
Okay?
So this is a theorem that we won't show this Durham,

88
00:05:37.121 --> 00:05:40.600
but we'll use it to show the proof that the linear term quarter is optimal.

89
00:05:43.450 --> 00:05:47.380
Okay,
so now let's do the proof of optimality of the linear a to encoder.

90
00:05:47.710 --> 00:05:52.520
It's truly just a sketch because I'm going to be making some assumptions.
Um,

91
00:05:53.230 --> 00:05:57.770
one assumption that I'm going to make is that the number of training,
uh,

92
00:05:57.800 --> 00:06:01.870
examples in a number of these x tees here,
uh,

93
00:06:01.880 --> 00:06:06.880
is going to be the same as a DEA number of dimensions in,

94
00:06:07.310 --> 00:06:09.320
uh,
in the input x.

95
00:06:09.321 --> 00:06:13.970
So essentially the sum here is over the same number of elements than this.

96
00:06:14.030 --> 00:06:14.863
Some here.

97
00:06:16.600 --> 00:06:17.220
<v 0>Yeah.</v>

98
00:06:17.220 --> 00:06:20.040
<v 1>So when we're training and not one quarter,
that's what we want to do here.</v>

99
00:06:20.041 --> 00:06:24.480
Want to find the parameters such that we minimize the average or the sum that's

100
00:06:24.481 --> 00:06:28.500
equivalent of the reconstruction era.
There's a,
so the square difference

101
00:06:31.300 --> 00:06:34.370
and uh,
we'll um,
uh,
in,
uh,

102
00:06:34.450 --> 00:06:37.780
let's assume that this here was computed by a linear and quarter.

103
00:06:37.781 --> 00:06:40.450
So we want to be able to analyze that case.

104
00:06:42.240 --> 00:06:42.720
<v 0>Okay.</v>

105
00:06:42.720 --> 00:06:45.930
<v 1>So another way of writing this expression would be as follows.</v>

106
00:06:46.170 --> 00:06:50.730
So we want to minimize this expression instead.
So this whole expression here,

107
00:06:50.731 --> 00:06:55.731
we can write it as half times to for Venus norm squared of this difference.

108
00:06:58.290 --> 00:07:01.020
Um,
so in particular in this expression,

109
00:07:02.430 --> 00:07:05.920
capitol x here is going to be the matrix,
uh,

110
00:07:05.940 --> 00:07:10.590
where the columns of that matrix are the different training examples.

111
00:07:11.280 --> 00:07:12.390
And so in other words,

112
00:07:12.391 --> 00:07:16.920
because in I'm assuming we have the same number of trainee examples as we have

113
00:07:16.921 --> 00:07:21.600
dimensionalities a number of,
um,
sort of the size of the,
uh,

114
00:07:21.630 --> 00:07:25.290
so the name number of training examples as the size of these vectors.

115
00:07:25.291 --> 00:07:28.400
Then this matrix x is going to be square and a,

116
00:07:28.410 --> 00:07:30.270
that's just going to facilitate the proof.

117
00:07:30.300 --> 00:07:33.810
We can generalize the proof to the general case,
but I'll just for simplicity,

118
00:07:33.811 --> 00:07:38.190
consider that specific case.
So that's what x here is going to be.

119
00:07:39.430 --> 00:07:39.710
<v 0>Okay.</v>

120
00:07:39.710 --> 00:07:43.820
<v 1>And uh,
then h of x,</v>

121
00:07:43.880 --> 00:07:47.660
I'll call that the matrix who's,
um,

122
00:07:47.690 --> 00:07:50.790
columns are going to be the,
uh,

123
00:07:51.000 --> 00:07:54.950
encoder outputs for each training example.
[inaudible].

124
00:07:55.850 --> 00:07:58.230
And then if we multiply by the,
uh,

125
00:07:58.250 --> 00:08:01.280
decoder weights w star,

126
00:08:01.520 --> 00:08:06.170
then by doing this,
uh,
so by doing this,
then,

127
00:08:06.230 --> 00:08:09.890
uh,
we would obtain our reconstruction because a WWE star,

128
00:08:09.891 --> 00:08:13.910
the first column of that expression here,

129
00:08:13.940 --> 00:08:15.840
that would be the result of multiplying up.

130
00:08:15.860 --> 00:08:20.850
You start by the first column of h of X.
And uh,

131
00:08:21.010 --> 00:08:23.060
and so all the columns of those matrix,

132
00:08:23.810 --> 00:08:26.870
that matrix would be all the reconstructed factors.

133
00:08:27.080 --> 00:08:30.980
And then if we take the square,
the for Venus norm,
we do get the uh,

134
00:08:31.120 --> 00:08:35.330
a double some.
So a nested,
some,
uh,
over,
uh,

135
00:08:35.360 --> 00:08:40.360
the squared of the reconstruction of each element in the inputs,

136
00:08:41.600 --> 00:08:46.370
training examples and their reconstruction that all this here that we have a

137
00:08:46.371 --> 00:08:48.320
greater or equal,
we don't have an equal.

138
00:08:48.350 --> 00:08:53.350
And that's because I'm also going to assume that age of x here is actually any

139
00:08:54.501 --> 00:08:57.840
matrix.
Uh,
so any matrix,

140
00:08:57.930 --> 00:09:02.930
a whatsoever of a size number of hidden units by a number of training examples.

141
00:09:05.520 --> 00:09:07.090
So we're not actually yet,

142
00:09:07.091 --> 00:09:11.560
it's going to constrain it to be the result of a linear encoder.
Well,

143
00:09:11.561 --> 00:09:16.540
just assume it's any matrix.
And then after that we'll actually show that,
uh,

144
00:09:16.800 --> 00:09:21.060
if we minimize this,
that the greater or equal is actually an equal,
uh,

145
00:09:21.090 --> 00:09:24.540
if we assume that this was the result of a linear in quarter.

146
00:09:25.320 --> 00:09:25.860
<v 0>Okay.</v>

147
00:09:25.860 --> 00:09:26.371
<v 1>But for now,</v>

148
00:09:26.371 --> 00:09:30.690
let's just look at this particular problem where h of x can be any matrix.

149
00:09:34.790 --> 00:09:35.623
<v 0>Okay?</v>

150
00:09:36.910 --> 00:09:37.720
<v 1>Okay.</v>

151
00:09:37.720 --> 00:09:42.310
So now what is the result of that minimization here,

152
00:09:42.430 --> 00:09:47.260
which is here?
Well,
from the theorem and just saw before.

153
00:09:47.440 --> 00:09:52.420
Then if we take x and we consider it's singular value decomposition like this

154
00:09:52.840 --> 00:09:54.040
and well we know this,

155
00:09:54.041 --> 00:09:58.480
that this here is actually a low rank matrix and the rank of that Matrix is

156
00:09:58.481 --> 00:10:01.480
going to be at most the number of hidden units.

157
00:10:02.020 --> 00:10:06.460
So if we think of k as being the hidden layer size and number of heating units

158
00:10:06.461 --> 00:10:07.360
and the encoder,

159
00:10:07.590 --> 00:10:12.590
then the previous theorem tells us that a solutions for this would be to have w

160
00:10:13.301 --> 00:10:17.590
star,
uh,
be equal to say the,
um,

161
00:10:17.980 --> 00:10:21.420
uh,
d,
uh,
part of the,
you may tricks that considers all the,

162
00:10:21.421 --> 00:10:26.421
the first k columns times the a sub matrix in sigma that considers only the

163
00:10:28.961 --> 00:10:30.700
first key rows and columns.

164
00:10:31.060 --> 00:10:35.590
And then h of x could be this,
it could be d,
the rest of the,

165
00:10:35.830 --> 00:10:36.990
uh,
expressions.

166
00:10:36.991 --> 00:10:41.140
So it could be a V transpose wherever you we've taken the k first columns.

167
00:10:41.680 --> 00:10:46.070
We could have put a sigma here as well.
That's another solution.
Uh,

168
00:10:46.150 --> 00:10:47.010
but let's just,
you know,

169
00:10:47.080 --> 00:10:51.010
this is one about a valid solution and it does minimize that expression based on

170
00:10:51.011 --> 00:10:55.870
the previous,
uh,
theorem.
All right.

171
00:10:57.690 --> 00:11:02.610
No,
let's see whether this particular h of x,

172
00:11:02.850 --> 00:11:05.030
which states that farm and minimize this,

173
00:11:05.031 --> 00:11:08.520
this can actually be expressed as a linear in quarter.

174
00:11:09.430 --> 00:11:09.730
<v 0>Okay.</v>

175
00:11:09.730 --> 00:11:11.530
<v 1>And then if we do that,
we finished our proof.</v>

176
00:11:14.240 --> 00:11:18.410
So h of x is equal to that.
That's exactly what we have here.

177
00:11:21.940 --> 00:11:26.890
And now I'll just essentially multiply by the end entity y d I a,
uh,

178
00:11:26.980 --> 00:11:29.500
because this times that is the identity.

179
00:11:29.501 --> 00:11:33.430
So this matrix times it's inverse here.
Uh,
that's just the identity.

180
00:11:33.431 --> 00:11:35.380
So I'm not changing anything in the equation.

181
00:11:37.980 --> 00:11:42.810
And I'm going to replace x here by,
it's a singular value decomposition.

182
00:11:43.200 --> 00:11:44.340
So x transpose,

183
00:11:44.341 --> 00:11:48.270
it's going to be the transpose of that expression if we put it in the singular

184
00:11:48.271 --> 00:11:52.280
value decomposition forms.
So that's fee times sigma transpose that.

185
00:11:52.620 --> 00:11:57.040
You transpose.
Thanks.
You thanks to sigma,
like times be transposed.

186
00:11:57.050 --> 00:12:01.360
So that's,
it comes from this,
this uh,
sorry,

187
00:12:02.860 --> 00:12:07.090
this is that here and this is here,

188
00:12:07.330 --> 00:12:09.070
this whole thing here,
this whole thing here.

189
00:12:09.790 --> 00:12:13.680
And then similarly for that matrix and that Matrix,
which,
uh,
uh,

190
00:12:13.690 --> 00:12:17.230
actually I'm just going to take this singular value decomposition of that
Matrix,

191
00:12:17.290 --> 00:12:18.123
which we find here

192
00:12:22.160 --> 00:12:25.160
now.
Uh,
then we,
uh,

193
00:12:25.190 --> 00:12:30.190
no status this times that that's just the identity matrix because you is oracle

194
00:12:30.651 --> 00:12:34.640
normal.
So we just get this expression,
these two cancel out.

195
00:12:38.270 --> 00:12:42.840
And then I'm going to write this,
uh,

196
00:12:43.040 --> 00:12:46.970
inversion here as that.
So indeed,
we know this,

197
00:12:46.971 --> 00:12:48.380
that if I take this,

198
00:12:48.410 --> 00:12:52.250
am I multiplied by what's within the inversion.

199
00:12:52.610 --> 00:12:55.010
So this times that,

200
00:12:55.340 --> 00:12:59.780
that's just the identity matrix cause this cancels out with that,
uh,

201
00:12:59.810 --> 00:13:00.980
this trends,

202
00:13:01.070 --> 00:13:05.030
this matrix times the inverse of that same matrix that's going to be the

203
00:13:05.031 --> 00:13:09.380
identity.
And then v Times v Transpose,

204
00:13:09.381 --> 00:13:14.330
that's also going to be the identity.
Okay.
So,

205
00:13:14.680 --> 00:13:18.140
uh,
I've just rewrote this part into that form instead.

206
00:13:22.380 --> 00:13:23.750
Next,
uh,

207
00:13:23.790 --> 00:13:28.320
we noticed that this times that that's also the identity because again,
the,

208
00:13:28.680 --> 00:13:32.400
is Ortho normal so we don't see it here anymore.

209
00:13:36.020 --> 00:13:38.930
Uh,
we apply the same thing here,
but now,

210
00:13:38.931 --> 00:13:43.650
because this is the transpose of only the k first columns of v,
uh,

211
00:13:43.670 --> 00:13:45.890
then we get the identity matrix,

212
00:13:45.891 --> 00:13:50.790
but where we've selected only the k first rose up that identity matrix.
Okay.

213
00:13:50.791 --> 00:13:53.100
So this will be a matrix of size,

214
00:13:53.310 --> 00:13:56.650
k times number of a number,

215
00:13:56.651 --> 00:13:58.590
a dimensionality of the input space.

216
00:13:59.940 --> 00:14:04.260
And it's going to be to have ones on the diagonal n zero elsewhere.

217
00:14:04.800 --> 00:14:07.200
So that's this matrix here.

218
00:14:11.640 --> 00:14:12.031
Uh,

219
00:14:12.031 --> 00:14:16.050
next I'll just apply the inversion and notice that if I inverse the product with

220
00:14:16.051 --> 00:14:19.840
Matrix,
then it's the product of the inverses.
But,
uh,

221
00:14:20.170 --> 00:14:23.500
the order has been inverted.
Okay.
So I,

222
00:14:23.870 --> 00:14:28.870
that's why this is equal to that matrix inverted times,

223
00:14:28.930 --> 00:14:30.370
that matrix inverted.

224
00:14:34.080 --> 00:14:37.830
Now I know this,
that's this matrix times it's inverse.

225
00:14:37.890 --> 00:14:41.100
That will be a day identity so they can sort it out.

226
00:14:41.840 --> 00:14:43.020
So I get this expression

227
00:14:46.790 --> 00:14:50.270
and then,
um,
now I have this,

228
00:14:50.300 --> 00:14:54.080
the result a,
that's matrix times stat matrix.

229
00:14:54.110 --> 00:14:59.060
I multiplying on the left by,
uh,
this identity matrix,

230
00:14:59.061 --> 00:15:02.750
which,
uh,
for which we consider only the first k roads.

231
00:15:03.350 --> 00:15:06.140
So what this matrix is going to do here,

232
00:15:06.350 --> 00:15:10.280
this matrix here is that what is happening here,

233
00:15:10.281 --> 00:15:14.960
it's only going to select the first key roles of the result of that expression

234
00:15:15.860 --> 00:15:20.420
and uh,
and in particular the entries in that Matrix.

235
00:15:20.900 --> 00:15:23.170
And so the k first roles,
uh,

236
00:15:23.300 --> 00:15:28.300
rose up that matrix is really can really be computed only by considering the sub

237
00:15:29.990 --> 00:15:34.820
matrix of a sigma in verse,
uh,

238
00:15:34.850 --> 00:15:39.850
that corresponds to their first key roles in the first k columns and multiply by

239
00:15:40.520 --> 00:15:42.020
the U Matrix transpose.

240
00:15:42.021 --> 00:15:45.020
But where we've taken a little bit k first columns at that you matrix,

241
00:15:45.210 --> 00:15:47.400
do you think about it a little bit,
uh,

242
00:15:47.420 --> 00:15:52.070
doing that is equivalent to this because this selects only the k first roles,

243
00:15:52.220 --> 00:15:53.990
rows of that Matrix.

244
00:15:54.230 --> 00:15:58.580
And these roles can be obtained by multiplying these two sub maitri season,

245
00:15:58.910 --> 00:16:00.860
some injuries of this and that.

246
00:16:03.850 --> 00:16:04.410
<v 0>Okay.</v>

247
00:16:04.410 --> 00:16:05.520
<v 1>And then we know this,</v>

248
00:16:05.521 --> 00:16:10.521
that we have and and quarter that was computed by taking all the inputs and then

249
00:16:12.181 --> 00:16:15.570
linearly transforming them.
Because this is just a linear matrix.

250
00:16:15.571 --> 00:16:18.900
So this could be our matrix w in an encoder.

251
00:16:20.250 --> 00:16:25.250
So h of x can be expressed as a linear and quarter h of x is also,

252
00:16:25.470 --> 00:16:29.790
which has this form is also the optimal solution for minimizing this.

253
00:16:30.450 --> 00:16:34.860
So if we want to optimize that with,
uh,
uh,
uh,

254
00:16:34.890 --> 00:16:38.020
if we want to optimize this thing here,
then,
uh,

255
00:16:38.130 --> 00:16:40.620
the best we could do would be this.

256
00:16:40.770 --> 00:16:44.220
And this could be obtained by having this be a linear in quarter.

257
00:16:45.000 --> 00:16:49.170
So the best reconstruction we could ever get if we have a linear decoder is to

258
00:16:49.171 --> 00:16:52.020
have this thing,
this matrix here being a to that.

259
00:16:52.290 --> 00:16:56.340
And this expression here can be obtained by a linear and quarter.
So that's,

260
00:16:56.341 --> 00:16:59.480
we've showed that the best autoencoder here,
uh,

261
00:16:59.520 --> 00:17:04.080
with the linear decoder is a achievable by Elinor in quarter.

262
00:17:06.990 --> 00:17:07.823
<v 0>Okay.</v>

263
00:17:07.920 --> 00:17:09.670
<v 1>All right.
So,
um,</v>

264
00:17:10.600 --> 00:17:15.070
so this is just what I've said in h of x,
which should,
uh,

265
00:17:15.400 --> 00:17:20.110
we should take x and multiplied by that matrix.
So that could be w and w star.

266
00:17:20.140 --> 00:17:24.110
It could be the other part of the expression in the,
uh,

267
00:17:24.280 --> 00:17:28.750
svd a theorem that you reuse for the low rank approximation.

268
00:17:29.790 --> 00:17:32.230
Um,
and,
and with these two expression,

269
00:17:32.231 --> 00:17:35.080
then we've reached the best we can in terms of reconstruction era.

270
00:17:35.081 --> 00:17:36.370
That's what we've just shown.

271
00:17:36.790 --> 00:17:41.050
It's the best in terms of the square difference error or squared difference
loss.

272
00:17:41.490 --> 00:17:43.810
Uh,
it's also the best if we have a linear the quarter,

273
00:17:43.811 --> 00:17:46.450
there's where the two assumptions we were making,
what we're making,

274
00:17:46.451 --> 00:17:48.520
that proof and also it's the best.

275
00:17:48.580 --> 00:17:52.500
So here means the lowest training reconstruction error.

276
00:17:52.530 --> 00:17:55.110
It doesn't necessarily say something about generalization error,

277
00:17:55.500 --> 00:18:00.040
but if we have more and more and more training data,
uh,
then eventually,
uh,

278
00:18:00.180 --> 00:18:04.860
training error convergence to generalization there.
So it's still,
that's uh,

279
00:18:04.920 --> 00:18:08.550
it would mean that for a large amount of training data we can expect that this

280
00:18:08.730 --> 00:18:10.530
linear and tone quarter we actually do really,

281
00:18:10.531 --> 00:18:12.270
really well in terms of reconstruction there.

282
00:18:13.640 --> 00:18:13.840
<v 0>Okay.</v>

283
00:18:13.840 --> 00:18:18.610
<v 1>Um,
so that's kind of a surprising result and that perhaps suggests that there's</v>

284
00:18:18.620 --> 00:18:20.790
something sort of broken about the auto encoder,

285
00:18:21.040 --> 00:18:23.450
a formulation that we've presented so far.

286
00:18:23.630 --> 00:18:26.270
If we want to consider non linear features,

287
00:18:26.300 --> 00:18:29.390
non linear encoders and uh,

288
00:18:29.420 --> 00:18:31.370
I'll find you two to finish up.

289
00:18:31.400 --> 00:18:36.400
I'll just mentioned that if we normalize the inputs such that we take each input

290
00:18:37.910 --> 00:18:42.770
and then we subtract the average of all training inputs,
and then,

291
00:18:42.830 --> 00:18:46.250
uh,
if we also divide by this factor here,

292
00:18:46.370 --> 00:18:51.370
then we get actually show that the encoder occurs funds to principal component

293
00:18:51.460 --> 00:18:55.550
analysis.
And,
uh,
uh,
so indeed,

294
00:18:55.880 --> 00:19:00.150
if we using this,
then,
um,
uh,
the,
uh,

295
00:19:00.260 --> 00:19:02.960
singular values and the left vectors,

296
00:19:02.970 --> 00:19:07.190
I actually associated with the Indian values and the vectors of the covariance

297
00:19:07.191 --> 00:19:11.750
matrix of the training examples.
By doing that transformation,

298
00:19:11.810 --> 00:19:15.920
I won't make the demonstration.
You can try it out.
It's not too complicated.

299
00:19:16.610 --> 00:19:18.530
But,
uh,
so in other words,

300
00:19:18.531 --> 00:19:22.070
there's a relationship between on tone quarters and principal component
analysis.

301
00:19:22.100 --> 00:19:27.100
And with that preprocessing principal component analysis is really optimal in

302
00:19:27.261 --> 00:19:32.150
terms of encoding data into a,
uh,
uh,

303
00:19:32.210 --> 00:19:37.130
a,
a hidden layer size of CISE gay,
uh,
uh,
and,
and,
uh,

304
00:19:37.190 --> 00:19:39.650
in such a way that we get a really good reconstruction there.

305
00:19:40.480 --> 00:19:40.950
<v 0>Okay.</v>

306
00:19:40.950 --> 00:19:44.980
<v 1>Right?
So that's it about this surprising result about the neuro tone quarters.</v>

