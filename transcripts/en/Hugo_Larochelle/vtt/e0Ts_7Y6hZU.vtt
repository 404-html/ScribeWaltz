WEBVTT

1
00:00:00.430 --> 00:00:00.680
Okay.

2
00:00:00.680 --> 00:00:01.251
<v 1>In this video,</v>

3
00:00:01.251 --> 00:00:05.120
we'll introduce the concept of free energy in a restricted posts for machine.

4
00:00:06.770 --> 00:00:07.603
<v 0>Okay.</v>

5
00:00:08.020 --> 00:00:10.570
<v 1>So we've seen that a restricted Boltzmann machine,
uh,</v>

6
00:00:10.571 --> 00:00:15.571
defines a distribution over the inputs and a latent factor of binary hidden

7
00:00:17.021 --> 00:00:18.700
units.
Now in this video,

8
00:00:18.701 --> 00:00:23.701
we'll look at what the actual marginal distribution over a x onlys over our

9
00:00:26.411 --> 00:00:29.110
inputs that we're interested in modeling.
Uh,

10
00:00:29.111 --> 00:00:33.280
it looks like to try to gain some intuition as to how restrictive Bolsa machine

11
00:00:33.281 --> 00:00:37.270
can make certain types of values for the input vector,

12
00:00:37.271 --> 00:00:40.120
more likely and other vectors,
that's likely.
So in other words,
high,

13
00:00:40.121 --> 00:00:43.250
it can actually model a distribution of refinery back.

14
00:00:43.340 --> 00:00:44.173
<v 0>Good.</v>

15
00:00:47.010 --> 00:00:51.090
<v 1>So what does p of x look like?
Uh,</v>

16
00:00:51.091 --> 00:00:54.460
so p of x,
if we wanted to write it down,
uh,

17
00:00:54.480 --> 00:00:58.500
well we can write it down that it's just the sum over all potential binary

18
00:00:58.501 --> 00:00:59.081
vectors.

19
00:00:59.081 --> 00:01:03.720
So binary patterns we could get at the hidden layer of the probability of

20
00:01:03.721 --> 00:01:07.530
observing that hidden layer with the given value of x.

21
00:01:08.550 --> 00:01:09.360
And,
uh,

22
00:01:09.360 --> 00:01:14.010
now we can just write down what p of x and h is just the exponential of the

23
00:01:14.011 --> 00:01:16.530
negative energy divided by d partition function.

24
00:01:17.580 --> 00:01:22.440
And what we can show is that this some of an exponential number of binary

25
00:01:22.441 --> 00:01:23.630
vectors,
the,

26
00:01:23.631 --> 00:01:28.500
so all the exponential number of patterns in the hit in the hidden layer can

27
00:01:28.501 --> 00:01:32.610
actually be written down in the more efficient expression and more efficient to

28
00:01:32.611 --> 00:01:33.444
compute.

29
00:01:33.510 --> 00:01:38.510
So we can write it down as the exponential of a bias term that involves on the x

30
00:01:40.620 --> 00:01:45.620
and it's biases plus a sum over all hidden layers of the logarithm logarithm of

31
00:01:49.201 --> 00:01:54.150
one plus the exponential of a Lennar transform of the input,

32
00:01:54.620 --> 00:01:54.960
uh,

33
00:01:54.960 --> 00:01:59.880
and the looking at the linear transfer for each hidden a hidden unit.

34
00:02:00.900 --> 00:02:01.681
So in other words,

35
00:02:01.681 --> 00:02:06.090
we can actually write it down as a linear term plus.

36
00:02:06.110 --> 00:02:10.170
So the exponential of an inner term plus the sum over all hidden units,

37
00:02:10.200 --> 00:02:15.200
Avi nonlinearly transform a version of the Lennar transformation associated with

38
00:02:16.170 --> 00:02:18.880
each hidden unit.
And a,

39
00:02:18.900 --> 00:02:21.180
if you look at the literature and restricted motion machine,

40
00:02:21.240 --> 00:02:24.630
if are you see mentioned the notion of a free energy.

41
00:02:24.840 --> 00:02:28.080
Well we just refereeing to minus this.

42
00:02:28.081 --> 00:02:30.570
So the negative of this whole term here.

43
00:02:30.660 --> 00:02:34.020
So that would be the free energy in a restricted posted machine.

44
00:02:35.010 --> 00:02:37.470
So that's first actually do this derivation here,

45
00:02:37.471 --> 00:02:42.471
how we go from that to this and notice again that you know,

46
00:02:43.321 --> 00:02:47.270
this might be a surprising result because we've transformed this summation over

47
00:02:47.271 --> 00:02:52.271
an exponential number of term of terms and to an expression that requires a uh,

48
00:02:52.860 --> 00:02:56.700
some over a linear number of terms,
linear in the number of hidden units.

49
00:02:58.560 --> 00:02:59.393
<v 0>Okay?
Yeah.</v>

50
00:02:59.960 --> 00:03:04.530
<v 1>Okay.
So p of x is,
uh,
the,
uh,</v>

51
00:03:04.760 --> 00:03:08.960
marginalization of the joint distribution by summing over all values of age.

52
00:03:09.440 --> 00:03:10.970
And so that here,

53
00:03:11.420 --> 00:03:14.540
this is just p of x and y.

54
00:03:19.040 --> 00:03:20.330
Now,
uh,

55
00:03:20.540 --> 00:03:25.540
let's actually notice that the exponential of that does not depend on the hidden

56
00:03:26.811 --> 00:03:30.800
layer so we can actually put it in front of the sun.
So he goes here,

57
00:03:32.140 --> 00:03:35.180
I'm running explicitly that this some of our,
uh,

58
00:03:35.240 --> 00:03:39.950
all binary vectors is we can write it down to z,
a series of nested,

59
00:03:39.951 --> 00:03:43.940
some over [inaudible],
up to age,
capital,
age.

60
00:03:44.750 --> 00:03:49.670
And then I'm writing down this term here in this term here.
Uh,

61
00:03:49.730 --> 00:03:52.940
similarly when like we've done in the demonstration for the conditional

62
00:03:52.941 --> 00:03:57.560
distribution and writing it as a sum over all hidden unit,
uh,

63
00:03:57.660 --> 00:04:02.630
indices of,
uh,
a term that involves on the a single

64
00:04:02.820 --> 00:04:04.040
<v 0>in unit HJ,</v>

65
00:04:08.250 --> 00:04:11.220
<v 1>then I'm using,
uh,
the same,
uh,</v>

66
00:04:11.221 --> 00:04:15.510
steps that have used in the derivation of the conditional distribution that

67
00:04:15.660 --> 00:04:19.350
essentially this is the exponential of a sum.

68
00:04:19.351 --> 00:04:24.351
So that's the product of the exponentiate it values for these terms here.

69
00:04:25.710 --> 00:04:29.310
And,
uh,
so in this product of terms,

70
00:04:29.370 --> 00:04:32.220
each factor on the depends on one hidden unit,

71
00:04:32.580 --> 00:04:37.580
which means that this nested sums cannot become a product of individual,

72
00:04:38.570 --> 00:04:41.310
uh,
sums over each individual

73
00:04:41.760 --> 00:04:42.970
<v 0>heating unit here</v>

74
00:04:47.610 --> 00:04:52.020
<v 1>and now,
uh,
because this sum is over just a two terms,</v>

75
00:04:52.021 --> 00:04:56.610
so a term where the hidden units is equal to zero and another one where it's

76
00:04:56.670 --> 00:05:01.590
equal to one,
then,
uh,
I can just write this down and explicitly.

77
00:05:01.591 --> 00:05:05.130
So when h one is equal to zero,
then this is the exponential of zero,

78
00:05:05.131 --> 00:05:09.470
which is one.
And when it's equal to one,
then it's just a,

79
00:05:09.480 --> 00:05:14.250
the exponential of B one plus the product of x with the first,

80
00:05:14.370 --> 00:05:16.080
uh,
row of w.

81
00:05:16.350 --> 00:05:21.350
And then I get a similar expression for all a hidden units until the last hidden

82
00:05:22.110 --> 00:05:25.410
unit.
So the hidden unit,
uh,
capital age.

83
00:05:29.250 --> 00:05:34.110
Then I'll also write this term here as an exponential,

84
00:05:34.270 --> 00:05:36.660
an expert,
the exponential of a term.

85
00:05:36.661 --> 00:05:41.661
So I'm just going to add the exponential of the log of the term that I hear.

86
00:05:43.080 --> 00:05:45.180
I'll do that for this one too.

87
00:05:45.181 --> 00:05:48.240
So I have the expansion of the luck of the original term.

88
00:05:50.220 --> 00:05:50.820
<v 0>Okay.</v>

89
00:05:50.820 --> 00:05:55.820
<v 1>This allows me to write the product of all of these exponentials here as a</v>

90
00:05:56.491 --> 00:06:01.310
single,
where I'm summing the terms inside the exponential.

91
00:06:02.570 --> 00:06:07.570
So I'm summing you buy a sturm and then I'm summing all of the logs of one plus

92
00:06:08.541 --> 00:06:13.541
the exponential of the linear transformations of x for each hidden unit,

93
00:06:14.210 --> 00:06:15.410
which I'm finding no

94
00:06:15.780 --> 00:06:16.613
<v 0>here.</v>

95
00:06:20.740 --> 00:06:25.740
<v 1>So we finished our demonstration that p of x can be written down like this.</v>

96
00:06:26.081 --> 00:06:28.510
So it's just the exponential of a,

97
00:06:28.660 --> 00:06:33.660
a sum of a bunch of terms and there's on the capital h number of such terms,

98
00:06:34.620 --> 00:06:36.460
I'm not in this form.

99
00:06:36.461 --> 00:06:40.480
What's nice is that we'll be able to get a little bit of intuition for how we

100
00:06:40.481 --> 00:06:43.030
can adjust the parameters,
uh,

101
00:06:43.031 --> 00:06:48.031
here to make the probability for a given input vector x,

102
00:06:48.560 --> 00:06:49.930
uh,
to increase.

103
00:06:49.931 --> 00:06:53.830
So I can we change it so that we increase the probability of a given input

104
00:06:53.831 --> 00:06:58.110
vector X.
First,
I actually introduced a,

105
00:06:58.150 --> 00:07:02.950
a notation for the,
uh,
nonlinear already log of one plus x.

106
00:07:03.310 --> 00:07:05.440
It's known as the sub plus function,

107
00:07:05.740 --> 00:07:08.620
which we're seeing in this plot here.

108
00:07:09.040 --> 00:07:11.110
So first consider the dotted line,

109
00:07:11.111 --> 00:07:15.220
which is just the maximum between zero and the input,

110
00:07:15.430 --> 00:07:19.840
which I'll just call x.
That's all.
So assuming that this access is x,

111
00:07:20.470 --> 00:07:23.800
uh,
and uh,
so we see that indeed it's linear.

112
00:07:23.801 --> 00:07:28.801
If the input of the nonlinearity the argument is greater than zero and otherwise

113
00:07:30.251 --> 00:07:33.140
it's clam at zero,
now stop.

114
00:07:33.141 --> 00:07:36.550
Plus you can think of it as a smooth version of that function.

115
00:07:36.640 --> 00:07:40.970
So we see that as the argument becomes greater and greater and greater,

116
00:07:40.990 --> 00:07:44.530
then it becomes closer and closer,
closer to a linear function.

117
00:07:45.040 --> 00:07:46.600
And if it's actually quite negative,

118
00:07:46.601 --> 00:07:49.570
then it's going to be a lower bounded by zero.

119
00:07:49.571 --> 00:07:52.540
And as we go towards minus the infinity,
uh,

120
00:07:52.541 --> 00:07:55.690
we are getting closer and closer to zero.
Okay.

121
00:07:55.691 --> 00:07:59.440
So it's kind of a soft version of the,
of this function here.

122
00:08:00.310 --> 00:08:03.800
Sometimes known as the,
uh,
sometimes,
uh,
I've actually,

123
00:08:03.820 --> 00:08:05.560
it's actually corresponds duty,

124
00:08:05.740 --> 00:08:10.630
rectify linear function that we've seen when we described a activation functions

125
00:08:10.660 --> 00:08:15.210
for neuro networks.
Okay.
So now what does it take?

126
00:08:15.600 --> 00:08:18.690
What can we do to make p of x big?

127
00:08:20.470 --> 00:08:24.040
Well,
we need that term to be big.
So,
uh,

128
00:08:24.220 --> 00:08:28.480
if the input vector x is well aligned with the bias vector,

129
00:08:28.481 --> 00:08:32.630
see that is if the elements of XR positive,
when a,

130
00:08:32.631 --> 00:08:34.000
our one Wentz,

131
00:08:34.060 --> 00:08:38.290
the terms of the associated terms in the sea vector are positive and zero,

132
00:08:38.291 --> 00:08:41.320
when they're negative,
then you know,
this,
uh,

133
00:08:41.380 --> 00:08:43.930
I can become great if this can become a bit.

134
00:08:45.190 --> 00:08:48.950
Now we also need these terms to become paying.
So we,
there's that tiered,

135
00:08:48.960 --> 00:08:52.240
this influences,
this only looks at individual values of x.

136
00:08:52.241 --> 00:08:54.940
So we can't really model,
um,
uh,

137
00:08:54.960 --> 00:08:58.350
dependencies between pairs of,
uh,

138
00:08:58.410 --> 00:09:00.030
inputs or even just the joint,

139
00:09:00.420 --> 00:09:03.960
a configuration of the elements in the input vector.

140
00:09:04.380 --> 00:09:09.000
So this term here is not going to model dependencies between the elements in the

141
00:09:09.001 --> 00:09:13.470
input vector.
So for that to become big,
then,

142
00:09:13.990 --> 00:09:16.880
uh,
we want the input vector to,
uh,

143
00:09:16.980 --> 00:09:21.750
be well aligned with the rows of w so far given,
uh,

144
00:09:21.960 --> 00:09:23.720
hidden unit j.
Uh,

145
00:09:23.790 --> 00:09:28.790
if the dot product between DGF row and the vector x is big,

146
00:09:30.510 --> 00:09:34.740
then this term is going to become big.
So far,
given hidden unit,
um,

147
00:09:35.310 --> 00:09:40.310
d f x is a positive for the inputs to which the JFN in layer is positive.

148
00:09:44.001 --> 00:09:47.520
We connected,
and if it's zero for the other,
uh,
uh,

149
00:09:47.550 --> 00:09:50.210
input neurons to which it's negatively correlated,
connected,

150
00:09:50.220 --> 00:09:53.940
then this has a chance of,
of being a bigger.
Um,

151
00:09:54.630 --> 00:09:58.710
and then we have the bias of the,
uh,
hidden unit,

152
00:09:59.760 --> 00:10:00.150
uh,

153
00:10:00.150 --> 00:10:05.150
which essentially controls how big this needs to be for the soft plus to,

154
00:10:05.910 --> 00:10:10.800
uh,
be a much bigger than a zero.
So,

155
00:10:10.860 --> 00:10:13.800
uh,
another way of seeing this is that we can think of each hidden unit that's

156
00:10:13.801 --> 00:10:18.801
acting as a feature where we look at whether a feature is present in the input x

157
00:10:19.021 --> 00:10:22.640
by multiplying by its associated weight.
And,
uh,

158
00:10:22.680 --> 00:10:27.680
so it's associated row in the matrix w and then the bias just determines what it

159
00:10:27.871 --> 00:10:31.560
means to four features actually be present.

160
00:10:31.740 --> 00:10:35.720
So how big must this term be for us to consider that,
uh,
in the,

161
00:10:35.730 --> 00:10:40.200
this particular pattern associated with that feature is a meaningfully present

162
00:10:40.201 --> 00:10:44.340
in the input.
So for instance,
imagine that Bj is quite negative.

163
00:10:44.400 --> 00:10:47.160
Like for instance,
imagine that it's minus four,
that it,

164
00:10:47.490 --> 00:10:50.430
it means that once I've completed that,

165
00:10:50.670 --> 00:10:54.060
then the selfless is actually quite close to zero.

166
00:10:54.390 --> 00:10:57.180
And so it means that if I wanted the soft plus to be none,

167
00:10:57.181 --> 00:11:00.800
zero and actually increase the probability,
uh,

168
00:11:00.840 --> 00:11:02.610
for the observation x,

169
00:11:02.790 --> 00:11:07.790
then I need to fight the contribution from the bias and um,

170
00:11:08.720 --> 00:11:12.540
be actually much greater than zero.

171
00:11:12.541 --> 00:11:16.770
Then say if the buyers had been zero.
And I started my activation competing,

172
00:11:16.771 --> 00:11:21.360
my activation here.
And,
um,
and,

173
00:11:21.450 --> 00:11:24.930
and so that's the role that each different parameters are playing.

174
00:11:25.260 --> 00:11:28.530
And now it makes it more explicit that really the hidden units,

175
00:11:28.531 --> 00:11:30.690
we can really think of them as features.

176
00:11:30.900 --> 00:11:35.900
And so if we want the probability of all of our training examples to be high,

177
00:11:37.290 --> 00:11:41.820
then we want the different roles of w to be well aligned and represent

178
00:11:41.850 --> 00:11:43.680
meaningful features that are present,

179
00:11:44.250 --> 00:11:47.240
that are often present in all of our training data sets.

180
00:11:47.430 --> 00:11:49.710
And so the more these terms are going to be positive,

181
00:11:49.860 --> 00:11:53.500
then the greater the probability associated with a given input is going to be.

182
00:11:53.800 --> 00:11:58.800
And so training will correspond to finding these rows of w along with their

183
00:11:59.321 --> 00:12:01.100
corresponding biases,
uh,

184
00:12:01.150 --> 00:12:06.150
which are such that this term tends to be quite high for the inputs that we have

185
00:12:07.360 --> 00:12:08.500
in our training set.

186
00:12:08.650 --> 00:12:13.190
So we would want these roads to essentially look at different,
uh,
uh,

187
00:12:13.240 --> 00:12:17.860
specific features of inputs that we tend to see for our,
in our training center.

188
00:12:18.940 --> 00:12:19.180
<v 0>Okay.</v>

189
00:12:19.180 --> 00:12:23.170
<v 1>All right.
So,
um,
that's,
so we've introduced the notion of a free energy,</v>

190
00:12:23.350 --> 00:12:28.210
which is involved in computing p of x.
We can compute p of x,

191
00:12:28.300 --> 00:12:32.230
uh,
attractively in the general case because again,
of the partition function.

192
00:12:32.530 --> 00:12:35.370
However,
introducing the free energy has allowed us to,
uh,

193
00:12:35.500 --> 00:12:40.500
get an expression where it becomes more obvious what an RBM needs to do to make

194
00:12:40.751 --> 00:12:44.470
certain inputs,
uh,
values of input vectors be,
uh,

195
00:12:44.590 --> 00:12:48.150
have high probability.
Uh,
and so in this case,

196
00:12:48.151 --> 00:12:49.870
supports your training or the training set,
how,

197
00:12:50.110 --> 00:12:52.540
what it needs to do to make the training set more likely.
Yeah.

