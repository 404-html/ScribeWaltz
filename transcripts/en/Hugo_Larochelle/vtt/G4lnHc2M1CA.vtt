WEBVTT

1
00:00:00.550 --> 00:00:03.670
And this veto will introduce the notion of a context window.

2
00:00:05.950 --> 00:00:08.570
The previous video we've introduced the,
uh,

3
00:00:08.620 --> 00:00:13.620
model of a linear chain conditional then and feel which consistent in adding

4
00:00:13.631 --> 00:00:18.631
these lateral connections parametrized rise by matrix fee between the

5
00:00:19.031 --> 00:00:23.370
computations made at different positions in the,
uh,

6
00:00:23.380 --> 00:00:24.970
in the sequence that we're modeling.

7
00:00:25.360 --> 00:00:30.360
And so this allows us to express some preferences for the assignment of a

8
00:00:31.100 --> 00:00:33.640
adjacent labels in a sequence.

9
00:00:34.750 --> 00:00:39.750
So the goal of doing this is so that the probability assigned at a given

10
00:00:39.911 --> 00:00:44.560
position is also influenced by the probability mass that we're assigning,
uh,
at,

11
00:00:44.620 --> 00:00:47.830
uh,
at Jason Positions.
And,
uh,

12
00:00:47.831 --> 00:00:52.150
and so this allows us to model essentially dependencies within the sequence.

13
00:00:52.720 --> 00:00:57.490
In other way of doing this could have been to have the inputs say,
uh,

14
00:00:57.500 --> 00:01:01.720
on the left of the center position here,

15
00:01:01.721 --> 00:01:06.721
k also influenced directly the computation of the pre activations that position

16
00:01:07.871 --> 00:01:12.180
came.
And similarly we could have had,
uh,
the input at a,

17
00:01:12.260 --> 00:01:13.570
the position on the right,

18
00:01:13.571 --> 00:01:18.280
a k plus one also be used directly in computing.
The,

19
00:01:18.680 --> 00:01:23.260
uh,
DRP activations at the output layer for position came.

20
00:01:23.840 --> 00:01:28.000
So we'll see two different ways of doing this in the conditional random fields.

21
00:01:31.280 --> 00:01:36.130
All right.
So,
um,
generally we'll talk about a context window,

22
00:01:36.490 --> 00:01:38.640
uh,
referring to,
uh,

23
00:01:38.830 --> 00:01:43.480
the set of inputs that will influence our prediction.

24
00:01:43.481 --> 00:01:44.860
So our,
uh,
the,

25
00:01:44.890 --> 00:01:49.300
our preference about the value of a given label at a given position.

26
00:01:49.510 --> 00:01:54.160
So,
uh,
can text window is going to include all of the inputs,

27
00:01:54.580 --> 00:01:59.140
uh,
at a given position that influenced the,
uh,
preference,

28
00:01:59.200 --> 00:02:03.310
uh,
that we are assigning to,
uh,
some,
uh,
uh,
two different labels.

29
00:02:03.790 --> 00:02:05.050
So more specifically,

30
00:02:05.800 --> 00:02:10.800
currently in the linear chain Crif we have the P of y given x is the exponential

31
00:02:12.190 --> 00:02:15.490
of as set of terms that depend,
uh,

32
00:02:15.550 --> 00:02:17.200
that involve each label.

33
00:02:17.201 --> 00:02:22.201
Why K for different positions,K and that involved the current input at the same

34
00:02:23.201 --> 00:02:27.280
position,
x,
k plus our pairwise term.
Now,

35
00:02:27.310 --> 00:02:30.550
if we consider the,
uh,
an example,
simple example,

36
00:02:30.551 --> 00:02:33.220
we have a context window of radius one.

37
00:02:33.580 --> 00:02:36.340
What this will mean is that

38
00:02:37.500 --> 00:02:39.600
we would also add two other terms,

39
00:02:40.410 --> 00:02:43.860
a set of terms or to a set of terms,

40
00:02:44.360 --> 00:02:48.930
a first set of terms which will essentially influence how,
uh,

41
00:02:48.931 --> 00:02:53.820
the input on the left of a position gay,
uh,
uh,

42
00:02:53.850 --> 00:02:58.550
how it influences how like we,
we think why k is so,
uh,

43
00:02:58.570 --> 00:03:02.330
we are essentially going to AV in neural network for a position,
uh,

44
00:03:02.331 --> 00:03:07.240
that will take the input that's always on the left of the current position.
K,

45
00:03:07.330 --> 00:03:12.330
so that's x came on as one and that will compute a pre activation outputs,

46
00:03:13.270 --> 00:03:18.100
which will assign for each value of y k a particular preference,
whether,
uh,

47
00:03:18.140 --> 00:03:22.210
uh,
this neural net thinks that,
uh,
how like he does this neural net thing,

48
00:03:22.211 --> 00:03:27.211
that different values of y k a r to be observed and similarly kind of an order

49
00:03:28.271 --> 00:03:30.760
in their own network,
but for the position on the right.

50
00:03:30.761 --> 00:03:35.761
So it's gonna tell us how we think why case likely given the input on the right

51
00:03:35.830 --> 00:03:40.360
of the position,
Kay in the sequence.
And so it's going to take x,

52
00:03:40.361 --> 00:03:43.270
k plus one.
And then,
uh,

53
00:03:43.271 --> 00:03:48.271
and then this neural network is going to compute a pre activation vector from

54
00:03:48.881 --> 00:03:53.680
which we'll be able to consult how this neural net things based on what's on the

55
00:03:53.681 --> 00:03:58.570
right,
uh,
that the value of yk,
whether the white,
you,
uh,

56
00:03:58.630 --> 00:04:00.910
the value of white gay is likely or not.

57
00:04:01.530 --> 00:04:02.350
<v 1>Okay.</v>

58
00:04:02.350 --> 00:04:05.170
<v 0>And notice that I've changed likely the notation here.</v>

59
00:04:05.171 --> 00:04:10.171
I've added here zero here minus one and here plus one just to highlight the fact

60
00:04:11.351 --> 00:04:16.351
that essentially these three reactivation vectors are computed by three

61
00:04:16.541 --> 00:04:17.860
different neural networks.

62
00:04:18.010 --> 00:04:22.030
So there's one neural network centered exactly on the uh,

63
00:04:22.180 --> 00:04:26.350
value k one that looks at the relative position on the left.

64
00:04:26.351 --> 00:04:31.090
So relative position minus one and another one relative position plus one.

65
00:04:31.150 --> 00:04:35.860
Okay.
So these are really three different neural networks that take a different,

66
00:04:36.070 --> 00:04:36.430
uh,

67
00:04:36.430 --> 00:04:41.200
input at a difference relative position with respect to a current position came.

68
00:04:42.040 --> 00:04:45.430
Okay.
So this is one way of having the,
uh,

69
00:04:45.700 --> 00:04:50.700
probability for a label at a given position to be dependent.

70
00:04:51.431 --> 00:04:54.070
Now directly under the input,
uh,

71
00:04:54.190 --> 00:04:57.910
that is not at the position gay,
but is in this,

72
00:04:58.000 --> 00:05:01.660
in this case of the position came at a swan on a position,
k plus one.

73
00:05:04.210 --> 00:05:08.710
So again,
if we look at the,
uh,
flow graph or computations,

74
00:05:08.890 --> 00:05:11.570
then it essentially means we're adding a,

75
00:05:11.670 --> 00:05:16.120
a set of connections that go from,
uh,

76
00:05:16.330 --> 00:05:21.330
the input on the left and then go eventually on the reactivation vector,

77
00:05:22.300 --> 00:05:26.170
uh,
at the position on a relatively on its right.

78
00:05:26.171 --> 00:05:28.780
So at position K and,
uh,

79
00:05:29.020 --> 00:05:32.440
similarly we have another set of connections which are different.

80
00:05:32.441 --> 00:05:35.530
So it's a different neural net that goes from x,

81
00:05:35.531 --> 00:05:40.531
k plus one and an influences the reactivation that determines the,

82
00:05:41.210 --> 00:05:44.410
uh,
that influences the probability of y k.

83
00:05:47.010 --> 00:05:47.843
<v 1>Okay.</v>

84
00:05:48.420 --> 00:05:48.781
<v 0>In other,</v>

85
00:05:48.781 --> 00:05:53.520
perhaps simpler alternative would be to use a single neural network,

86
00:05:53.880 --> 00:05:56.070
but feed it the whole context window.

87
00:05:56.071 --> 00:06:01.070
That is the concatenation of all the input vectors that belonged to the context

88
00:06:01.071 --> 00:06:02.720
window at a given position.

89
00:06:03.410 --> 00:06:07.820
And so this will allow their own neural network to learn about the whole
context.

90
00:06:07.821 --> 00:06:12.150
Junk lead the relationship of the whole context with respect to,
uh,

91
00:06:12.240 --> 00:06:15.950
the labels at the different positions.
And so in this case,

92
00:06:16.490 --> 00:06:19.670
the notation is much simpler.
So we have a single neural network,

93
00:06:19.671 --> 00:06:23.180
we don't have a comma zero here,
uh,

94
00:06:23.210 --> 00:06:28.210
but it's input is a computed based on the both ex gay as we had before,

95
00:06:29.631 --> 00:06:32.630
but also x came on the Swan and x k plus one.

96
00:06:33.050 --> 00:06:36.560
So one way of doing this is just concatenate these three vectors and feed that

97
00:06:36.561 --> 00:06:40.090
to,
uh,
uh,
set that as the input layer of,
uh,
uh,

98
00:06:40.130 --> 00:06:42.330
of the neural network and then,
uh,

99
00:06:42.380 --> 00:06:46.160
perform a Ford propagation until we reached the last hidden layer.

100
00:06:46.370 --> 00:06:47.300
So of course in this case,

101
00:06:47.301 --> 00:06:50.480
we'd have more parameters for the first hidden layers because we'd have to

102
00:06:50.481 --> 00:06:54.880
connect also x k minus one and escape plus one to the hidden layer.
But,
uh,

103
00:06:54.920 --> 00:06:58.040
the other hidden layer is computation with would not really change.

104
00:06:58.460 --> 00:07:02.090
It's just the first hidden in there that would now depend on the whole context

105
00:07:02.091 --> 00:07:02.924
window.

106
00:07:02.980 --> 00:07:03.380
<v 1>Okay.</v>

107
00:07:03.380 --> 00:07:07.730
<v 0>And as before we have our linear chain parameters here,
which influence,
uh,</v>

108
00:07:07.760 --> 00:07:12.760
how much we liked seeing a at Jason values of the labeling in our sequence for

109
00:07:13.731 --> 00:07:18.530
which we computing a probability and a not because this is some runs from one,

110
00:07:18.531 --> 00:07:23.531
two k a and because x k minus one equals one is x zero and that does not exist

111
00:07:25.161 --> 00:07:26.090
in our sequence.

112
00:07:26.360 --> 00:07:31.360
And the same thing for at a k equals capital k then x Cape at capitol x Capitol

113
00:07:33.321 --> 00:07:35.690
capable plus one does not exist as well.

114
00:07:35.691 --> 00:07:40.320
Well arbitrarily you can just choose to set those vectors to zero.
Um,

115
00:07:40.400 --> 00:07:44.960
or we could choose some special vector that would essentially inform the neural

116
00:07:44.961 --> 00:07:49.910
network that it's on either edge of the sequence if we think that's a practical

117
00:07:49.911 --> 00:07:50.571
information,

118
00:07:50.571 --> 00:07:55.571
that useful information for influencing the probability of the symbol on the

119
00:07:56.690 --> 00:07:58.010
borders of the sequence.

120
00:08:01.220 --> 00:08:06.110
So now are a linear change conditional random field would look like this.
Uh,

121
00:08:06.230 --> 00:08:08.820
so now the input,
uh,

122
00:08:08.900 --> 00:08:13.250
at position k would be the concatenation of x,

123
00:08:13.251 --> 00:08:17.870
k minus one and x k plus one.
And so now I would have the,

124
00:08:17.871 --> 00:08:21.110
all these pixels and these pixels and these pixels as input,

125
00:08:21.410 --> 00:08:25.990
and then it would propagate until we reach the reactivation.

126
00:08:25.991 --> 00:08:30.590
I'll put layer,
uh,
so the upper layer and compute the reactivation there,
uh,

127
00:08:30.591 --> 00:08:34.970
to,
uh,
get our term in the,
uh,
uh,
the conditional random fields.

128
00:08:35.600 --> 00:08:38.060
And,
uh,
so similarly,
for instance,

129
00:08:38.061 --> 00:08:42.830
this character would be on the right here and it would be on the left of the

130
00:08:42.831 --> 00:08:43.611
context here.

131
00:08:43.611 --> 00:08:48.230
So the same character will actually be present at different positions in the,

132
00:08:48.260 --> 00:08:52.300
uh,
in the,
uh,
in the,
uh,
uh,
input of the,
uh,

133
00:08:52.340 --> 00:08:54.230
here three neural network.

134
00:08:54.350 --> 00:08:55.183
<v 2>That's</v>

135
00:08:58.670 --> 00:09:00.980
<v 0>now we'll introduce a different notation,</v>

136
00:09:00.981 --> 00:09:04.970
which is simpler and we'll simplify the math for the rest of it.

137
00:09:05.120 --> 00:09:07.700
<v 2>Uh,
uh,
the v of the videos.</v>

138
00:09:08.560 --> 00:09:12.560
<v 0>Um,
so first we'll introduce the notion of the urinary luck factor.</v>

139
00:09:13.310 --> 00:09:17.960
Uh,
so we'll see more a,
why we call these luck factor.
Uh,

140
00:09:17.961 --> 00:09:18.840
but their unit re,

141
00:09:18.841 --> 00:09:23.841
because they are the terms that only depend on the single why at a single

142
00:09:23.931 --> 00:09:26.210
position came.
So,
for instance,

143
00:09:26.360 --> 00:09:30.200
if we look at the case where we had the three different neural networks,

144
00:09:30.201 --> 00:09:33.860
one that looked,
uh,
at the center and other one on it's left,

145
00:09:33.870 --> 00:09:37.250
another one on it's right.
Well all of these terms here,

146
00:09:37.251 --> 00:09:42.251
the each depend on the dare expressional each depends only on yk.

147
00:09:42.980 --> 00:09:47.980
So essentially these are the terms that express a preference for the value of y

148
00:09:48.321 --> 00:09:52.340
at the position.
Uh,
k only end up at that.

149
00:09:52.520 --> 00:09:53.590
That is based on the,

150
00:09:53.660 --> 00:09:58.270
it's a preference only on why gain not on a pair of values of four wives.

151
00:09:58.730 --> 00:10:03.320
And similarly for the case where we have just a single neural net with a uh,

152
00:10:03.350 --> 00:10:04.730
context window has input.

153
00:10:05.060 --> 00:10:10.060
While in this case it's only the term p activation term that looks at the whole

154
00:10:11.570 --> 00:10:12.500
context window.

155
00:10:12.660 --> 00:10:17.530
And this term only depends on the single y value at the specific position came.

156
00:10:17.840 --> 00:10:19.550
Okay.
So for those,
for both of those,

157
00:10:19.551 --> 00:10:24.551
I'll just use the notation a Uy gay you is for you neri because it depends on

158
00:10:24.681 --> 00:10:29.681
only one label and why k to specify a which for which variable are we computing?

159
00:10:31.150 --> 00:10:34.340
<v 2>Uh,
the,
uh,
the urinary factor.</v>

160
00:10:35.220 --> 00:10:40.220
<v 0>And you'll notice here that the key here also specifies the position.</v>

161
00:10:40.921 --> 00:10:44.760
So whyK is a variable,
but also,
uh,
uh,

162
00:10:44.820 --> 00:10:48.030
we'll use just to simplify again the notation.
Uh,

163
00:10:48.031 --> 00:10:52.200
we use the value of k here to inform us that uh,

164
00:10:52.230 --> 00:10:56.340
this term is computed based on the position came.

165
00:10:57.600 --> 00:10:58.620
And uh,

166
00:10:58.621 --> 00:11:03.390
so these will be the urinary lot factors and we also have pairwise luck factors

167
00:11:03.420 --> 00:11:07.740
and there are pairwise because they involve a pair of labels,

168
00:11:07.800 --> 00:11:12.660
adjacent labels.
And so we'll have a p four,
sorry,

169
00:11:12.661 --> 00:11:17.250
four pairwise.
And then in this case it's only the term that looks,

170
00:11:17.570 --> 00:11:21.720
uh,
at the entry.
Why K Y K plus one in the matrix.

171
00:11:21.930 --> 00:11:26.040
It's the only pairwise luck factor.
And so a quickly,

172
00:11:26.041 --> 00:11:30.600
the reason why we call them lot factor is because,
because here,

173
00:11:30.810 --> 00:11:34.140
uh,
so if you write down p of y given x with this new invitation,

174
00:11:34.141 --> 00:11:35.600
we get the exponential of the,

175
00:11:35.601 --> 00:11:40.601
some of the urinary log factors plus the sum of the pairwise luck factors where

176
00:11:40.801 --> 00:11:43.830
we iterate over a possible positions.

177
00:11:44.910 --> 00:11:46.140
And a why,

178
00:11:46.150 --> 00:11:50.240
the reason why they are locked factors is that we could write this as the,
uh,

179
00:11:50.280 --> 00:11:54.010
because the exponential of Assam is the product of exponentials could write this

180
00:11:54.011 --> 00:11:57.770
back as a product of some exponentials of June.

181
00:11:57.780 --> 00:12:01.780
They're re log factors and pairwise luck factors.

182
00:12:02.050 --> 00:12:06.550
And so the exponential of the lug would a,
would just be a factor.

183
00:12:06.970 --> 00:12:11.080
And so for this reason,
because they appear in the,
uh,
in the,
uh,

184
00:12:11.100 --> 00:12:15.700
they are because they are exponentiate it in the,
uh,
uh,
this expression here,

185
00:12:15.760 --> 00:12:19.600
we say that they,
instead of being factors that are being multiplied together,

186
00:12:19.601 --> 00:12:23.620
there are lot of factors which are being an exponentiate it and then multiplied

187
00:12:23.621 --> 00:12:24.790
together.
Okay.

188
00:12:25.060 --> 00:12:29.920
So we'll continue using this notation and just know that,

189
00:12:29.950 --> 00:12:34.730
well,
for the pairwise factor,
we always use this,
uh,

190
00:12:34.840 --> 00:12:38.560
for the next videos.
Uh,
and for the unit factors,
well,

191
00:12:38.561 --> 00:12:43.561
we could either use this case here or we could use a single neural net with the

192
00:12:43.840 --> 00:12:44.800
big context window.

