WEBVTT

1
00:00:00.350 --> 00:00:00.900
Yeah.

2
00:00:00.900 --> 00:00:01.461
<v 1>In this video,</v>

3
00:00:01.461 --> 00:00:06.461
we're finally ready to put the different pieces together to get our final

4
00:00:06.920 --> 00:00:08.390
dictionary learning algorithm.

5
00:00:09.620 --> 00:00:10.453
<v 0>Okay.</v>

6
00:00:10.860 --> 00:00:15.720
<v 1>So,
uh,
we remember that a Ds Pars coding problem.</v>

7
00:00:15.900 --> 00:00:16.411
Uh,

8
00:00:16.411 --> 00:00:20.520
so the problem that corresponds to learning a good dictionary for being able to

9
00:00:20.730 --> 00:00:25.730
infer some sparse representations for our Dataset is a corresponds to a to

10
00:00:26.971 --> 00:00:31.860
minimization.
One with respect to the dictionary and one retrospect to the spars

11
00:00:31.861 --> 00:00:36.861
representations for all of our training examples and what we've seen before,

12
00:00:37.140 --> 00:00:41.220
how given the training example and a dictionary Matrix d,

13
00:00:41.310 --> 00:00:46.310
How can I infer the spars coats with the Ista Algorithm and given some fixed

14
00:00:48.240 --> 00:00:51.690
sponsored representations,
we've seen two algorithms,
uh,

15
00:00:51.810 --> 00:00:56.220
for updating the dictionary matrix and so performing this minimization,

16
00:00:56.280 --> 00:00:59.640
assuming that this minimization has been performed.
Okay.

17
00:01:00.000 --> 00:01:04.340
So now we're ready to put these two pieces together to get a joint,
uh,

18
00:01:04.420 --> 00:01:09.060
a global algorithm for performing this whole minimization here.
Uh,

19
00:01:09.061 --> 00:01:12.300
so retrospective,
both d n a h d.

20
00:01:15.630 --> 00:01:17.010
So the idea is very simple.

21
00:01:17.040 --> 00:01:22.040
We'll just alternate between performing the inference that is computing the age

22
00:01:22.260 --> 00:01:26.170
of x t vectors,
parse codes,
and then,
uh,

23
00:01:26.200 --> 00:01:29.580
updating the dictionary.
And A,

24
00:01:29.581 --> 00:01:33.450
so we'll go back and forth like this until convergence.
Uh,

25
00:01:33.451 --> 00:01:35.400
so here's a Sudoko for the algorithm.

26
00:01:35.790 --> 00:01:40.410
As long as our Matrix d has not converged,
are as far as coding Matrix,

27
00:01:40.590 --> 00:01:41.760
our dictionary Matrix,

28
00:01:42.210 --> 00:01:46.140
a first for each input XD in my training set,

29
00:01:46.260 --> 00:01:50.430
I'll stuck in memory and performed the inference of it's sparse coding

30
00:01:50.431 --> 00:01:53.430
representation and so forth that I can use Ista.

31
00:01:54.540 --> 00:01:55.650
Once that's done,

32
00:01:56.040 --> 00:02:01.040
then I am ready to update my dictionary using these fixed precomputed

33
00:02:01.351 --> 00:02:05.400
representations.
And so if we use the block corner at the center algorithm,

34
00:02:05.750 --> 00:02:10.410
I again compute my matrix a matrix B,
which corresponds to these,

35
00:02:10.650 --> 00:02:14.400
uh,
sums over the Dataset of two different types of outer products,

36
00:02:14.580 --> 00:02:18.210
one between the input and there are other,
uh,
with the uh,

37
00:02:18.390 --> 00:02:21.840
between the input and these first and representation and the other one,

38
00:02:22.080 --> 00:02:25.430
the other product between,
uh,
the uh,
uh,

39
00:02:25.440 --> 00:02:27.390
as far as going representation with itself.

40
00:02:28.140 --> 00:02:33.030
And then I can run the block corner descent algorithm to until convergence,

41
00:02:33.330 --> 00:02:36.030
uh,
to get my new value 40.

42
00:02:36.930 --> 00:02:40.940
And then I come back and I recompute new sparse representations.

43
00:02:40.970 --> 00:02:44.550
So now these are going to be different.
Is Diaz changed.

44
00:02:44.551 --> 00:02:48.810
And that's because when we're performing an update of the dictionary Matrix d,

45
00:02:49.020 --> 00:02:51.540
we were assuming that these were precomputed.

46
00:02:51.541 --> 00:02:55.780
We're not changing when these changing,
but that's,
that's actually false.
Uh,

47
00:02:56.100 --> 00:02:59.170
performing disinterests requires knowing a particular matrix team.

48
00:02:59.890 --> 00:03:01.780
And so what we'll do is that,
uh,

49
00:03:01.880 --> 00:03:06.880
will update the representation for all of our training examples and then that's

50
00:03:08.171 --> 00:03:10.330
going to give us new matrix and B,

51
00:03:10.540 --> 00:03:15.040
and then we can rerun our block corner does scent with these new matrix,

52
00:03:15.200 --> 00:03:20.200
a matrix amb that's going to give us a new Matrix d and then we go back,

53
00:03:20.350 --> 00:03:21.041
we update the,

54
00:03:21.041 --> 00:03:25.500
as far as codes and we continue like this until the Matrix d has not changed

55
00:03:25.620 --> 00:03:27.310
significantly has converged.

56
00:03:28.420 --> 00:03:33.410
Now the reason why this will this alternating,
uh,

57
00:03:33.460 --> 00:03:36.130
so alternating between these two optimization problems.

58
00:03:36.131 --> 00:03:40.420
The reason why this was con this will converge is that well performing this

59
00:03:40.421 --> 00:03:45.040
optimization strictly will improve with respect to our previous,
uh,

60
00:03:45.130 --> 00:03:48.130
value of the objective for our previous value of d and h.

61
00:03:48.131 --> 00:03:50.020
And that's because we keeping the fix,

62
00:03:50.230 --> 00:03:53.530
but we're are improving the optimization,

63
00:03:53.531 --> 00:03:56.860
which respect the objective optimization,
uh,

64
00:03:56.861 --> 00:04:01.210
with respect to our sparse codes age.
And then when we come here,

65
00:04:01.390 --> 00:04:03.160
if we optimize it well,

66
00:04:03.161 --> 00:04:07.660
we are strictly improving our objective with respect to,
uh,
d actually.

67
00:04:08.800 --> 00:04:12.530
Um,
and so,
uh,
for,
for that reason,
uh,

68
00:04:12.540 --> 00:04:14.710
whenever we're a by alternating,

69
00:04:14.711 --> 00:04:19.000
we're always improving over the objective but just with respect to one variable.

70
00:04:19.330 --> 00:04:22.420
And so,
uh,
in,
in that sense it's a,

71
00:04:22.421 --> 00:04:27.421
it's a kind of similar to this by itself is sort of like a block coordinate a

72
00:04:28.511 --> 00:04:31.990
algorithm because we alternate between optimizing one part of the problem,

73
00:04:32.140 --> 00:04:35.620
keeping the other part fixed and then going back and optimizing and other part

74
00:04:35.621 --> 00:04:38.350
of the problem.
Yeah.
Keeping the other part fixed.

75
00:04:38.650 --> 00:04:42.490
So for that reason it's going to converge.
And a,
you might,

76
00:04:42.491 --> 00:04:44.350
if you're familiar with the eem algorithm,

77
00:04:44.351 --> 00:04:48.420
the expectation maximisation algorithm say port training mixtures of Galoshins

78
00:04:48.460 --> 00:04:52.210
or other graphical models with the,
uh,
with the latent,

79
00:04:52.690 --> 00:04:56.450
a latent,
uh,
random variables.
Uh,

80
00:04:56.500 --> 00:04:58.890
this is kind of similar where,
uh,

81
00:04:58.930 --> 00:05:02.680
inferring the sparse code would be akin to the east step.

82
00:05:03.010 --> 00:05:07.270
And the optimizing the dictionary would be akin to the m step.

83
00:05:08.770 --> 00:05:11.410
And so that's it for the sparse calling all of them.

84
00:05:11.590 --> 00:05:16.330
And so now we have all the pieces for you to implement it and run it on,
say,

85
00:05:16.331 --> 00:05:19.390
some,
uh,
data set of images and so on.

86
00:05:19.391 --> 00:05:23.920
So get a good value for our dictionary,
which will,
uh,

87
00:05:23.950 --> 00:05:27.250
allow us to say,
for instance,
extract some good,

88
00:05:27.251 --> 00:05:29.320
sparse features for a given dataset.

