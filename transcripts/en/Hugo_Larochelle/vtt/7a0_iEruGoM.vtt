WEBVTT

1
00:00:00.340 --> 00:00:03.700
This video will introduce and define the sparse coding model.

2
00:00:05.890 --> 00:00:10.870
So sparse coding is a model in the context of unsupervised learning.
So a,

3
00:00:10.871 --> 00:00:14.740
that's in the context where we have training data,
which is not labeled.

4
00:00:14.741 --> 00:00:19.030
So we only have a set of,
uh,
x factors in our training set.

5
00:00:19.600 --> 00:00:24.490
And a,
it's going to be a model which would also help us extract some interesting

6
00:00:24.491 --> 00:00:28.570
features out of,
uh,
some training set,
uh,

7
00:00:28.571 --> 00:00:32.770
and allow us to perhaps leverage a large set of unlabeled data,

8
00:00:32.771 --> 00:00:36.220
see a lot of unlabeled images that we extracted from the web.

9
00:00:37.600 --> 00:00:42.280
And uh,
as,
uh,
so we've seen before,
uh,

10
00:00:42.281 --> 00:00:44.140
other neural networks for,
uh,

11
00:00:44.141 --> 00:00:47.550
unsupervised learning or we seeing a wreck suspected Bolsa machine.
And you know,

12
00:00:47.551 --> 00:00:52.030
it's one quarter in the previous videos.
And so now we'll look at another,

13
00:00:52.290 --> 00:00:56.290
uh,
as far as calling,
sorry,
another unsupervised,
a neural network,

14
00:00:56.380 --> 00:00:58.390
which is known as this par scoring model.

15
00:01:00.040 --> 00:01:00.873
<v 1>Okay.</v>

16
00:01:01.160 --> 00:01:01.820
<v 0>All right.</v>

17
00:01:01.820 --> 00:01:06.290
So the idea behind sparse coding is that for our,

18
00:01:06.380 --> 00:01:11.300
uh,
for any input x,
we want to find a latent representation.

19
00:01:11.390 --> 00:01:14.060
So a hidden layer,
h t ah,

20
00:01:14.070 --> 00:01:19.010
till is to note that it's the hidden layer for that training example.
And A,

21
00:01:19.011 --> 00:01:23.690
we want to lead the representation that is first a sparse,

22
00:01:23.960 --> 00:01:27.990
which means that in the Layton representation we want there to be many Zeros,

23
00:01:27.991 --> 00:01:31.650
the only a few,
none,
zero elements.
And uh,

24
00:01:31.670 --> 00:01:36.620
we also want the representation to,
uh,
contain meaningful information about x t.

25
00:01:36.950 --> 00:01:39.600
And that we were going to formulate that is that we want a liter and

26
00:01:39.601 --> 00:01:44.280
representation to be able to reconstruct the original input,
uh,

27
00:01:44.330 --> 00:01:49.070
as well as possible.
So to translate these two,
uh,

28
00:01:49.360 --> 00:01:53.370
uh,
concepts that we want to implement in sparse coding,
uh,

29
00:01:53.390 --> 00:01:58.280
we'll translate that into an objective function,
uh,
which,
uh,

30
00:01:58.281 --> 00:02:01.670
which we see here.
So first,
whoops.

31
00:02:04.980 --> 00:02:05.813
<v 1>Yeah.</v>

32
00:02:06.220 --> 00:02:11.220
<v 0>So first we want to be able to reconstruct the original data as well as</v>

33
00:02:11.681 --> 00:02:15.660
possible.
And so for that reason,
uh,
will,
uh,

34
00:02:15.830 --> 00:02:18.650
enforce XD,
uh,

35
00:02:18.700 --> 00:02:23.700
we won't XD to be close to a reconstruction a,

36
00:02:24.880 --> 00:02:29.800
which is going to be my Leeton representation times a matrix of weights and

37
00:02:29.801 --> 00:02:30.580
inspires coding.

38
00:02:30.580 --> 00:02:35.200
We usually refer to that matrix of weights as a dictionary matrix.

39
00:02:35.590 --> 00:02:39.040
And we'll see a little bit more later why we use the term dictionary.

40
00:02:39.610 --> 00:02:44.110
And so this we will call our reconstruction and we want the reconstruction to be

41
00:02:44.111 --> 00:02:47.320
as close as possible to uh,
the original input.

42
00:02:48.240 --> 00:02:49.073
<v 1>Okay.</v>

43
00:02:49.330 --> 00:02:52.810
<v 0>But we also want that latent representation to be sparse.
So for,</v>

44
00:02:52.840 --> 00:02:57.840
because of this will penalize the l one norm of a delete and representation that

45
00:02:58.331 --> 00:03:03.280
we've seen before.
We mentioned the l one norm,
actually,
uh,
uh,

46
00:03:03.310 --> 00:03:08.200
we'll force certain elements to be a exactly zero.
So we'll achieve our,

47
00:03:08.650 --> 00:03:12.700
uh,
our goal of getting a lead and representation,
which contains many Zeros.

48
00:03:14.890 --> 00:03:19.510
And now this lamb,
the term here is going to control to what extent,

49
00:03:19.860 --> 00:03:21.890
uh,
we wish to,
uh,

50
00:03:22.030 --> 00:03:26.530
get a good reconstruction error compared to achieving high sparsity.

51
00:03:26.920 --> 00:03:31.150
And so these two objectives sort of fight each other.
Um,
the sparsity term,

52
00:03:31.151 --> 00:03:35.770
we'd be very happy if all the ht vector was only Zeros.

53
00:03:36.010 --> 00:03:39.370
However,
the reconstruction would be very bad.
It would be always zero.

54
00:03:39.940 --> 00:03:40.571
On the other hand,

55
00:03:40.571 --> 00:03:45.430
we could get a perfect reconstruction of XD if we had no constraints of what the

56
00:03:45.431 --> 00:03:49.600
value of ht could be.
The later representation.
However,
the l one,

57
00:03:49.650 --> 00:03:52.960
a penalty here would not be happy.
It would be high,
uh,

58
00:03:52.990 --> 00:03:57.300
because presumably the values in age who need to be,
uh,
you know,
none,

59
00:03:57.310 --> 00:04:00.730
zero and perhaps fairly large to get a good reconstruction.

60
00:04:00.880 --> 00:04:05.470
So this is to control the trade off between a good reconstruction and sparsity

61
00:04:05.471 --> 00:04:09.880
in the representation.
Okay.
And so that's the objective.

62
00:04:09.881 --> 00:04:11.650
We want to optimize,
uh,

63
00:04:11.680 --> 00:04:16.180
and one to optimize it for each training example XD.

64
00:04:16.660 --> 00:04:21.660
So we'll have a sum over all the training examples and a notice here that the

65
00:04:23.801 --> 00:04:28.180
loss is,
uh,
requires a minimization over the later and representation.

66
00:04:28.330 --> 00:04:32.740
So for each XD we,
uh,
wanted to,
uh,

67
00:04:32.770 --> 00:04:37.000
we want to optimize our problems such that if I tried to find the best

68
00:04:37.001 --> 00:04:38.490
representation that reconstruct,

69
00:04:38.710 --> 00:04:42.190
well the input and sparse and is also sparse,

70
00:04:42.370 --> 00:04:46.400
then the sum of the reconstruction term and the sparsity terms going to be as

71
00:04:46.401 --> 00:04:50.890
small as possible.
And so at the,
uh,

72
00:04:51.100 --> 00:04:55.120
outer loop of this some,
what we do is for all the training set,

73
00:04:55.150 --> 00:04:59.300
we want to find the dictionary Matrix d,
uh,

74
00:04:59.590 --> 00:05:02.440
which is such that it's going to lead to a very small reconstruction.

75
00:05:02.470 --> 00:05:05.470
And the sparsity a combined with as far city penalty,

76
00:05:05.471 --> 00:05:08.380
a very small loss for all the training examples.

77
00:05:08.920 --> 00:05:12.590
So that's all we're going to formulate our a learning problem.
So,
uh,

78
00:05:12.640 --> 00:05:16.420
we're going to formulate as this optimization problem here.
Um,

79
00:05:17.020 --> 00:05:21.550
notice that we have to constrain the columns of d somehow.

80
00:05:21.551 --> 00:05:23.410
That's because,
uh,

81
00:05:23.440 --> 00:05:28.440
say we are penalizing age if we're not constraining dean anyway,

82
00:05:29.200 --> 00:05:33.850
uh,
then we could increase the size of the elements in d and the decrees

83
00:05:34.000 --> 00:05:34.840
accordingly.

84
00:05:34.841 --> 00:05:39.360
The size of the elements in age and that would make the sparsity determine,

85
00:05:39.370 --> 00:05:42.160
are more happy with it would become lower and lower and lower.

86
00:05:42.550 --> 00:05:47.550
And so it could sort of cheat by transferring the size of age into d and then a

87
00:05:48.670 --> 00:05:53.260
satisfied those parts of the penalty.
So to avoid this and get a badly,

88
00:05:53.261 --> 00:05:56.070
a defined objective,
uh,

89
00:05:56.080 --> 00:06:00.050
what we'll do is that will the columns of d to have a norm of one.

90
00:06:00.051 --> 00:06:03.440
So all the vectors that form the columns at the got up norm what?

91
00:06:04.850 --> 00:06:06.500
Um,
and uh,

92
00:06:06.750 --> 00:06:11.330
I've sometimes in the literature you find that people instead a for the
constant,

93
00:06:11.331 --> 00:06:14.120
they'll just constraint the norm to be no greater than one.

94
00:06:14.330 --> 00:06:18.530
It's just an alternative formulation.
But the idea behind it is a,
is the same.

95
00:06:21.790 --> 00:06:24.250
So,
um,
if,
uh,

96
00:06:24.370 --> 00:06:28.570
if we remember a d autumn quarter that was discussed in previous videos,

97
00:06:28.870 --> 00:06:33.870
well we can think of Ddi as the equivalent of the auto encoder output weight

98
00:06:34.241 --> 00:06:36.580
matrix.
So indeed it takes a,

99
00:06:36.690 --> 00:06:40.400
they don't representation and uh,
and uh,

100
00:06:40.480 --> 00:06:43.390
our auto encoder weight matrix multiplies that and that becomes our

101
00:06:43.391 --> 00:06:44.224
reconstruction.

102
00:06:44.230 --> 00:06:48.370
So it's kind of like we have in the twin quarter with a linear output activation

103
00:06:48.371 --> 00:06:52.100
function.
What changes though is that,
uh,

104
00:06:52.120 --> 00:06:56.470
the way we are going to obtain the latent representation for an input,

105
00:06:56.530 --> 00:07:01.180
it's gotta be much more complicated,
uh,
for,
uh,
as part squirting model.

106
00:07:01.240 --> 00:07:05.540
So in no color,
it was just a linear transformation followed by nonlinearity.

107
00:07:06.090 --> 00:07:08.510
Uh,
but in spar schooling,
uh,

108
00:07:08.560 --> 00:07:13.560
the age for which will be measuring and counting this loss is going to be the

109
00:07:14.260 --> 00:07:18.160
age that minimizers does some of the loss and the sparsity term.

110
00:07:18.700 --> 00:07:23.020
So another way of seeing it is that a d h term here,

111
00:07:23.470 --> 00:07:27.910
uh,
which depends on XD is the argument of the,

112
00:07:27.911 --> 00:07:30.670
some of the reconstruction term and the sparsity term.

113
00:07:31.210 --> 00:07:33.940
So now the encoding function,

114
00:07:33.970 --> 00:07:37.350
given some input is no longer the inner activation followed by knowing the

115
00:07:37.351 --> 00:07:42.340
narrative,
it's the result of the more complicated optimization problem.

116
00:07:46.310 --> 00:07:50.160
All right,
so,
um,
let's talk about the dictionary.

117
00:07:50.161 --> 00:07:54.120
Why do we call this a dictionary and what does it represent?
Um,

118
00:07:54.121 --> 00:07:58.620
so we said that the x had the reconstruction is just my,

119
00:07:58.680 --> 00:08:03.440
a latent representation extracted from D,
which is the result of optimizing,

120
00:08:03.460 --> 00:08:06.240
finding the best code in terms of reconstruction and sparsity.

121
00:08:06.780 --> 00:08:11.710
And then multiplying that by d.
That's my reconstruction now because,
uh,

122
00:08:11.790 --> 00:08:16.170
this is spars.
Uh,
it means that many of its elements are going to be zero.

123
00:08:16.860 --> 00:08:17.850
And,
uh,

124
00:08:17.880 --> 00:08:22.880
so it means that we can write it this a term here as the sum over all k four,

125
00:08:24.301 --> 00:08:27.660
which the element in the late and representation,

126
00:08:27.840 --> 00:08:31.710
the Keith Element is not zero of the column.

127
00:08:31.890 --> 00:08:36.890
The key if column of the Dictionary Matrix Times the value of decay if elements

128
00:08:38.071 --> 00:08:41.640
in the late and representation.
So,
uh,

129
00:08:41.730 --> 00:08:46.530
if we think of this as some sort of texts,
then this is saying that,

130
00:08:46.630 --> 00:08:47.010
uh,
were,

131
00:08:47.010 --> 00:08:51.690
are decomposing our texts into a sum of a bunch of words,

132
00:08:51.691 --> 00:08:55.020
which would be the column of d.
And,
uh,

133
00:08:55.110 --> 00:08:59.960
the sort of quote unquote frequency of that word is going to be the,
uh,

134
00:09:00.270 --> 00:09:04.820
value of,
uh,
the latent representation and so forth.

135
00:09:04.821 --> 00:09:08.880
Those are zero.
It's kind of like saying this document doesn't contain that word.

136
00:09:09.560 --> 00:09:12.480
Uh,
instead of calling the dictionary elements words,

137
00:09:12.481 --> 00:09:15.430
we often call them atoms instead.
But,
uh,

138
00:09:15.530 --> 00:09:17.550
you can see that dictionary is essentially,

139
00:09:17.730 --> 00:09:22.350
it's kind of like a language with which you're going to try to describe your

140
00:09:22.770 --> 00:09:27.330
inputs that your observations and you're going to assume where,

141
00:09:27.331 --> 00:09:31.980
assuming that each observation is a composition of a fairly limited number of

142
00:09:31.981 --> 00:09:35.640
these elements in my language,
in the dictionary.
So for instance,

143
00:09:35.790 --> 00:09:40.410
if we run the spar scoring model of uh,
images of characters,

144
00:09:41.040 --> 00:09:43.830
well one thing we could get a,
so by the way,

145
00:09:43.831 --> 00:09:47.950
this is another Australian from a paper by Mark Rela was runs Thatoh,
um,

146
00:09:48.660 --> 00:09:51.150
and colleagues.
So if we had this image,

147
00:09:51.300 --> 00:09:55.560
then one thing we could obtain is that this image can be decomposed as the,

148
00:09:55.860 --> 00:09:59.220
uh,
as a linear combination of the different pen strokes.

149
00:09:59.221 --> 00:10:03.930
We have a pen stroke here,
one like this one,
a little bit similar,

150
00:10:04.000 --> 00:10:05.970
another one here like that.

151
00:10:06.690 --> 00:10:11.100
Then more like the tail of the seven.
Another one like this one,

152
00:10:11.280 --> 00:10:12.860
maybe like that.
And uh,

153
00:10:12.861 --> 00:10:17.640
so we see that we've decomposed this fairly complicated image in terms of pixels

154
00:10:18.090 --> 00:10:22.440
into representation,
which just identifies which pen strokes are present,

155
00:10:22.800 --> 00:10:26.160
uh,
into,
in this image of a character.

156
00:10:26.970 --> 00:10:31.140
And a,
and so if we think that our complex observations,

157
00:10:31.141 --> 00:10:34.920
which might be very high dimensional,
can all be decomposed into uh,
uh,

158
00:10:35.070 --> 00:10:36.000
sort of language.

159
00:10:36.001 --> 00:10:40.050
So a sparse representation where everything is just a linear combination of a

160
00:10:40.051 --> 00:10:44.370
few vectors and as far as coding is going to be a good option for representing

161
00:10:44.371 --> 00:10:48.480
that type of data.
And then now we can use the h vector,

162
00:10:48.690 --> 00:10:53.580
which is mostly going to contain zero except for the non zero elements,

163
00:10:53.770 --> 00:10:57.210
uh,
as a representation say for performing classification.

164
00:10:58.850 --> 00:11:01.620
Um,
so in certain application,
uh,

165
00:11:01.680 --> 00:11:06.000
will actually know what so good a dictionary to use.

166
00:11:06.510 --> 00:11:10.800
Uh,
but in general we will actually want to adapt our dictionary or learn it from

167
00:11:10.801 --> 00:11:13.650
data,
learn a good dictionary for describing our data.

168
00:11:13.920 --> 00:11:18.630
And so that's what we'll see in the coming videos.
And,

169
00:11:19.050 --> 00:11:21.840
uh,
just to be explicit.
So now these little image,

170
00:11:21.900 --> 00:11:26.280
they would correspond to the images we have here of each character.

171
00:11:26.870 --> 00:11:31.110
They would correspond to the columns of d and then the values here,

172
00:11:31.290 --> 00:11:35.760
they correspond to the Leighton representation values.
Now,

173
00:11:35.780 --> 00:11:38.650
this example,
they turn out to be mostly one,
but they can,

174
00:11:38.700 --> 00:11:42.530
we'll generally value a bit,
a very big,
uh,
between,
uh,
uh,

175
00:11:42.540 --> 00:11:45.210
they can be either positive or negative.
In this example,

176
00:11:45.211 --> 00:11:48.960
it happens to be positive,
but it can be anything between Madison 20%.
Venti.

177
00:11:49.290 --> 00:11:53.490
It's just that many of them because of sparsity term,
are going to be zero.

178
00:11:54.160 --> 00:11:59.100
All right?
So that's just,
uh,
uh,
intuitive and,
and,
uh,

179
00:11:59.160 --> 00:12:03.570
well formal and then a intuitive,
uh,
description of sparse code in model.

