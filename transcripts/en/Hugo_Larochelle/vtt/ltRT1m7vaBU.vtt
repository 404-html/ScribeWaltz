WEBVTT

1
00:00:00.780 --> 00:00:00.950
Okay.

2
00:00:00.950 --> 00:00:04.580
<v 1>In this video we'll mention an alternative to maximum likelihood training of</v>

3
00:00:04.581 --> 00:00:07.910
conditional random fields,
Nolan as a sort of likelihood.

4
00:00:10.280 --> 00:00:15.110
So we've seen in the previous video that,
uh,
we could get an expression for,
uh,

5
00:00:15.230 --> 00:00:19.640
the,
uh,
negative log likelihood gradients,
uh,

6
00:00:19.700 --> 00:00:24.290
with respect to the parameters,
which allows us to,
uh,
then,
you know,
get a way,

7
00:00:24.500 --> 00:00:28.790
an expression to compute for performing gradient descent and perform minimizing

8
00:00:28.820 --> 00:00:31.910
negative log likelihood or perform maximum likelihood training of our

9
00:00:31.911 --> 00:00:36.680
conditional random fields.
However,
it did involve an expectation over,

10
00:00:36.710 --> 00:00:41.360
uh,
the,
uh,
label,
uh,
or the target vector y.

11
00:00:42.080 --> 00:00:44.900
And while it might only involve a few variables,

12
00:00:44.920 --> 00:00:48.650
it's still requires performing an inference of what the marginal distribution

13
00:00:48.651 --> 00:00:52.310
over the subset of labels that are involved in the gradient,

14
00:00:52.730 --> 00:00:56.270
uh,
might be.
So we have to perform that in France.

15
00:00:56.480 --> 00:01:00.680
And unless we have a,
uh,
uh,
uh,
graph,

16
00:01:00.740 --> 00:01:05.420
which is a linear chain or a tree,
uh,
this,
uh,

17
00:01:05.450 --> 00:01:07.960
this approximate inference,
uh,

18
00:01:07.961 --> 00:01:12.920
so a estimating this marginal conditional distribution,
uh,

19
00:01:13.040 --> 00:01:16.580
it's just an approximation and it might actually be very bad in practice.

20
00:01:16.760 --> 00:01:20.480
So some people have explored in instead,
uh,
an alternative,

21
00:01:20.540 --> 00:01:24.110
which is to just do without maximum likelihood training and just changed the

22
00:01:24.111 --> 00:01:27.860
last function to be used for training the conditional Mendham field.

23
00:01:29.890 --> 00:01:34.050
So I just want to mention one such approach,
which is known as pseudo likelihood.

24
00:01:34.890 --> 00:01:39.890
So the idea is that instead of trying to maximize the probability of the whole

25
00:01:40.291 --> 00:01:43.170
wide vector,
given the input,
uh,

26
00:01:43.171 --> 00:01:48.171
we might want to maximize instead each of the conditional of each why k given

27
00:01:50.850 --> 00:01:51.750
everything else,

28
00:01:51.751 --> 00:01:56.751
so all otherwise and the input so specifically will minimize the negative,

29
00:01:58.021 --> 00:02:03.021
some of the log of the probability of the true label at position k given all

30
00:02:05.071 --> 00:02:07.800
other labels at position one up two k minus one,

31
00:02:07.860 --> 00:02:10.410
then k plus one up to capitol came.

32
00:02:11.760 --> 00:02:16.760
So you can think of it as trying to predict a intern each y k not just from

33
00:02:17.220 --> 00:02:21.630
expert,
from all other elements in the vector y.
Uh,

34
00:02:21.631 --> 00:02:26.450
what's nice is that,
uh,
for this expression,
we can usually tractably,
uh,

35
00:02:26.550 --> 00:02:31.380
compute the exact gradience for optimizing that objective.
Uh,
so,

36
00:02:31.381 --> 00:02:35.220
and that's because each of these conditionals essentially just require that we

37
00:02:35.221 --> 00:02:39.820
normalize over all values of y,
k and a usually white.

38
00:02:39.830 --> 00:02:42.600
You will take a fairly small number of values.

39
00:02:42.690 --> 00:02:45.200
So it's going to be similar to performing irregular,

40
00:02:45.270 --> 00:02:48.180
soft Max computation and the regular neural network.

41
00:02:48.181 --> 00:02:52.440
Though now this probability will be influenced not just by the input but also by

42
00:02:52.441 --> 00:02:54.300
other label values.
Uh,

43
00:02:54.301 --> 00:02:59.290
and the other label values are actually going to be the true label values,
uh,

44
00:02:59.320 --> 00:03:04.090
in the target vector.
And,
uh,

45
00:03:04.150 --> 00:03:07.860
and the other thing that's nice is that each of these conditionals for,
uh,

46
00:03:08.020 --> 00:03:12.340
the conditional random fields is typically only really going to depend on a few

47
00:03:12.341 --> 00:03:14.260
of the other target variables.

48
00:03:14.530 --> 00:03:18.090
And that's because of the local mark of property of a conditional random fields.

49
00:03:18.091 --> 00:03:22.750
If we look at its markup network,
then we know that,
uh,
the,
uh,

50
00:03:22.780 --> 00:03:23.410
probably have,

51
00:03:23.410 --> 00:03:28.000
why can't given everything else reduces to the property of white gay given only

52
00:03:28.001 --> 00:03:28.691
its neighbors.

53
00:03:28.691 --> 00:03:32.860
So it's actually going to be a much simpler expression than just this depending

54
00:03:32.861 --> 00:03:37.600
on the graph structure on the mark of network.
Um,
so,

55
00:03:37.660 --> 00:03:40.660
uh,
that being said,
we still to make prediction,

56
00:03:40.661 --> 00:03:44.680
we would still normally a need to compute the marginal distribution,

57
00:03:44.710 --> 00:03:48.490
which might still be intractable.
So it makes training more tractable.

58
00:03:48.520 --> 00:03:50.710
But at test time we make a prediction.

59
00:03:50.770 --> 00:03:53.410
If you want to use prediction based on the marginals,

60
00:03:53.440 --> 00:03:55.660
we still have to do some approximate inference.

61
00:03:55.900 --> 00:03:59.200
So that's doesn't entirely solve our problem.

62
00:04:00.190 --> 00:04:03.940
So all of this that the reason we have to do this and that this isn't a problem

63
00:04:03.941 --> 00:04:07.450
here is that here we are conditioning on the true labels.

64
00:04:07.480 --> 00:04:11.260
So we don't have to infer what the other labels might be.

65
00:04:11.261 --> 00:04:14.440
We actually know them at training time.
So we exploiting that our training time.

66
00:04:14.441 --> 00:04:15.550
But at test time we can't,

67
00:04:16.240 --> 00:04:20.410
and I should say that in practice we often find that pseudo likelihood works

68
00:04:20.411 --> 00:04:20.921
less well.

69
00:04:20.921 --> 00:04:25.921
So it gives a models are not as good as models that are trained by maximum

70
00:04:25.961 --> 00:04:29.650
likelihood.
And there's some theory for,
uh,

71
00:04:29.680 --> 00:04:32.590
backing up why we should observe this.

72
00:04:32.800 --> 00:04:37.150
So I would not necessarily a suggest someone uses,
so the likelihood,

73
00:04:37.330 --> 00:04:40.690
except perhaps on a very,
very complicated model.
Uh,

74
00:04:40.750 --> 00:04:44.770
but it's good to know that there are alternatives to conditional random fields

75
00:04:44.920 --> 00:04:48.730
than,
uh,
training them by maximum likelihood,
uh,
and,
uh,

76
00:04:48.910 --> 00:04:53.890
started alternatives will be sort of nicer in terms of the computations that are

77
00:04:53.891 --> 00:04:55.480
required for,
for training them.

78
00:04:55.870 --> 00:05:00.010
And a training conditional random fields in general is a very active field of

79
00:05:00.011 --> 00:05:04.900
research.
Uh,
but with all we've seen in the Er,
this veto in the previous ones,

80
00:05:05.080 --> 00:05:09.160
that should give you a,
uh,
a headstart for looking at that literature more.

