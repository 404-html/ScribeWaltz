WEBVTT

1
00:00:00.620 --> 00:00:00.830
Yeah.

2
00:00:00.830 --> 00:00:05.610
<v 1>And this video will introduce a very useful tool known as d look like you who</v>

3
00:00:05.611 --> 00:00:06.740
had variational bound.

4
00:00:07.970 --> 00:00:08.803
<v 0>Yeah.</v>

5
00:00:09.380 --> 00:00:13.120
<v 1>So in the previous video we talked that,
uh,
we introduced the uh,</v>

6
00:00:13.280 --> 00:00:18.280
model deep belief network and then I talked about the fact that this idea of

7
00:00:18.381 --> 00:00:21.860
retraining neural networks by stacking rbms,
uh,

8
00:00:21.861 --> 00:00:23.930
actually came from deep belief networks.

9
00:00:24.230 --> 00:00:28.910
And then I tried to give an idea for how this was useful in the context of a

10
00:00:28.911 --> 00:00:31.090
deep belief network where,
uh,

11
00:00:31.250 --> 00:00:36.230
we would try to initialize a three hidden layer deep belief networks by first

12
00:00:36.231 --> 00:00:41.231
training and RBM and taking its weights first hidden layer weights to initialize

13
00:00:42.241 --> 00:00:43.550
the sigmoid belief network,

14
00:00:43.551 --> 00:00:48.551
part of a two hidden layer DBN at which case we would keep those fix to

15
00:00:48.720 --> 00:00:53.720
pretrained the second a hidden layer RBM weights in the two hidden layer,

16
00:00:53.981 --> 00:00:56.780
the BN.
And then after doing that for awhile,

17
00:00:56.870 --> 00:01:01.210
we would use the two first hidden layer weights initialize the sigma belief

18
00:01:01.220 --> 00:01:01.641
network,

19
00:01:01.641 --> 00:01:06.641
part of the three hidden layer DBN and then we would be left with the procedure

20
00:01:07.281 --> 00:01:11.720
of training the top RBM weights of the three hidden there.
DBN.

21
00:01:12.770 --> 00:01:17.270
And so to actually describe exactly how this procedure will,
uh,

22
00:01:17.390 --> 00:01:18.580
come about.
And,
and,

23
00:01:18.581 --> 00:01:22.820
and the actual details of the algorithm will need a tool known as the log

24
00:01:22.821 --> 00:01:25.070
likelihood variational,
uh,
bound.

25
00:01:25.550 --> 00:01:28.310
And so in this video will introduce what this is exactly

26
00:01:31.670 --> 00:01:34.790
to introduce the a variational bound.
Uh,

27
00:01:34.800 --> 00:01:39.800
we will need the concavity explored the concavity property of the logarithm

28
00:01:41.521 --> 00:01:45.630
function.
So,
uh,
the logarithm function is a concave function,

29
00:01:45.631 --> 00:01:46.620
which looks like this.

30
00:01:47.340 --> 00:01:50.820
And a concave function has the following property.

31
00:01:51.210 --> 00:01:53.310
So if you take a,

32
00:01:53.311 --> 00:01:56.220
so let's just think about the log as are concave function.

33
00:01:56.300 --> 00:02:01.050
If you take the log of a weighted average of some numbers,
Ai,

34
00:02:01.230 --> 00:02:06.230
where the weights of each AI is going to be this Amigo I here and a Omega I is

35
00:02:09.240 --> 00:02:12.260
that it can be any,
uh,
wait,
as long as it's a,

36
00:02:12.310 --> 00:02:17.130
they're all greater than equal than zero.
And uh,
they sent two.
One.
Okay.
So this,

37
00:02:17.131 --> 00:02:20.780
so that this is a weighted average of each of these numbers.
Ais,

38
00:02:21.960 --> 00:02:22.590
well,

39
00:02:22.590 --> 00:02:26.520
the log of this weighted average for a concave function is always going to be

40
00:02:26.580 --> 00:02:27.780
greater or equal.

41
00:02:27.930 --> 00:02:32.930
Then the same way that average but of the logarithm of the numbers Ai.

42
00:02:34.200 --> 00:02:37.410
So,
uh,
visually if we had two numbers,

43
00:02:37.470 --> 00:02:39.810
a one and a two a,

44
00:02:39.830 --> 00:02:44.830
then what this would correspond to is that the weighted average between the log

45
00:02:45.271 --> 00:02:48.680
of a one and the log of a two,

46
00:02:49.260 --> 00:02:52.510
which corresponds to this line here.
So this line is,
uh,

47
00:02:52.530 --> 00:02:55.410
the way that average between log of a one and luck of a two.

48
00:02:55.411 --> 00:02:58.320
So luck of a one is here,
not going to two is here.

49
00:02:58.710 --> 00:03:01.870
And so any way that average would fall between the straight line between these

50
00:03:01.871 --> 00:03:03.520
two points.

51
00:03:04.180 --> 00:03:09.180
So that line is always going to be below the logarithm of that function between

52
00:03:09.640 --> 00:03:12.190
a one and a two.
So the Logarithm,

53
00:03:12.550 --> 00:03:17.550
in other words of any weighted average between a one and a two using the same

54
00:03:17.620 --> 00:03:21.250
weights.
Okay.
So that this is visually,

55
00:03:21.251 --> 00:03:24.660
we see that for a concave function would always get this,
we get this,
this,

56
00:03:24.770 --> 00:03:27.280
that this line would be below the actual function.

57
00:03:28.090 --> 00:03:32.380
And so that's what this property refers to.
But for any number of points,

58
00:03:32.381 --> 00:03:37.330
not just,
uh,
not necessarily just to,
but any weighted combination,
uh,

59
00:03:37.390 --> 00:03:41.170
between,
uh,
any number of weeks.
Sorry,
Alpha for numbers.

60
00:03:41.950 --> 00:03:45.810
So that's an important property that will leverage to get our variational
bounds.

61
00:03:48.240 --> 00:03:49.073
<v 0>Yeah.</v>

62
00:03:49.300 --> 00:03:49.870
<v 1>Okay.</v>

63
00:03:49.870 --> 00:03:54.870
So the idea of a variational bound of the log likelihood for some model,

64
00:03:55.580 --> 00:03:55.911
uh,

65
00:03:55.911 --> 00:04:00.911
is actually very useful for a lot of graphical models with a latent variables.

66
00:04:01.630 --> 00:04:06.630
So imagine we have any model p of x n h one A.

67
00:04:06.941 --> 00:04:11.650
I could have written just h,
uh,
to be general,
but we'll use it,
uh,

68
00:04:11.790 --> 00:04:14.080
in the previous,
uh,
in the next slides,

69
00:04:14.440 --> 00:04:18.370
in the next video for this specific case of the first thing than they are in
DBN.

70
00:04:18.640 --> 00:04:23.410
But any model that defines the distribution of an x using some latent variable,

71
00:04:23.470 --> 00:04:28.470
each one is going to have the following version of the idea of the various,

72
00:04:28.901 --> 00:04:32.590
you know,
bound is to take the log likelihood,
which might be too complicated,

73
00:04:32.680 --> 00:04:34.030
too complicated to compute.

74
00:04:34.480 --> 00:04:38.490
So log of Q of x would requires summing over each one.
And,
uh,

75
00:04:38.500 --> 00:04:41.230
there are some models.
If there are,
uh,
uh,

76
00:04:41.231 --> 00:04:45.290
that might be too complicated in the interactions between x and h one or the

77
00:04:45.291 --> 00:04:50.200
number of values of each one to allow for performing that some,
uh,
exactly.

78
00:04:50.800 --> 00:04:51.611
So instead we'll,

79
00:04:51.611 --> 00:04:56.611
we'll derive is an expression where we've added these new parameters,

80
00:04:57.700 --> 00:05:01.810
this sort of separate a model if you want to think about it this way,
uh,

81
00:05:01.840 --> 00:05:06.820
which we call queue of h one given x u of h one given x is sort of a separate

82
00:05:06.821 --> 00:05:09.340
model.
It's a,
and you can think of it,

83
00:05:09.341 --> 00:05:13.630
that's an approximation of what would be the true posterior under our model p of

84
00:05:13.631 --> 00:05:14.650
h one given x.

85
00:05:15.100 --> 00:05:19.150
So cube h one given x can be different from p u of h one given that's it's a

86
00:05:19.151 --> 00:05:21.520
separate,
it has its separate set of parameters.

87
00:05:22.720 --> 00:05:26.500
And by introducing these extra variables,

88
00:05:26.590 --> 00:05:31.590
we'll actually be able to formulate a lower bound on the log likelihood.

89
00:05:32.470 --> 00:05:36.940
And this lower bound we have sort of properties that are interesting for a,

90
00:05:36.941 --> 00:05:40.900
if we wanted to actually optimize this look like we could then optimize instead

91
00:05:40.901 --> 00:05:44.140
the lower bound.
And by pushing the lower bound up,

92
00:05:44.260 --> 00:05:47.730
we can hope that we also pushing the actual lug likelihood up.

93
00:05:47.950 --> 00:05:52.330
So that's the idea that the main usage for the Farish northbound that we often

94
00:05:52.331 --> 00:05:53.380
seen the literature.

95
00:05:54.550 --> 00:05:55.100
<v 0>Yeah.</v>

96
00:05:55.100 --> 00:05:58.460
<v 1>Okay.
So how do we actually get this,
uh,
lower bound?</v>

97
00:05:59.200 --> 00:06:03.890
So we have that log of p of x is equal to this expression here.
So let's,

98
00:06:03.891 --> 00:06:07.760
uh,
go into a two steps.
So first,

99
00:06:07.850 --> 00:06:09.830
ignore this term here in this term here.

100
00:06:10.400 --> 00:06:14.240
So what this sustain saying is that the log of p of x is the log of the sum over

101
00:06:14.241 --> 00:06:17.600
h one of p of x and h one.
Okay.

102
00:06:17.601 --> 00:06:21.050
So where's marginalizing h one?
So that gives us p of x.
So that's equal.

103
00:06:22.310 --> 00:06:27.000
And now what we're doing is that we're just introducing this virginal.

104
00:06:27.350 --> 00:06:32.350
A model will perceive your queue of h one given x by multiplying and dividing by

105
00:06:33.171 --> 00:06:35.300
it.
Okay.
So these would normally cancel.

106
00:06:35.450 --> 00:06:38.240
So that's why we haven't changed anything.
So that's why we have the equality.

107
00:06:39.450 --> 00:06:39.880
<v 0>Yeah.</v>

108
00:06:39.880 --> 00:06:44.880
<v 1>And now we are already ready to use the property of a concavity concavity of the</v>

109
00:06:45.730 --> 00:06:49.750
log function.
So by using those,

110
00:06:49.751 --> 00:06:53.560
considering those as our weights and those here,

111
00:06:53.561 --> 00:06:58.561
the ratio p of x and h one divided by Q of h one given x as our numbers a I then

112
00:07:02.621 --> 00:07:07.621
we can exploit the concavity of the lug by saying that this is great or an

113
00:07:08.021 --> 00:07:10.270
equal,
then the weighted average.

114
00:07:10.660 --> 00:07:15.660
So we're summing over all values of h one weighted by Q of h one given x of

115
00:07:15.701 --> 00:07:17.110
dialogue of this number here.

116
00:07:17.111 --> 00:07:22.111
So the racial p of x and h one divided by Q of h one given x.

117
00:07:22.900 --> 00:07:27.880
Okay.
So this here we can think of it as a huge way that some over an exponential

118
00:07:27.881 --> 00:07:31.540
number of values of age.
Uh,
if we think of an Rbm,
for instance,

119
00:07:31.541 --> 00:07:33.870
where these are binary in the take,
uh,

120
00:07:33.960 --> 00:07:38.390
so globally to take two to the number of hidden units values.
Um,

121
00:07:38.590 --> 00:07:42.850
and so now this a way that some weekend,
uh,
uh,

122
00:07:42.940 --> 00:07:46.720
obtain it slower bound by doing the a way that some,

123
00:07:46.750 --> 00:07:51.750
instead of the log of the ratio here and not exploiting the fact that the log of

124
00:07:51.791 --> 00:07:55.840
a ratio is the,
uh,
differences of the logs,

125
00:07:56.680 --> 00:07:58.450
then we get that,
uh,

126
00:07:58.451 --> 00:08:02.620
this whole expression here as just,
uh,

127
00:08:03.370 --> 00:08:07.300
there's some over each one of Qa,
h one given x lug of the numerator here,

128
00:08:07.301 --> 00:08:12.301
p of x and h one minus the sum over each one of Q h one given x times the log of

129
00:08:14.111 --> 00:08:18.640
Qh one given X.
Okay.
So we just,
uh,

130
00:08:18.850 --> 00:08:23.850
almost straight forwardly exploited the concavity of the log function and that

131
00:08:23.861 --> 00:08:28.861
gave us our lower bound on the log probability under a model p of x,

132
00:08:29.710 --> 00:08:34.710
which is a lower bounded by this expression here for any choice of what Qa h one

133
00:08:35.441 --> 00:08:39.730
given x is.
And that's because for any way to the average,
this is true.

134
00:08:39.731 --> 00:08:43.120
So these are the weights of our average or for any chores of our posterior

135
00:08:43.121 --> 00:08:46.360
approximate posterior Qa,
Chuang given x.
We have this,
uh,

136
00:08:46.361 --> 00:08:50.050
that this is a greater or equal than this whole expression here.

137
00:08:52.550 --> 00:08:53.310
<v 0>Okay,</v>

138
00:08:53.310 --> 00:08:58.080
<v 1>so this is called the variational or variational lower bound.
Uh,
so I just,</v>

139
00:08:58.190 --> 00:09:02.220
uh,
remove that.
What was in between this and this whole expression here.

140
00:09:03.540 --> 00:09:05.490
One property of this,
uh,

141
00:09:05.580 --> 00:09:10.580
various northbound is that if our separate pacira Qa h one given x is actually

142
00:09:12.481 --> 00:09:16.770
exactly the same as the true conditional in our model,
you have h one given x,

143
00:09:16.980 --> 00:09:20.370
then we actually have an equality.
So this would be equal.

144
00:09:21.060 --> 00:09:24.720
So let's do a very quick and dirty demonstration of this.
Um,

145
00:09:24.990 --> 00:09:29.370
so log of p of x and h one,
we can write it as the,

146
00:09:29.750 --> 00:09:30.110
uh,

147
00:09:30.110 --> 00:09:35.110
lug of a p of h one given x plus log of p of x.

148
00:09:40.800 --> 00:09:45.800
So that's because p of x one and an h one is p of h one times p of h one given x

149
00:09:46.411 --> 00:09:49.080
times p of x.
Then take the log of that.

150
00:09:49.170 --> 00:09:52.960
That's just going to be the sum of the lugs.
Now,
uh,

151
00:09:52.980 --> 00:09:56.340
we are considering the case where Q is exactly the same sp.

152
00:09:56.341 --> 00:09:57.570
So we'll just change the

153
00:09:58.540 --> 00:09:59.590
<v 2>choose two PS.</v>

154
00:10:01.730 --> 00:10:06.380
<v 1>And now we notice that we have uh,
the sum over,</v>

155
00:10:06.530 --> 00:10:10.640
uh,
each one of Poah,
one given x times the log of p of h one given x.

156
00:10:10.910 --> 00:10:13.400
And then we have minus the,
exactly the same thing here.

157
00:10:13.790 --> 00:10:18.790
So this term would disappear with this one would be canceled out by this one.

158
00:10:21.770 --> 00:10:25.840
And now we just have some of her,
uh,
h one of Poah,

159
00:10:25.850 --> 00:10:29.960
one given x times the log of p of x.
Well,

160
00:10:29.961 --> 00:10:32.360
the log of p of x does not depend on each one,

161
00:10:32.361 --> 00:10:37.361
so we could actually put it right in front of the sum and then we'd some over

162
00:10:37.491 --> 00:10:40.070
just p of h one given x,
which would be one.

163
00:10:40.071 --> 00:10:44.870
So we would be left with just log of p of x,
which is exactly this year.

164
00:10:45.350 --> 00:10:46.910
Okay.
That's a very,

165
00:10:47.240 --> 00:10:52.160
we can dirty demonstration that for Qa Schwann given x,
uh,

166
00:10:52.190 --> 00:10:54.500
equal to p of y.
Given x,
we actually have inequality.

167
00:10:56.210 --> 00:11:01.010
And just generally the more Q of h one given x is going to be different from the

168
00:11:01.011 --> 00:11:04.100
trooper steer,
then the less tight this bound will be.

169
00:11:04.101 --> 00:11:07.850
The bigger the difference between this and that whole expression is going to be.

170
00:11:08.240 --> 00:11:12.950
And in fact,
the difference between the left and right a part of this,
uh,

171
00:11:12.980 --> 00:11:15.950
bound here is going to be the,
uh,

172
00:11:16.070 --> 00:11:20.510
something known as the KL divergence between Q of h one and p of h one,

173
00:11:20.720 --> 00:11:25.190
which is this expression here.
So in general,
the Kale divergence is a,

174
00:11:25.191 --> 00:11:29.810
you can think of it as a kind of difference,
uh,
distance between,
um,
uh,

175
00:11:29.840 --> 00:11:32.870
distributions.
It's not a distance because it's not symmetric.

176
00:11:32.900 --> 00:11:37.370
If we changed the order of these two arguments,
we get different numbers.
Uh,

177
00:11:37.460 --> 00:11:41.390
we can get different numbers.
Uh,
but we see that point since it's zero.

178
00:11:41.450 --> 00:11:45.950
If Q is equal to p.
So,
um,
in this expression,

179
00:11:46.010 --> 00:11:51.010
if we have p here and p here than this cancels out with that.

180
00:11:51.561 --> 00:11:55.960
So we get log of one like of one is zero.
So all the terms are going to be Zeros.

181
00:11:56.490 --> 00:11:57.323
Okay.

182
00:11:58.630 --> 00:12:03.090
So what's interesting with the various nor bound is that it's a suggests and a

183
00:12:03.100 --> 00:12:07.900
procedure for optimizing the log probability of our training data.

184
00:12:08.320 --> 00:12:11.710
Uh,
even if you have a model where we can compute that term exactly.

185
00:12:12.280 --> 00:12:16.030
And the approach we'll take is to try to instead work with the lower bound and

186
00:12:16.031 --> 00:12:17.440
tried to push up the lower bound.

187
00:12:18.160 --> 00:12:23.140
And the way that this is usually done is that we would first,

188
00:12:23.590 --> 00:12:25.120
um,
uh,

189
00:12:25.121 --> 00:12:29.830
at the first step we would try to find our approximation Q of h one that is as

190
00:12:29.831 --> 00:12:32.170
close as possible to the trooper steer.

191
00:12:32.200 --> 00:12:36.550
So we would optimize the Kale divergence between two NP for some family of

192
00:12:36.551 --> 00:12:41.110
models for our posterior RQ.
And then once that's done,

193
00:12:41.350 --> 00:12:46.000
then we will keep Q fixed and we'll actually optimize with respect to just the

194
00:12:46.001 --> 00:12:50.620
log of p of x and h one.
And if we make good choices for what this,

195
00:12:50.710 --> 00:12:54.670
uh,
approximate posterior here is,
then,
uh,

196
00:12:54.700 --> 00:12:59.260
it turns out that for a certain cases and certain models,
uh,

197
00:12:59.261 --> 00:13:00.760
we actually,
uh,

198
00:13:00.790 --> 00:13:04.300
can much more easily optimize this expression here.

199
00:13:04.690 --> 00:13:05.920
So then we alternate,

200
00:13:05.950 --> 00:13:09.280
the algorithm would be alternate between updating Q and updating,

201
00:13:09.730 --> 00:13:13.420
updating cue of dating Pete.
And you do this until convergence.

202
00:13:13.780 --> 00:13:16.600
And so by doing this,
so we,
when I say updating,

203
00:13:16.601 --> 00:13:19.420
we say maximize this expression with respect to Q,

204
00:13:19.421 --> 00:13:24.040
and then maximize this expression with respect to p and then alternate,

205
00:13:24.330 --> 00:13:26.350
uh,
until convergence.
So by doing this,

206
00:13:26.351 --> 00:13:30.550
we push up the lower bounds or where ideally a,
you know,

207
00:13:30.640 --> 00:13:35.080
we can hope that we actually pushing this and also the Emr with them is based on

208
00:13:35.081 --> 00:13:39.910
this idea.
But where at the east step,
uh,
it corresponds to finding Q,

209
00:13:39.911 --> 00:13:44.770
which is exactly equal to p if you're familiar with the stuff here.
So that's,

210
00:13:44.771 --> 00:13:47.200
uh,
that's why I'm talking about it here.
It's a very,

211
00:13:47.201 --> 00:13:52.120
very useful tool in general for training,
uh,
probabilistic graphical models.

212
00:13:52.330 --> 00:13:54.640
And we'll also use it for,
uh,

213
00:13:54.740 --> 00:13:59.740
in the next video for deriving the greedy layer wise training procedure for a

214
00:13:59.860 --> 00:14:00.760
deep belief network.

