WEBVTT

1
00:00:00.170 --> 00:00:00.590
Yeah.

2
00:00:00.590 --> 00:00:04.880
<v 1>And this video will derive the gradients of the loss function with respect to</v>

3
00:00:04.881 --> 00:00:07.700
the,
uh,
different hidden layers in the neural network.

4
00:00:09.780 --> 00:00:14.780
So now we are again pushing forward with getting an expression for the private

5
00:00:16.011 --> 00:00:20.280
or gradients,
uh,
so that we can put those in into,
uh,

6
00:00:20.400 --> 00:00:23.310
the description of this took gastic great in the center with them,

7
00:00:23.311 --> 00:00:24.780
applied for neural networks.

8
00:00:25.970 --> 00:00:26.803
<v 0>Okay.</v>

9
00:00:27.250 --> 00:00:30.030
<v 1>All right.
So,
uh,
we looked at the gradients,
which is sector,</v>

10
00:00:30.040 --> 00:00:34.480
the outputs of the neural network.

11
00:00:34.510 --> 00:00:39.370
And then we looked at the pre activation and a,
at the output layer.

12
00:00:39.730 --> 00:00:44.710
And so we could continue like this,
deriving it for any other,
a hint in there.

13
00:00:44.711 --> 00:00:48.550
But if were to do this every time by hand,
like we've done so far,

14
00:00:48.730 --> 00:00:53.350
this might get a little bit complicated.
So can we sort of take a step back and,

15
00:00:53.380 --> 00:00:58.240
uh,
try to see how,
instead of looking at individual neurons and then computing,

16
00:00:58.241 --> 00:01:03.220
what's the,
uh,
what's the actual,
um,
what's the actual,

17
00:01:03.300 --> 00:01:07.660
a great intro,
partial derivative,
and then putting those back into,
uh,
gradients.

18
00:01:07.960 --> 00:01:12.960
Can we get a more general formulation for the other near a hidden layers?

19
00:01:13.690 --> 00:01:16.000
Uh,
the other layers in the neural network.

20
00:01:18.750 --> 00:01:23.750
So what we can do is actually use the chain rule to get a more genero

21
00:01:25.171 --> 00:01:28.650
formulation for the greatest respect to any hidden layer.

22
00:01:29.040 --> 00:01:32.950
And then from that we'll be able to derive a procedure for computing the

23
00:01:32.951 --> 00:01:35.100
gradients for all,
uh,
hidden layers.

24
00:01:35.760 --> 00:01:39.510
So we know that from the chain rule that if we have a function p of a,

25
00:01:39.511 --> 00:01:43.950
which depends on a,
and if dysfunction is such that he can,
uh,

26
00:01:43.980 --> 00:01:48.510
we written as a function of intermediate results,
which we'll call queue.

27
00:01:48.511 --> 00:01:49.344
I have a,

28
00:01:49.860 --> 00:01:54.580
and so where I indexes the intermediate results then from the chain rule,

29
00:01:54.581 --> 00:01:59.581
we know that the partial derivative of Pov with respect to a is just the sum

30
00:02:00.031 --> 00:02:04.080
over all partial results of the,
uh,

31
00:02:04.110 --> 00:02:09.110
partial derivative of Pov with respect to the IMF parcel results times deep

32
00:02:09.221 --> 00:02:12.630
partial.
They were the of of Q,
I have a which respect to eight.

33
00:02:13.370 --> 00:02:15.720
So Feinstein's uh,
in the neural network,

34
00:02:15.750 --> 00:02:19.950
if we're honored the derivative with respect to the activation of this neuron

35
00:02:20.430 --> 00:02:24.430
and well then we,
we'll need to consider,
uh,
and the,
and,

36
00:02:24.500 --> 00:02:26.130
and then we could use say,

37
00:02:26.131 --> 00:02:31.131
the pre activation gradient here and here as the partial a result.

38
00:02:31.441 --> 00:02:34.260
Because if we have this,
then we can compute the last function.

39
00:02:35.070 --> 00:02:38.780
So what chain would would tell us is that to get the partial there,
they've,

40
00:02:39.150 --> 00:02:43.370
for the activation here,
we would need to combine the A,

41
00:02:43.380 --> 00:02:47.970
we need to take a sum over all paths between,
between what we're,
uh,

42
00:02:48.000 --> 00:02:52.760
making,
uh,
parts of their,
the over and what,
uh,
we,
uh,
want to,

43
00:02:53.140 --> 00:02:56.700
uh,
partial,
uh,
with respect to what we want our partial derivative.

44
00:02:57.150 --> 00:03:02.040
So in particular we would have to add the product of the partial dividends of

45
00:03:02.050 --> 00:03:05.000
our last function with respect to the pre.

46
00:03:05.400 --> 00:03:08.560
So sort of this part of the path times the partial,

47
00:03:08.561 --> 00:03:13.000
did it live up the pre activation at the output layer,
which respect to this.

48
00:03:13.001 --> 00:03:17.670
So that's this part of their path plus the other,
uh,

49
00:03:17.680 --> 00:03:22.060
components in the other path between the activation here and the output.

50
00:03:22.330 --> 00:03:26.920
So that would be the partial derivative of the loss with respect to the pre

51
00:03:26.921 --> 00:03:29.560
activation times,
the uh,

52
00:03:30.050 --> 00:03:33.700
preact the parts of their lives of the pre activation with respect to the

53
00:03:33.701 --> 00:03:36.160
activation here.
Okay.

54
00:03:36.161 --> 00:03:41.161
So we get invoked this chain rule and use it in computing are gradients by using

55
00:03:42.401 --> 00:03:45.790
for EI a,
the activation of a unit in some layer,

56
00:03:46.270 --> 00:03:51.270
the parcel or intermediate results would be the pre activation in the layer

57
00:03:51.311 --> 00:03:56.030
above for all the,
uh,
uh,
for each I have,

58
00:03:56.050 --> 00:03:58.570
uh,
layer,
uh,
unit in the layer above.

59
00:03:58.920 --> 00:04:03.310
And then p of a would be our function would be the loss function.

60
00:04:03.760 --> 00:04:03.941
Okay.

61
00:04:03.941 --> 00:04:07.370
So that's what we'll use to derive the Greens with respect to the hidden there.

62
00:04:08.820 --> 00:04:09.653
<v 0>Okay.</v>

63
00:04:09.660 --> 00:04:10.493
<v 1>All right.</v>

64
00:04:10.670 --> 00:04:15.670
So if we are at d kith hidden layer and won the partial derivative of the loss,

65
00:04:16.041 --> 00:04:19.920
which was with respect to the GF,
uh,
uh,

66
00:04:20.010 --> 00:04:23.470
GF activation,
uh,
up the GF,
uh,

67
00:04:23.510 --> 00:04:26.840
hidden unit for the Kia [inaudible] layer.
Well,

68
00:04:26.870 --> 00:04:31.730
the a chain rule tells me that if I want to use as the intermediate result,

69
00:04:31.731 --> 00:04:36.240
the pre activation a at the k plus one,
if,

70
00:04:36.380 --> 00:04:39.410
uh,
hidden in there.
So that's the hidden there above.

71
00:04:40.200 --> 00:04:45.200
But I need to sum over all of the APP reactivation that the layer above,

72
00:04:45.580 --> 00:04:50.270
I need to multiply the partial derivative of the last week,
respected the,

73
00:04:50.390 --> 00:04:55.390
that reactivation and then multiplied that by the partial derivative of that

74
00:04:56.061 --> 00:04:56.894
reactivation,

75
00:04:56.930 --> 00:05:01.930
the IFP or activation of the layer above with respect to the activation at the

76
00:05:03.230 --> 00:05:07.310
Keith layer with respect to the GF,
uh,
hidden unit,

77
00:05:07.400 --> 00:05:12.000
which is the thing we want to get our parcel derivative,
uh,
with respect to,
uh,

78
00:05:12.080 --> 00:05:16.370
what we want our partial derivative.
So,
uh,

79
00:05:16.371 --> 00:05:17.390
so for instance,

80
00:05:17.660 --> 00:05:21.680
if we are here where something over this pack and this back or if we were at

81
00:05:21.681 --> 00:05:26.510
this hidden layer,
then we'd need to take into account,
uh,

82
00:05:26.570 --> 00:05:30.670
the gradient that comes from this partial,
uh,

83
00:05:30.680 --> 00:05:31.940
this pre activation.

84
00:05:31.941 --> 00:05:36.920
And then this practice reactivation and this practice is p activation and so on.

85
00:05:37.800 --> 00:05:38.410
<v 0>Yeah,</v>

86
00:05:38.410 --> 00:05:40.510
<v 1>it's remove some ink.
Okay.</v>

87
00:05:40.810 --> 00:05:45.400
So let's assume that we've already computed that before.

88
00:05:45.730 --> 00:05:48.400
And so for instance,
if we started at the output layer,

89
00:05:48.570 --> 00:05:52.850
or we could assume that we've already computed the reactivation gradient,
uh,

90
00:05:52.860 --> 00:05:55.480
at the output layer.
Now,

91
00:05:55.481 --> 00:05:58.370
the part of this expression that we don't yet is this part here,

92
00:05:58.371 --> 00:06:02.770
what's the partial derivative of the pre activation at the layer above with

93
00:06:02.771 --> 00:06:06.800
respect to the activation that the layer below for Dj JF unit?

94
00:06:07.670 --> 00:06:08.990
Well,
uh,

95
00:06:09.020 --> 00:06:12.560
let's remind ourselves the formula for the reactivation activation that a given

96
00:06:12.561 --> 00:06:16.400
layer.
So a,
so this is k here and a half came in this one here.

97
00:06:16.401 --> 00:06:18.620
So this could instead be k plus one,

98
00:06:18.621 --> 00:06:23.180
and then I would have just gay here and I keep last one here,
last one here.

99
00:06:24.560 --> 00:06:28.880
So,
um,
for the unit at the layer above,
uh,

100
00:06:28.881 --> 00:06:31.310
it's computation is just a linear transformation.

101
00:06:31.310 --> 00:06:36.310
So a bias plus a linear combination of all of the activation of the units in the

102
00:06:36.771 --> 00:06:41.750
layer below.
So all the,
uh,
j unit in the layer of below.

103
00:06:42.230 --> 00:06:42.921
So I can find one,

104
00:06:42.921 --> 00:06:47.921
the partial derivative of that expression with respect to a specific hidden

105
00:06:48.771 --> 00:06:51.780
units.
Uh,
so for that specific hidden unit,

106
00:06:51.800 --> 00:06:55.940
I all the other terms except the one for the actual value of jam interested in,

107
00:06:56.090 --> 00:06:57.950
uh,
the partial,
their lives would be zero.

108
00:06:58.400 --> 00:07:02.420
And so for the case where this Jay is the same JFF as this one here,

109
00:07:03.700 --> 00:07:04.310
um,

110
00:07:04.310 --> 00:07:08.960
then I have a constant times the actual uh,

111
00:07:09.170 --> 00:07:10.370
activation of the unit.

112
00:07:10.690 --> 00:07:14.240
And the partial derivative of that would just be the scaler it would be just be

113
00:07:14.390 --> 00:07:15.530
wk I Jane.

114
00:07:15.740 --> 00:07:20.740
So this term here is simply equal to the connection between the JF new neuron in

115
00:07:23.211 --> 00:07:25.040
the hidden layer,

116
00:07:25.041 --> 00:07:29.840
Kay and the IFE neuron in the hidden layer above.
Okay.

117
00:07:30.950 --> 00:07:32.120
Again,
this removes some ink.

118
00:07:33.380 --> 00:07:36.830
So now this expression corresponds to,

119
00:07:36.860 --> 00:07:41.860
we can think of it as taking the vector of gradients which respect to the

120
00:07:43.250 --> 00:07:48.250
reactivation that the layer cake loss one multiplied by the column of that

121
00:07:49.101 --> 00:07:52.780
Matrix.
And so it's multiplied by the column,
the,
the,

122
00:07:52.781 --> 00:07:57.680
the product of the vector here with the vector corresponding to the JF column.

123
00:07:57.860 --> 00:08:01.220
And that's because we're summing over the row index.

124
00:08:01.280 --> 00:08:05.330
So then we'd be going from the first role,
second row and so on.

125
00:08:05.540 --> 00:08:09.050
So we're taking the product of these terms along the column,

126
00:08:09.080 --> 00:08:13.970
the JF column up that Matrix.
And so we can write it in this way.
We can say,

127
00:08:14.000 --> 00:08:17.570
I'll take DJF column of Matrix W in,
sorry,

128
00:08:17.571 --> 00:08:21.890
there should be a k plus one here.
So I get the get the uh,

129
00:08:22.250 --> 00:08:26.540
uh,
and sorry,
this should be in there should be changed here.

130
00:08:27.650 --> 00:08:32.650
So that means I'm taking all the elements for the row index and Jay here means

131
00:08:33.591 --> 00:08:37.820
that I'm taking Dj if uh,
elements,
uh,
DJ column.

132
00:08:38.300 --> 00:08:39.470
So I think the [inaudible] column,

133
00:08:39.510 --> 00:08:42.620
that Matrix and so that would be a column vector.

134
00:08:42.650 --> 00:08:47.060
So I take the transpose to make it into a row vector and then I can multiply

135
00:08:47.061 --> 00:08:50.600
that by my gradient retrospect to their pre activation.

136
00:08:51.080 --> 00:08:53.900
And so this would be exactly equivalent to that.

137
00:08:56.240 --> 00:08:57.073
<v 0>Yeah.</v>

138
00:08:57.400 --> 00:08:58.990
<v 1>Now if I want the gradient,</v>

139
00:08:58.991 --> 00:09:03.991
so if I want the a vector partial derivatives for all activations in the hidden

140
00:09:04.811 --> 00:09:09.670
layer k,
then all I can do is I take my whole matrix,

141
00:09:09.700 --> 00:09:10.570
I transpose it.

142
00:09:10.960 --> 00:09:15.370
And then I multiplied that by the factor of gradients for the reactivation that

143
00:09:15.371 --> 00:09:17.050
the hidden layer k plus one.

144
00:09:17.590 --> 00:09:21.790
And that's because the definition of multiplying in vector by a matrix is just

145
00:09:21.791 --> 00:09:26.170
multiplying that factor with all the rows of that Matrix.

146
00:09:26.980 --> 00:09:28.840
And so,
uh,
if you,

147
00:09:29.330 --> 00:09:32.710
if we go back at this formula here,
uh,

148
00:09:32.950 --> 00:09:36.280
so here and here we had the thought in here we had a j.

149
00:09:37.390 --> 00:09:42.100
And so by taking the transpose a,
it means that,

150
00:09:42.420 --> 00:09:47.180
uh,
we're multiplying by this row vector and,
uh,

151
00:09:47.200 --> 00:09:52.200
this rule vector would curse fun to the JF row vector of the transpose of that

152
00:09:54.581 --> 00:09:55.870
Matrix.
Okay?

153
00:09:56.440 --> 00:10:00.550
So that's one way of taking our partial their lives and then converting that

154
00:10:00.551 --> 00:10:03.610
into a vector of partial derivative.
So the gradient,

155
00:10:03.640 --> 00:10:07.460
we simply take the partial derivatives of a salary,

156
00:10:07.480 --> 00:10:10.090
the gradient of the activation at the layer above,

157
00:10:10.240 --> 00:10:13.660
and then multiply multiplying by the transpose of the connections between these

158
00:10:13.780 --> 00:10:14.890
two adjacent layers.

159
00:10:19.060 --> 00:10:20.440
And now if one of the gradient,

160
00:10:20.441 --> 00:10:24.280
which respected the pre activation at a given layer.

161
00:10:24.580 --> 00:10:27.220
So if you're on the partial limited of what they are hidden,

162
00:10:27.221 --> 00:10:32.080
they are k of the GEF reactivation.
Introspect all lost function.

163
00:10:32.350 --> 00:10:36.250
But again,
I can use the chain rule,
you know,
now,
uh,

164
00:10:36.280 --> 00:10:39.340
in this case I could,
uh,
look,
uh,

165
00:10:39.400 --> 00:10:43.630
computed as as the product of the partial derivative of the last retrospective,

166
00:10:43.631 --> 00:10:48.631
the GF activation in layer k times the partial there live of the hidden there,

167
00:10:50.470 --> 00:10:55.470
k JF activation with respect to the JFF pre activation of hidden layer cake.

168
00:10:57.640 --> 00:11:01.300
So in this case,
I don't have a sum and that's because,
uh,

169
00:11:01.510 --> 00:11:05.230
I compute as I see here,
the uh,

170
00:11:05.290 --> 00:11:08.380
JF activation in the layer is,

171
00:11:08.381 --> 00:11:12.360
does not depend on the reactivation of the other,
uh,

172
00:11:12.460 --> 00:11:15.490
units in that layer is just a nonlinear function.

173
00:11:15.491 --> 00:11:20.491
The activation function applied on the JF pre activation of the reactivation

174
00:11:21.251 --> 00:11:23.200
vector.
So in this case,

175
00:11:23.201 --> 00:11:28.201
I only need this only depends on the JF reactivation.

176
00:11:30.510 --> 00:11:35.220
And now,
uh,
so for this term,
this is simply the,

177
00:11:35.510 --> 00:11:37.130
uh,
partial there.

178
00:11:37.140 --> 00:11:41.790
They've of my activation function evaluated at the,
uh,

179
00:11:42.000 --> 00:11:44.730
value of their pre activation.
Okay.

180
00:11:44.731 --> 00:11:49.450
So this would be simply the pre,
uh,
the,
the gradient or the,
the,

181
00:11:49.451 --> 00:11:53.710
the,
the narrative of my chosen activation function.

182
00:11:56.740 --> 00:11:56.921
Now,

183
00:11:56.921 --> 00:12:01.360
if I want to compute the gradients of the last function with respect to my pre

184
00:12:01.361 --> 00:12:04.000
activations or the vector of partial derivatives,

185
00:12:04.670 --> 00:12:09.110
I have to do is to take all the partial lives and put them in the vector that we

186
00:12:09.111 --> 00:12:11.060
remember,
that,
uh,
uh,

187
00:12:11.100 --> 00:12:16.100
it only corresponded to taking the partial derivative of the last function with

188
00:12:16.461 --> 00:12:21.220
respect to the activation of a neuron and multiplying it by the gradient or the

189
00:12:21.330 --> 00:12:26.330
derivative of the activation evaluated at the pre activation of that same

190
00:12:27.701 --> 00:12:31.870
neuron.
So we can just achieve that by taking the factor.
Uh,

191
00:12:31.871 --> 00:12:33.970
so the gradient,
uh,

192
00:12:34.130 --> 00:12:38.590
up the last with respect to the activations and multiplying that element.

193
00:12:38.590 --> 00:12:43.570
Why do we need another doing an element wise multiplication with a vector that

194
00:12:43.571 --> 00:12:45.650
contains all of these,
uh,

195
00:12:45.790 --> 00:12:50.210
gradients of the activation function for all neurons,
neurons,
uh,

196
00:12:50.620 --> 00:12:54.970
one,
two up to Jay,
then up to the number of neurons in that layer.

197
00:12:56.110 --> 00:13:00.670
Now those are familiar with a vector calculus.
Uh,

198
00:13:00.700 --> 00:13:05.080
they might have taken this step just taking the,
uh,
and that's,
uh,

199
00:13:05.081 --> 00:13:10.000
you can think of this as the,
uh,
vector version of the chain rule.

200
00:13:10.001 --> 00:13:15.001
You take the gradient with respect to all your partial results and then you

201
00:13:15.011 --> 00:13:17.850
multiply that by the,
uh,

202
00:13:17.950 --> 00:13:22.720
gradient of the partial results with respect or the intermediate results with

203
00:13:22.721 --> 00:13:25.840
respect to the,
uh,
in our case,
the activation.

204
00:13:26.650 --> 00:13:30.010
And now I just want to,
uh,
uh,

205
00:13:30.100 --> 00:13:35.100
I just want to mention that this expression here is college Jacobian.

206
00:13:35.410 --> 00:13:37.120
It contains the grain,

207
00:13:37.330 --> 00:13:41.230
the partial derivative of all elements of that vector with respect to all

208
00:13:41.231 --> 00:13:45.670
elements of that vector.
So the partial derivatives of all activation,
uh,

209
00:13:45.700 --> 00:13:50.500
in the hidden layer with respect to all pre activation and a,

210
00:13:50.501 --> 00:13:55.240
and it turns out in this case that this Jacobian is actually a diagonal matrix.

211
00:13:55.241 --> 00:13:57.730
So all off diagonal elements are zero.

212
00:13:57.731 --> 00:14:02.731
And that's because the activation for a neuron on the depends on it's pre

213
00:14:02.771 --> 00:14:06.450
activation and doesn't depend on DP activations of the other neurons.

214
00:14:06.451 --> 00:14:09.520
So the partial data,
food,
their lives would be zero.

215
00:14:10.090 --> 00:14:15.090
And also talk about that be to justify why I didn't use that notation to mean

216
00:14:17.380 --> 00:14:18.490
that vector,

217
00:14:19.070 --> 00:14:23.030
if I remove some ink actually actually wrote sort of use,
uh,

218
00:14:23.450 --> 00:14:27.280
a bit strange notation by saying I'm going to construct a vector where all

219
00:14:27.281 --> 00:14:31.240
elements is the narrative of the activation function.

220
00:14:31.390 --> 00:14:35.980
And that's because this here is not equal to that.

221
00:14:36.370 --> 00:14:38.150
But doing the matrix,

222
00:14:38.230 --> 00:14:43.230
the vector product of this with that Matrix actually in this case corresponds to

223
00:14:43.331 --> 00:14:47.720
taking that Matrix and doing an element wise multiplication with,
uh,

224
00:14:47.790 --> 00:14:52.220
sorry that vector in doing it otherwise multiplication with that vector,

225
00:14:52.221 --> 00:14:56.600
the vector of,
uh,
narratives,
uh,
for,
uh,

226
00:14:56.601 --> 00:15:01.090
all of the activation functions evaluate that.
All of the,
uh,
uh,

227
00:15:01.130 --> 00:15:05.060
p activation,
uh,
uh,
values.
All right,

228
00:15:05.061 --> 00:15:08.750
so this is how we compute the,
uh,
last gradient,

229
00:15:08.770 --> 00:15:12.590
which respect to hidden there is both at the activation and pre activation
level.

