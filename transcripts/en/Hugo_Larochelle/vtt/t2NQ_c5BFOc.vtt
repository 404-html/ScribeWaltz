WEBVTT

1
00:00:00.610 --> 00:00:02.670
In this video will introduce a different type of,

2
00:00:02.671 --> 00:00:05.380
of doing quarter known as DT noise ingo twin quarter.

3
00:00:07.570 --> 00:00:12.520
We talked in the previous video how a over complete hidden layer in the regular

4
00:00:12.521 --> 00:00:17.260
auto encoder,
uh,
doesn't make a lot of sense in terms of what we can expect.

5
00:00:17.261 --> 00:00:22.170
The auto encoder can learn.
And that's because it could actually just learn,
uh,

6
00:00:22.270 --> 00:00:25.800
to copy individual elements in the input vector.

7
00:00:26.160 --> 00:00:30.870
And then h of x would just be a copy essentially of the input.
So would not sell,

8
00:00:30.930 --> 00:00:35.670
be encouraged to learn meaningful features.
And so now we'll see that,

9
00:00:35.920 --> 00:00:39.380
uh,
with this new formulation known as the [inaudible],

10
00:00:39.570 --> 00:00:44.570
we will get a set of hidden units which extract interesting structure from the,

11
00:00:45.151 --> 00:00:46.470
uh,
from our training set.

12
00:00:49.710 --> 00:00:52.840
There's an idea is actually quite simple and uh,

13
00:00:52.841 --> 00:00:56.650
we'll see that it essentially directly addresses the problem that an auto

14
00:00:56.651 --> 00:01:01.360
encoder could learn.
A regular coder could,
could learn to just copy the input.

15
00:01:02.530 --> 00:01:05.130
So the idea is to train,
uh,

16
00:01:05.160 --> 00:01:07.900
so the model is known as Dt nosing or two a quarter.

17
00:01:08.020 --> 00:01:10.600
And you'll see very clearly why it's called like that.

18
00:01:11.440 --> 00:01:16.210
So the idea is to learning representation and extracted by the end quarter that

19
00:01:16.211 --> 00:01:19.510
is robust to the introduction of noise into the input.

20
00:01:20.590 --> 00:01:25.590
And so the idea will be that instead of feeding to deal in quarter,

21
00:01:25.840 --> 00:01:27.620
the original input x,

22
00:01:29.120 --> 00:01:33.660
well actually feed a noisy version of it.
And this noisy version,

23
00:01:33.661 --> 00:01:35.460
which I'm calling x stilled,

24
00:01:35.880 --> 00:01:40.880
is going to be the result of taking x and then passing it through a noise

25
00:01:41.161 --> 00:01:45.900
process or some probabilistic process which will take x and in generate this

26
00:01:45.960 --> 00:01:49.620
instilled in some way based on x.
Um,

27
00:01:49.621 --> 00:01:52.590
so one popular way of,
uh,

28
00:01:52.650 --> 00:01:57.480
for this noise pro public choice of this lowest process is to just randomly

29
00:01:57.481 --> 00:02:01.050
assign certain elements of the inputs to zero,

30
00:02:01.290 --> 00:02:06.120
where a will choose to assign an input to a value of zero with some probability,

31
00:02:06.300 --> 00:02:11.280
uh,
which is a hyper parameter would have to tune.
So in this example here,

32
00:02:11.281 --> 00:02:14.460
say the probability was half,
uh,
then,
um,

33
00:02:14.580 --> 00:02:19.230
we would iterate over the inputs and then say for this input,
I flip a coin.

34
00:02:19.440 --> 00:02:24.150
Uh,
maybe the coin says that I should add some noise.
So in this case,

35
00:02:24.420 --> 00:02:27.420
instead of copying x,
I would just,

36
00:02:27.480 --> 00:02:32.480
I would actually not copying and set the value of that unit in excel two zero.

37
00:02:33.960 --> 00:02:37.740
Here I flip a coin,
I get a different outcome which says copies.
So I copy it,

38
00:02:38.220 --> 00:02:42.880
I copy it here,
here,
I don't copy it.
Uh,
and again,
uh,

39
00:02:42.930 --> 00:02:46.650
for all and so on for all the input elements.
So we do this sort of randomly.

40
00:02:46.860 --> 00:02:49.410
So if we did that another time for the same input x,

41
00:02:49.411 --> 00:02:54.411
we get a different pattern of Zeros and that's I actually crucial.

42
00:02:55.601 --> 00:02:59.680
So now what we've done is that we're feeding the to the interim quarter noisy

43
00:02:59.681 --> 00:03:04.681
variant where some of the elements in for this noise process has been

44
00:03:05.021 --> 00:03:09.310
essentially erased in this sense.
We could choose other types of noise,

45
00:03:09.311 --> 00:03:14.311
we could add Goshen additive noise by adding a see Gulshan those with the mean

46
00:03:14.711 --> 00:03:16.640
zero in a particular variants,

47
00:03:16.690 --> 00:03:19.090
which would be a hyper parameter would have to tune.

48
00:03:20.050 --> 00:03:22.630
And then to train the Dooney denoising auto quarter.

49
00:03:23.200 --> 00:03:27.400
So from [inaudible] we get a encoder value h of x still,

50
00:03:27.430 --> 00:03:30.850
and then we get a reconstruction x hat and not x hat.

51
00:03:30.880 --> 00:03:35.880
We don't compare it to external that we actually compare it to the original

52
00:03:36.460 --> 00:03:40.210
input.
So the last function compares x stilled.

53
00:03:40.480 --> 00:03:43.800
So x hat to ex not ex stilled.

54
00:03:44.520 --> 00:03:47.200
And now we see that in this case,
uh,

55
00:03:47.290 --> 00:03:50.710
for one thing we can get a perfect reconstruction error by just copying

56
00:03:50.740 --> 00:03:55.660
instilled,
uh,
into different units here.
Uh,
so for instance,

57
00:03:55.690 --> 00:03:58.530
because they tone quarter gets a zero here,
uh,

58
00:03:58.750 --> 00:04:03.130
but this zero was actually the result of some noise then at the reconstruction.

59
00:04:03.160 --> 00:04:06.010
If it's trying to actually reconstruct that value,

60
00:04:06.570 --> 00:04:10.270
it can't entirely trust this value because in this case,
actually it's zero.

61
00:04:10.271 --> 00:04:11.710
So it's actually not the right valve.

62
00:04:11.711 --> 00:04:16.090
We can not just copy at the output the value of zero.
That would be a terrible,

63
00:04:16.610 --> 00:04:19.720
uh,
prediction to do in terms of the loss.

64
00:04:20.410 --> 00:04:24.250
So instead they do nosing autoencoder will be forced to use other inputs,

65
00:04:24.570 --> 00:04:27.700
for instance,
that input and that input to a,

66
00:04:27.760 --> 00:04:32.200
and then learn about what's the relationship normally between these two inputs.

67
00:04:32.200 --> 00:04:36.040
Say,
and this other input here.
And,
uh,

68
00:04:36.240 --> 00:04:40.900
and then using by learning what that relationship can be a,

69
00:04:40.901 --> 00:04:45.190
then it might be able then to infer from it a more reliable prediction of what

70
00:04:45.191 --> 00:04:47.770
this input was based say on these two inputs.

71
00:04:48.460 --> 00:04:52.060
So that's what the hidden layer will be forced to try to do.

72
00:04:52.270 --> 00:04:56.500
And this will thus force it to have hidden units that extract particular types

73
00:04:56.590 --> 00:05:00.670
of,
of,
of structures that we tend to see.
So predict,
uh,
you know,

74
00:05:00.680 --> 00:05:04.610
different types of correlations between the inputs that we tend to see,
uh,

75
00:05:04.930 --> 00:05:09.550
in the input vectors in our training distribution.
So by doing this,

76
00:05:09.551 --> 00:05:14.530
we really broke and made impossible,
not optimal,
uh,
this,

77
00:05:14.630 --> 00:05:15.610
uh,
uh,

78
00:05:15.670 --> 00:05:19.750
behavior in the Taco of just copying each input because there's noise in it.

79
00:05:19.960 --> 00:05:20.890
So by adding noise,

80
00:05:20.891 --> 00:05:25.480
we're forcing you to learn about the structure of the input distribution and a,

81
00:05:25.481 --> 00:05:28.720
and then we're forcing it to us,
uh,

82
00:05:28.780 --> 00:05:32.350
at the same time learn more meaningful features,
uh,

83
00:05:32.410 --> 00:05:34.000
as we'll see in the next slide.

84
00:05:37.270 --> 00:05:41.200
So here's an illustration of,
of,
of how it's training.
Um,

85
00:05:41.320 --> 00:05:45.720
so imagine in this,
uh,
cartoon example that,
uh,

86
00:05:45.820 --> 00:05:49.300
we can reconstructing vectors in two dimensions and that our training

87
00:05:49.301 --> 00:05:51.850
distribution is such that,
uh,

88
00:05:51.851 --> 00:05:56.680
really all examples sort of lie around this one dimensional mind manifold.

89
00:05:56.710 --> 00:06:01.340
This one dimensional curved lines here essentially corresponds to where,

90
00:06:01.720 --> 00:06:02.000
uh,

91
00:06:02.000 --> 00:06:07.000
almost all of the data from my input distribution lines and when I'm training,

92
00:06:08.000 --> 00:06:10.670
what I'm doing is that I'm taking one of these ex,
uh,

93
00:06:10.790 --> 00:06:14.270
training examples with these different x's would be samples in my training set

94
00:06:14.720 --> 00:06:19.040
and am I adding some noise?
So that's pushing it away from the manifold.

95
00:06:19.550 --> 00:06:21.590
And then the,
uh,

96
00:06:21.620 --> 00:06:25.160
what the auto encoder is trying is to take this x still,

97
00:06:25.161 --> 00:06:29.870
which has been pushed away from the manifold and projected back on the metaphor

98
00:06:29.960 --> 00:06:33.980
especially it's strange to projected exactly two x.
And so on average,

99
00:06:34.040 --> 00:06:39.040
what it's going to do is sort of tried to push back the data where most of the

100
00:06:39.591 --> 00:06:42.380
training data usually is.
And so by doing this,

101
00:06:42.381 --> 00:06:45.290
really it's learning about this structures.
In this case,

102
00:06:45.291 --> 00:06:49.280
the structure of the data is essentially the shape of that manifold.

103
00:06:49.700 --> 00:06:53.500
And so we see here in this visual illustration that,
uh,

104
00:06:53.510 --> 00:06:56.960
we're forcing you are towing quarter to learn about what that structure is

105
00:06:56.961 --> 00:07:00.440
because it needs to know what that structure is in order to know where to

106
00:07:00.441 --> 00:07:04.370
project back.
The,
uh,
noisy,
uh,
uh,

107
00:07:04.430 --> 00:07:09.020
the noisy inputs x still,
uh,
to be able to get a good reconstruction,
uh,

108
00:07:09.140 --> 00:07:09.973
x hat.

109
00:07:10.580 --> 00:07:10.900
<v 1>Okay.</v>

110
00:07:10.900 --> 00:07:15.790
<v 0>So for our images,
you know,
uh,
we wouldn't be in two D,
But,
uh,</v>

111
00:07:15.910 --> 00:07:16.870
for images,
we,

112
00:07:17.190 --> 00:07:21.940
we do have the notion that most images a lie on their lower dimensional

113
00:07:21.941 --> 00:07:26.230
metaphor,
which represent variations such as translations and rotations and small

114
00:07:26.231 --> 00:07:31.000
deformations.
And so,
uh,
if this was an example,

115
00:07:31.210 --> 00:07:34.150
then a noisy example would look something like this.

116
00:07:34.151 --> 00:07:36.190
It would kind of look like a character,

117
00:07:36.191 --> 00:07:39.010
but not exactly would be slightly off the manifold.

118
00:07:39.550 --> 00:07:44.550
And then we will learn to push this back into this here and a,

119
00:07:45.221 --> 00:07:46.480
again,
in a sense,

120
00:07:46.481 --> 00:07:51.481
we're also learning to distinguish between true inputs and a noisy inputs.

121
00:07:52.890 --> 00:07:56.800
Uh,
if we had an a lot of noise,
then we would be generating something like this.

122
00:07:56.980 --> 00:08:01.570
And so we'd be learning essentially that we don't want to be able to reconstruct

123
00:08:01.571 --> 00:08:02.470
this perfectly.

124
00:08:02.590 --> 00:08:06.280
What do you want to be able to reconstruct perfectly is things that look like

125
00:08:06.281 --> 00:08:09.280
digits like this.
Even if they have a little bit of noise,

126
00:08:09.281 --> 00:08:12.460
we'd be able to reconstruct him back into a nice looking digit,

127
00:08:12.700 --> 00:08:14.590
whereas this which has a lot of noise,

128
00:08:14.591 --> 00:08:19.360
we will not learn to reconstruct it into it's all noisy version.
Um,

129
00:08:19.361 --> 00:08:19.871
so again,

130
00:08:19.871 --> 00:08:23.990
we were filed back in the setting where we have hidden units that are good at

131
00:08:24.000 --> 00:08:27.160
reconstructing things that look like the data,
that training data.

132
00:08:27.520 --> 00:08:32.410
And that is not good at reconstructing things that are very noisy because noisy

133
00:08:32.411 --> 00:08:36.670
things like this or less noisy things like this are going to be projected back

134
00:08:36.730 --> 00:08:38.560
into a nun that we,
the image.

135
00:08:42.330 --> 00:08:46.500
So,
uh,
let's look at the actual features we get when we trained the Nemnus.

136
00:08:46.530 --> 00:08:51.300
So this is an example of a feature is trained by an auto encoder without

137
00:08:51.301 --> 00:08:54.150
corrupting the inputs.
So that's just a regular old twin quarter here.

138
00:08:56.960 --> 00:09:01.460
Now if we take 25% of the input,
25% of the input,

139
00:09:01.461 --> 00:09:06.080
and we set it to zero.
So we have this masking noise type of,
uh,
corruption.

140
00:09:06.840 --> 00:09:08.760
Yeah,
we see now that a lot of the hidden units,

141
00:09:08.761 --> 00:09:12.510
their filters start looking more like edge detectors.
For instance,

142
00:09:12.511 --> 00:09:17.140
we have these sort of pen stroke like detectors here,
here and here.
Uh,

143
00:09:17.141 --> 00:09:20.580
this is also kind of like a pen stroke detector in the sense that we have some,

144
00:09:20.770 --> 00:09:25.410
uh,
ink here.
And then it's saying there's background,
right?
Uh,
besides the,

145
00:09:25.411 --> 00:09:29.880
uh,
the ink.
Um,
we also noticed that,
uh,

146
00:09:29.910 --> 00:09:31.620
the filter for instance,

147
00:09:31.650 --> 00:09:35.880
this one which was not looking too bad is no,
so for you,

148
00:09:36.890 --> 00:09:38.210
keep your eyes on this one.

149
00:09:38.740 --> 00:09:38.900
<v 1>Okay.</v>

150
00:09:38.900 --> 00:09:42.700
<v 0>We see that I'd,
the filter actually got bigger and uh,</v>

151
00:09:42.740 --> 00:09:45.460
and that's part of it also because if we add a lot of noise,

152
00:09:45.640 --> 00:09:48.040
like 25% of noise in the input,

153
00:09:48.400 --> 00:09:53.200
then each unit to be able to say what kind of structure is present?

154
00:09:53.201 --> 00:09:55.030
Is it a sort of edge like this?

155
00:09:55.270 --> 00:09:59.830
Then it needs to integrate out many more pixels because,
uh,

156
00:09:59.860 --> 00:10:02.320
because uh,
you know,
many of them might be noisy,

157
00:10:02.321 --> 00:10:06.310
so you want to cover enough that you,
you have enough in your set of pixels,

158
00:10:06.311 --> 00:10:10.120
your feet,
your,
that this hidden unit is doing it's computation over.

159
00:10:10.210 --> 00:10:14.800
So that sort of the,
the noise can be washed out of the computation.

160
00:10:15.850 --> 00:10:16.230
<v 1>Okay.</v>

161
00:10:16.230 --> 00:10:20.790
<v 0>And then feed,
I add even more,
we get even more a bigger,
uh,
filters,</v>

162
00:10:20.791 --> 00:10:23.790
bigger receptive fields.
So when we're talking about receptive fields,

163
00:10:23.791 --> 00:10:26.010
we were talking about the,
uh,

164
00:10:26.400 --> 00:10:31.400
number of pixels for which the weights with a is significantly far from zero.

165
00:10:31.441 --> 00:10:34.470
So this would be white or black in these images.

166
00:10:35.100 --> 00:10:38.160
So we see again that we get some interesting features.

167
00:10:38.161 --> 00:10:43.160
We are kind of like pen stroke detectors and uh,
uh,
there are actually,
uh,

168
00:10:43.290 --> 00:10:46.650
oops,
sorry.
There actually again,
bigger because we've added more noise.

169
00:10:46.651 --> 00:10:51.390
So it's trying to cover a larger part of the image to be able to extract,

170
00:10:51.440 --> 00:10:55.860
uh,
and,
and figure out more accurately what the structure is mean.
We get,
again,

171
00:10:55.861 --> 00:11:00.840
like a lot of these sort of a pen stroke detectors here in here that we didn't

172
00:11:00.841 --> 00:11:02.400
get without any noise.

173
00:11:06.190 --> 00:11:10.690
Uh,
here's another experiment where,
uh,
we train a natural limits patches.

174
00:11:10.691 --> 00:11:11.910
So we taken,
that's what they mentioned,

175
00:11:11.911 --> 00:11:13.900
like the image of a forest or something like that.

176
00:11:14.200 --> 00:11:18.520
And then we take small windows or patches of pixels.

177
00:11:18.521 --> 00:11:22.690
So these are like a few pixels by a few pixels on these with their very small,

178
00:11:22.870 --> 00:11:25.780
tiny images,
extract him from the larger image of a natural,

179
00:11:26.170 --> 00:11:30.130
a natural scene.
And,
uh,

180
00:11:30.160 --> 00:11:34.720
if we tried an auto encoder with the squared difference loss and a,

181
00:11:34.721 --> 00:11:38.740
we add gaussian noise,
then the types of filters we get are really interesting.

182
00:11:38.741 --> 00:11:40.660
Do you look a lot like edge detectors?

183
00:11:40.661 --> 00:11:44.820
So they're like high intensity or positive intensive?
I,
yes.
Yeah,

184
00:11:44.890 --> 00:11:49.150
high intensity compared with low intensity.
So for instance,
this would be active.

185
00:11:49.200 --> 00:11:53.860
We get something,
you know,
high intensity pixels next to low pixels.

186
00:11:54.640 --> 00:11:56.650
So that's really an edge.
Uh,

187
00:11:56.740 --> 00:12:00.940
and if we actually had trained a PCA on these images,

188
00:12:00.941 --> 00:12:05.740
so we're not showing these here,
but that's not the type of filters we would get.

189
00:12:05.741 --> 00:12:10.741
We actually get a lot of these sort of spatial high frequency filters that we

190
00:12:10.781 --> 00:12:12.340
saw in the,
uh,

191
00:12:12.341 --> 00:12:17.230
regular auto encoder where some of the filters were mostly gray and we saw like

192
00:12:17.380 --> 00:12:21.450
smaller ridges,
uh,
kind of,
uh,
waves in the,
uh,

193
00:12:21.460 --> 00:12:22.420
in the filter,

194
00:12:22.640 --> 00:12:23.680
<v 2>uh,
uh,</v>

195
00:12:23.840 --> 00:12:24.980
<v 0>with high frequency.</v>

196
00:12:25.070 --> 00:12:29.090
So we would see these types of things if we actually trained the PCA model on

197
00:12:29.091 --> 00:12:30.020
the same images.

198
00:12:30.230 --> 00:12:34.400
So now we see that we don't get that for the nosing auto encoder.

199
00:12:34.401 --> 00:12:36.410
The optimal solution is PCA.

200
00:12:36.411 --> 00:12:39.680
It's actually a very different solution which extracts features,

201
00:12:39.681 --> 00:12:42.050
which are much more meaningful.
Uh,

202
00:12:42.140 --> 00:12:45.650
in terms of the types of things we'd like to know about the image,

203
00:12:45.651 --> 00:12:49.430
say fort recognizing objects.
So detecting the,
uh,

204
00:12:49.440 --> 00:12:54.010
our contours of an object should be useful for being able to identify a,

205
00:12:54.080 --> 00:12:54.830
what's the nature

206
00:12:54.830 --> 00:12:55.663
<v 2>that object.</v>

207
00:12:58.270 --> 00:12:59.220
<v 0>And um,</v>

208
00:12:59.290 --> 00:13:02.920
and also it's not equivalent to taking the regular tone quarter he had and

209
00:13:02.921 --> 00:13:04.180
adding a lot of weight.
Tk.

210
00:13:04.330 --> 00:13:09.330
So I mentioning this because it's been shown that adding Goshen noise in the

211
00:13:09.791 --> 00:13:10.450
input,

212
00:13:10.450 --> 00:13:12.660
<v 2>uh,
uh,
uh,</v>

213
00:13:13.200 --> 00:13:17.810
<v 0>and then Tony Quarter,
if we,
if discussion Norris was very,
very small,</v>

214
00:13:17.811 --> 00:13:21.060
then in the limit of towards being zero,
uh,

215
00:13:21.061 --> 00:13:26.061
then it's essentially equivalent to some sort of weight decay or regularization

216
00:13:26.540 --> 00:13:29.330
on the weight.
And,
uh,
but in fact,

217
00:13:29.331 --> 00:13:33.900
if you trained irregular autoencoder,
which some way decay,
you,

218
00:13:33.920 --> 00:13:35.750
we actually don't recover the same solution.

219
00:13:35.751 --> 00:13:38.780
We get these high frequency filters that,

220
00:13:38.920 --> 00:13:40.570
<v 2>uh,
uh,</v>

221
00:13:40.830 --> 00:13:44.440
<v 0>once we got on the images of,
of characters,
for instance,
at our more,</v>

222
00:13:44.680 --> 00:13:46.810
<v 2>uh,
uh,
you know,
that are,
um,</v>

223
00:13:47.530 --> 00:13:51.280
<v 0>more silver alert to what we'd expect with a PCA,
for instance.</v>

224
00:13:51.520 --> 00:13:55.870
So really this is giving you a different solution from either strong way decay

225
00:13:56.050 --> 00:13:59.250
or a,
or just PCA.
This is really a,
uh,

226
00:13:59.320 --> 00:14:04.270
a solution in terms of filters that,
uh,
is much more satisfying.
And indeed,

227
00:14:04.480 --> 00:14:07.330
when we tried to use these features for classifying images,

228
00:14:07.420 --> 00:14:11.380
we do get much better performances than just with a regular touring quarter.

229
00:14:12.130 --> 00:14:14.140
So that's it for the nosing or throwing quarters.

230
00:14:14.870 --> 00:14:14.940
<v 2>Okay.</v>

