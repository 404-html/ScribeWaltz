WEBVTT

1
00:00:01.250 --> 00:00:05.400
In this video will introduce a new type of neural network called the restricted

2
00:00:05.401 --> 00:00:08.280
Boltzmann machine and will present its definition.

3
00:00:10.560 --> 00:00:11.910
So this is our,

4
00:00:11.940 --> 00:00:16.230
our first example of a neural network for unsupervised learning.

5
00:00:16.410 --> 00:00:20.400
So what have we seen before is neural networks that would learn to predict from

6
00:00:20.401 --> 00:00:22.560
an input a particular target.

7
00:00:22.590 --> 00:00:27.150
So he could learn to predict a class label or I could learn to predict from an

8
00:00:27.151 --> 00:00:29.790
input sequence,
a sequence of class labels.

9
00:00:29.850 --> 00:00:33.840
That's what we've seen before with the feed forward neural network and with the

10
00:00:33.841 --> 00:00:34.890
conditional random fields.

11
00:00:35.490 --> 00:00:39.780
Now we're going to talk about the neural network that performs unsupervised

12
00:00:39.810 --> 00:00:40.860
learning.
That is,

13
00:00:40.861 --> 00:00:45.150
it's going to learn something about the data based on the training set that only

14
00:00:45.151 --> 00:00:49.310
contains input vectors,
uh,
they use for that.
Uh,

15
00:00:49.380 --> 00:00:53.120
also application of unsupervised learning and,
uh,

16
00:00:53.160 --> 00:00:56.460
an unsupervised learning or neural network like a restricted Boltzmann machine

17
00:00:56.820 --> 00:01:01.170
is going to be a to extract meaningful features,
uh,

18
00:01:01.210 --> 00:01:05.970
about your data that will hopefully make it more explicit.

19
00:01:06.230 --> 00:01:07.620
Uh,
what is,
for instance,

20
00:01:07.621 --> 00:01:11.910
the class to which a particular input belongs to or make it easier to predict

21
00:01:11.940 --> 00:01:14.330
other,
uh,
types of,
uh,

22
00:01:14.850 --> 00:01:17.640
information about your data that you might be interested in.

23
00:01:18.520 --> 00:01:23.160
It's also going to allow us to leverage the availability of unlabeled data.

24
00:01:23.161 --> 00:01:26.400
So imagine you actually have a very small label training set,

25
00:01:26.580 --> 00:01:30.480
but you have a lot of unlabeled examples.
So for instance,

26
00:01:30.481 --> 00:01:34.290
you might have a classification of image problem.
Uh,

27
00:01:34.320 --> 00:01:37.590
perhaps you only have a few labeled images,
but you could go on the Internet.

28
00:01:37.591 --> 00:01:41.760
They actually find a lot of unlabeled images that have not been classified into

29
00:01:42.120 --> 00:01:45.600
a as belonging to the particular labels you're interested in.

30
00:01:45.720 --> 00:01:50.480
So that's going to allow us to do that.
And that's actually for,
those are,
uh,

31
00:01:50.520 --> 00:01:53.790
no more about machine learning.
And that's known as semi supervised learning.

32
00:01:53.791 --> 00:01:58.791
The problem of learning from a Dataset which contains a little bit of labeled

33
00:01:59.790 --> 00:02:01.590
data and a lot of unlabeled data.

34
00:02:02.670 --> 00:02:06.750
And we've also discussed that if we perform generative learning as opposed to

35
00:02:06.751 --> 00:02:10.710
discriminative learning,
uh,
then we in the,
our objective function,

36
00:02:10.711 --> 00:02:15.360
we have a turn that looks like this minus the log of the probability of,

37
00:02:15.420 --> 00:02:19.340
uh,
the,
uh,
input and,
uh,
we've,
uh,

38
00:02:19.380 --> 00:02:24.090
justified using generative learning as a performing some sort of regular riser

39
00:02:24.091 --> 00:02:26.580
that's data dependent.
And uh,

40
00:02:26.581 --> 00:02:31.050
so by introducing some unsupervised learning,
uh,
and sort of in models,
we can,

41
00:02:31.080 --> 00:02:35.670
we will be able to think about it as a way of performing this data dependent

42
00:02:35.820 --> 00:02:40.800
regularization by doing unsupervised learning in particular this term,

43
00:02:41.100 --> 00:02:44.100
uh,
we can actually compute it for uh,
uh,

44
00:02:44.101 --> 00:02:48.740
an input vector even if we don't have a label for it.
So again,
you know,
uh,

45
00:02:48.780 --> 00:02:52.370
dysregulation,
we can think of it as helping us,
uh,

46
00:02:52.500 --> 00:02:57.060
perform a particular castigation problem.
If a or any other prediction problem,

47
00:02:57.061 --> 00:03:01.930
if we have some on labeled data.
So I'm in the following videos,

48
00:03:01.931 --> 00:03:04.420
we'll see three different neural networks for unsupervised learning.

49
00:03:04.450 --> 00:03:06.730
And then the,
uh,

50
00:03:06.760 --> 00:03:11.620
next few videos we'll concentrate on the restrictive bolsa machine and neural

51
00:03:11.621 --> 00:03:12.454
network.

52
00:03:15.400 --> 00:03:20.350
So here's an illustration of an RBM.
So an RBM is,

53
00:03:20.440 --> 00:03:21.800
uh,
a,
uh,

54
00:03:21.801 --> 00:03:26.801
an undirected graphical model that defines a distribution over some input vector

55
00:03:27.550 --> 00:03:32.470
x.
And uh,
it's going to model the distribution of that,
uh,

56
00:03:32.471 --> 00:03:35.530
those vectors in my training data ex,
uh,

57
00:03:35.590 --> 00:03:39.730
using a layer of a binary,
uh,
hidden units,

58
00:03:40.090 --> 00:03:44.320
which I'm going to call h.
So here h is actually a random variable.

59
00:03:44.620 --> 00:03:48.660
Much like x in my model is also a random variable.
Uh,

60
00:03:48.670 --> 00:03:52.930
so I'm going to assume that my visible,
uh,
layer,

61
00:03:53.110 --> 00:03:58.110
which corresponds to the random variable x is going to consist of a binary

62
00:04:00.011 --> 00:04:03.850
units.
Uh,
and we'll talk a bit later about how,

63
00:04:04.060 --> 00:04:07.480
whether we can and how we can actually generalize the restrictive Bolsa machine

64
00:04:07.481 --> 00:04:11.530
to other types of observations that might be real value door,
uh,

65
00:04:11.570 --> 00:04:13.990
discrete value and not just binary value.

66
00:04:15.460 --> 00:04:18.640
So the respect of the most of the machine is going to define the distribution

67
00:04:18.670 --> 00:04:19.750
over x.

68
00:04:20.020 --> 00:04:25.020
And that distribution is actually going to involve some latent variables which

69
00:04:25.511 --> 00:04:29.410
correspond to my binary hidden units that we were going to get.

70
00:04:29.411 --> 00:04:33.910
That distribution is that will for the fun and energy function,
uh,

71
00:04:33.911 --> 00:04:37.640
which is as follows,
it's linear in either age or x.

72
00:04:37.950 --> 00:04:42.910
So it's going to be the product of the vector of hidden units,

73
00:04:42.940 --> 00:04:47.240
times a matrix of connections w times x.
Uh,

74
00:04:47.290 --> 00:04:48.860
then it's also going to involve,

75
00:04:48.861 --> 00:04:53.060
there's going to be minus a bias vector c times,
uh,

76
00:04:53.110 --> 00:04:58.110
my vector x and then minus my bias vector B times h.

77
00:04:59.860 --> 00:05:04.240
So we can also write it into scalar form where we have explicit sons over the,

78
00:05:04.260 --> 00:05:08.710
uh,
hidden units or the inputs and,

79
00:05:08.711 --> 00:05:09.810
uh,
and,

80
00:05:09.860 --> 00:05:14.380
and so we see that essentially the energy is going to be the sum of each of

81
00:05:14.381 --> 00:05:19.381
these terms here and to obtain a distribution or probability to obtain

82
00:05:21.221 --> 00:05:24.220
probabilities from this energy function.
Uh,

83
00:05:24.250 --> 00:05:28.060
well we'll do what's the most sensible thing to do,

84
00:05:28.240 --> 00:05:30.010
a much like in a physicist goal system,

85
00:05:30.011 --> 00:05:34.070
the probability of observing a particular configuration of our variable of

86
00:05:34.370 --> 00:05:39.370
interest is going to be the exponential of minus the energy associated with the

87
00:05:40.541 --> 00:05:42.700
value of x and h.

88
00:05:42.970 --> 00:05:46.510
And then we're going to divide by normalization constant or partition function,

89
00:05:47.020 --> 00:05:50.710
which unfortunately for restrictive Bolsa machine is actually in tractable.

90
00:05:50.980 --> 00:05:51.940
So this is that.

91
00:05:51.941 --> 00:05:56.941
Here is the sum of the numerator overall values of x and h and since x n h r a

92
00:05:59.480 --> 00:06:04.460
binary,
uh,
there's an exponential number of values that they can take.

93
00:06:04.880 --> 00:06:09.300
And so computing that partition function here in practice going to be,
uh,

94
00:06:09.380 --> 00:06:11.030
in the general case in tractable.

95
00:06:11.330 --> 00:06:16.220
So we have to correct for that problem and we'll see how we do that in the

96
00:06:16.280 --> 00:06:20.360
subsequent videos.
Now know this,
that,
uh,

97
00:06:20.390 --> 00:06:24.830
first of bias vector,
if a bias CK is negative,

98
00:06:25.720 --> 00:06:30.490
then it means that if x case equal to one,
that's going to,
uh,

99
00:06:30.650 --> 00:06:33.380
decrease this term.
So it's because we have a minus here,

100
00:06:33.381 --> 00:06:36.290
it's going to increase the energy and,
uh,

101
00:06:36.440 --> 00:06:39.440
since high energies associated with low probabilities,

102
00:06:39.680 --> 00:06:44.680
then having a bias vector that's negative means that we're are going to,

103
00:06:44.870 --> 00:06:49.490
um,
express a preference for ex gay,
uh,
not being equal to one.

104
00:06:49.491 --> 00:06:53.210
So instead of would prefer if it was zero,
uh,
and uh,

105
00:06:53.240 --> 00:06:58.080
the opposite would also be true.
If a CK was positive,
then we'll,
uh,

106
00:06:58.100 --> 00:07:01.970
this would mean that we'd actually prefer xk being equal to one then being

107
00:07:01.971 --> 00:07:05.300
called to zero just based on that term,
uh,

108
00:07:05.301 --> 00:07:10.301
the probability of x kb a one if CK is positive will actually be a greater,

109
00:07:12.290 --> 00:07:16.760
we have a similar thing with the biases be Jay here.
If they're positive,

110
00:07:16.761 --> 00:07:20.930
then it means we have a preference for HJ being or Taiwan if they're negative at

111
00:07:20.931 --> 00:07:24.290
zero.
And then the most important part of the model is really the,

112
00:07:24.291 --> 00:07:25.640
this connection Matrix here.

113
00:07:26.420 --> 00:07:31.420
So what each connection is going to model is our preference for both h j an xk

114
00:07:33.060 --> 00:07:36.530
being equal to a one.
So if it's negative,

115
00:07:36.560 --> 00:07:41.560
then it means that if h j or h and h k is equal to one,

116
00:07:42.620 --> 00:07:43.580
if this is negative,

117
00:07:43.581 --> 00:07:47.750
then it means that the probable the of observing this under a model is going to

118
00:07:47.751 --> 00:07:51.380
be decreased.
Whereas if either one was zero,
uh,

119
00:07:51.440 --> 00:07:54.380
then this would be preferred.
This would because either one,
if he,

120
00:07:54.560 --> 00:07:56.360
if he didn't want a zero,
then we multiply.

121
00:07:56.770 --> 00:07:59.900
Then this is zero times whichever value is here.

122
00:07:59.960 --> 00:08:04.280
So that's going to be associated with a lower energy.
And similarly,

123
00:08:04.281 --> 00:08:09.281
if this is positive that it means that when j n x gay is equal to one,

124
00:08:09.440 --> 00:08:12.950
then the energy is going to be decreased because of the minus here.

125
00:08:13.010 --> 00:08:15.440
So the probability is going to be greater.
Okay.

126
00:08:15.441 --> 00:08:20.240
So this is just a intuitive description of how these different parameters affect

127
00:08:20.510 --> 00:08:25.490
the probability of observing a particular configuration for the vector x and the

128
00:08:25.491 --> 00:08:30.380
vector h.
All right,
so that's for the definition or restricted Boltzmann machine.

129
00:08:30.500 --> 00:08:35.180
We have bias vectors.
We have a matrix of connections.
Uh,

130
00:08:35.210 --> 00:08:39.770
the part of the model that models the inputs that involves the variables

131
00:08:39.771 --> 00:08:42.350
corresponding to our input vectors.
A,

132
00:08:42.351 --> 00:08:46.910
we call it the visible layer visible because this is the data who actually see

133
00:08:46.911 --> 00:08:48.780
where has the hidden layer,
uh,

134
00:08:48.860 --> 00:08:52.130
which here is a random variable is a hidden,

135
00:08:52.131 --> 00:08:55.230
because we don't actually know for a given input vector x,

136
00:08:55.320 --> 00:08:58.880
what's the vector of hidden units?
This is a latent variable nar

137
00:08:59.030 --> 00:08:59.863
<v 1>model.</v>

138
00:09:02.630 --> 00:09:06.200
<v 0>We can represent that as a mark of network.
Um,</v>

139
00:09:06.201 --> 00:09:11.150
so what we had before was more a informal representation of what the model is.

140
00:09:11.151 --> 00:09:14.240
But if we use the mark of network illustration and a,

141
00:09:14.241 --> 00:09:18.590
if we assume that now our nodes are going to be the vectors x and h Dan,

142
00:09:18.591 --> 00:09:21.020
we just a have a,
uh,

143
00:09:21.080 --> 00:09:24.530
an edge between x and h because they're sharing a factor.

144
00:09:24.650 --> 00:09:28.700
So indeed p of x h is the exponential of minus the energy.

145
00:09:28.970 --> 00:09:32.930
And remember that minus the n or the energy is a den negative.

146
00:09:32.931 --> 00:09:35.990
Some of this term,
this term and this term.

147
00:09:36.230 --> 00:09:38.180
So the two minuses cancel out.

148
00:09:38.181 --> 00:09:41.480
So we get the exponential of that term plus that term.
What's that term?

149
00:09:42.020 --> 00:09:42.771
And because the,

150
00:09:42.771 --> 00:09:46.880
some of the exponential of this psalm is the product of exponentials,

151
00:09:46.881 --> 00:09:51.860
then we get this factor here at times this factor times this factor.

152
00:09:52.130 --> 00:09:52.950
Okay.

153
00:09:52.950 --> 00:09:53.350
<v 1>Yeah.</v>

154
00:09:53.350 --> 00:09:56.560
<v 0>So then what they should base on an energy function is really just an</v>

155
00:09:56.561 --> 00:10:01.150
alternative way of representing a product of factors,
uh,

156
00:10:01.180 --> 00:10:05.340
by just taking the exponential of minus the energy.
Uh,
all this,

157
00:10:05.380 --> 00:10:06.550
some terms here,

158
00:10:06.551 --> 00:10:11.400
they will translate into factors in the more factorial representation of the

159
00:10:11.401 --> 00:10:12.234
distribution.

160
00:10:13.710 --> 00:10:14.370
<v 1>Okay.</v>

161
00:10:14.370 --> 00:10:18.520
<v 0>A more representative,
a illustration of this would be to use,
uh,</v>

162
00:10:18.580 --> 00:10:21.970
a node for each scaler in our models.

163
00:10:21.971 --> 00:10:26.971
So for each scale or value in the vector x and each a component in our vector h

164
00:10:27.670 --> 00:10:30.430
and then in this case we see that,
uh,

165
00:10:30.460 --> 00:10:35.460
we must draw edges between each pair of units in the visible layer with an end,

166
00:10:37.570 --> 00:10:40.030
uh,
hidden there.
Uh,

167
00:10:40.031 --> 00:10:42.910
and that's because if we write it as a product of factors,

168
00:10:42.911 --> 00:10:47.900
we do get this pairwise factor here,
which involves both h j,

169
00:10:47.901 --> 00:10:52.240
an ex ex gay for all values of j and k.
And then the,

170
00:10:52.510 --> 00:10:56.590
this factor is parameterized by the entry in the Matrix w.
And similarly,

171
00:10:56.591 --> 00:10:58.510
we have these funerary factors here,

172
00:10:58.511 --> 00:11:02.770
which I express a preference individually for either x or h.

173
00:11:03.130 --> 00:11:06.640
And notice that here we don't have any interactions between either day hidden

174
00:11:06.641 --> 00:11:10.240
units when each other or the visible units with each other.

175
00:11:10.300 --> 00:11:13.810
And that's why we say it's a restricted Boltzmann machine.
Uh,

176
00:11:13.811 --> 00:11:18.811
we restricted the connectivity by allowing only connections between the visible

177
00:11:19.781 --> 00:11:21.100
and the hidden layers.

178
00:11:24.450 --> 00:11:25.111
And finally,

179
00:11:25.111 --> 00:11:29.340
if we instead look at the factor graph visualization of the restricted Boltzmann

180
00:11:29.341 --> 00:11:31.400
machine,
then of course for a,

181
00:11:31.410 --> 00:11:36.410
we get a factor for each pair wise connection between elements in the visible

182
00:11:38.401 --> 00:11:42.240
and hidden units,
or each of our pairwise factors are in the street up here.

183
00:11:42.241 --> 00:11:44.850
But it also have the urinary factors here,

184
00:11:44.851 --> 00:11:49.851
which involves just a single a scale or a variable in our model.

185
00:11:51.990 --> 00:11:52.141
Yeah.

186
00:11:52.141 --> 00:11:56.050
So that's just what we've gained from the fact they're graphic illustration is

187
00:11:56.051 --> 00:12:01.051
that we now explicitly see that we actually have,

188
00:12:01.930 --> 00:12:06.250
uh,
that we have a urinary factors for both h and x.

189
00:12:07.220 --> 00:12:10.960
All right.
So that's,
uh,
our definition of the restrictive bolsa machine.

190
00:12:11.140 --> 00:12:14.560
And in the subsequent video we'll see how to do France in that model and how to

191
00:12:14.561 --> 00:12:15.550
train it on some data.

