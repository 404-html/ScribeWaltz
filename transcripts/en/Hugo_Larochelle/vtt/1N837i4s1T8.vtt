WEBVTT

1
00:00:01.040 --> 00:00:05.360
In this video,
we'll look at and derive the gradient of the last function,
uh,

2
00:00:05.361 --> 00:00:09.870
with respect to the output layer of a neural network.
Now,

3
00:00:09.871 --> 00:00:14.220
in our derivation of stochastic gradient descent for neural network training,

4
00:00:14.400 --> 00:00:18.330
we are now at this part where we need a procedure for computing the parameter

5
00:00:18.331 --> 00:00:20.590
gradients.
This is actually a,

6
00:00:20.591 --> 00:00:24.570
so making the whole derivation of the gradients is,
uh,

7
00:00:24.750 --> 00:00:27.180
actually a complicated step.
So,
Eh,

8
00:00:27.210 --> 00:00:29.940
we'll take more than one video is to make the full duration.

9
00:00:30.000 --> 00:00:33.040
And in this video we'll first look at,
uh,

10
00:00:33.390 --> 00:00:37.890
the writing their gradients of the loss function at the output of the neural

11
00:00:37.891 --> 00:00:41.590
network.
All right,

12
00:00:42.400 --> 00:00:44.320
so will compute the partial dividends.

13
00:00:44.380 --> 00:00:47.780
And then what we'll do is that we'll combine the partial derivatives with

14
00:00:47.781 --> 00:00:50.410
respect to all output units,
uh,

15
00:00:50.411 --> 00:00:55.210
into a vector farms or to get the actual ingredient.
Now,

16
00:00:55.230 --> 00:00:56.920
uh,
this is our loss function here,

17
00:00:57.160 --> 00:01:00.910
the negative log likelihood for the true target.
Why?

18
00:01:01.240 --> 00:01:06.240
And now we're interested in what is the partial derivative with respect to DCF

19
00:01:06.700 --> 00:01:10.930
output.
Okay,
so in this illustration here,
um,

20
00:01:10.960 --> 00:01:15.370
the true class might be this neuron.
Uh,
and now we're looking at the partial,

21
00:01:15.371 --> 00:01:20.050
they're there,
which is back to the,
uh,
output of the neuron.
It's activation.

22
00:01:20.051 --> 00:01:24.700
So this is restricted by orange here on the upper part of the circle.

23
00:01:25.900 --> 00:01:30.780
And,
uh,
and this neurons see might be any of the output neuron.
They could be,

24
00:01:30.820 --> 00:01:33.190
could be the first one,
which would be in,

25
00:01:33.250 --> 00:01:36.730
if this was the label with the incorrect one or it could be the last one,

26
00:01:36.910 --> 00:01:39.970
which would be the correct one.
This made up example.

27
00:01:40.890 --> 00:01:41.723
<v 1>All right,</v>

28
00:01:42.460 --> 00:01:47.460
<v 0>so taking the partial liver div minus the log simply gives us minus one divided</v>

29
00:01:48.371 --> 00:01:53.080
by the quantity question f of x a y.

30
00:01:53.081 --> 00:01:55.150
So that's because there are different,

31
00:01:55.151 --> 00:02:00.030
the log is one over the input of the log.
And,

32
00:02:00.070 --> 00:02:05.070
but now notice that this here affects of why DYF output.

33
00:02:05.970 --> 00:02:08.410
It might be a different output than the CF output.

34
00:02:08.890 --> 00:02:13.300
So actually this might be a constant,
if c is not equal to y.
So for this reason,

35
00:02:13.510 --> 00:02:16.090
we also multiply by one,
uh,

36
00:02:16.120 --> 00:02:20.950
the indicator function that looks at whether y is equal to c and outputs one if

37
00:02:20.951 --> 00:02:23.110
y is equal to c and otherwise it outputs zero.

38
00:02:23.560 --> 00:02:28.000
So we'll get a partial liver zero if why is not,
see and otherwise it's going,

39
00:02:28.060 --> 00:02:32.320
this is going to be one.
So we'll get a partial derivative of one over f of x

40
00:02:32.470 --> 00:02:34.600
<v 1>of uh,
uh,
uh,
of why.</v>

41
00:02:36.500 --> 00:02:39.240
<v 0>And now if we put this into vector form to get the gradient,</v>

42
00:02:39.241 --> 00:02:43.700
so the full gradients of dislikes function with respect to output units,

43
00:02:44.150 --> 00:02:48.470
then we noticed that we have minus one divided by f of x of y.

44
00:02:48.710 --> 00:02:50.000
Uh,
no,

45
00:02:50.060 --> 00:02:54.080
no matter which output a neuron we are looking at,

46
00:02:54.110 --> 00:02:55.520
no matter what's the value of c.

47
00:02:55.880 --> 00:03:00.700
And the only thing that changes between different values of c is we have a one

48
00:03:00.701 --> 00:03:04.540
here or zero right or the indicator function outputs one or zero.

49
00:03:05.230 --> 00:03:09.550
So then we can factor this out and get a instead write this into vector form by

50
00:03:09.551 --> 00:03:13.760
constructing a vector which contains weather.
Uh,

51
00:03:13.810 --> 00:03:18.010
the indicator function before c equals zero c equals one.

52
00:03:18.011 --> 00:03:21.820
And so I'm up to c goals.
See my capitol c minus one.

53
00:03:21.850 --> 00:03:26.850
So I'll just assume at this point that I'm a indexing the labels from zero to

54
00:03:27.300 --> 00:03:29.350
uh,
the number of classes minus one.

55
00:03:30.220 --> 00:03:30.790
<v 1>Okay.</v>

56
00:03:30.790 --> 00:03:35.590
<v 0>And so this vector actually noted e of why,</v>

57
00:03:35.890 --> 00:03:40.480
uh,
we call it the a one hot vector or a one out of sea vector.

58
00:03:40.630 --> 00:03:45.460
So it's vector that has a bunch of zeroes except at a position and the position

59
00:03:45.520 --> 00:03:50.230
in,
uh,
and question here is why.
So the,
uh,

60
00:03:50.231 --> 00:03:55.000
at the position where we have y equals two,
its actual value.

61
00:03:55.001 --> 00:03:58.720
We'll get a one and otherwise it's going to be zero elsewhere.
Okay?

62
00:03:58.750 --> 00:04:03.430
So our gradient of the output is going to be minus the,

63
00:04:03.490 --> 00:04:04.180
uh,

64
00:04:04.180 --> 00:04:09.160
one heart vector for y divided by f of x of Y.

65
00:04:12.130 --> 00:04:12.850
<v 1>Okay.</v>

66
00:04:12.850 --> 00:04:16.630
<v 0>Now let's look at what's the uh,
gradient,</v>

67
00:04:16.690 --> 00:04:19.840
but for the pre activation at the output layer,

68
00:04:19.960 --> 00:04:24.960
so now we want the partial derivative of the loss function with respect to the

69
00:04:25.840 --> 00:04:28.900
CF pre activation at the output layer.

70
00:04:28.960 --> 00:04:33.280
So that's a albus us one of x,
and then the CF element of that factor.

71
00:04:34.420 --> 00:04:36.580
Now we'll look at the duration in the second,

72
00:04:36.581 --> 00:04:38.650
but it turns out it has a very simple form.

73
00:04:38.710 --> 00:04:43.710
It's just minus whether c is the true target or not,

74
00:04:44.770 --> 00:04:49.750
minus what's the probability that our neural network has assigned to Class C?

75
00:04:49.810 --> 00:04:54.670
So what's the CFG output at the output layer?
That's actually a very simple form,

76
00:04:55.080 --> 00:04:59.860
if only right into a vector form to get the gradient.
Well,

77
00:05:00.250 --> 00:05:04.360
uh,
this term in vector form would become the one heart factor.

78
00:05:05.020 --> 00:05:07.600
And then this term would just be a,

79
00:05:07.601 --> 00:05:11.680
would become the vector of probabilities.
So the output vector,

80
00:05:12.310 --> 00:05:16.800
ef bold of X.
Okay,
so this is actually a great Ian for the reactivation,

81
00:05:16.990 --> 00:05:20.110
which actually very,
very simple,
uh,
mathematically.

82
00:05:22.760 --> 00:05:26.540
Now we'll look at the derivation of this partial derivative.

83
00:05:27.530 --> 00:05:31.940
Uh,
so the,
uh,
I like doing this derivation because it's,

84
00:05:32.220 --> 00:05:36.050
uh,
uh,
if that's necessarily the result is a simple,

85
00:05:36.051 --> 00:05:39.640
but getting there is not as simple and it's uh,
uh,

86
00:05:39.710 --> 00:05:44.090
we'll go through a bunch of mathematical steps that are useful in general for

87
00:05:44.091 --> 00:05:47.720
understanding and working with neural networks.
Okay.

88
00:05:47.780 --> 00:05:50.630
So what we're interested in is computing this here.

89
00:05:52.530 --> 00:05:52.930
<v 1>Okay.</v>

90
00:05:52.930 --> 00:05:57.560
<v 0>Now let's first do the narrative with respect to minus log.</v>

91
00:05:58.070 --> 00:06:01.760
Uh,
so let's pass it.
They're the two minus log.

92
00:06:01.880 --> 00:06:05.540
So it means that we would get minus one divided by f of x,
y.

93
00:06:05.840 --> 00:06:09.320
So that's the derivative a of minus log of something.

94
00:06:09.890 --> 00:06:14.870
And now using the chain rule out where I have left to do is to multiply by the

95
00:06:15.020 --> 00:06:19.700
partial derivative of f of x,
y with respect to our pre activation.

96
00:06:20.260 --> 00:06:22.520
I guess that's just the application of the chain will

97
00:06:25.140 --> 00:06:29.730
now let's replace f of x,
y by uh,

98
00:06:29.790 --> 00:06:31.500
what it actually corresponds to.

99
00:06:31.530 --> 00:06:36.480
It's the softmax applied on the reactivation vector and we just look at the y

100
00:06:36.650 --> 00:06:38.120
<v 1>Alan of that factor.</v>

101
00:06:40.050 --> 00:06:44.340
<v 0>And that's actually replace a softmax with its definition.</v>

102
00:06:44.630 --> 00:06:49.200
We remember that soft Max is the exponentiate it a pre activations.

103
00:06:49.320 --> 00:06:50.730
And if we will look at the wife element,

104
00:06:50.760 --> 00:06:55.760
then we have the Wyeth reactivation normalize by dividing by four.

105
00:06:55.950 --> 00:06:59.250
All possible classes see prime.
So I'm using c prime here,

106
00:06:59.251 --> 00:07:02.790
four s as my index for summing over all classes,
uh,

107
00:07:02.890 --> 00:07:07.890
of d exponentiate a pre activations for all output units for all values of c

108
00:07:08.521 --> 00:07:09.354
prime.

109
00:07:10.010 --> 00:07:10.843
<v 1>Yeah.</v>

110
00:07:10.930 --> 00:07:13.870
<v 0>So I've just,
now we have this partial there live here to do.</v>

111
00:07:15.690 --> 00:07:17.010
Um,
so this is,
uh,

112
00:07:17.360 --> 00:07:22.360
we have the directive to do for a ratio of two function and now we remember that

113
00:07:22.471 --> 00:07:26.460
the partial derivative of the ratio of to function with respect to x is the

114
00:07:26.461 --> 00:07:31.461
partial derivative of g of x with respect to x times one over h of x minus g of

115
00:07:33.211 --> 00:07:38.010
x divided by h of x squared.
So the numerator divided by the denominator,

116
00:07:38.011 --> 00:07:41.640
but squared times,
uh,
the,
uh,

117
00:07:41.700 --> 00:07:45.770
partial derivative with respect to h of X.
Okay.

118
00:07:46.600 --> 00:07:48.370
So this is a fairly simple,

119
00:07:48.780 --> 00:07:50.210
<v 1>uh,
uh,</v>

120
00:07:50.640 --> 00:07:53.820
<v 0>fact about the partial there,
the,
with respect to a ratio.</v>

121
00:07:54.420 --> 00:07:58.770
So we'll in in our case,
g of x would be the numerator here.

122
00:07:59.190 --> 00:08:01.320
H of x would be the denominator here.

123
00:08:01.380 --> 00:08:06.380
An x would actually be deep reactivation for the CF output a unit.

124
00:08:08.610 --> 00:08:09.443
<v 1>Okay.</v>

125
00:08:09.570 --> 00:08:13.970
<v 0>So here I'm just replacing,
I'm using this formula here and uh,
uh,</v>

126
00:08:13.980 --> 00:08:16.930
so replacing the whole partial narrative,
uh,

127
00:08:16.980 --> 00:08:21.980
by this term here where I'm replaced g evacs h of x and x by the actual term.

128
00:08:24.260 --> 00:08:28.700
And so now we just have to do this partial they would have here and this partial

129
00:08:28.770 --> 00:08:33.440
derivative here.
All right,
well the partial,

130
00:08:33.441 --> 00:08:37.790
they live of the exponentiate id a why if pre activation,

131
00:08:37.940 --> 00:08:40.850
which respected,
uh,
the CFP activation,

132
00:08:41.180 --> 00:08:45.530
it's going to be zero if why is not equal to c and otherwise the partial,

133
00:08:45.531 --> 00:08:49.070
they were live with the exponential function is the exponential function.

134
00:08:49.100 --> 00:08:52.820
So we'd get the exponential of the YFP activation,

135
00:08:53.030 --> 00:08:57.870
which is exactly this term here.
Now if we do the partial derivative,

136
00:08:57.871 --> 00:09:02.871
which respected you see it output of the sum over all outputs of the

137
00:09:03.211 --> 00:09:07.530
exponentiate a pre activation.
So we see,
we think the exponential of all,

138
00:09:07.860 --> 00:09:12.780
uh,
the c prime a pre activations,

139
00:09:13.800 --> 00:09:18.540
well if c prime is not,
see in this some Dan,
this is,

140
00:09:18.570 --> 00:09:20.370
we're doing the derivative with respect to a constant.

141
00:09:20.371 --> 00:09:24.330
So the term is zero and otherwise for c prime equal to see,

142
00:09:24.690 --> 00:09:29.250
well then we add the derivative of the exponential,
which is the exponential.

143
00:09:29.670 --> 00:09:32.720
Okay.
So they're partially,
we give up this whole sun,
uh,

144
00:09:32.910 --> 00:09:37.860
reduces to just the exponential of the CF pre activation.
Okay.

145
00:09:37.861 --> 00:09:40.980
Because because it's the only term that's uh,
uh,
uh,

146
00:09:40.981 --> 00:09:45.780
that depends on the reactivation,
uh,
uh,
the CFP activation.

147
00:09:46.770 --> 00:09:51.060
And so the other thing that I've done is that I've taken this part here,

148
00:09:51.180 --> 00:09:55.950
which is squared.
I just wrote that as the denominator times itself.

149
00:09:56.280 --> 00:10:00.780
So I can actually separate that into a first fraction here,

150
00:10:00.900 --> 00:10:05.790
which takes essentially,
so let me remove some ink which takes,

151
00:10:05.820 --> 00:10:09.690
that's with one times the denominator.

152
00:10:10.140 --> 00:10:13.770
And then,
uh,
this other term is just a,

153
00:10:13.780 --> 00:10:17.110
we'll use the other,
um,
denominator there.

154
00:10:17.480 --> 00:10:18.960
The other part of this square here.

155
00:10:21.980 --> 00:10:25.220
Now let's use again the definition of soft Max.

156
00:10:25.221 --> 00:10:30.221
We actually noticed that this here is the why if,

157
00:10:30.820 --> 00:10:35.420
uh,
element of the softmax under p activation vector.
Uh,

158
00:10:35.550 --> 00:10:38.930
same thing can be said for this term here.

159
00:10:39.520 --> 00:10:42.800
And now this term is actually not the wife,

160
00:10:42.801 --> 00:10:47.150
but the sea of element of the softmax of their p activation.

161
00:10:48.490 --> 00:10:50.110
So I have this year here,

162
00:10:53.040 --> 00:10:55.940
and again,
we realize that while the sea itself,

163
00:10:56.010 --> 00:10:59.400
Max of the activation is just the outputs,
uh,

164
00:10:59.401 --> 00:11:03.020
so the activation of the output layer,
so that's f of X.
Second,

165
00:11:03.060 --> 00:11:05.820
replace the softmax by f of x here,
here,
and here,

166
00:11:06.990 --> 00:11:07.823
<v 1>okay.</v>

167
00:11:07.960 --> 00:11:12.700
<v 0>And now we know this,
that we're multiplying in front by minus one over F of x,</v>

168
00:11:12.701 --> 00:11:17.410
Y I have have a f f of X.
Well,
why here f of x,
y here is,

169
00:11:17.411 --> 00:11:21.400
I can actually cancel that with both these factors.

170
00:11:22.460 --> 00:11:23.190
<v 1>Okay?</v>

171
00:11:23.190 --> 00:11:25.620
<v 0>And so this gives me indeed what's left,</v>

172
00:11:25.621 --> 00:11:30.621
which is minus the indicator function of whether y is equal to c minus F of x a

173
00:11:32.670 --> 00:11:35.400
seat.
So the CFO,
uh,

174
00:11:35.460 --> 00:11:39.450
output value at the output of layer,
okay?

175
00:11:39.451 --> 00:11:44.451
So that's how you can derive the partial derivative of the loss function with

176
00:11:44.881 --> 00:11:49.320
respect to the pre activation for any output neuron.
See,

177
00:11:49.650 --> 00:11:54.030
and we see how that actually reduces by reusing the definition of the soft Max

178
00:11:54.180 --> 00:11:59.070
actually reduces into an expression that only depends on the output activation.

179
00:11:59.440 --> 00:12:01.080
So we get a very simple expression,

180
00:12:02.260 --> 00:12:02.310
<v 1>okay.</v>

