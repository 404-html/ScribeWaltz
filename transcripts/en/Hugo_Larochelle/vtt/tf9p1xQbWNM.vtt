WEBVTT

1
00:00:00.550 --> 00:00:04.270
In this video,
we'll look at the directives of different activation functions

2
00:00:06.370 --> 00:00:09.450
now or previously,
the rive,
the gradients of,
uh,

3
00:00:09.520 --> 00:00:13.600
the last function with respect to the pre activation vectors.
And,
uh,

4
00:00:13.601 --> 00:00:17.770
we saw that actually involved the narrative of the activation function.

5
00:00:17.771 --> 00:00:22.000
So let's look at what those are.
Four different choices of activation functions.

6
00:00:24.240 --> 00:00:28.720
If we're using a linear activation function,
then,
uh,

7
00:00:28.770 --> 00:00:32.310
the partial they live or did it,
there it is actually very simple.

8
00:00:32.311 --> 00:00:36.620
So if you take the derivative of Gfa,
which is back to a,
uh,
then,
uh,

9
00:00:36.660 --> 00:00:40.770
we easily see that it only corresponds to one.
So in this case,
actually,

10
00:00:41.010 --> 00:00:42.120
we just multiply by one.

11
00:00:42.121 --> 00:00:46.590
So we don't even have to take into consideration where we do our computations of

12
00:00:46.591 --> 00:00:50.430
the gradient of the reactivation function,
the reactivation of the neurons,

13
00:00:53.350 --> 00:00:57.280
more common choice as we've seen as the sigmoid activation function.

14
00:00:57.670 --> 00:00:59.410
And so it takes this form,

15
00:00:59.411 --> 00:01:04.411
it's one over one plus the exponential of minus the pre activation.

16
00:01:04.930 --> 00:01:05.081
Now,

17
00:01:05.081 --> 00:01:09.280
if we take the devil develop that with respect to the pre activation function,

18
00:01:09.370 --> 00:01:12.250
the reactivation,
sorry,
a a,

19
00:01:12.251 --> 00:01:15.130
then it turns out if we make the derivation that uh,

20
00:01:15.140 --> 00:01:17.290
it has a fairly simple form,

21
00:01:17.590 --> 00:01:21.580
it's simply the value of uh,
the activation function,

22
00:01:21.581 --> 00:01:26.080
the sigmoid times one minus the value of a,

23
00:01:26.081 --> 00:01:30.890
again,
a gov of the sigmoid of the reactivation.
And,

24
00:01:31.000 --> 00:01:35.070
and so it we see here that it makes sense indeed.
If we there,

25
00:01:35.080 --> 00:01:39.280
there would be the slope of the function and uh,

26
00:01:39.310 --> 00:01:44.310
we see that if either gov or a one minus g is equal to zero,

27
00:01:44.980 --> 00:01:48.550
that is if Giovanni zero or if [inaudible] is equal to one,

28
00:01:48.580 --> 00:01:52.630
in which case we have one minus one.
So we get zero.
So,

29
00:01:53.020 --> 00:01:56.080
uh,
then we'd get a derivative of zero.

30
00:01:56.560 --> 00:02:01.560
And so we can only get gov equal zero if the reactivation is uh,

31
00:02:02.390 --> 00:02:04.050
as a minus,
uh,

32
00:02:04.120 --> 00:02:07.660
is close to minus infinite and a or is minus infinite,

33
00:02:07.661 --> 00:02:11.620
and then Jay is going to be equal to one if a is equal to infinite.

34
00:02:12.250 --> 00:02:17.250
And that indeed we see that as we increase the value of the pre activation,

35
00:02:17.530 --> 00:02:22.510
as we decrease it,
then we see that the slope is,
is becoming,

36
00:02:22.530 --> 00:02:25.870
uh,
the shape of the curve is becoming flatter and flatter and flatter.

37
00:02:26.020 --> 00:02:29.260
So indeed we see that the gradient,
the derivative,

38
00:02:29.290 --> 00:02:34.290
as we increase the activation or decrease it becomes closer to zero.

39
00:02:35.620 --> 00:02:38.800
And so this actually means that when we compute the gradients,

40
00:02:38.980 --> 00:02:42.640
we'd be multiplying by zero.
If the,
uh,

41
00:02:42.670 --> 00:02:45.490
a hidden unit has its pre activation,

42
00:02:45.750 --> 00:02:50.250
a very large and either a negative or positive.
So,
uh,
the grain,

43
00:02:50.251 --> 00:02:54.120
it is going to be close to zero.
If a hidden unit is a saturated,

44
00:02:54.190 --> 00:02:59.030
we'll say that the value is saturated because it's close to one.
If it's a,
uh,

45
00:02:59.230 --> 00:03:02.470
pos potential value add,
uh,
uh,
in its range.

46
00:03:03.310 --> 00:03:05.800
So this means that,
uh,

47
00:03:05.801 --> 00:03:10.450
actually training a neural network when hidden inside saturated is hard.

48
00:03:10.451 --> 00:03:13.240
And that's because the gradients are going to be very,
very small.

49
00:03:13.241 --> 00:03:16.780
And so we'll get very little feedback.
The,
uh,
uh,

50
00:03:16.781 --> 00:03:18.830
hidden units that are below that hidden,

51
00:03:18.860 --> 00:03:22.480
it's all going to get very little feedback from that,
a hidden unit.

52
00:03:22.810 --> 00:03:26.620
Because it's essentially going to propagate a partial derivative of zero

53
00:03:29.020 --> 00:03:34.020
and other activation function that is popular is the tension or a hyperbolic

54
00:03:34.121 --> 00:03:38.950
tangent.
Uh,
it has this form here that we've seen,

55
00:03:38.951 --> 00:03:43.570
or we can write it this way.
And again,
from a derived the a narrative,

56
00:03:43.930 --> 00:03:48.930
we again can write it in a fairly simple form is just one minus the activation.

57
00:03:50.380 --> 00:03:54.370
So the activation function applying on the reactivation square,
and again,

58
00:03:54.371 --> 00:03:58.030
we get the same behavior.
Uh,
if,
uh,

59
00:03:58.060 --> 00:04:00.910
the neuron is close to it's saturated value,

60
00:04:00.911 --> 00:04:03.910
which is either minus one or one,

61
00:04:04.150 --> 00:04:08.380
then this term will be squared would be one.
So he had one minus one.

62
00:04:08.590 --> 00:04:13.270
So we get a derivative of zero.
So again,
if this neuron is saturated,

63
00:04:13.540 --> 00:04:16.090
then a,
when we propagate the greatest,

64
00:04:16.091 --> 00:04:20.350
when you compute the gradients with respect to the hidden layers below,

65
00:04:20.440 --> 00:04:24.580
it would actually be multiplied.
Uh,
the,
the partial,
then that goes through,

66
00:04:24.581 --> 00:04:29.470
that neuron will be multiplied by zero if the neuron is saturated.
Okay?

67
00:04:29.471 --> 00:04:31.010
So those are the different,
uh,

68
00:04:31.200 --> 00:04:34.480
there this for the different activation functions that we have,

69
00:04:34.481 --> 00:04:35.740
the news in neural networks.

