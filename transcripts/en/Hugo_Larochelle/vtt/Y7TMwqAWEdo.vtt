WEBVTT

1
00:00:00.690 --> 00:00:00.870
Okay.

2
00:00:00.870 --> 00:00:04.560
<v 1>In this video,
we'll describe the operation of a discrete convolution,</v>

3
00:00:04.620 --> 00:00:08.640
which will be used for computing before propagation in the conditional neural

4
00:00:08.641 --> 00:00:09.474
network.

5
00:00:11.230 --> 00:00:12.000
<v 0>Okay.</v>

6
00:00:12.000 --> 00:00:13.830
<v 1>So I mentioned before that,
uh,</v>

7
00:00:13.860 --> 00:00:16.800
when we computing the reactivation of a hidden unit,

8
00:00:17.190 --> 00:00:19.430
what would be doing is that,
uh,

9
00:00:19.470 --> 00:00:22.200
each in the end would be segmented into different feature maps.

10
00:00:22.500 --> 00:00:27.000
And then for each feature map I mentioned that the last video that will actually

11
00:00:27.001 --> 00:00:30.420
perform a discrete come pollution operation.
Uh,
this,

12
00:00:30.421 --> 00:00:35.421
this week convolution operation would actually give me the pre activation that

13
00:00:35.551 --> 00:00:39.690
comes from each of the IAF,
uh,
channel.

14
00:00:40.110 --> 00:00:43.560
So performing this with correspond to a,
for each hidden units,

15
00:00:43.860 --> 00:00:47.370
multiplying it's weights with the it's receptive field,

16
00:00:47.371 --> 00:00:51.630
and then summing the accumulated p activation and then with some,

17
00:00:51.631 --> 00:00:54.720
also over our possible input channels.

18
00:00:55.290 --> 00:00:59.010
So that would give us the P activations of all the hidden units in the same

19
00:00:59.011 --> 00:01:02.640
feature map,
and then we could apply some nonlinearity to get an activation.

20
00:01:03.720 --> 00:01:04.800
And so now in this video,

21
00:01:04.801 --> 00:01:09.210
we'll look at these specific definition of this convolution operation and how it

22
00:01:09.211 --> 00:01:14.211
relates to a computing reactivations in neuro and the feed forward neural

23
00:01:14.371 --> 00:01:15.204
network.

24
00:01:17.260 --> 00:01:18.040
<v 0>Okay.</v>

25
00:01:18.040 --> 00:01:20.810
<v 1>All right.
So let x be some input image.</v>

26
00:01:20.960 --> 00:01:25.960
So it's going to be a matrix and a k is going to be a colonel and it's also

27
00:01:27.261 --> 00:01:28.160
going to be a matrix.

28
00:01:28.460 --> 00:01:33.080
So now this year I'm not using my usual dictation of using capital letters and

29
00:01:33.081 --> 00:01:36.680
bold to uh,
uh,
refered to matrices.

30
00:01:36.860 --> 00:01:40.070
That's just because I'm trying to stick with the notation of Jared at all in

31
00:01:40.071 --> 00:01:40.904
their paper.

32
00:01:41.000 --> 00:01:45.300
I just want to sort of look back at the original paper and be able to,
uh,

33
00:01:45.620 --> 00:01:48.350
relate the notation I'm using with,
there's a,

34
00:01:48.351 --> 00:01:49.790
but normally I would use capital letters,

35
00:01:49.791 --> 00:01:52.820
so just don't forget that this is a matrix and that's a matrix.

36
00:01:53.690 --> 00:01:58.190
So if you performing the district convolution of that image x with some colonel

37
00:01:58.191 --> 00:02:01.850
came,
uh,
we will perform the following computation.

38
00:02:02.570 --> 00:02:07.570
So the convolution of x with k that's going to yield a matrix as well and image

39
00:02:09.530 --> 00:02:14.530
and at position I j so to the AIA throw and the GF column in that Matrix will

40
00:02:16.640 --> 00:02:17.930
have the following value.

41
00:02:18.740 --> 00:02:23.740
So what we'll do is that will sum over all offset possibles possible where the

42
00:02:24.231 --> 00:02:29.231
opposite p and Q range between zero and r minus one,

43
00:02:29.810 --> 00:02:33.440
where r is the number of rows and the number of columns of gate.

44
00:02:33.441 --> 00:02:35.660
So we're assuming the kernel is a square matrix.

45
00:02:36.290 --> 00:02:41.290
So for all possible offsets all combinations of these numbers here we'll look at

46
00:02:41.961 --> 00:02:45.890
the Pixel at position I and J,

47
00:02:45.891 --> 00:02:48.260
but offset it by p and Q.

48
00:02:48.500 --> 00:02:53.500
And we multiply that by the element in the kernel matrix at upset p and Q.

49
00:02:54.800 --> 00:02:59.800
But where now the offset is taken with respect to the bottom right corner of the

50
00:03:00.071 --> 00:03:04.900
matrix.
So if I have a matrix for my criminal matrix,

51
00:03:05.320 --> 00:03:08.600
which is a three by three and say hi,

52
00:03:08.620 --> 00:03:12.910
have a p equal one and Q equals zero,

53
00:03:13.690 --> 00:03:17.440
then it means that the elements,
uh,

54
00:03:17.740 --> 00:03:22.150
K R minus P and r minus Q,
where are always three p's and q's,

55
00:03:22.160 --> 00:03:27.020
zero with Curtis,
fun to,
uh,
essentially counting,
uh,

56
00:03:27.070 --> 00:03:32.050
and uh,
relatively to the bottom,
uh,
right element of that Matrix.

57
00:03:32.440 --> 00:03:36.730
So I would be at row one.
That's uh,
sorry,

58
00:03:36.731 --> 00:03:41.710
it's the second row.
So that's uh,
r which is three minus one,
which is one.

59
00:03:41.711 --> 00:03:46.300
So that's two.
And then I would be at our three minus Q zero.

60
00:03:46.301 --> 00:03:51.220
So at the third column,
so this would be that element.

61
00:03:51.280 --> 00:03:55.270
So upset one zero instead of being,
uh,

62
00:03:55.300 --> 00:04:00.300
this element here where counting if we're counting relatively to the top left

63
00:04:00.581 --> 00:04:05.480
corner like we would normally do when we accessing elements in the colonel were

64
00:04:06.190 --> 00:04:10.140
counting a relatively to the bottom left corner of the matrix.

65
00:04:10.600 --> 00:04:13.720
In this particular example,
we would eat this.
So in this,

66
00:04:13.721 --> 00:04:18.721
some were doing over all possible offsets where we go from zero to a two,

67
00:04:19.451 --> 00:04:23.110
four articles,
three and two would be zero one or two.

68
00:04:23.111 --> 00:04:25.960
And we take all combinations.
And so in each case,

69
00:04:25.961 --> 00:04:30.961
when multiplied the element that I've said P and Q at position I and j and d

70
00:04:31.211 --> 00:04:35.980
image and multiply it by the corresponding element in the kernel matrix.

71
00:04:37.420 --> 00:04:39.790
So that's for the mathematical definition.

72
00:04:39.850 --> 00:04:43.950
Now let's look at visually what this computation corresponds to and it's going

73
00:04:43.951 --> 00:04:45.790
to be much more intuitive once we do that.

74
00:04:46.690 --> 00:04:51.690
So we can think of this operation as first taking the Colonel Matrix.

75
00:04:52.600 --> 00:04:56.530
Kay.
So here's an example of a kernel matrix,
two by two Colonel Matrix.

76
00:04:57.070 --> 00:05:01.600
And then we will do is that we'll flip the rows and the columns.
Uh,

77
00:05:01.601 --> 00:05:04.750
what would this we'll do is we'll make everything more intuitive where we won't

78
00:05:04.751 --> 00:05:06.360
have to uh,
uh,

79
00:05:06.550 --> 00:05:10.270
compute the offset with respect to the bottom right corner,

80
00:05:10.271 --> 00:05:12.460
but instead will completely offset the prospective.

81
00:05:12.461 --> 00:05:15.670
That top left corner of that flipped matrix.

82
00:05:16.000 --> 00:05:19.420
So this matrix,
so I'm going to use still the four,

83
00:05:19.421 --> 00:05:22.810
like the operation of flipping the rows and columns.
It would be,

84
00:05:22.811 --> 00:05:24.730
it would correspond to that Matrix here.

85
00:05:24.890 --> 00:05:29.890
So we see we've switch the rows and we've switched the columns with respect to

86
00:05:31.581 --> 00:05:34.090
that matrix here.
Okay.

87
00:05:34.690 --> 00:05:37.300
So once we have this Kate tilled,

88
00:05:37.510 --> 00:05:41.290
what we do is that for each position where we can fit that kernel,

89
00:05:41.560 --> 00:05:46.010
we do an element wise multiplication of all elements with the,
uh,

90
00:05:46.090 --> 00:05:49.810
part of the image that is at the current,
uh,

91
00:05:49.811 --> 00:05:53.920
at any given position.
So in this example,
we're starting at the top left corner.

92
00:05:53.921 --> 00:05:57.220
So I Jay,
this would be dead for the elements.

93
00:05:57.410 --> 00:06:01.460
I equal one j equals one.
So we do see this,

94
00:06:01.461 --> 00:06:03.590
this element wise multiplication,
and then we suck.

95
00:06:04.010 --> 00:06:09.010
So we have one times zero plus 0.5 times 80

96
00:06:10.760 --> 00:06:13.250
plus two point 25 times time,

97
00:06:13.340 --> 00:06:17.830
times 20 and plus zero times 40.

98
00:06:18.680 --> 00:06:22.370
She's here.
So that gives us 45

99
00:06:23.900 --> 00:06:27.080
and then we continue this but for all values of ing.

100
00:06:27.081 --> 00:06:31.520
So for our positions where we can position the top left,
uh,

101
00:06:31.550 --> 00:06:35.180
element of the kernel matrix in the image.

102
00:06:37.110 --> 00:06:41.250
So then we perform the same operation,
element wise,
multiplication,

103
00:06:41.280 --> 00:06:45.600
and then some summation of about multiplication here,
here,
and here.

104
00:06:46.080 --> 00:06:51.080
So all of these terms and then summation of all these four values.

105
00:06:52.110 --> 00:06:52.531
In this case,

106
00:06:52.531 --> 00:06:57.531
this gives 110 and we continue like this for all positions.

107
00:06:58.530 --> 00:07:03.530
And that gives us the convolution of k with this three by three image here that

108
00:07:04.741 --> 00:07:06.240
I made up for this example.

109
00:07:07.290 --> 00:07:07.890
<v 0>Okay.</v>

110
00:07:07.890 --> 00:07:11.910
<v 1>And so notice that here I'm using a,
uh,
some sort of great color coding,</v>

111
00:07:12.550 --> 00:07:15.420
uh,
uh,
illustration,
uh,

112
00:07:15.440 --> 00:07:19.270
to sort of illustrate the intensity of the,
uh,

113
00:07:19.440 --> 00:07:23.240
resulting image.
So we can sort of think of it that is visualize eating it,

114
00:07:23.250 --> 00:07:26.310
that's uh,
uh,
as an actual image.

115
00:07:29.050 --> 00:07:31.210
So we know this,
that this operation,

116
00:07:31.211 --> 00:07:35.860
this convolution operations where we're taking a kernel and we're placing it in

117
00:07:35.861 --> 00:07:40.400
all positions in the image.
And then we're doing these elements wise,
uh,

118
00:07:40.710 --> 00:07:44.920
uh,
multiplications with a sum that's very similar to the types of operations we

119
00:07:44.921 --> 00:07:49.080
have to do.
When will computing reactivation and a feet forward conclusion,

120
00:07:49.081 --> 00:07:54.081
all neural network and a in turns out that indeed we can convert a,

121
00:07:54.790 --> 00:07:59.440
what I described as the way we compute the activations using this,
uh,

122
00:07:59.500 --> 00:08:04.030
with a convolutional neural network kind of connectivity with the layers,

123
00:08:04.390 --> 00:08:09.010
uh,
using a convolutional,
uh,
operation.
So,

124
00:08:09.700 --> 00:08:11.170
uh,
let's,
uh,

125
00:08:11.171 --> 00:08:16.171
so actually the pre activations that are going to be computed from channel XII

126
00:08:17.290 --> 00:08:21.580
in order to compute the feature map.
Why Jay?
Where,
uh,

127
00:08:21.610 --> 00:08:24.800
why'd you would be after the nonlinear already here?
Uh,

128
00:08:24.910 --> 00:08:29.620
I could compute that part of the p activation by just taking a,

129
00:08:29.630 --> 00:08:31.450
deriving a convolution kernel,

130
00:08:31.480 --> 00:08:36.070
which is just going to be my connection Matrix Wij Mike and boosting all neural

131
00:08:36.071 --> 00:08:39.730
network and then flipping it's columns and rows.
Okay.

132
00:08:39.850 --> 00:08:44.490
And that will become my conversion or kernel so that when I'm applying the

133
00:08:44.750 --> 00:08:49.360
discrete convolution,
then,
uh,
as we've seen in the description I gave before,

134
00:08:49.361 --> 00:08:52.390
I'm going to be re flipping the rows and columns.
So I'll,
I'll,

135
00:08:52.660 --> 00:08:57.660
I'll get back to the actual matrix w so from that colonel then to get my p

136
00:08:58.561 --> 00:09:03.030
activations that are computed from the IAFF a channel,

137
00:09:03.060 --> 00:09:06.760
I just perform a convolution between the IIF channels.

138
00:09:06.761 --> 00:09:08.220
So this slides in this image,

139
00:09:08.221 --> 00:09:13.221
if I have multiple channels with the kernel k I checked and then to compute all

140
00:09:14.581 --> 00:09:15.570
of why,

141
00:09:15.780 --> 00:09:20.780
then I would sum the result of these convolutions across the input channels.

142
00:09:21.120 --> 00:09:23.300
And then I'd apply some,
none of them the already in it.

143
00:09:23.301 --> 00:09:28.301
I would give me the activations of that particular feature map and a to get all

144
00:09:28.921 --> 00:09:32.250
the feature maps that I would do this for all values of Jay's,

145
00:09:32.251 --> 00:09:34.950
for all feature maps.
I have in my neuro network.

146
00:09:36.030 --> 00:09:36.470
<v 0>Okay.</v>

147
00:09:36.470 --> 00:09:41.400
<v 1>All right.
So,
um,
it might seem complicated that we would,
uh,
you know,</v>

148
00:09:41.610 --> 00:09:45.750
after this operation where have to flip columns,
like I said,
uh,
cons and rose,

149
00:09:45.870 --> 00:09:47.340
like I said,
we're,
uh,

150
00:09:47.540 --> 00:09:50.250
one thing I mentioned this connection because that's where the name

151
00:09:50.251 --> 00:09:51.930
convolutional neural network comes from.

152
00:09:52.170 --> 00:09:55.770
And also because you might have access to a library that computes convolutions

153
00:09:55.771 --> 00:09:58.710
very efficiently,
in which case you want to do this transformation.

154
00:09:59.610 --> 00:10:03.540
There's another type of signal processing operation known as the correlation,

155
00:10:03.541 --> 00:10:07.680
which actually corresponds to doing the same essentially what I've described

156
00:10:07.681 --> 00:10:11.960
before,
but without flipping rows in columns.
So it would be exactly that,

157
00:10:11.961 --> 00:10:16.320
the quick correlation between x and my matrix of connections.

158
00:10:16.800 --> 00:10:19.890
And um,
uh,
so there's a relationship here again,

159
00:10:19.891 --> 00:10:23.550
between the discrete correlation and uh,
the discrete convolution,

160
00:10:23.551 --> 00:10:27.480
which I won't get more into besides just saying that it's essentially the same

161
00:10:27.481 --> 00:10:31.170
thing without the row and column flipping before we do the element wise

162
00:10:31.171 --> 00:10:35.040
multiplications with [inaudible] at every position where we placed a colonel.

163
00:10:36.210 --> 00:10:38.910
But so from this,
it means that now we have,
uh,

164
00:10:39.030 --> 00:10:42.660
we have some efficient code that do,
uh,
uh,
convolutions.

165
00:10:42.690 --> 00:10:47.690
Then we can compute all of my pre activations and compute all my feature maps,

166
00:10:48.180 --> 00:10:49.740
uh,
fairly efficiently.

167
00:10:52.210 --> 00:10:53.043
<v 0>Okay.</v>

168
00:10:53.070 --> 00:10:57.950
<v 1>So,
uh,
to get an idea of the kind of,
uh,
the,
the result of applying,
uh,</v>

169
00:10:57.960 --> 00:11:02.370
these discrete convolutions or computing reactivations between,
uh,

170
00:11:02.371 --> 00:11:06.880
some inputs,
uh,
channel for some GF,

171
00:11:07.360 --> 00:11:09.500
uh,
uh,
feature maps.

172
00:11:09.501 --> 00:11:14.501
So imagine that I have a matrix Wij which corresponds to this matrix here where

173
00:11:14.550 --> 00:11:19.230
have zeros on the diagonal and 0.5 on the,
uh,
other elements.

174
00:11:19.440 --> 00:11:20.161
So in this case,

175
00:11:20.161 --> 00:11:24.210
it simplifies everything because flipping rows in columns is exactly equivalent.

176
00:11:24.450 --> 00:11:25.770
It gives the same matrix.

177
00:11:26.460 --> 00:11:29.160
But if I were to do this discrete convolution on this image,

178
00:11:29.400 --> 00:11:34.350
then I would get this contribution to the p activations of that particular

179
00:11:34.351 --> 00:11:38.390
feature map.
And we see that we have a fairly high value,
uh,

180
00:11:38.520 --> 00:11:43.520
at places that curse fund to regions of the original image where I had a pattern

181
00:11:45.271 --> 00:11:48.540
that is similar.
So like this with a kind of a diagonal edge.

182
00:11:48.540 --> 00:11:53.540
So we have two 55 that corresponds to this super position here up the uh,

183
00:11:54.461 --> 00:11:58.600
up the filter or the kernel and two 50 dive again,

184
00:11:59.050 --> 00:12:03.100
uh,
where I had this value here.
And so,

185
00:12:03.370 --> 00:12:08.280
uh,
uh,
I see that once I've computed this,
I already have some uh,

186
00:12:08.320 --> 00:12:13.270
illustration of where do this particular kind of feature with an edge,

187
00:12:13.390 --> 00:12:16.780
uh,
with that angle is present in the image.

188
00:12:17.770 --> 00:12:22.030
And then the idea is that with a nonlinearity that we have in commercial neural

189
00:12:22.031 --> 00:12:26.200
network that we might be able to emphasize,
uh,
uh,

190
00:12:26.260 --> 00:12:31.180
whenever this a value of correspondence between the filter and a particular

191
00:12:31.181 --> 00:12:35.590
region of the image.
So emphasize when it reaches a certain threshold.

192
00:12:36.010 --> 00:12:38.980
So in particular,
if I were to apply this element wise,

193
00:12:38.981 --> 00:12:42.670
none in the red on the feature map,
uh,
pre activation,

194
00:12:42.850 --> 00:12:47.830
assuming we have only one input channel.
Uh,
so Francis by multiplying 0.02,

195
00:12:48.190 --> 00:12:51.430
the result of the conclusion and subtracting four in passing this through a

196
00:12:51.431 --> 00:12:56.431
sigmoid dad get fairly close to one values and the part of the image that has

197
00:12:57.371 --> 00:13:01.120
this particular kind of pattern and everywhere else,
something closer to zero.

198
00:13:01.690 --> 00:13:06.550
And that's the kind of a feature detection that a convolutional a hidden layer

199
00:13:06.551 --> 00:13:08.380
is going to do.
Uh,

200
00:13:08.410 --> 00:13:12.900
it's going to be able to detect certain type of features.
And,
and the,
uh,

201
00:13:12.940 --> 00:13:14.350
in a feature map,
again,

202
00:13:14.351 --> 00:13:18.160
we will have the same feature of being detected everywhere in the image.

203
00:13:20.360 --> 00:13:21.193
<v 0>Yeah.</v>

204
00:13:21.470 --> 00:13:26.330
<v 1>Final notion related to convolution is the idea of a zero panning.</v>

205
00:13:26.550 --> 00:13:30.080
Uh,
it's going to be useful in particular when we talk about computing the

206
00:13:30.590 --> 00:13:31.423
gradients,

207
00:13:31.610 --> 00:13:36.500
when we're going to be deriving the procedure for a computer ingredients with

208
00:13:36.501 --> 00:13:39.020
respect to some last ones,
the classification,

209
00:13:39.340 --> 00:13:43.610
a lost that we've been using in regular neural networks.
So,
uh,

210
00:13:43.870 --> 00:13:44.703
so essentially,
uh,

211
00:13:44.780 --> 00:13:48.340
a procedure for back propagating gradients across the conclusion on their own

212
00:13:48.341 --> 00:13:49.174
network.

213
00:13:49.790 --> 00:13:54.170
Zero panning is when we allow to go over the borders by essentially panning

214
00:13:54.171 --> 00:13:59.171
Zeros everywhere around the image enough so that we can fit our filter,

215
00:14:01.540 --> 00:14:06.420
uh,
everywhere as long as there's at least one element from the filter that,
uh,

216
00:14:06.480 --> 00:14:10.970
or the kernel that overlaps with the original image.
So in this case,

217
00:14:11.260 --> 00:14:14.630
uh,
by adding,
if I add zeros here,

218
00:14:14.680 --> 00:14:18.350
it's a one row of zero and I have a filter which is the same as before,

219
00:14:18.351 --> 00:14:23.060
which is just two by two.
And this way I'm able to fit the filter,

220
00:14:23.410 --> 00:14:26.930
uh,
in the position where it overlaps with just one element here.

221
00:14:27.410 --> 00:14:32.000
The other types of padding,
we can do a kind of reflective padding where,
uh,

222
00:14:32.001 --> 00:14:36.250
we would act as if there were like a mirror here.
And then,
uh,
uh,

223
00:14:36.270 --> 00:14:39.170
so if the value here would be two 55,

224
00:14:39.171 --> 00:14:43.490
and the value if we're to a pad here would be zero because it's a zero here.

225
00:14:43.640 --> 00:14:45.090
So I sorta symmetric,
uh,

226
00:14:45.110 --> 00:14:49.160
kinds of padding a zero padding is probably the one that's most used in

227
00:14:49.161 --> 00:14:50.900
convolutional neural networks.
And,

228
00:14:51.500 --> 00:14:54.650
and that's the one I'm going to be referring to whenever I'm adding this.

229
00:14:54.710 --> 00:14:58.940
Underline a notation here with a convolution operation.

230
00:14:59.210 --> 00:15:02.060
That's the one that we're going to involve in particular when we talk about

231
00:15:02.061 --> 00:15:06.440
great guns.
So if we do the convolution with the same granolas before,

232
00:15:06.441 --> 00:15:10.220
but with zero panning that we get this,
uh,
uh,

233
00:15:10.250 --> 00:15:12.920
convolutional output in stem.
And we know this,

234
00:15:12.921 --> 00:15:15.620
that because we were able to fit the colonel in more positions,

235
00:15:15.621 --> 00:15:19.810
now we get a larger,
uh,
larger,
uh,
output,

236
00:15:19.820 --> 00:15:23.660
the larger matrix after the contribution.
I guess.

237
00:15:23.690 --> 00:15:25.460
So that's it for this convolutions.

