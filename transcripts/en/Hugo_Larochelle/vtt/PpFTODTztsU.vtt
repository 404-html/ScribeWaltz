WEBVTT

1
00:00:00.130 --> 00:00:00.560
Yeah.

2
00:00:00.560 --> 00:00:04.050
<v 1>In this video we'll define the loss function we'll be using for training are all</v>

3
00:00:04.060 --> 00:00:08.750
networks.
So we've seen the stochastic gradient descent algorithm.

4
00:00:08.770 --> 00:00:13.040
So in this video we're talking about defining this loss function here that we

5
00:00:13.041 --> 00:00:16.940
need,
uh,
as an ingredient for performing stochastic.
Great.
In the sense.

6
00:00:18.490 --> 00:00:19.323
<v 0>Okay.</v>

7
00:00:19.860 --> 00:00:23.900
<v 1>Um,
so we know that a neural network for classification,</v>

8
00:00:24.440 --> 00:00:28.070
we can think of the output layer as giving us an estimate,
uh,

9
00:00:28.140 --> 00:00:32.090
of a given some input.
Uh,

10
00:00:32.091 --> 00:00:34.790
what is the probability that it belongs to class?

11
00:00:34.791 --> 00:00:37.040
See what is the property that the,
uh,

12
00:00:37.050 --> 00:00:42.050
why the target associated with x is actually equal to some given class c?

13
00:00:43.040 --> 00:00:46.340
So F of x,
a capital f of x,

14
00:00:46.370 --> 00:00:49.910
which is the output layer of our neural network.
That's a vector.

15
00:00:50.150 --> 00:00:55.150
And so the CFO element would give us the probability that x belongs to the CF

16
00:00:55.711 --> 00:01:00.140
class and the whole vector gives us the whole distribution conditional

17
00:01:00.141 --> 00:01:04.810
distribution of what the target could be given x,
uh,

18
00:01:04.880 --> 00:01:06.500
as estimated by the neural network.

19
00:01:07.430 --> 00:01:12.430
So something I would be fairly intuitive would be to maximize the probability of

20
00:01:12.681 --> 00:01:17.000
the correct target yt given some given,

21
00:01:17.090 --> 00:01:19.770
uh,
input XD from our training center.

22
00:01:20.930 --> 00:01:25.580
Now we're framing our training as a minimization problem that as a maximisation

23
00:01:25.581 --> 00:01:27.320
problem.
So we'll convert,

24
00:01:27.440 --> 00:01:31.970
there's problem maximizing the probabilities into a minimization by using what

25
00:01:31.971 --> 00:01:34.670
is known as the negative log likelihood.

26
00:01:35.240 --> 00:01:37.400
So the negative log likelihood is a,

27
00:01:37.401 --> 00:01:42.140
so l of f of x and y where x is the input and wise,

28
00:01:42.170 --> 00:01:47.170
the true target is going to be minus the log and it's going to be the natural

29
00:01:47.631 --> 00:01:51.560
log.
I'll use still log even though sometimes we write ln four,
the natural log.

30
00:01:51.980 --> 00:01:53.540
So it's going to be minus log.

31
00:01:54.020 --> 00:01:58.400
The YF element of my output layer.

32
00:01:58.870 --> 00:02:02.330
Why is,
because this is why is this is the why here.

33
00:02:02.331 --> 00:02:04.550
So this is the actual true target.

34
00:02:04.760 --> 00:02:09.760
So I'm going to minimize minus log of the white of the output layer,

35
00:02:10.491 --> 00:02:15.491
which is the estimate of x belonging to a label.

36
00:02:15.741 --> 00:02:19.040
Why?
Um,
so,
uh,

37
00:02:19.070 --> 00:02:21.490
because log is um,

38
00:02:21.990 --> 00:02:26.930
sort of relationship between maximizing the probability and minimizing the lug.

39
00:02:27.090 --> 00:02:31.760
My,
the negative log prop is that,
uh,
maximizing the,

40
00:02:31.790 --> 00:02:35.930
maximizing something is the same thing as maximizing the lug of something

41
00:02:35.931 --> 00:02:39.740
because log is a monotonically increasing function.

42
00:02:40.130 --> 00:02:45.130
And then maximizing something is the same thing as minimizing the negative of

43
00:02:46.430 --> 00:02:47.480
something.
Okay.

44
00:02:47.481 --> 00:02:52.481
So these two problems are actually equivalent and other way of writing the

45
00:02:53.041 --> 00:02:55.100
negative log likelihood is as follows,

46
00:02:55.220 --> 00:02:58.490
minus the sum over all possible classes.

47
00:02:58.491 --> 00:03:02.570
See of d,
uh,
an identifier,

48
00:03:02.580 --> 00:03:06.820
the function which is one if y,
which is the true target,

49
00:03:07.240 --> 00:03:10.180
is equal to see this index over which I'm summing.

50
00:03:10.840 --> 00:03:15.610
And then so this identity function is one if this is true in zero otherwise.

51
00:03:15.790 --> 00:03:17.590
So if y is equal to c,

52
00:03:17.591 --> 00:03:21.880
then multiply one by the lug of f of x,
F c,

53
00:03:21.881 --> 00:03:25.960
which is my estimate of the probability of the true class being c.

54
00:03:26.530 --> 00:03:30.540
And otherwise,
if why is not equal to c,
then this term is zero.

55
00:03:30.570 --> 00:03:35.440
So this term disappears.
So effectively,
it means that you know,
this,

56
00:03:35.530 --> 00:03:40.060
the only non zero term is this one.
So that's why these two things are equal.

57
00:03:41.000 --> 00:03:45.340
Um,
so we take the log to simplify,
uh,

58
00:03:45.400 --> 00:03:50.050
and uh,
also to get some numerical stability.
Uh,
um,

59
00:03:50.440 --> 00:03:51.280
uh,
so yeah,
so we,

60
00:03:51.281 --> 00:03:55.300
we take the log to get some numerical stability and also to simplify the math

61
00:03:55.301 --> 00:03:56.980
when we were going to compute a,

62
00:03:57.010 --> 00:04:01.060
in particular their relatives of the parameters and uh,

63
00:04:01.630 --> 00:04:06.130
uh,
this,
uh,
[inaudible] is also known as the cross entropy.
Uh,

64
00:04:06.131 --> 00:04:10.180
so if you're familiar with,
uh,
you know,
a little bit about,
uh,

65
00:04:10.450 --> 00:04:12.510
information theory,
um,

66
00:04:12.760 --> 00:04:17.650
the cross entropy between two distribution is a,
actually takes this form.

67
00:04:17.651 --> 00:04:18.484
So in fact,

68
00:04:18.580 --> 00:04:23.580
this would be the cross entropy or with the minus would be the cross entropy

69
00:04:24.371 --> 00:04:26.890
between this distribution here,

70
00:04:26.891 --> 00:04:29.830
which puts it probably have one over there,

71
00:04:30.010 --> 00:04:33.680
true class and zero over all other class.
So the Co,

72
00:04:33.820 --> 00:04:37.840
the cross entropy do twin,
this distribution and that distribution,

73
00:04:37.841 --> 00:04:42.550
which is the distribution that is output by our neural network.
So,

74
00:04:42.551 --> 00:04:43.384
um,

75
00:04:43.570 --> 00:04:47.080
this is going to be the last function we'll use for training on neural network.

