WEBVTT

1
00:00:00.510 --> 00:00:03.640
And this video will introduce the idea of multitask learning.

2
00:00:04.720 --> 00:00:05.553
<v 1>Okay.</v>

3
00:00:05.950 --> 00:00:10.360
<v 0>So in the previous video we discussed the conventional architecture for neural</v>

4
00:00:10.361 --> 00:00:15.361
network that would give us a model for the urinary potential in a linear chain,

5
00:00:15.521 --> 00:00:19.120
conditional random them fields for solving any individual,

6
00:00:19.210 --> 00:00:20.620
a word tagging problem,

7
00:00:21.460 --> 00:00:25.540
and a then at the end I mentioned that,
uh,
uh,

8
00:00:25.630 --> 00:00:30.310
so we could use such a neural network individually and the single specific

9
00:00:30.311 --> 00:00:35.110
neural network for each task and solve them individually.
However,
uh,

10
00:00:35.200 --> 00:00:39.560
this might seem some optimal in the sense that these tasks,
my are,

11
00:00:39.970 --> 00:00:44.360
they appear a little bit related to,
for instance,
pause tagging,
which,
uh,

12
00:00:44.410 --> 00:00:47.230
helps us identify nouns and verbs in a sentence.

13
00:00:47.440 --> 00:00:51.160
It's probably related to the problem of identifying noun phrases and verb

14
00:00:51.161 --> 00:00:51.994
phrases.

15
00:00:52.130 --> 00:00:57.130
Also identifying verb and noun phrases might be useful for identifying what are

16
00:00:57.851 --> 00:01:01.910
the segments which correspond to a single role,
which respectable verbs.

17
00:01:01.950 --> 00:01:06.540
So there are probably relationships between a chunk game between identifying

18
00:01:06.541 --> 00:01:11.541
noun and verb phrases and identifying the semantic roles of a words with respect

19
00:01:12.401 --> 00:01:15.640
to different forbs in the sentence.
Well,
semantic role labeling.

20
00:01:16.260 --> 00:01:16.620
<v 1>Okay.</v>

21
00:01:16.620 --> 00:01:21.090
<v 0>And so one way of tackling this problem would be to die,
uh,</v>

22
00:01:21.150 --> 00:01:25.500
to essentially develop a single conditional random fields,
uh,

23
00:01:25.620 --> 00:01:27.010
along,
uh,
um,

24
00:01:27.480 --> 00:01:31.440
tomorrow the joint distribution of all the labels for all the different tasks.

25
00:01:31.950 --> 00:01:33.960
Uh,
however,
if we do this by say,

26
00:01:33.961 --> 00:01:38.580
introducing connections are or pairwise potentials between the labels of

27
00:01:38.581 --> 00:01:40.380
different tasks,

28
00:01:40.410 --> 00:01:45.410
then we'll miss eventually face a problem were performing inference at test time

29
00:01:46.021 --> 00:01:49.020
and during training,
uh,
will be,
uh,

30
00:01:49.050 --> 00:01:53.040
will not be possible to do exactly what we have to do.
Approximate inference,

31
00:01:53.280 --> 00:01:56.760
which is some,
uh,
it's just a little bit inconvenient.

32
00:01:57.300 --> 00:02:01.080
And so here we'll look at a different approach that columbarium Winston decided

33
00:02:01.081 --> 00:02:06.081
to use in order to still allow some sharing between the different tasks while

34
00:02:06.151 --> 00:02:10.680
not having to design and conditional random fields.
It would be to,
uh,

35
00:02:10.710 --> 00:02:15.240
complex,
uh,
to allow for efficient and none approximately inference.

36
00:02:17.580 --> 00:02:20.400
The approach is actually very,
very simple.
Um,

37
00:02:20.910 --> 00:02:25.910
so we'll use individual neural networks for each of the different tasks except

38
00:02:26.671 --> 00:02:29.010
for the lookup tables.
That is,

39
00:02:29.040 --> 00:02:34.040
except for the matrices that contain the word and feature representations fall

40
00:02:35.310 --> 00:02:36.960
the words in our vocabulary.

41
00:02:37.560 --> 00:02:42.560
So the idea is that each task will go and update the parameters of the lookup

42
00:02:43.771 --> 00:02:45.900
tables.
And the only thing,

43
00:02:46.080 --> 00:02:51.080
the only parameters of the unit were potentials are going to be specific for a

44
00:02:51.361 --> 00:02:55.620
given task are a,
is the rest of the neural networks.
So the convolution,

45
00:02:55.621 --> 00:03:00.520
the linear transformation,
uh,
into hard times and so on,

46
00:03:00.850 --> 00:03:04.150
everything else is actually going to be a,

47
00:03:04.151 --> 00:03:07.420
so all of these parameters are going to be specific to different tasks.

48
00:03:07.450 --> 00:03:10.300
The only thing is that they're all shared the same word representations.

49
00:03:11.290 --> 00:03:16.290
And so we'll be able essentially to have a better representation for a word by

50
00:03:16.960 --> 00:03:20.830
combining information about what it's pasta idea is,
what is a,

51
00:03:20.831 --> 00:03:25.831
it's a role in participants participating in two noun phrases or the semantic

52
00:03:27.011 --> 00:03:32.011
roles with respect to verbs or whether it's in a named entity or not and so on.

53
00:03:33.070 --> 00:03:35.620
And so this will help,
uh,

54
00:03:35.650 --> 00:03:39.010
make the word representations across the different task richer.

55
00:03:39.070 --> 00:03:43.040
But we'll still have specific parameters for solving each of these tasks

56
00:03:43.090 --> 00:03:43.923
individually.

57
00:03:45.220 --> 00:03:48.850
So this is a form of multitask learning in general.

58
00:03:48.851 --> 00:03:52.630
We refer to multitask learning as a type of,
uh,

59
00:03:52.660 --> 00:03:57.120
learning where we tried to solve multiple tasks,
uh,

60
00:03:57.460 --> 00:03:58.480
simultaneously.

61
00:03:58.870 --> 00:04:02.860
And it's a very simple one where we just share parameters across different

62
00:04:02.861 --> 00:04:06.670
models,
which otherwise would be,
uh,
would be treated separately.

63
00:04:07.030 --> 00:04:08.470
And so it's a very simple,

64
00:04:08.471 --> 00:04:12.280
straightforward way of at least allowing for some sharing of information between

65
00:04:12.281 --> 00:04:15.400
data sets of different,
uh,
problems.

66
00:04:19.140 --> 00:04:20.300
And,
um,

67
00:04:20.340 --> 00:04:25.340
and other thing we could do is actually invent certain tasks to try to make even

68
00:04:25.800 --> 00:04:29.910
more richer the word representations that we learned.
Um,

69
00:04:30.090 --> 00:04:32.640
so one task that they thought about is a,

70
00:04:32.730 --> 00:04:36.150
a task inspired by language modeling.
Uh,

71
00:04:36.240 --> 00:04:39.660
so what you would do is that,
uh,

72
00:04:39.661 --> 00:04:44.661
so the task that they thought about was that it would take a window of tech,

73
00:04:44.771 --> 00:04:45.990
say five words,

74
00:04:47.100 --> 00:04:52.100
and then the task would be to determine if I give you a particular window of

75
00:04:52.591 --> 00:04:53.250
words,

76
00:04:53.250 --> 00:04:58.140
whether the word that is place in the middle is actually the original word that

77
00:04:58.141 --> 00:05:02.270
was taken,
that,
that was observed in the original text.
Uh,

78
00:05:02.480 --> 00:05:05.130
so one way of,
of,
uh,
performing a,

79
00:05:05.131 --> 00:05:09.140
of generating a data set for that is to take all the word windows.

80
00:05:09.141 --> 00:05:14.100
They're fab words.
And uh,
so we pick,
uh,

81
00:05:14.370 --> 00:05:18.060
these different windows of words from some unlabeled corpus like Wikipedia,

82
00:05:18.930 --> 00:05:23.810
and then we generate a bunch of negative or imposter,
uh,

83
00:05:23.820 --> 00:05:24.361
words,

84
00:05:24.361 --> 00:05:29.361
context by just replacing the middle word with a different randomly chosen word.

85
00:05:30.210 --> 00:05:34.290
So the actual original windows would correspond to the positive example of that

86
00:05:34.291 --> 00:05:38.330
problem.
And then the negative example would be these other,
uh,

87
00:05:38.460 --> 00:05:42.990
word windows where we've taken the middle word and replaced it by some randomly

88
00:05:43.170 --> 00:05:47.080
chosen word.
Now you might think that,
um,

89
00:05:48.030 --> 00:05:50.010
you might actually pick random,
the,

90
00:05:50.011 --> 00:05:53.340
a word that would still make sense as a,

91
00:05:53.370 --> 00:05:56.070
if we actually observed it in that context.
So for instance,

92
00:05:56.960 --> 00:06:00.930
if we were replacing the,
and instead of the,

93
00:06:00.931 --> 00:06:05.300
we picked randomly,
so it'd be cat sat on a mat.

94
00:06:05.770 --> 00:06:07.640
A that would still make sense.

95
00:06:07.641 --> 00:06:11.330
So it is perhaps a little bit strange to treat that as a negative example.

96
00:06:11.331 --> 00:06:14.690
Something that we don't want the neural network to think is likely.

97
00:06:14.960 --> 00:06:19.560
But I'm still,
this is only trying to influence the,

98
00:06:19.561 --> 00:06:22.010
the,
the,
the word representations.

99
00:06:22.130 --> 00:06:24.950
And because there are many more words aren't going to be wrong than there are

100
00:06:24.951 --> 00:06:28.760
words that are going to be good words.
This is still a good way,

101
00:06:29.090 --> 00:06:32.450
which works well in practice for generating a data for,

102
00:06:32.451 --> 00:06:37.250
for this task particular the um,
uh,
this,

103
00:06:37.330 --> 00:06:37.610
uh,

104
00:06:37.610 --> 00:06:41.870
extra task which is going to be solved using the same word representations and

105
00:06:42.240 --> 00:06:45.590
the neural net that tries to do word tagging.
Uh,

106
00:06:45.740 --> 00:06:48.530
the way they're going to tackle it is using a neural network,

107
00:06:48.531 --> 00:06:52.190
which takes just this context of five words.
So it's,
in this case,

108
00:06:52.220 --> 00:06:55.310
this neural network is not going to be a convolutional neural net that's going

109
00:06:55.311 --> 00:06:56.750
to be a regular neural net,

110
00:06:57.020 --> 00:07:01.220
but that uses where representations and tries also to influence them through

111
00:07:01.221 --> 00:07:05.510
gradient descent.
And the last function that they decided to use a,

112
00:07:05.610 --> 00:07:10.610
so we could design some sort of binary classification problem using a soft Max

113
00:07:11.211 --> 00:07:14.480
output with two possibilities fund since that that could have worked.

114
00:07:14.481 --> 00:07:18.940
But instead they use a,
a margin inspire,
uh,
sorry,

115
00:07:18.950 --> 00:07:20.750
am I lost function?

116
00:07:20.750 --> 00:07:25.550
That's a inspired by the idea of maximum margin between,
uh,

117
00:07:25.580 --> 00:07:26.660
between different classes.

118
00:07:26.661 --> 00:07:31.661
And specifically what they're trying to do is that if we know f data x as the

119
00:07:32.721 --> 00:07:34.280
output of the neural net,

120
00:07:34.310 --> 00:07:39.260
which outputs a score as to how good the particular configuration of the word

121
00:07:39.261 --> 00:07:41.110
window and his input,
uh,

122
00:07:41.180 --> 00:07:44.450
is how meaningful it is based on according to their neural net.

123
00:07:44.690 --> 00:07:48.800
So if we have f data vacs for the original window,

124
00:07:49.100 --> 00:07:53.870
and then for a window of texts where we've replaced the,
uh,
window,

125
00:07:53.910 --> 00:07:57.350
uh,
the middle word in the window by some random word w,

126
00:07:57.940 --> 00:08:00.560
so f data of x Doubleu,

127
00:08:00.561 --> 00:08:03.530
if we know that has the score for this imposter window,

128
00:08:04.160 --> 00:08:07.430
then they would try to minimize this expression here.

129
00:08:07.460 --> 00:08:11.730
That is the maximum between zero and one minus,
uh,

130
00:08:11.930 --> 00:08:16.430
this core plus that score.
So in other,
we're brightening,
this is,

131
00:08:16.700 --> 00:08:21.680
we'll put parenthesis here and then replaced them that buy a minus because it

132
00:08:21.681 --> 00:08:24.620
will be canceled out by this minus.
That's why it's a,

133
00:08:24.680 --> 00:08:26.240
it's a plus in the original expression.

134
00:08:26.870 --> 00:08:30.050
So effectively what we're doing is that we're looking at the difference between

135
00:08:30.051 --> 00:08:32.210
the score as assigned by the neural net,

136
00:08:32.240 --> 00:08:36.590
which has just a single output and it provides us with the score for the input,

137
00:08:37.070 --> 00:08:39.320
uh,
the,
the context window we're providing it.

138
00:08:39.640 --> 00:08:43.880
So we look at that score and we subtracted by the score for an imposter window.

139
00:08:44.630 --> 00:08:48.230
And now if that difference is larger than one,

140
00:08:48.410 --> 00:08:52.400
then we have my one minus a number that's larger than one,
which is negative.

141
00:08:52.401 --> 00:08:55.530
So the maximum between zero and that is going to be zero.

142
00:08:56.250 --> 00:08:59.220
And so in this case,
the loss is exactly zero.

143
00:08:59.221 --> 00:09:03.150
We're not incurring any loss from that prediction.
However,

144
00:09:03.180 --> 00:09:05.860
if the s Corp for the true,
uh,

145
00:09:06.030 --> 00:09:11.030
the original window and subtracted by the score of a D impasto window is smaller

146
00:09:14.101 --> 00:09:14.820
than one.

147
00:09:14.820 --> 00:09:19.380
So if the margin between the score of the true window and the score of the

148
00:09:19.381 --> 00:09:23.520
impossible window is not bigger than one,
um,

149
00:09:23.970 --> 00:09:26.670
in this case,
then we're going to have one minus the number,

150
00:09:26.671 --> 00:09:30.360
which is smaller than one.
So we actually be incurring a loss,
which is none,
zero.

151
00:09:30.900 --> 00:09:35.870
So what the neural network is trying to do is to push the score of the original

152
00:09:35.880 --> 00:09:40.880
window to be at least one more than the score of any other impasto window.

153
00:09:41.850 --> 00:09:43.800
Okay.
So that's it.
Uh,
you know,

154
00:09:43.980 --> 00:09:47.580
intuitively essentially trying to separate imposter windows from,

155
00:09:47.640 --> 00:09:51.320
from true windows.
And,
uh,
like I said,
it's,
it's so it's,
it's,

156
00:09:51.321 --> 00:09:54.510
it's trying to enforce the margin between the true windows,

157
00:09:54.720 --> 00:09:57.120
the original windows and the imposter windows.

158
00:09:57.750 --> 00:09:58.260
<v 1>Okay.</v>

159
00:09:58.260 --> 00:10:03.030
<v 0>So this is similar also to language modeling in spirit in the sense that instead</v>

160
00:10:03.031 --> 00:10:06.630
of predicting the next word,
we're predicting what is the word in the middle.

161
00:10:06.631 --> 00:10:11.370
And we're trying to make sure that we assign a higher score to the true,
uh,

162
00:10:11.610 --> 00:10:16.110
to,
to true a windows that contained the right word instead of the,
uh,

163
00:10:16.170 --> 00:10:19.640
instead of an imposter word.
And so for that reason,
in the original paper,

164
00:10:19.641 --> 00:10:22.440
sometimes they talk about this task as a language modeling tasks,

165
00:10:22.470 --> 00:10:24.270
even though it's not a language model.

166
00:10:24.600 --> 00:10:29.220
And based on the definition of a language model as being a probabilistic model,

167
00:10:29.520 --> 00:10:34.050
uh,
that assigns a probability to any sequence of text.
But,
uh,
just so you know,

168
00:10:34.051 --> 00:10:37.620
if you look at the paper,
that's why they call it language modeling task.

169
00:10:37.621 --> 00:10:41.190
It's because it's similar in spirit with language modeling in general.

170
00:10:44.250 --> 00:10:48.140
So let's look at some results taken from,
uh,
the,
uh,

171
00:10:48.150 --> 00:10:51.440
general version of that paper by color bag and western.
Um,

172
00:10:51.441 --> 00:10:55.320
so we have here the results for the past tagging,
chunking,
name,

173
00:10:55.321 --> 00:10:59.460
entity recognition in semantic role,
labeling tasks for some benchmark systems.

174
00:10:59.461 --> 00:11:02.760
So I take him from the literature,
so they're usually not neural networks.

175
00:11:03.360 --> 00:11:04.750
And then the,
uh,

176
00:11:04.800 --> 00:11:09.800
neural net that chooses the convolutional approach and a different convolutional

177
00:11:10.470 --> 00:11:15.110
neural net for each different tasks,
uh,
that is,
uh,
used as the union rep,

178
00:11:15.111 --> 00:11:18.620
potential for conditional and them feel.
And,
uh,

179
00:11:18.660 --> 00:11:22.590
also in this particular neural net,
uh,
no features are used.

180
00:11:22.591 --> 00:11:25.530
The only feature that's used as the original word,

181
00:11:25.531 --> 00:11:27.600
but we're not extracting suffixes and prefixes.

182
00:11:27.601 --> 00:11:31.140
So we're trying to see whether the neural net can learn a representation from

183
00:11:31.141 --> 00:11:35.010
scratch without any information as to what are useful features,

184
00:11:35.011 --> 00:11:39.270
the extract from the original word,
uh,
to get a good performance,

185
00:11:39.271 --> 00:11:41.660
which is something that these benchmarks systems do.

186
00:11:42.600 --> 00:11:44.850
And we sit a performance is not too bad,

187
00:11:44.851 --> 00:11:48.420
but it's usually below the performance of the benchmark systems.

188
00:11:50.720 --> 00:11:53.770
However,
if now we,
uh,
either,
uh,

189
00:11:53.771 --> 00:11:58.771
use the language model as an extra task that is used to influence the word

190
00:11:59.351 --> 00:12:04.090
representations that we're learning.
And also if we add up on,
uh,

191
00:12:04.091 --> 00:12:07.200
on top of that,
the possibility of,
uh,

192
00:12:07.330 --> 00:12:10.390
having all tasks influenced the word representation.

193
00:12:10.391 --> 00:12:12.040
So if you do multitask learning,

194
00:12:12.280 --> 00:12:16.450
then we see that we get a very big improvement with respect to the original

195
00:12:16.451 --> 00:12:18.130
result we got with the neural net.

196
00:12:18.400 --> 00:12:20.860
And we're actually quite close to the baseline system.

197
00:12:20.861 --> 00:12:24.900
So here were almost exactly at the baseline system.
Uh,

198
00:12:24.940 --> 00:12:27.690
here we're a bit more below but not,

199
00:12:27.730 --> 00:12:30.840
that's a much worse again here,

200
00:12:30.850 --> 00:12:34.190
88 x instead of 89.
Uh,

201
00:12:34.191 --> 00:12:39.040
and so we got a big jump from 81 that we had initially and here is 74,

202
00:12:39.360 --> 00:12:43.510
uh,
which is low and in 77,
but much better than the 70,

203
00:12:43.630 --> 00:12:45.550
almost 71 that we have before.

204
00:12:46.460 --> 00:12:50.060
So what's interesting here is that just based on the learning,

205
00:12:50.090 --> 00:12:55.040
the training data and without any information about what are the good features

206
00:12:55.041 --> 00:12:58.710
to extracts or whether we should use suffixes and prefixes,
uh,

207
00:12:58.790 --> 00:13:03.200
it actually was able to do a really good job and almost match the benchmark

208
00:13:03.201 --> 00:13:06.590
systems which have a lot of hen tuning in terms of what the,

209
00:13:06.620 --> 00:13:08.390
what are the features that were extracted.

210
00:13:08.600 --> 00:13:12.140
So which required a lot of work from the,
uh,

211
00:13:12.230 --> 00:13:15.530
the human experts behind the model that,
that designed it.

212
00:13:16.930 --> 00:13:17.520
<v 1>Yeah.</v>

213
00:13:17.520 --> 00:13:20.130
<v 0>And then they tried to see,
okay,
well what if for these different tasks,</v>

214
00:13:20.131 --> 00:13:25.131
we actually introduce these other features that are known to help for,

215
00:13:25.440 --> 00:13:26.970
uh,
trying to,
uh,

216
00:13:27.030 --> 00:13:31.290
solve these other problems or suffixes are known to help or pass tagging in.

217
00:13:31.291 --> 00:13:34.100
Indeed,
then we matched the performance of the best paths,

218
00:13:34.101 --> 00:13:38.610
stagger gazzard tier features are helpful and indeed then we matched the

219
00:13:38.611 --> 00:13:43.230
performance for a named entity recognition,
pos,
stags.
Uh,

220
00:13:43.260 --> 00:13:46.020
if we add as the input,
the past tags at the words,

221
00:13:46.510 --> 00:13:46.820
<v 1>okay,</v>

222
00:13:46.820 --> 00:13:51.720
<v 0>uh,
this is useful for chunking and indeed we matched the performance of the best</v>

223
00:13:51.721 --> 00:13:52.141
system,

224
00:13:52.141 --> 00:13:56.940
which usually do this kind of post tagging extraction and a force,

225
00:13:57.150 --> 00:13:59.490
a cementing cruel labeling.
Uh,

226
00:13:59.491 --> 00:14:03.660
we get a little bit of a boost compared to the original performance,

227
00:14:03.661 --> 00:14:07.920
but we don't match yet the benchmark system more for the three of the,

228
00:14:07.980 --> 00:14:11.640
these three other tasks.
We actually did a sometimes a little bit better.

229
00:14:12.240 --> 00:14:13.060
So we see that as a,

230
00:14:13.060 --> 00:14:18.060
this is a very competitive approach for a word tagging for a bunch of different

231
00:14:18.870 --> 00:14:22.950
work,
tagging problems,
uh,
and uh,
based on,
uh,

232
00:14:22.951 --> 00:14:26.910
and this is all based on the neural network that learns word representations.

233
00:14:29.900 --> 00:14:32.380
And a fun thing to do then is also to look,
uh,

234
00:14:32.510 --> 00:14:35.120
try to visualize the quality of the word representation.

235
00:14:35.390 --> 00:14:38.750
And one way of doing this is to take different words,
say friends,

236
00:14:39.080 --> 00:14:43.640
which has a word id,
four 54,
that's not really meaningless.

237
00:14:43.641 --> 00:14:44.990
It could have been anything else really.

238
00:14:45.890 --> 00:14:50.890
And then look at what are the nearest neighbors in word representation.

239
00:14:51.650 --> 00:14:53.300
If we consider all other words.

240
00:14:53.301 --> 00:14:56.270
So we go over all the words in our vocabulary and look at their word

241
00:14:56.271 --> 00:14:59.230
representation compared the,
uh,
distance.

242
00:14:59.231 --> 00:15:03.020
So euclidean distance with the word representation of safe friends and then

243
00:15:03.030 --> 00:15:07.100
report the nearest neighbors.
For instance,
we see here,
that's if our friends,

244
00:15:07.101 --> 00:15:11.660
we get a lot of other different countries in,
in Europe.
Uh,

245
00:15:11.810 --> 00:15:16.550
so for Jesus we got a lot of rigid village,
unrelated words as Xbox.

246
00:15:16.580 --> 00:15:21.440
We got some,
uh,
technology and,
and often gaming console words,

247
00:15:21.890 --> 00:15:25.490
uh,
and read,
we get greenish,
bluish,
pinkish,

248
00:15:25.910 --> 00:15:29.990
uh,
mega bits,
other computer science related terms.

249
00:15:30.140 --> 00:15:34.640
So we see that it seems to have in some,
in some fairly real way,

250
00:15:34.641 --> 00:15:38.450
understood the,
what these words are.
And a,

251
00:15:38.451 --> 00:15:43.310
you can also try to get a two d visualization of these word representations and

252
00:15:43.311 --> 00:15:44.300
you can look on that,

253
00:15:44.800 --> 00:15:48.170
a URL here to get the two d mapping of the words and you'd see it.

254
00:15:48.171 --> 00:15:51.440
It's quite impressive.
Uh,
the,
uh,
understanding of the,

255
00:15:51.441 --> 00:15:56.420
it was words that was learned by this model.
And,
uh,

256
00:15:56.421 --> 00:15:56.961
and so that's,

257
00:15:56.961 --> 00:16:01.070
it's for how to solve word tagging problems with the neural networks.

