WEBVTT

1
00:00:00.760 --> 00:00:04.120
In this video,
we'll look at the concept of regularization and uh,

2
00:00:04.180 --> 00:00:07.510
see what it's good for and how we introduce it into the training of a neural

3
00:00:07.511 --> 00:00:08.344
network.

4
00:00:09.690 --> 00:00:10.320
<v 1>Yeah.</v>

5
00:00:10.320 --> 00:00:14.280
<v 0>So now in,
uh,
describing the full procedure for us,
the Cassie,</v>

6
00:00:14.281 --> 00:00:18.480
great in the sense where are at the step where we have to specify a irregular

7
00:00:18.481 --> 00:00:20.200
riser and a,

8
00:00:20.330 --> 00:00:24.450
and also how we actually compute the gradients of the regular riser with respect

9
00:00:24.451 --> 00:00:27.600
to our parameters.
So we see the regular risers here.

10
00:00:27.601 --> 00:00:31.620
So it's involved in how we update the parameters of the neural network.

11
00:00:32.010 --> 00:00:36.840
The role of the regular riser is to penalize certain types of functions and

12
00:00:36.841 --> 00:00:38.950
specifically we will want to,

13
00:00:38.951 --> 00:00:43.470
it's best to penalize a neural network style are very complicated in the sense

14
00:00:43.471 --> 00:00:48.471
that they correspond to a function that is very nonlinear and uh,

15
00:00:48.540 --> 00:00:53.460
might be able to overfit on the training set that is essentially learn more or

16
00:00:53.461 --> 00:00:56.640
less by heart the information that's contained in the training set and not be

17
00:00:56.641 --> 00:01:01.200
able to generalize well to new examples.
So we'll first look at a few,
uh,

18
00:01:01.310 --> 00:01:04.410
common regular riser that are used for neural network training.

19
00:01:04.560 --> 00:01:08.400
And then we'll try to build some intuition for why it's actually important to

20
00:01:08.401 --> 00:01:11.130
use regularization in neural networks.

21
00:01:13.130 --> 00:01:13.570
<v 1>Yeah.</v>

22
00:01:13.570 --> 00:01:18.410
<v 0>Now,
uh,
first regularize it,
it's very common is l two norm regularization.</v>

23
00:01:18.920 --> 00:01:21.670
Uh,
typically we only regularize the weights,

24
00:01:21.671 --> 00:01:26.671
so the connection weights between the hidden layers and in Ltl regularization

25
00:01:26.811 --> 00:01:31.130
what we do is that we penalize for each a hidden layer,

26
00:01:31.620 --> 00:01:36.020
uh,
and for each connection I j between the,
uh,

27
00:01:36.050 --> 00:01:39.560
JF a neuron in the previous,
uh,

28
00:01:39.590 --> 00:01:42.560
layer and the IIF neuron in the lyric,
Kay,

29
00:01:42.710 --> 00:01:47.710
we will penalize the square of the weight value and other way too,

30
00:01:47.781 --> 00:01:48.081
right?

31
00:01:48.081 --> 00:01:53.081
This is to notice that this sum here actually corresponds to the,

32
00:01:53.600 --> 00:01:55.030
uh,
for being this norm.

33
00:01:55.031 --> 00:01:59.310
X is the square of the food Venus norm there for being as normally is just the

34
00:01:59.840 --> 00:02:03.050
square root of the sum of the square of the elements in the Matrix.

35
00:02:03.290 --> 00:02:06.770
And so if we think the square of that,
that corresponds to this whole sun here,

36
00:02:07.910 --> 00:02:12.230
uh,
so this is essentially shrinking the weights towards zero with a squared

37
00:02:12.231 --> 00:02:16.250
penalty,
uh,
the gradient of the regular riser,
uh,

38
00:02:16.251 --> 00:02:20.480
with respect to the weights for the KF,
uh,
layer.
Uh,

39
00:02:20.481 --> 00:02:24.980
it's actually very simple.
It's just two times the weight matrix.

40
00:02:25.100 --> 00:02:27.800
So a,
as an exercise we can try to do it,

41
00:02:27.801 --> 00:02:31.250
but you'll see it's very simple to obtain very easy result to the right.

42
00:02:32.240 --> 00:02:32.721
Like I said,

43
00:02:32.721 --> 00:02:36.020
usually we only apply it on the weights and not so much on the biases we don't

44
00:02:36.021 --> 00:02:38.510
expect to over fit the training set,
uh,

45
00:02:38.540 --> 00:02:41.870
by changing the biases a lot and more by changing the weights,

46
00:02:41.871 --> 00:02:46.380
which really dictate how the function can become more or less nonlinear.

47
00:02:46.480 --> 00:02:51.050
And so more or less flexible for a fitting and a for those that are familiar

48
00:02:51.051 --> 00:02:55.340
with,
uh,
graphical models and,
uh,
the idea of probabilistic modeling,

49
00:02:55.520 --> 00:02:59.740
we can actually think of Alto regularization as,

50
00:02:59.830 --> 00:03:04.240
uh,
a,
uh,
sending where we assume that the weights of the neural network,

51
00:03:04.630 --> 00:03:09.250
uh,
came from a [inaudible] prior.
So that's because of the square here.

52
00:03:09.490 --> 00:03:11.500
Uh,
if,
um,

53
00:03:11.990 --> 00:03:16.810
if you were to do maximum a posterial refitting of your problems stick model on

54
00:03:16.811 --> 00:03:17.650
the training set,

55
00:03:17.890 --> 00:03:22.890
then you'd have a term which would correspond to the sum of the square of the

56
00:03:23.230 --> 00:03:24.063
parameters.

57
00:03:24.250 --> 00:03:27.250
So that comes out of assuming that there's a gosh in prior and the weights,

58
00:03:27.251 --> 00:03:30.340
I want to discuss this so much,
but uh,
uh,
this is a,

59
00:03:30.341 --> 00:03:34.030
a notion that people are familiar with probabilistic modeling and graphical

60
00:03:34.031 --> 00:03:35.590
models will be familiar with.

61
00:03:38.500 --> 00:03:43.030
Then there's El one regularization in which case we replaced the sum over the

62
00:03:43.031 --> 00:03:46.720
square of the weight values with an absolute value.

63
00:03:46.721 --> 00:03:50.050
So we penalize the absolute value of the weights and a,

64
00:03:50.051 --> 00:03:53.560
if you look at the gradient,
um,
actually the grading,

65
00:03:53.561 --> 00:03:58.240
there's only well-defined for weights that are non zero.
And uh,

66
00:03:58.270 --> 00:04:01.570
if,
uh,
so that's because the absolute value looks like this.

67
00:04:02.080 --> 00:04:05.950
And so where this would be zero.
And so if we're here,

68
00:04:05.951 --> 00:04:08.800
we see that the gradient would be negative.

69
00:04:08.801 --> 00:04:11.590
So if the weight is negative than the gradient is negative.

70
00:04:11.830 --> 00:04:13.150
And otherwise if it's positive,

71
00:04:13.151 --> 00:04:16.330
then the gradient is positive and it's actually equal to one.

72
00:04:16.360 --> 00:04:19.030
This is a linear function with a slope of minus one,

73
00:04:19.031 --> 00:04:23.080
and this would be a linear function with a slope of one and at zero it's not

74
00:04:23.081 --> 00:04:26.170
defined.
Uh,
so,
uh,
uh,
we own the,

75
00:04:26.370 --> 00:04:30.520
we essentially assume that the gradient is Dan zero,
uh,
to be more technical.

76
00:04:30.520 --> 00:04:34.930
This corresponds to you taking a subgrade.
And so,
uh,
if you ever see that name,

77
00:04:34.931 --> 00:04:39.931
that's just the idea that we just take the gradient of a slope that's below the

78
00:04:40.211 --> 00:04:44.820
function locally.
And so if we had a function that's flat,
it would be a,

79
00:04:44.830 --> 00:04:48.890
it would be below the function that the 0.0.
So,
um,
instead of using a gradient,

80
00:04:48.920 --> 00:04:52.920
which is not the fine,
we use the subgrade into zero.
Um,

81
00:04:52.960 --> 00:04:54.340
so if you write this in math,

82
00:04:54.670 --> 00:04:59.560
the gradient of the regular riser is going to,
with respect to some weights,

83
00:04:59.561 --> 00:05:03.040
it's just going to be two going to be the sign of the weights.

84
00:05:03.340 --> 00:05:06.400
So a sign of the weight,
that's just a,

85
00:05:06.401 --> 00:05:08.410
we're going to define it as one.

86
00:05:08.411 --> 00:05:12.040
If the weight is greater than zero and minus one,

87
00:05:12.070 --> 00:05:16.090
if the way it is instead smaller than zero in otherwise the two conditions with

88
00:05:16.150 --> 00:05:17.050
not be satisfied.

89
00:05:17.050 --> 00:05:21.320
So it would be the sine function would return zero for the case where the way to

90
00:05:21.340 --> 00:05:23.200
zero.
Um,

91
00:05:23.410 --> 00:05:26.590
one nice thing with l one which is different from l two,

92
00:05:26.591 --> 00:05:30.460
you don't get this from l two regularization is that with a strong

93
00:05:30.461 --> 00:05:31.480
regularization,

94
00:05:31.510 --> 00:05:36.070
the weights I actually going to be pushed towards being exactly zero.

95
00:05:36.640 --> 00:05:40.600
And so at the solution,
once the stochastic gradient descent as converge,

96
00:05:40.700 --> 00:05:43.990
You have a certain number of whites which are going to be exactly at zero,

97
00:05:43.991 --> 00:05:48.220
which can be understood as a procedure for pruning away some of the connections

98
00:05:48.221 --> 00:05:51.250
between the weights.
So with Ellen regularization,

99
00:05:51.550 --> 00:05:55.630
it's kind of equivalent to learning which neuron should be connected,
uh,

100
00:05:55.990 --> 00:05:56.823
to each other.

101
00:05:57.110 --> 00:06:00.470
And then we see also that if we remove connections between neurons,

102
00:06:00.471 --> 00:06:04.360
then implicitly that means that the neural network is less complex.

103
00:06:04.361 --> 00:06:08.100
So really by penalizing the [inaudible] a norm of the weights,
uh,

104
00:06:08.120 --> 00:06:09.650
we should be getting a neural network,

105
00:06:09.651 --> 00:06:13.730
which is a less flexible and less able to overfit on the training set.

106
00:06:14.450 --> 00:06:17.890
And in this case,
there's also a probabilistic interpretation where,
uh,

107
00:06:17.930 --> 00:06:21.110
this is equivalent to assuming that we are putting a,
uh,
some,
uh,

108
00:06:21.140 --> 00:06:25.730
La Plaza priors or plus in distribution as the prior distribution for what could

109
00:06:25.731 --> 00:06:28.930
have generated the weights of the neural net,
which,
uh,

110
00:06:29.080 --> 00:06:32.210
under our assumption as generated our training data.
So again,

111
00:06:32.300 --> 00:06:35.670
I will discuss this more,
but,
uh,
uh,
these are,
uh,

112
00:06:35.750 --> 00:06:40.750
topics that are concepts that are frequent in province stick modeling graphical

113
00:06:41.031 --> 00:06:44.480
models.
Okay.

114
00:06:44.481 --> 00:06:46.980
So why do we want regularization?
Uh,

115
00:06:46.981 --> 00:06:50.840
and what we really want is not do well in the training set is actually do well

116
00:06:50.841 --> 00:06:52.370
on new examples.
For instance,

117
00:06:52.371 --> 00:06:56.210
examples that would come that would be in the test set and a,

118
00:06:56.211 --> 00:06:57.700
so that's the,
uh,

119
00:06:57.740 --> 00:07:02.690
we're interested then in the generalization performance of our model and,

120
00:07:02.720 --> 00:07:06.020
uh,
it can be shown more formally that he,

121
00:07:06.021 --> 00:07:11.021
the generalization error of elearning or algorithm decomposes into a,

122
00:07:11.481 --> 00:07:15.920
the sum of two terms,
a bias term and the [inaudible] term.

123
00:07:16.510 --> 00:07:16.770
Uh,

124
00:07:16.770 --> 00:07:21.770
so the variants of a learning algorithm a is essentially a measure of how much

125
00:07:24.530 --> 00:07:29.150
will would the model very if I changed the training set.
So for instance,

126
00:07:29.151 --> 00:07:34.151
if I had an infinite amount of data and I was able to construct many separate

127
00:07:35.211 --> 00:07:40.130
training sets and then I ran my algorithm on these different training sets,
uh,

128
00:07:40.131 --> 00:07:43.730
to what extent would the model very between training sets,

129
00:07:43.731 --> 00:07:47.610
to what extent would it be different than an correspond to a different,
uh,

130
00:07:47.690 --> 00:07:51.770
f function that we're,
that we're trying to learn?
In our case,
a neural network.

131
00:07:52.640 --> 00:07:55.250
The bias,
uh,
will instead be,

132
00:07:55.580 --> 00:07:58.520
to what extent is the average model I'm,

133
00:07:58.610 --> 00:08:01.010
I would be getting by changing the training set.

134
00:08:01.160 --> 00:08:03.110
And so sampling many different training set.

135
00:08:03.200 --> 00:08:06.300
To what extent would the sort of average model,
uh,

136
00:08:06.470 --> 00:08:10.790
be close to the true solution of my learning problem?

137
00:08:10.791 --> 00:08:15.200
That is the true function,
which I could call f star of x,
the true function,

138
00:08:15.201 --> 00:08:19.010
which really has generated these trainings,
uh,
samples and,

139
00:08:19.070 --> 00:08:21.140
and is behind the any new training,

140
00:08:21.230 --> 00:08:24.680
any new test example I would encounter and have to do predictions on.

141
00:08:26.000 --> 00:08:30.590
Um,
and so to get the more intuitive understanding what this means,

142
00:08:30.680 --> 00:08:33.920
here's a cartoon illustration.
So,
uh,

143
00:08:33.921 --> 00:08:37.490
so the gray areas correspond to different neural nets.

144
00:08:37.700 --> 00:08:42.700
So each point in a gray area is a neural net that I would be able to obtain for

145
00:08:43.110 --> 00:08:47.630
a particular training set amongst all training that I could sample,

146
00:08:47.950 --> 00:08:51.710
uh,
for a given problem.
So these are the possible F's,

147
00:08:51.711 --> 00:08:55.160
the possible neural nets I could be obtaining by training,
uh,

148
00:08:55.170 --> 00:08:58.380
on a given training set and so forth.
Different training sets,

149
00:08:58.381 --> 00:09:01.260
I'd be getting all of these different neural nets.

150
00:09:01.680 --> 00:09:05.310
And F star is going to be the point in my space of function,

151
00:09:05.311 --> 00:09:07.930
which corresponds to the true solution,
the,
the,

152
00:09:07.950 --> 00:09:11.850
the true function that actually generates my,
uh,

153
00:09:11.851 --> 00:09:16.050
my labels that I'm serving.
So in this case here,

154
00:09:16.051 --> 00:09:20.910
we have low variance because we see that the region of potential neural networks

155
00:09:20.911 --> 00:09:23.850
I could get by varying the training set is actually quite small,

156
00:09:24.210 --> 00:09:28.260
but I actually have high bias because on average or let's say maybe that's this

157
00:09:28.261 --> 00:09:28.631
point,

158
00:09:28.631 --> 00:09:32.910
the average sort of typical neural net that could be getting is actually quite

159
00:09:32.911 --> 00:09:37.830
far from the true solution.
For that reason,
I have high bias,
uh,

160
00:09:37.831 --> 00:09:41.920
on the opposite of that spectrum,
I have a case where I've had high Verun.

161
00:09:41.920 --> 00:09:45.510
So the region of potential neural networks is actually quite large,

162
00:09:46.110 --> 00:09:48.870
but low bias because on average,

163
00:09:48.900 --> 00:09:52.230
maybe the point on average would be sort of here then that's actually quite

164
00:09:52.231 --> 00:09:56.310
close to,
uh,
what the true solution is here.

165
00:09:56.940 --> 00:10:00.090
So let's think about why this is bad.
And this is also bad.

166
00:10:00.630 --> 00:10:02.970
So this is bad because,
uh,

167
00:10:03.030 --> 00:10:08.030
in a given situation I get one training set so I can get a point that's anywhere

168
00:10:08.551 --> 00:10:11.100
here as solution.
The neural net that can be anywhere here.

169
00:10:11.310 --> 00:10:13.260
So say it might be here,

170
00:10:13.500 --> 00:10:17.880
but because the region is large and I won't be very far from what the average

171
00:10:17.881 --> 00:10:20.430
case would be,
so that's good.

172
00:10:20.431 --> 00:10:24.210
But unfortunately here the average case because I have high bias is actually

173
00:10:24.211 --> 00:10:27.510
quite far from what would be the best solution,

174
00:10:27.540 --> 00:10:32.220
which would be to report the true function at the opposite case because of a

175
00:10:32.221 --> 00:10:36.120
high by a high variance,
uh,
for a given training set,

176
00:10:36.121 --> 00:10:40.290
I could be very unlucky and get this neural net or I could get this neural net.

177
00:10:40.590 --> 00:10:43.110
Actually the probability of getting a neural net,

178
00:10:43.111 --> 00:10:46.980
which is fairly close to the true solution is actually quite small.

179
00:10:46.981 --> 00:10:48.330
This region is very small.

180
00:10:48.470 --> 00:10:52.290
It's very small compared to the whole region of potential neural nets.

181
00:10:52.291 --> 00:10:53.640
I could be getting.
So,

182
00:10:53.641 --> 00:10:57.630
even though on average I might be lucky and get something that's close to here,

183
00:10:57.631 --> 00:11:00.480
even though the average neural net,
quote unquote,

184
00:11:00.630 --> 00:11:02.370
could be close to the true solution,

185
00:11:02.550 --> 00:11:05.010
because I have so much fermions for a given training set,

186
00:11:05.011 --> 00:11:07.230
it's fair likely that I won't be in this region.

187
00:11:07.231 --> 00:11:09.360
I would be actually quite far from the truth solution.

188
00:11:10.260 --> 00:11:14.550
So the best thing to do would be to sort of explore a trade off between low

189
00:11:14.551 --> 00:11:18.180
variance and high bias and high Verun slow buys.

190
00:11:18.600 --> 00:11:21.090
And so we have a trade off here,
which seems fairly reasonable.

191
00:11:21.150 --> 00:11:26.150
So I have a bias because the average case is not a close to f star.

192
00:11:26.900 --> 00:11:31.800
Uh,
and I have some parents.
So this region here is bigger than that region here.

193
00:11:32.130 --> 00:11:35.430
However,
I see that,
you know,
for different training set,

194
00:11:35.460 --> 00:11:39.900
I'm always fairly close to what the true solution might be,
the f star.

195
00:11:41.010 --> 00:11:45.630
And so,
uh,
back to the idea of regularization,
uh,

196
00:11:45.750 --> 00:11:49.350
the amount of regularization,
which is,
uh,
uh,

197
00:11:49.410 --> 00:11:54.130
controlled by the lambda wait,
uh,
which is multiplied by their regular riser.

198
00:11:54.140 --> 00:11:59.140
So it determines how important I think it is to minimizing the regularization

199
00:11:59.411 --> 00:12:04.360
penalty by changing diff for different values of,
of the lender hyper parameter.

200
00:12:04.760 --> 00:12:09.580
Uh,
I can essentially control whether I'm in Laverne's high bias region or high

201
00:12:09.581 --> 00:12:12.510
viral low bias region.
If dilemma lambda,

202
00:12:12.580 --> 00:12:15.730
the penalization on the regularization is very high,

203
00:12:15.731 --> 00:12:19.980
then I'm going to get very little variation because,
uh,

204
00:12:20.020 --> 00:12:24.610
I can only explore neural nets that have very small weights and are fewer neural

205
00:12:24.611 --> 00:12:26.590
nets.
Uh,
we can't count them,

206
00:12:26.591 --> 00:12:29.710
but essentially the regional neuro that's with small weights as much smaller

207
00:12:29.711 --> 00:12:34.570
than the region of neural nets with small and big weights.
And so with a big,

208
00:12:34.810 --> 00:12:38.290
uh,
lambda,
I'm in the,
uh,
uh,

209
00:12:38.320 --> 00:12:40.060
low variance a region,

210
00:12:40.360 --> 00:12:44.700
but with the small lambda them in the high variance region,

211
00:12:44.830 --> 00:12:49.240
low bias region.
And so by trying different values of lambda,

212
00:12:49.600 --> 00:12:54.550
I'm able to explore this tradeoff between bias and parents and hopefully get a

213
00:12:54.551 --> 00:12:59.440
lender that provides the best trade off and better generalization.

214
00:13:00.040 --> 00:13:03.140
So this is the idea behind generalization and,
uh,

215
00:13:03.280 --> 00:13:06.460
the use of a regularization in neural nets.

216
00:13:06.580 --> 00:13:11.410
And so now we know how to introduce two different regularization l two and l one

217
00:13:11.710 --> 00:13:13.000
into neural network training.

