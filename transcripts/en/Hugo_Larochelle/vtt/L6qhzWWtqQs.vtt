WEBVTT

1
00:00:00.320 --> 00:00:00.830
Okay.

2
00:00:00.830 --> 00:00:04.880
<v 1>In this video we'll see how we can perform imprints and a spar scoring model and</v>

3
00:00:04.881 --> 00:00:07.760
compute delayed and representation for giving input.

4
00:00:08.640 --> 00:00:09.473
<v 0>Okay,</v>

5
00:00:09.760 --> 00:00:13.710
<v 1>so we've seen the objective in a sparse coding.</v>

6
00:00:13.990 --> 00:00:18.990
We're trying to find a dictionary matrix such that we can encode each of our

7
00:00:19.241 --> 00:00:24.241
training input into a representation which contains information about what was

8
00:00:25.901 --> 00:00:29.740
the original input and is also sparse and a,

9
00:00:29.741 --> 00:00:34.190
so to compute this objective and also to perform,
to optimize it,
uh,

10
00:00:34.330 --> 00:00:39.330
we need in the inner lobe of this inner loop of the Psalm to actually be able to

11
00:00:39.581 --> 00:00:44.110
perform the minimization here.
In other words,
given an x t,
uh,

12
00:00:44.111 --> 00:00:48.610
we need to be able to encode our input into the sparse representation.

13
00:00:48.611 --> 00:00:53.350
So to find these parts representation that optimizes the,
uh,

14
00:00:53.470 --> 00:00:57.700
reconstruction error plus a sparsity penalty,
uh,
objective.

15
00:00:58.270 --> 00:01:00.550
So in this video,
we'll see now go to them for doing that.

16
00:01:01.880 --> 00:01:02.713
<v 0>Okay,</v>

17
00:01:02.990 --> 00:01:05.790
<v 1>so let's assume for now that,
uh,</v>

18
00:01:05.930 --> 00:01:08.150
we are given a dictionary matrix.

19
00:01:08.730 --> 00:01:11.420
Now we have to infer the latent representation,

20
00:01:11.421 --> 00:01:15.080
the sparse representation h of x team.

21
00:01:15.890 --> 00:01:20.890
So a what do we have to do is optimize this combination of the squared or

22
00:01:21.081 --> 00:01:25.040
reconstruction term and uh,
sparsity term on the HD.

23
00:01:25.100 --> 00:01:28.640
We have to optimize that with respect to the later and representation.

24
00:01:29.290 --> 00:01:31.700
So we have to optimize the combination,

25
00:01:31.701 --> 00:01:36.020
the sum of a quadratic function with a,

26
00:01:36.110 --> 00:01:40.830
uh,
an l one type function.
Now I,

27
00:01:40.880 --> 00:01:43.190
one thing we could do is great dissent.
We've uh,

28
00:01:43.290 --> 00:01:47.780
used gradient descent algorithms before,
so why not for this particular problem?

29
00:01:48.590 --> 00:01:52.230
Um,
and so we could compute the initialize,

30
00:01:52.231 --> 00:01:56.120
the later representation to some value computer gradient with respect to the

31
00:01:56.121 --> 00:02:01.121
current value of the late and representation and updated using the gradient and

32
00:02:01.831 --> 00:02:04.790
formation.
So by going in the opposite direction of the gradient.

33
00:02:05.390 --> 00:02:08.000
So we see that the gradient has this term here,

34
00:02:08.001 --> 00:02:11.300
which comes from the reconstruction term and a term here,

35
00:02:11.301 --> 00:02:15.110
which we can recognize from a,
you know,
with the sine function.

36
00:02:15.370 --> 00:02:20.370
And we typically see when we have an l one type function to optimize what are we

37
00:02:20.751 --> 00:02:24.520
looking at the grading of another one function so we could perform great in the

38
00:02:24.680 --> 00:02:28.640
updating the Leighton representation,
uh,
uh,

39
00:02:28.700 --> 00:02:31.880
until it converges.
And,
uh,
uh,

40
00:02:32.030 --> 00:02:35.980
we found that an optimal value for this,
uh,

41
00:02:36.230 --> 00:02:39.680
one thing that could be said about this particular problem is that because it's

42
00:02:39.681 --> 00:02:44.530
the sum of two convex functions,
the,
uh,

43
00:02:44.570 --> 00:02:49.370
than the actual objective is convex.
So unlike,
say,
training a neural network,

44
00:02:49.371 --> 00:02:54.290
this optimization problem is,
uh,
has a unique global Optima.

45
00:02:54.830 --> 00:02:59.150
And so,
uh,
and that's,
it's,
it's a fairly easy problem to optimize.

46
00:03:02.680 --> 00:03:06.470
Let's look at the gradient for single hidden unit.
So,
uh,

47
00:03:06.520 --> 00:03:09.200
we have the green and from the or the partial there,

48
00:03:09.201 --> 00:03:11.390
they've from the squared reconstruction.

49
00:03:11.470 --> 00:03:15.280
So it's the difference between the reconstruction and the actual input

50
00:03:15.700 --> 00:03:18.810
multiplied by the,
uh,
uh,

51
00:03:18.880 --> 00:03:23.830
the vector that corresponds to the,
uh,
the,
uh,
that hidden units,

52
00:03:23.831 --> 00:03:26.240
weights with the input.
So that hidden units,
uh,

53
00:03:26.260 --> 00:03:30.040
Adam or dictionary element to have this part.

54
00:03:30.100 --> 00:03:32.350
And then we have the sign here.

55
00:03:33.170 --> 00:03:37.810
Now we know that the l one norm encourages values.
In this case,

56
00:03:37.870 --> 00:03:42.640
the hidden units to be a or forces summer will force some of them to be exactly

57
00:03:42.641 --> 00:03:46.720
zero.
Now if we do regular great in dissent,
um,

58
00:03:47.590 --> 00:03:52.360
uh,
actually it's actually very unlikely that we'll,
uh,

59
00:03:52.420 --> 00:03:56.410
do a grain and audit and we'll fall exactly on an h value,

60
00:03:56.411 --> 00:03:59.950
which is equal to zero.
And just actually generally speaking,

61
00:03:59.951 --> 00:04:04.570
the l one norm is not differentiable at zero.
So in that sense,
we can't really,

62
00:04:04.850 --> 00:04:09.130
uh,
this is great in the sentence is not to properly define,
uh,

63
00:04:09.131 --> 00:04:12.640
for this objective because we don't have the gradient at exactly zero,

64
00:04:12.910 --> 00:04:16.370
even though we actually expect that some of the hidden units will fall exactly.

65
00:04:16.371 --> 00:04:20.170
It should fall exactly on zero because,
uh,
he had one Norman courageous Parsi.

66
00:04:21.580 --> 00:04:26.060
So a solution for this,
uh,
uh,
is uh,

67
00:04:26.080 --> 00:04:27.010
to do the following.

68
00:04:27.280 --> 00:04:31.750
So I won't explain the rational theory behind why this is a good thing to do.

69
00:04:31.751 --> 00:04:36.720
I'll just give you an intuition for why this could work.
So,
um,

70
00:04:36.730 --> 00:04:40.390
what we'll do is that we'll do something very close to grade in the sense,

71
00:04:40.840 --> 00:04:45.790
and a,
I will alternate between performing an update based on the grain and

72
00:04:45.791 --> 00:04:49.930
reconstruction error and then performing it update based on the gradient of the,

73
00:04:50.170 --> 00:04:53.560
uh,
s a l one.
So,
uh,
based on this sign here,

74
00:04:54.550 --> 00:04:59.550
but what we'll do is that if the value of the of a hidden unit changes sign only

75
00:05:00.611 --> 00:05:05.611
because the l one norm gradient is pushing it towards Zeros and uh,

76
00:05:06.310 --> 00:05:10.750
because of the step size or learning rate we've been using,
it's a,
uh,

77
00:05:10.840 --> 00:05:15.140
it's actually gone past zero.
So it changed the sign then,
uh,

78
00:05:15.190 --> 00:05:16.420
if it changes sign,

79
00:05:16.450 --> 00:05:21.450
we actually clamp the a latent representation unit or the hidden unit to exactly

80
00:05:23.230 --> 00:05:28.230
zero and a so intuitively the idea here is that if it's the gradient from these

81
00:05:30.881 --> 00:05:35.130
parts of the term that is switching the sign,
uh,

82
00:05:35.290 --> 00:05:36.590
well actually the sparsity in turn,

83
00:05:36.640 --> 00:05:38.800
we know that it prefers everything to be zero.

84
00:05:39.040 --> 00:05:42.670
So presumably it's trying to set it exactly at zero.
So we'll do that.

85
00:05:42.790 --> 00:05:45.490
We'll set it exactly two zero if it changes sign.

86
00:05:46.360 --> 00:05:48.340
So that yields the following algorithm.

87
00:05:49.570 --> 00:05:54.570
So first we are update each of the unit based on the gradient descent update

88
00:05:55.870 --> 00:05:57.830
rule for only the reconstruction.

89
00:05:57.831 --> 00:06:02.831
So we're taking the previous value and then we're subtracting some step size or

90
00:06:03.051 --> 00:06:05.810
learning rate times degraded or the reconstruction.

91
00:06:07.540 --> 00:06:12.100
And then we check whether the sign of,

92
00:06:12.190 --> 00:06:14.620
um,
uh,
yeah,

93
00:06:14.621 --> 00:06:19.621
does sign of the new value for the hidden units is different from the sign of

94
00:06:21.760 --> 00:06:25.900
the same hidden unit.
But after we would apply,
uh,
the,
uh,

95
00:06:25.940 --> 00:06:30.760
gradient update based on the gradient for the l one norm.
So in other words,

96
00:06:30.761 --> 00:06:35.530
this is another way of,
of,
uh,
writing or this in words,

97
00:06:35.531 --> 00:06:40.180
this would correspond to checking whether h a key t is changing sign.

98
00:06:40.510 --> 00:06:42.910
If it's changing sign,
then we set it to zero.

99
00:06:43.600 --> 00:06:45.970
And otherwise we just applied the regular updates.

100
00:06:45.971 --> 00:06:49.150
So we just subtract a alpha times lambda,

101
00:06:49.151 --> 00:06:53.710
which is the regularization or the sparsity wait times the sign of,

102
00:06:53.740 --> 00:06:58.120
uh,
of H.
Okay.
So we essentially shrinking age towards zero.

103
00:06:59.350 --> 00:07:02.200
And so we applied these updates for all hidden units,

104
00:07:02.440 --> 00:07:05.020
and then we repeat the procedure until,
uh,

105
00:07:05.230 --> 00:07:07.810
di values of the hidden units doesn't change too much.

106
00:07:08.290 --> 00:07:11.110
So by defining some sort of convergence criteria.

107
00:07:13.690 --> 00:07:17.380
So this algorithm is known as [inaudible] iterative shrinkage and threshold and

108
00:07:17.490 --> 00:07:21.500
algorithm.
Uh,
so we have the web,
it's version in the,

109
00:07:21.501 --> 00:07:22.990
and the vector form.

110
00:07:23.170 --> 00:07:27.660
We initialize all the whole vector of uh,
uh,

111
00:07:27.661 --> 00:07:29.560
as far as coding representations,
Francis,

112
00:07:29.561 --> 00:07:34.561
we could initialize it to everything to zero and then until the factor has

113
00:07:35.591 --> 00:07:36.424
converged,

114
00:07:36.950 --> 00:07:40.870
we updated based on the reconstruction era gradient.

115
00:07:41.590 --> 00:07:45.700
And then we applied this shrink function here.
So the string function is,

116
00:07:45.730 --> 00:07:50.080
it does exactly what I said before for each of the unit.
It looks at whether the,

117
00:07:50.230 --> 00:07:54.610
uh,
update,
uh,
would make the HA each hidden unit change sign if it,

118
00:07:54.640 --> 00:07:58.240
and if it does,
you can pick to zero.
Otherwise you just shrink it towards zero.

119
00:07:59.020 --> 00:08:01.390
Okay,
so the shrink function,
uh,

120
00:08:01.391 --> 00:08:05.080
I define it more formally here between a and B.

121
00:08:05.081 --> 00:08:09.190
So a would be the age here and B would be this,
um,

122
00:08:09.250 --> 00:08:13.940
it's actually a scaler,
um,
uh,
in this case it's a scalar,
it's the,

123
00:08:14.050 --> 00:08:17.440
it's a vector a that consists of on the disk value.

124
00:08:18.250 --> 00:08:22.210
So what does shrink operation does is that it takes the absolute value of each

125
00:08:22.211 --> 00:08:27.010
element in the vector subtracts it a value here,
which has got to be positive.

126
00:08:27.011 --> 00:08:30.460
So this is always going to be positive.
So it shrinks it towards zero.

127
00:08:30.910 --> 00:08:34.090
But now if by shrinking it,
it actually becomes negative,

128
00:08:34.540 --> 00:08:38.260
we're taking the absolute balance or in other words it's changing sign.

129
00:08:38.710 --> 00:08:40.210
Then we're going to set it to zero.

130
00:08:40.211 --> 00:08:45.010
So we're taking the Max between this n zero and then we multiply back by the

131
00:08:45.011 --> 00:08:49.270
sign because we have had we moved the sign up.
The element here.
Okay,

132
00:08:49.780 --> 00:08:53.410
so visually this is what the shrink function looks like.

133
00:08:53.770 --> 00:08:56.670
If the input of district function,

134
00:08:56.671 --> 00:09:01.650
the part in a here is smaller than be ISO,

135
00:09:01.680 --> 00:09:04.650
which in this case bi corresponded to 10 it'll be,

136
00:09:04.651 --> 00:09:06.930
I would be this Alpha Times lambda here.

137
00:09:07.590 --> 00:09:12.300
If it's smaller than that value in absolute value.
So either,

138
00:09:12.301 --> 00:09:15.650
so it's eh within this interval then the shrink it,

139
00:09:16.250 --> 00:09:21.030
it's going to output exactly zero and otherwise it's going to grow linearly.

140
00:09:21.270 --> 00:09:26.270
So re so notice that one step 1.5 the output value is 0.5 so that's for bi

141
00:09:27.931 --> 00:09:30.060
equals two equal to one.
So it's just,

142
00:09:30.330 --> 00:09:35.330
it shrank towards zero by subtracting one and if subtracting one would have

143
00:09:36.090 --> 00:09:39.840
changed a sign of the element that it just outputs zero.
Exactly.

144
00:09:41.190 --> 00:09:41.580
<v 0>Okay.</v>

145
00:09:41.580 --> 00:09:43.380
<v 1>And now this algorithm is known to converge.</v>

146
00:09:43.450 --> 00:09:48.100
If a one over the step size,
Eh,
uh,

147
00:09:48.210 --> 00:09:52.570
is bigger than the largest can value of the,
uh,

148
00:09:52.630 --> 00:09:57.370
of this matrix here.
So it's just a theoretical condition for,
uh,

149
00:09:57.460 --> 00:10:01.000
for convergence.
So in other words,
if a alpha,

150
00:10:01.090 --> 00:10:03.700
so if one over Alpha is big enough or in other words,

151
00:10:03.701 --> 00:10:07.060
if Alpha is small enough and then the dis will converge.

152
00:10:07.660 --> 00:10:12.130
And uh,
and this is a fairly popular algorithm for performing infants.

153
00:10:13.090 --> 00:10:14.800
So once we're done,

154
00:10:15.070 --> 00:10:18.880
the algorithm returns what we call h of x t.

155
00:10:18.881 --> 00:10:23.550
It returns the sparse representation for,
uh,
an input.

156
00:10:23.830 --> 00:10:25.480
And we know this,
that this,

157
00:10:25.840 --> 00:10:30.160
which is equivalent to encoding the input into a hidden layer is much more

158
00:10:30.161 --> 00:10:34.850
complex.
Gated is they're fairly complicated iterative process,
uh,

159
00:10:34.910 --> 00:10:38.480
just more complicated compared to in an auto encoder.
We have,
uh,
uh,

160
00:10:38.510 --> 00:10:42.130
linear transform,
uh,
followed by non linearity.
So in fact,

161
00:10:42.131 --> 00:10:45.610
what happens here is that the hidden units are sort of competing with each other

162
00:10:45.640 --> 00:10:49.900
to explain it and they're competing because there's a parcel sparsity terms that

163
00:10:49.901 --> 00:10:51.220
wants everything to be zero.

164
00:10:51.490 --> 00:10:55.300
And so effectively what's going to happen is the hidden units that best explains

165
00:10:55.330 --> 00:10:58.460
something in the input and explains it better than another,

166
00:10:58.700 --> 00:11:00.260
a hidden units are going to be none,
zero.

167
00:11:00.261 --> 00:11:05.120
And the other things that are sort of overshadowed by other hitting you and
star,

168
00:11:05.360 --> 00:11:08.510
uh,
that are more appropriate for the input are going to be shrink to zero.

169
00:11:09.260 --> 00:11:13.310
So there's this spark scoring model one property it has,
is that hitting you?

170
00:11:13.311 --> 00:11:15.590
It's compete with each other for explaining the input.

171
00:11:18.330 --> 00:11:20.370
Uh,
there are other,
um,
uh,

172
00:11:20.400 --> 00:11:24.110
improvements we can make so fun since it's the updates all hidden units

173
00:11:24.111 --> 00:11:27.720
simultaneously,
which might be a wasteful because,
uh,

174
00:11:27.750 --> 00:11:31.460
maybe some hidden units will come verge faster than others.
And,
uh,

175
00:11:31.470 --> 00:11:36.420
so one idea is to update only the most promising hidden units at every
iteration.

176
00:11:36.421 --> 00:11:39.970
So we do less computers in every iteration.
Um,

177
00:11:40.660 --> 00:11:45.130
and uh,
if I suggest,
I recommend you look at this paper here,

178
00:11:45.490 --> 00:11:49.960
uh,
which,
uh,
it actually mentions,
uh,
so this,
uh,

179
00:11:49.990 --> 00:11:54.360
paper doesn't propose this algorithm,
but it describes and the,
uh,

180
00:11:54.400 --> 00:11:55.870
fairly well.
Uh,

181
00:11:55.871 --> 00:12:00.750
so it describes the coroner descent algorithm for performing this inference

182
00:12:00.760 --> 00:12:02.530
problem.
In freeing this part's codes.

183
00:12:02.830 --> 00:12:06.760
And one advantage it has is that it does not require a learning rate or step

184
00:12:06.761 --> 00:12:11.320
size for,
uh,
like Ista,
which requires an Alpha in updating the,

185
00:12:11.380 --> 00:12:16.330
a latent representation.
So,
uh,
if you're interested,
I,
uh,

186
00:12:16.520 --> 00:12:20.980
if you want those faster algorithm that also has the advantage of not requiring

187
00:12:21.220 --> 00:12:23.560
specifying a learning rate for doing inference,

188
00:12:23.830 --> 00:12:28.660
I suggest you look at this particular paper.
All right?
So that sums up,

189
00:12:29.020 --> 00:12:30.490
uh,
uh,

190
00:12:30.610 --> 00:12:34.540
what we have to say about how to we in first schools for a given input.

