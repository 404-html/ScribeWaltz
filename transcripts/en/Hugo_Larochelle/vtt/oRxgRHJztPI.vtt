WEBVTT

1
00:00:00.580 --> 00:00:04.000
In this video,
we'll now discuss how to train a recursive neural network.

2
00:00:04.440 --> 00:00:05.273
<v 1>Okay.</v>

3
00:00:06.380 --> 00:00:10.270
<v 0>So we've discussed the two main components behind recursive neural networks.
Uh,</v>

4
00:00:10.320 --> 00:00:15.320
the part that merges pairs of vector representations to obtain a new

5
00:00:15.351 --> 00:00:20.351
representation that covers more words in the sentence and also the part which,

6
00:00:20.690 --> 00:00:24.770
uh,
assigns a score for the quality of mergers that from which we get get a

7
00:00:24.771 --> 00:00:29.540
procedure for determining what is the best tree structure according to the
model.

8
00:00:29.790 --> 00:00:33.590
Uh,
so the best the tree structure in terms of the score it science for the full

9
00:00:33.591 --> 00:00:34.370
tree.

10
00:00:34.370 --> 00:00:39.170
And in order to get an inferred the correct sync tactic tree for a given

11
00:00:39.171 --> 00:00:40.004
sentence.

12
00:00:40.370 --> 00:00:43.280
So now we'll see how we can actually train the recursive neural network to

13
00:00:43.281 --> 00:00:45.010
actually produce the,
uh,

14
00:00:45.260 --> 00:00:49.700
true wanted parse tree for a given sentence,
uh,

15
00:00:49.720 --> 00:00:52.070
that is provided in some training data.

16
00:00:55.520 --> 00:01:00.520
So I'll refer to why as the true power street are given sentence and why hat as

17
00:01:01.881 --> 00:01:04.340
the parser that's predicted,
uh,

18
00:01:04.550 --> 00:01:08.660
by the recursive neural network using the procedure we saw in the previous
video.

19
00:01:09.560 --> 00:01:13.910
So,
uh,
what we would like is for the score,
the full tree,
why,

20
00:01:13.920 --> 00:01:15.740
which is the correct tree.
Uh,

21
00:01:15.741 --> 00:01:20.741
so we called Das s of y to actually be hired than the score of the uh,

22
00:01:21.740 --> 00:01:24.770
tree that we predicted y hat where a,

23
00:01:24.771 --> 00:01:26.960
unless of course y hat is actually why,

24
00:01:26.961 --> 00:01:31.400
in which case our neural network is doing the,
uh,
doing the right prediction.

25
00:01:31.430 --> 00:01:33.200
But if it's doing an incorrect prediction,

26
00:01:33.201 --> 00:01:38.120
then what we want is for the score of s y to be higher than the score of,

27
00:01:38.250 --> 00:01:41.270
uh,
sorry,
the score of y to be higher than the score of y hat.

28
00:01:41.360 --> 00:01:46.360
So that next time around it's more likely that y will be the predicted a parse

29
00:01:46.761 --> 00:01:49.680
tree.
So,
uh,

30
00:01:49.710 --> 00:01:54.560
and not do them for are trying to achieve this is that whenever y hat is not why

31
00:01:54.561 --> 00:01:59.561
we just increased the score of y a and then we decrease the score of y hat.

32
00:02:01.100 --> 00:02:02.360
So,
uh,

33
00:02:02.390 --> 00:02:06.590
procedure for doing this would be to first take the sentence and infer the

34
00:02:06.591 --> 00:02:11.591
predicted parse tree and then we will increase the score s why have the correct

35
00:02:13.581 --> 00:02:17.870
guy and the crazy decreases a score of a y hat?
No,

36
00:02:17.871 --> 00:02:22.871
that s why had by doing an update in the direction of the gradient of the score,

37
00:02:25.400 --> 00:02:27.920
s why the gradient,
which respected of parameters,

38
00:02:27.990 --> 00:02:32.990
a model of this core s y minus the gradients of the parameter with respect to

39
00:02:33.801 --> 00:02:38.600
the parameters of the score for the predicted three y hat.

40
00:02:39.170 --> 00:02:39.711
So in other words,

41
00:02:39.711 --> 00:02:43.550
we have to take these two gradients and subtracting because this is giving us a

42
00:02:43.551 --> 00:02:48.551
direction that will increase the score a four y and this is going to give us a

43
00:02:48.771 --> 00:02:53.270
direction that is going to decrease the score,
four of the parse tree y hat.

44
00:02:54.410 --> 00:02:57.200
And now to get the gradients,
uh,

45
00:02:57.260 --> 00:03:01.400
each of these two gradients and to combine it to get the single gradient,

46
00:03:01.890 --> 00:03:05.370
uh,
uh,
or,
uh,
saw a single direction,
uh,

47
00:03:05.530 --> 00:03:08.030
in which to update our parameters.
Then we'll,

48
00:03:08.031 --> 00:03:12.970
we'll have to do is do a regular backpropagation through the recursive network

49
00:03:12.971 --> 00:03:17.290
that follows the structure of a y and y hat.

50
00:03:17.710 --> 00:03:22.450
So I'm essentially for say why,
uh,
had,
uh,

51
00:03:22.660 --> 00:03:26.590
was a parse tree corresponding to that structure.

52
00:03:28.450 --> 00:03:30.820
Then in this case,
it means we'd have a neural net,

53
00:03:30.821 --> 00:03:34.810
which is taking the word representation here in here and computing a

54
00:03:34.811 --> 00:03:36.910
representation,
which is like a hidden,

55
00:03:36.911 --> 00:03:41.050
they are at this level in computing a hidden layer at this level by combining

56
00:03:41.051 --> 00:03:41.780
this,

57
00:03:41.780 --> 00:03:46.570
a vector representation with the effect of representation here from both of
them,

58
00:03:46.571 --> 00:03:51.550
we'd get as the output a score,
the score of that merge and that merge.

59
00:03:51.820 --> 00:03:56.820
And these would be some together to get the score of the full parse tree.

60
00:03:57.820 --> 00:04:00.550
Why?
So that would be for propagation.

61
00:04:00.551 --> 00:04:04.570
And then backward propagation would be to assume that we have a great in

62
00:04:04.571 --> 00:04:09.571
retrospect to s why have one and then moving in the other direction back

63
00:04:11.261 --> 00:04:16.210
propagating the gradient,
respect to all of our parameters,
uh,

64
00:04:16.330 --> 00:04:18.160
to get the gradient,
uh,

65
00:04:18.190 --> 00:04:21.430
which affects all the parameters for that four s y.

66
00:04:21.880 --> 00:04:26.140
And then if we wanted to get the gradient minus s y hat,

67
00:04:26.380 --> 00:04:30.830
then so imagine that y hat corresponded to different,
uh,
parse trees.

68
00:04:30.831 --> 00:04:33.220
So maybe this parse tree here,

69
00:04:35.280 --> 00:04:35.870
<v 1>uh,</v>

70
00:04:35.870 --> 00:04:39.920
<v 0>then I'd beat would be competing this vector representation and then this one</v>

71
00:04:39.921 --> 00:04:44.921
and from both of them we'd get a score which would be some together and this

72
00:04:45.171 --> 00:04:49.380
would give us s y hat.
Then again,

73
00:04:49.381 --> 00:04:51.020
I would assume that the gradient,

74
00:04:51.250 --> 00:04:55.500
just like to s y hat is minus one cause we're interested in the gradient in,

75
00:04:55.630 --> 00:04:56.990
in the negative gradient.

76
00:04:57.140 --> 00:05:01.400
And then we'd back propagate through the neural network that was constructed,

77
00:05:01.720 --> 00:05:03.440
a recursively using this,

78
00:05:03.620 --> 00:05:08.620
this syntactic structure of y hat and then back propagate through to all the

79
00:05:09.081 --> 00:05:14.060
parameters.
And so these two procedures would give us,
uh,

80
00:05:14.120 --> 00:05:17.180
this part and this part of our update direction.

81
00:05:17.181 --> 00:05:19.160
And then we'd combine them by subtracting them.

82
00:05:19.340 --> 00:05:22.310
And that gives us the update direction to follow,

83
00:05:22.311 --> 00:05:24.710
to update all of the parameters.

84
00:05:25.460 --> 00:05:28.910
So that's how training would proceed in this case.

85
00:05:31.450 --> 00:05:32.283
<v 1>Okay.</v>

86
00:05:32.660 --> 00:05:37.310
<v 0>Now,
um,
in the parse tree,
uh,
often the nodes are also labeled.</v>

87
00:05:37.340 --> 00:05:38.990
They're labeled retrospect too,

88
00:05:38.991 --> 00:05:43.991
whether they correspond to a noun phrase or verb phrase and uh,

89
00:05:44.440 --> 00:05:47.570
uh,
using these,
these labels actually available in the training data.

90
00:05:47.960 --> 00:05:52.960
So one thing we could do is to just also add from each a representation at every

91
00:05:54.141 --> 00:05:57.320
level at a connections to a softmax,

92
00:05:57.860 --> 00:06:00.470
which would try to predict what that label is.

93
00:06:00.800 --> 00:06:03.920
So instead of having just as,
uh,

94
00:06:04.190 --> 00:06:08.700
from each vector representation of a score that is output,

95
00:06:08.710 --> 00:06:13.710
it will also have other parameters of the neural network that would also try to

96
00:06:14.420 --> 00:06:19.420
predict what's the distribution over the label of that at that node and a,

97
00:06:20.451 --> 00:06:24.140
and then we could use the negative log likelihood of the true label as a

98
00:06:24.141 --> 00:06:27.650
gradient,
which could be done back propagated through the whole network.

99
00:06:28.310 --> 00:06:31.730
And this weekend only do for the true parts tree.
Why?

100
00:06:31.760 --> 00:06:33.830
Because for the predictive process,
three,

101
00:06:34.070 --> 00:06:37.950
we don't necessarily have notes that correspond to the original parts street.

102
00:06:37.951 --> 00:06:40.040
So we don't know what the label should actually be.

103
00:06:40.460 --> 00:06:45.350
So this other backpropagation that we can do to integrate also,

104
00:06:45.710 --> 00:06:49.340
uh,
the notion of labels at the Inter internal note,

105
00:06:49.850 --> 00:06:54.110
uh,
in the parse tree,
we can only do it for the true part street.

106
00:06:54.530 --> 00:06:56.530
And so then we,
we,
uh,

107
00:06:56.570 --> 00:07:00.530
we checked what's the negative log likelihood at every internal note.

108
00:07:00.830 --> 00:07:03.680
And this would give us a gradient for that.

109
00:07:03.681 --> 00:07:08.681
Those output layers layers are which would be back propagated through the whole

110
00:07:09.290 --> 00:07:12.500
network.
Similar knee has me back propagated back,

111
00:07:12.501 --> 00:07:15.500
propagate the gradient with respect to the score of the treats.

112
00:07:17.920 --> 00:07:18.753
<v 1>Okay.</v>

113
00:07:18.920 --> 00:07:23.070
<v 0>Some other details about how uh,
training proceeds,
um,</v>

114
00:07:23.180 --> 00:07:27.650
in their paper they actually use the word representation using the language

115
00:07:27.651 --> 00:07:30.800
modeling task of color bag and western.
Uh,

116
00:07:30.801 --> 00:07:34.190
and then so they used that to initialize the representation and then they would

117
00:07:34.191 --> 00:07:38.150
find tune them.
He sent in Chile because they use them as initializations.

118
00:07:38.450 --> 00:07:41.600
They would,
they would still train them,
uh,

119
00:07:41.660 --> 00:07:45.920
using the recursive auto encoder,
sorry,
recursive network,

120
00:07:46.250 --> 00:07:49.070
uh,
uh,
training procedure that I've described.

121
00:07:49.250 --> 00:07:52.340
So this addition of the auto encoder is good.
This should be network.

122
00:07:54.070 --> 00:07:54.740
<v 1>Okay.</v>

123
00:07:54.740 --> 00:07:57.140
<v 0>Uh,
so that's another detail that's uh,</v>

124
00:07:57.200 --> 00:08:01.520
that that was important for getting good results.
The other thing,

125
00:08:01.521 --> 00:08:06.521
which I don't want to describe in more details because it goes a bit beyond what

126
00:08:06.861 --> 00:08:07.820
we've seen so far,

127
00:08:07.821 --> 00:08:12.821
is that the training criteria they actually use for training the recursive

128
00:08:13.221 --> 00:08:15.890
network to a gift.

129
00:08:15.891 --> 00:08:20.710
The correct a parse tree is actually a margin based criteria.
Uh,

130
00:08:20.780 --> 00:08:25.780
essentially they don't want the correct parts tree to have a strictly greater,

131
00:08:26.570 --> 00:08:31.400
only a strictly greater or score it then any other parts,
street y hat,
sorry.

132
00:08:31.401 --> 00:08:33.830
Why Star?
They actually want the score,

133
00:08:33.831 --> 00:08:38.831
the correct post street to be at at least a some margin greater than the

134
00:08:40.040 --> 00:08:44.990
correct.
Uh,
sorry.
Then any other parts street and that margin for a parse tree.

135
00:08:45.020 --> 00:08:49.280
Why Star,
uh,
is going to be this delta?
Why,
why star here,

136
00:08:49.281 --> 00:08:54.281
which corresponds to essentially the number of errors in predicting why star,

137
00:08:54.651 --> 00:08:56.430
when the true tree is.

138
00:08:56.460 --> 00:09:01.230
Why and this number of errors is actually,
uh,

139
00:09:01.260 --> 00:09:04.380
the number of,
of incorrect spends.

140
00:09:04.590 --> 00:09:08.430
So just give an example to make this more concrete.
So imagine that the,

141
00:09:08.590 --> 00:09:11.000
our street was y and then,
um,

142
00:09:11.070 --> 00:09:14.440
we considering what's the number of incorrect span with respect to say,

143
00:09:14.441 --> 00:09:18.810
predicting why star here.
Uh,
then there would be one incorrect span.

144
00:09:18.811 --> 00:09:23.811
And that's because at this internal node where spanning the words from,

145
00:09:24.750 --> 00:09:27.000
uh,
from position three to four,

146
00:09:27.270 --> 00:09:31.210
whereas in the predicted,
uh,
or this,

147
00:09:31.211 --> 00:09:34.860
this other incorrect Parse,
streetwise star,
uh,

148
00:09:34.890 --> 00:09:37.680
there are no notes that span from three to four.

149
00:09:37.681 --> 00:09:41.100
There's instead a node that spanned from one to three,

150
00:09:41.101 --> 00:09:44.040
which is not present in the,
um,

151
00:09:44.160 --> 00:09:47.240
in the original and the true par street.
Uh,

152
00:09:47.280 --> 00:09:50.550
so we don't incur an error for emerging this year.

153
00:09:50.700 --> 00:09:54.960
And that's because we also merged that,
uh,
these two together.

154
00:09:55.020 --> 00:09:59.020
And also we don't incur an error at the global tree.

155
00:09:59.030 --> 00:10:03.960
So I have this note here because this is spanning all words and similar to here

156
00:10:03.961 --> 00:10:08.370
at this level of the parse tree,
we're also spending a all words,

157
00:10:09.860 --> 00:10:10.620
um,

158
00:10:10.620 --> 00:10:15.620
and it turns out that using we can just do a very small modification of the

159
00:10:15.631 --> 00:10:19.790
approximate,
uh,
inference algorithm that we saw before,
uh,

160
00:10:19.830 --> 00:10:22.620
which is essentially a beam search kind of approach.

161
00:10:22.880 --> 00:10:24.330
And so we can actually modify,

162
00:10:24.331 --> 00:10:28.800
it's to instead try to find the tree that,
uh,

163
00:10:28.830 --> 00:10:33.790
most violates this margin criteria.
I'm going very fast over it as a,

164
00:10:33.870 --> 00:10:37.470
I don't have a really want to give more details about that,

165
00:10:37.500 --> 00:10:41.970
but I encourage you to know more about this training procedure.
Uh,

166
00:10:42.000 --> 00:10:46.850
if you still have questions to look at the social al paper where more details
is,

167
00:10:46.910 --> 00:10:50.310
is given,
uh,
about that.
Um,
and we're respected.

168
00:10:50.320 --> 00:10:55.170
This training procedure essentially corresponds to trying to do approximate

169
00:10:55.380 --> 00:10:58.500
structured as VM kind of training,
uh,

170
00:10:58.560 --> 00:11:01.920
of the score given by the recursive neural network.
But again,

171
00:11:01.921 --> 00:11:05.340
to get more details about everything that's described in this video about the

172
00:11:05.341 --> 00:11:08.280
training procedure,
I encourage you to look at this paper here.

173
00:11:10.560 --> 00:11:13.810
And in terms of the performance,
if we look at the uh,
uh,

174
00:11:13.890 --> 00:11:16.080
performance for a syntactic parsing,

175
00:11:16.290 --> 00:11:21.180
a which is usually measured using the f one,
uh,
measure,
uh,

176
00:11:21.210 --> 00:11:25.680
the recursive neural network gets a performance of 90 point 25%.

177
00:11:26.010 --> 00:11:30.030
While,
uh,
the one of the state of the art parser,
the Berkeley Pars or,

178
00:11:30.840 --> 00:11:34.790
uh,
gets 91.6 33%.
So it's a,

179
00:11:34.830 --> 00:11:39.240
it's a little bit worse,
uh,
but it's quite close.
Uh,
and,

180
00:11:39.270 --> 00:11:41.590
and so that's,
that's pretty satisfying.
Uh,

181
00:11:41.591 --> 00:11:45.300
if you look at the paper by rich Richard Socher,
uh,

182
00:11:45.301 --> 00:11:50.301
you actually see that you can use the same algorithm for also parsing visual

183
00:11:50.851 --> 00:11:55.720
scenes and for visual scene parsing actually,
uh,
got at,

184
00:11:55.750 --> 00:11:58.030
uh,
at that time a state of the art results.

185
00:11:58.060 --> 00:12:02.620
So while it might look like this is only applicable to parsing sentences,

186
00:12:02.621 --> 00:12:06.160
you can also parse visual scenes and,
and,
and in this case,

187
00:12:06.161 --> 00:12:09.550
it actually gets state of the art results.
And finally in the paper,

188
00:12:09.551 --> 00:12:13.690
they also provide a,
an analysis to try to see these,
uh,

189
00:12:14.260 --> 00:12:17.260
representation of phrases and then sentences,
uh,

190
00:12:17.261 --> 00:12:21.490
whether they actually capture a similarity that's meaningful.

191
00:12:21.850 --> 00:12:24.480
So for instance,
if we took that sentence,
would you,

192
00:12:24.510 --> 00:12:28.600
is that why gained 52?
Uh,
so,
uh,
UNC,

193
00:12:28.601 --> 00:12:32.860
that's because this is for an unknown word or,
or o v word,

194
00:12:32.861 --> 00:12:36.280
that's just the token they use for representing Ovi.

195
00:12:37.210 --> 00:12:39.070
And we look at,
uh,
uh,

196
00:12:39.100 --> 00:12:43.990
other sentences that have a or phrases that have very similar vector

197
00:12:43.991 --> 00:12:48.940
representations.
We get,
get phrases that indeed maintain some of the,
you know,

198
00:12:48.941 --> 00:12:53.440
have very similar information as this contained in,
in this particular phrase.

199
00:12:53.800 --> 00:12:54.041
Again,

200
00:12:54.041 --> 00:12:58.330
in the other example here where the dollar dropped was judged to be similar to

201
00:12:58.331 --> 00:13:02.740
the dollar retreated or dollar gained or bunk prices rallied.

202
00:13:03.080 --> 00:13:07.390
Where here we get a fairly different phrase than this,

203
00:13:07.391 --> 00:13:11.320
but we see it's talking about similar concepts.
So again,

204
00:13:11.321 --> 00:13:13.690
I encourage you to look at,
uh,
this paper vibe,

205
00:13:13.720 --> 00:13:18.430
social and there are actually many more extensions that have been developed,

206
00:13:18.431 --> 00:13:21.400
developed in the following years.
Uh,

207
00:13:21.430 --> 00:13:25.810
and so I encourage you to also look at that.
And,
uh,
that's it.

208
00:13:25.900 --> 00:13:27.460
Or recursive neural networks.

