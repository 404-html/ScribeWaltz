WEBVTT

1
00:00:00.250 --> 00:00:00.850
Okay.

2
00:00:00.850 --> 00:00:01.351
<v 1>In this video,</v>

3
00:00:01.351 --> 00:00:06.030
we'll talk about how we can infer the syntactic tree of a sentence using a

4
00:00:06.031 --> 00:00:07.320
recursive neural network.

5
00:00:08.300 --> 00:00:09.133
<v 0>Okay.</v>

6
00:00:09.650 --> 00:00:12.530
<v 1>So in the previous video,
we discussed how,
uh,</v>

7
00:00:12.560 --> 00:00:16.400
in the recursive neural network approach,
we,
uh,
had the neural network,

8
00:00:16.401 --> 00:00:19.220
which could merge the Paris of,
uh,

9
00:00:19.240 --> 00:00:24.240
a pair of representations of different nodes in the syntactic tree and then

10
00:00:25.160 --> 00:00:28.190
output the representation of a parent node.

11
00:00:28.790 --> 00:00:32.410
And we also discussed that we,
uh,
in that neural network,

12
00:00:32.420 --> 00:00:35.000
we would also have a part of the neural network,

13
00:00:35.001 --> 00:00:39.440
which would also output a score for the quality of that merged.

14
00:00:39.710 --> 00:00:43.550
And what we see now is that we could use that score in order to score entire

15
00:00:43.551 --> 00:00:44.384
trees.

16
00:00:44.450 --> 00:00:49.450
And then it's essentially a search in the space of trees to find a,

17
00:00:50.060 --> 00:00:53.180
uh,
the trio would,
which would be most appropriate as the,
the,

18
00:00:53.181 --> 00:00:57.230
the best thing tactic tree,
uh,
to explain a given sentence.

19
00:00:57.650 --> 00:01:01.910
And at which point it means that we can just perform syntactic parsing using a

20
00:01:01.911 --> 00:01:03.200
recursive neural network.

21
00:01:06.720 --> 00:01:10.920
So this is the neural net we saw before.
It takes a,
this is emerging neural net,

22
00:01:10.921 --> 00:01:15.750
which takes representation of two,
uh,
children in a tree,

23
00:01:16.110 --> 00:01:17.580
uh,
representing a,

24
00:01:17.630 --> 00:01:21.760
there were a single word or a phrase in that tree and then outputs a merge

25
00:01:21.761 --> 00:01:24.210
representation for the parent in the tree.

26
00:01:24.630 --> 00:01:27.270
And then it also gives us a,

27
00:01:27.370 --> 00:01:31.860
a score for the quality of that merge as just the linear transformation of the

28
00:01:31.861 --> 00:01:36.000
merged representation.
Now let's assume for now that this,

29
00:01:36.170 --> 00:01:37.720
uh,
this,

30
00:01:37.730 --> 00:01:42.660
this prediction score is actually a good score for identifying where there,
uh,

31
00:01:42.690 --> 00:01:45.150
these two parts of the sentences,

32
00:01:45.151 --> 00:01:49.560
these two phrases or two words or pair of words and phrase,
uh,

33
00:01:49.620 --> 00:01:54.420
we're actually,
uh,
appropriate for being merged and be part of,

34
00:01:54.530 --> 00:01:57.510
uh,
uh,
a more global,
a sink tactic tree.

35
00:01:57.960 --> 00:02:02.940
Now how can we use that in order to assign scores to a entire trees?

36
00:02:03.150 --> 00:02:07.930
And then how could we try to at least approximately search for the best treat?

37
00:02:07.931 --> 00:02:09.420
It has the biggest score.

38
00:02:11.960 --> 00:02:12.793
<v 0>Okay.</v>

39
00:02:13.100 --> 00:02:13.700
<v 1>Well,</v>

40
00:02:13.700 --> 00:02:18.260
the simplest approach in terms of scoring an entire tree is to just assign it

41
00:02:18.261 --> 00:02:18.951
the score,

42
00:02:18.951 --> 00:02:23.390
which is going to be the sum of all of the merging scores.

43
00:02:23.391 --> 00:02:26.450
So essentially the is going to be as good as the,

44
00:02:26.451 --> 00:02:31.451
some of the qualities of each mergers that are required.

45
00:02:32.570 --> 00:02:35.990
Then I that are done by the recursive neural network for that part street.

46
00:02:36.530 --> 00:02:38.780
So for instance,
imagine that we had the small sentence,

47
00:02:38.781 --> 00:02:42.600
the cat is here and that uh,
we,
uh,

48
00:02:42.650 --> 00:02:47.240
considered this particular part story then the merchants we'd have to do is we'd

49
00:02:47.241 --> 00:02:52.241
have to merge the representation of the and cat to get the representation here

50
00:02:53.150 --> 00:02:57.560
and then are merging neural network would also give us as the output a score,

51
00:02:57.561 --> 00:03:01.310
which I'm calling s one,
two,
one two,
because uh,

52
00:03:01.330 --> 00:03:06.330
it's essentially a scoring this subtree which merges one with two.

53
00:03:07.070 --> 00:03:08.740
Uh,
we'd also have to emerge is,

54
00:03:08.741 --> 00:03:12.860
and here this would give us this word representation from a,

55
00:03:12.970 --> 00:03:16.390
sorry phrase representation from these two word representations.
And again,

56
00:03:16.391 --> 00:03:20.920
we get a score for that merge.
And then following,
this part's true.

57
00:03:20.921 --> 00:03:24.610
You have to merge these two notes to get the parent node for the full sentence.

58
00:03:24.611 --> 00:03:29.611
So we're merging this representation and that representation by concatenating

59
00:03:30.191 --> 00:03:34.360
them multiplying by w applying a sigmoid,
which gives us this factor.

60
00:03:34.570 --> 00:03:39.340
And from that vector multiplying by w score gives us the score for that merge,

61
00:03:39.610 --> 00:03:42.560
which I'm calling s one,
two and three four.

62
00:03:42.561 --> 00:03:46.810
So we see that the index is essentially representation with parent thesis of the

63
00:03:46.811 --> 00:03:50.290
actual tree.
That's the notation and I'm using here.

64
00:03:50.680 --> 00:03:54.580
And then the score for the full tree is just going to be s one,

65
00:03:54.581 --> 00:03:58.720
two plus s three four plus s one,
two,
three,

66
00:03:58.721 --> 00:04:03.180
four as written here.
So in other words,
the some,
uh,
the,

67
00:04:03.181 --> 00:04:07.750
the score of the tree is just a sum of all the scores.
I computed as I was,

68
00:04:08.210 --> 00:04:12.340
uh,
performing all the,
uh,
representation mergers,

69
00:04:12.700 --> 00:04:13.533
uh,

70
00:04:13.540 --> 00:04:18.250
as I was going up towards the root of the corresponding part street.

71
00:04:20.500 --> 00:04:23.830
Now if we were considering instead this parse tree,

72
00:04:24.040 --> 00:04:29.040
then I'd have to merge the with gap giving us this and this score here.

73
00:04:29.970 --> 00:04:33.430
Uh,
then I take that representation and this word representation,

74
00:04:33.460 --> 00:04:38.460
merging it to get the representation for their cat is which is obtained by,

75
00:04:38.750 --> 00:04:39.640
uh,

76
00:04:39.700 --> 00:04:44.700
taking this representation here concatenated with this one multiplied by w by

77
00:04:44.771 --> 00:04:47.680
sigmoid,
then multiplying by w score,

78
00:04:47.681 --> 00:04:50.950
we get the score for that particular emerge.

79
00:04:51.280 --> 00:04:56.140
And so on to get the score also for the merge at the root of the tree.

80
00:04:56.380 --> 00:05:01.380
And then this syntactic true would have instead of score s one to s one,

81
00:05:01.631 --> 00:05:05.950
two with three and s,
uh,
one,
two with three,

82
00:05:05.951 --> 00:05:10.120
and then one,
two with three merged with one.
Okay,

83
00:05:10.990 --> 00:05:15.990
so now the question is for a given [inaudible] neural net that performs these

84
00:05:16.331 --> 00:05:21.160
merges.
So you know,
this,
this,
uh,
neural network that would,
uh,

85
00:05:21.161 --> 00:05:25.210
the W's and the w score and the biases as well.

86
00:05:25.270 --> 00:05:28.000
These are the parameters of their recursive neural network.

87
00:05:28.001 --> 00:05:31.750
It corresponds to the recursive neural network really.
And,
uh,

88
00:05:31.751 --> 00:05:34.090
so far a particular Chris neural network,

89
00:05:34.360 --> 00:05:38.170
what is the tree that has the highest score?
Um,

90
00:05:38.200 --> 00:05:42.550
how do we find this particular tree?
Unfortunately,
we can't do this exactly.

91
00:05:42.700 --> 00:05:47.110
However,
Richard Socher and,
uh,
his colleagues propose and approximate algorithm,

92
00:05:47.111 --> 00:05:51.980
which they claim works well,
uh,
uh,
in practice and indeed,
uh,

93
00:05:52.150 --> 00:05:55.870
the report good results by using this particular inference procedure.

94
00:05:57.020 --> 00:06:00.350
So let's look
at a disenfranchise procedure.
I'm,

95
00:06:01.090 --> 00:06:04.850
I'm going to describe it essentially using an example to give you an intuition

96
00:06:04.851 --> 00:06:05.780
for how it works.

97
00:06:08.620 --> 00:06:13.620
So we want to find the tree which has the highest sum of all of the merging

98
00:06:13.931 --> 00:06:14.764
scores.

99
00:06:15.460 --> 00:06:19.070
And so because the sum of a bigger tree is going through a,

100
00:06:19.150 --> 00:06:24.060
we can think of it as essentially the composite as the sum of,
uh,

101
00:06:24.320 --> 00:06:26.800
a d score associated with it sucked trees.

102
00:06:26.830 --> 00:06:29.590
Then that's essentially how will obtain an algorithm,

103
00:06:29.591 --> 00:06:34.591
which will first look at the best a sub trees and then move toward,

104
00:06:36.600 --> 00:06:41.530
uh,
move to,
uh,
trees that cover more and more words to eventually reach,

105
00:06:41.920 --> 00:06:46.630
uh,
Eh,
an inference of the tree,
which would cover all of the words.

106
00:06:47.380 --> 00:06:50.950
So specifically we'll have,
and the rea,
which I'm calling eight here,

107
00:06:51.160 --> 00:06:55.440
which is indexed by the first and last word that the,
uh,

108
00:06:55.540 --> 00:07:00.400
tree for this particular selling dra covers.
So for instance,

109
00:07:00.430 --> 00:07:02.660
this cell would correspond to,
uh,

110
00:07:02.710 --> 00:07:06.520
trying to infer the treat that covers the words from position one,
two,
three.

111
00:07:06.520 --> 00:07:11.520
So the cat and it's a two four would be covering instead,

112
00:07:11.710 --> 00:07:14.950
cat is here and one,

113
00:07:14.951 --> 00:07:18.880
two would just be the tree that covers just the end cap and so on.

114
00:07:20.050 --> 00:07:23.500
And,
uh,
as we'll see as I described the algorithm out,

115
00:07:23.501 --> 00:07:26.530
I'll be able to reuse what I'm looking at,

116
00:07:26.531 --> 00:07:29.440
the scores of the trees that cover one and three,

117
00:07:29.441 --> 00:07:34.110
I'll be able to reuse these scores of the [inaudible],
uh,

118
00:07:34.240 --> 00:07:37.570
below in the Ndra.
And uh,

119
00:07:37.580 --> 00:07:41.320
and so this will be able to reuse the computations that I've done for a smaller

120
00:07:41.321 --> 00:07:45.250
sub trees to,
uh,
performing maximisation for larger centuries.

121
00:07:46.810 --> 00:07:50.300
So,
um,
let's first consider,
uh,

122
00:07:50.320 --> 00:07:53.440
what is the best subtree that covered just two words.
So one,

123
00:07:53.441 --> 00:07:57.890
two and to three and three,
four.
And this gives it,

124
00:07:57.891 --> 00:08:01.240
its just one tree.
The tree that merges the two corresponding words.

125
00:08:02.530 --> 00:08:07.530
So from that we would get then compute what is the score associated with each

126
00:08:08.261 --> 00:08:12.670
such subtree.
So in this case,
this is a score of merging one with two,

127
00:08:12.790 --> 00:08:16.420
which could,
could be computed using our recursive neural network there.

128
00:08:16.560 --> 00:08:20.840
The part that merges the scores and then computes a,

129
00:08:21.100 --> 00:08:25.240
the emerges that word representations and then computes a score.
Uh,

130
00:08:25.241 --> 00:08:27.580
we also probably have a,
uh,

131
00:08:27.581 --> 00:08:32.320
another [inaudible] which would actually store the representation for,
uh,

132
00:08:32.340 --> 00:08:34.600
for that tree,
which is going to be used later on.

133
00:08:34.750 --> 00:08:38.330
But for now it's consider computing the finding the,
uh,

134
00:08:38.650 --> 00:08:41.560
maximizing value for the sum of scores.

135
00:08:43.450 --> 00:08:46.870
Next we can move to tweeze of size two.

136
00:08:46.871 --> 00:08:50.620
So this covered trees of size one.
Now let's look at trees of size too.

137
00:08:51.490 --> 00:08:55.410
So for the tree that would,
um,
and by size too,

138
00:08:55.411 --> 00:08:58.530
I mean the number of internal notes.
So these trees covered,
uh,

139
00:08:58.620 --> 00:09:01.740
you of size do with cover,
uh,
three words.

140
00:09:03.330 --> 00:09:08.310
Um,
so now imagine the tree that covers,
uh,
one,
uh,
the first word,

141
00:09:08.311 --> 00:09:12.570
the second word.
And the third word,
uh,
with are two such trees.

142
00:09:12.600 --> 00:09:15.810
There's the tree which merges one,
two first,

143
00:09:16.290 --> 00:09:21.290
and then merge is the representation of one two with the third word or the other

144
00:09:21.631 --> 00:09:24.300
way around.
Merges two,
three together first,

145
00:09:24.301 --> 00:09:28.110
and then merges the first word with their result of merging two and three.

146
00:09:28.980 --> 00:09:33.250
So the sum of the scores in this case would be the,
uh,

147
00:09:33.300 --> 00:09:35.790
score of the first merge here.

148
00:09:35.940 --> 00:09:40.940
And then the numeral that corresponds to the root of this sub tweet that covers

149
00:09:41.910 --> 00:09:46.500
a words from one,
two,
three.
So I'll read the computed the emerge up one and two.

150
00:09:46.501 --> 00:09:47.730
That's a one too.

151
00:09:48.510 --> 00:09:53.510
So I can just take that score and then add a new score of merging the result of

152
00:09:53.761 --> 00:09:57.540
emerging one two with uh,
the third word,
which is it's,

153
00:09:57.630 --> 00:10:01.810
so that's why we need that corresponding word fee.
Uh,

154
00:10:02.000 --> 00:10:05.330
we're a site,
an upward vector representation somewhere too.

155
00:10:05.331 --> 00:10:07.530
We use it in order to compute that score.

156
00:10:08.830 --> 00:10:09.240
<v 0>Okay.</v>

157
00:10:09.240 --> 00:10:13.650
<v 1>So then I could compare that with the possibility of merging two and three first</v>

158
00:10:14.010 --> 00:10:16.800
a witch and the score for that as being computed already.

159
00:10:16.801 --> 00:10:19.410
It's in this cell of the array.

160
00:10:19.980 --> 00:10:23.790
And then I would just add the score of then merging one with the result of

161
00:10:23.791 --> 00:10:25.200
having merged two and three.

162
00:10:25.650 --> 00:10:29.250
I'm assuming the APP that already somewhere that I can pass the word

163
00:10:29.251 --> 00:10:33.750
representation for one,
the representation for two and three and then again,

164
00:10:33.760 --> 00:10:35.820
query mind neural nets to get a score,

165
00:10:35.850 --> 00:10:40.830
which would be this and then add that to the value in the cell two and three.

166
00:10:41.790 --> 00:10:45.990
And then I do a comparison.
And then let's imagine that in this example,

167
00:10:46.170 --> 00:10:49.500
this sub tree is actually more likely a den.

168
00:10:49.680 --> 00:10:52.380
That's the value that I would keep in this area.

169
00:10:52.680 --> 00:10:57.680
And also somewhere I would keep what is the resulting vector at this node in

170
00:10:58.141 --> 00:11:00.420
that sub tree so I can reuse it later.

171
00:11:02.440 --> 00:11:02.990
<v 0>Okay.</v>

172
00:11:02.990 --> 00:11:06.360
<v 1>Similarly for the tree that covers,
uh,</v>

173
00:11:06.440 --> 00:11:08.540
words from two to a position to,

174
00:11:08.541 --> 00:11:11.450
to visit for af two possibilities.

175
00:11:11.690 --> 00:11:16.690
Either I reuse the subtree covering two n three and then I merged the result of

176
00:11:18.171 --> 00:11:20.660
that with the fourth word in the sentence.

177
00:11:20.720 --> 00:11:23.270
Or I do the other way around you as a,

178
00:11:23.290 --> 00:11:27.650
assuming I've merged three and four together and then merging the result of that

179
00:11:27.860 --> 00:11:32.540
with the second work.
And so similarly,
Zach do a comparison.

180
00:11:32.541 --> 00:11:36.950
Maybe this is the most likely,
uh,
sorry,
the,
the,
the,
the,

181
00:11:37.010 --> 00:11:38.810
some of scores that is highest.

182
00:11:39.140 --> 00:11:42.590
And so I started that instead in the Dra and uh,

183
00:11:42.620 --> 00:11:45.050
and then also take the result of this,

184
00:11:45.051 --> 00:11:49.190
merge the vector representation and put it somewhere so I can reuse it later.

185
00:11:52.300 --> 00:11:52.900
<v 0>Okay.</v>

186
00:11:52.900 --> 00:11:55.180
<v 1>Not,
if I reach that part of the IRA,</v>

187
00:11:55.181 --> 00:11:59.970
then now I have to consider all the trees that,
uh,
cover,
uh,

188
00:12:00.030 --> 00:12:04.780
forwards.
And this gives does own the one.
Uh,
so in this case it can,

189
00:12:05.200 --> 00:12:08.190
if we're coming forwards,
it can only cover from the first the forward.

190
00:12:08.200 --> 00:12:13.060
So all words in the sentence and then there are three possibilities.
Uh,

191
00:12:13.061 --> 00:12:14.680
so what can I do?

192
00:12:15.070 --> 00:12:19.930
I can either assume that,
um,

193
00:12:20.740 --> 00:12:25.390
I have already merged all of the words from the first two,
the third position,

194
00:12:25.810 --> 00:12:30.810
and then all that's left is to merge the fourth word with my representation of,

195
00:12:31.540 --> 00:12:33.730
uh,
with,
uh,
uh,
of the,
uh,

196
00:12:33.731 --> 00:12:38.731
phrase from the covers were a position one to position three.

197
00:12:39.490 --> 00:12:42.400
So,
uh,
so this would be the score of that sub for you.

198
00:12:42.401 --> 00:12:47.401
Plus the new score from merging the fourth word with the representation of the

199
00:12:47.570 --> 00:12:52.000
uh,
phrase from one to three,
the cat is,

200
00:12:52.780 --> 00:12:57.780
and now I want to compare that with instead of having merge one to two and also

201
00:12:57.941 --> 00:13:00.070
having merged three to four separately,

202
00:13:00.100 --> 00:13:04.540
and then just taking [inaudible] and joining them with a parent node and,

203
00:13:04.650 --> 00:13:07.800
and merging the representation of these two separate treats.

204
00:13:08.230 --> 00:13:12.910
So one and two and three and four,
uh,

205
00:13:12.940 --> 00:13:15.990
there,
uh,
here.
And here's,

206
00:13:16.000 --> 00:13:20.020
I can reuse those computations again.
And uh,

207
00:13:20.021 --> 00:13:24.000
so these are raise the cells in these arrays contain the scores of these two sub

208
00:13:24.010 --> 00:13:24.291
trees.

209
00:13:24.291 --> 00:13:29.291
And I just need to increment the score of a so and incorporate the score of

210
00:13:31.091 --> 00:13:34.930
merging the representation of the phrase from one to two.

211
00:13:34.931 --> 00:13:39.880
So the phrase the cat with the representation of the phrase three for the phrase

212
00:13:39.940 --> 00:13:43.090
is here.
Um,

213
00:13:43.570 --> 00:13:47.770
and then finally I can instead assume that I've merged already.

214
00:13:47.771 --> 00:13:51.760
Everything that's from two to force.
I have a,
I think the tree that,

215
00:13:51.940 --> 00:13:56.320
the best subject I found for covering a words at position two to four.

216
00:13:56.321 --> 00:13:59.740
And then I add to this,
the,
uh,
remaining merge,

217
00:13:59.741 --> 00:14:03.280
which is to take the representation of the first word and merge it with the

218
00:14:03.281 --> 00:14:08.050
representation of the phrase from the second position to the fourth position.

219
00:14:08.080 --> 00:14:12.940
So now I have three things to compare.
And now let's imagine that in this case,

220
00:14:13.300 --> 00:14:17.080
uh,
it's this option here that is most likely,

221
00:14:17.260 --> 00:14:19.480
that is assuming I've already covered,
uh,

222
00:14:19.780 --> 00:14:24.220
one to two and three to four and then just merging these two subjects together.

223
00:14:25.510 --> 00:14:27.350
And now in that case,
um,

224
00:14:27.760 --> 00:14:31.810
if I then wanted to infer what is the syntactic tree that corresponds to that

225
00:14:31.840 --> 00:14:34.780
when in this case it would correspond to disinfect Dick Tree.

226
00:14:35.080 --> 00:14:40.000
So indeed here we've made the decisions that we wanted to have merged.
Uh,
the,

227
00:14:40.030 --> 00:14:40.690
uh,
have,
uh,

228
00:14:40.690 --> 00:14:45.240
use the subtree between one and two and uh,

229
00:14:45.280 --> 00:14:48.790
the and joined with the subtree between three and four.

230
00:14:49.090 --> 00:14:52.000
And then the subject between one and two,
well,
there's only one,

231
00:14:52.120 --> 00:14:54.830
it's the one that merges the first word,
the second word.

232
00:14:55.070 --> 00:15:00.070
So it's that sub tree and then is here the century that the only possible

233
00:15:00.420 --> 00:15:04.940
separately is this one and a here at the decision that have made was to merge

234
00:15:04.941 --> 00:15:08.210
these two.
So we get the global tree,
which is like this.

235
00:15:08.980 --> 00:15:13.640
So that's a very simple,
uh,
uh,
example for this,
this,

236
00:15:13.700 --> 00:15:18.190
uh,
our with them.
Uh,
I just wanted to give you an intuitive,
uh,

237
00:15:18.260 --> 00:15:21.440
impression of,
I actually works for more details.

238
00:15:21.441 --> 00:15:26.330
I encourage you to look at the original Richard social paper and the references

239
00:15:26.331 --> 00:15:31.331
and contains to know more about how you could actually perform this for general

240
00:15:31.881 --> 00:15:32.990
length sentences.

241
00:15:33.740 --> 00:15:38.740
But effectively the general procedure is you try to infer the best subtree for

242
00:15:40.730 --> 00:15:45.160
each possible span,
uh,
or phrase,
uh,
uh,

243
00:15:45.200 --> 00:15:47.290
lengths starting from,
uh,

244
00:15:47.330 --> 00:15:50.660
phrases that covered two words as in here.

245
00:15:50.870 --> 00:15:53.450
And then phrases that cover three words as in here.

246
00:15:53.451 --> 00:15:57.770
And then phrases that cover forwards as in here.
And by moving like this,

247
00:15:57.771 --> 00:16:02.771
you can always use the computations that you've performed previously for a,

248
00:16:03.321 --> 00:16:08.180
where you inferred the subsidiary that was best,
uh,
um,

249
00:16:08.210 --> 00:16:12.200
that was best,
uh,
for these,
uh,
these,
uh,
smaller phrases.

250
00:16:13.670 --> 00:16:18.040
Also,
I'm not going to explain why,
but this is actually not returning the best.

251
00:16:18.110 --> 00:16:23.060
Subtree.
Uh,
so this is actually an approximation of the best tree.
Uh,

252
00:16:23.061 --> 00:16:27.100
and,
um,
I courage you to think more about this and,
uh,

253
00:16:27.380 --> 00:16:32.380
to see why you might have to look at the original algorithm that stews for

254
00:16:33.170 --> 00:16:37.820
performing syntactic inference of,
uh,
a syntactic parsing.

255
00:16:38.270 --> 00:16:39.350
But,
uh,

256
00:16:39.351 --> 00:16:42.830
for now you need to know really is that this is the general procedure that,

257
00:16:42.920 --> 00:16:46.790
that is used with recursive neural networks for inferring the syntactic tree

258
00:16:46.791 --> 00:16:49.100
structure of a given sentence.

