WEBVTT

1
00:00:01.290 --> 00:00:06.120
And this be new,
we'll discuss and contrast two different types of learning,

2
00:00:06.210 --> 00:00:08.520
a discriminative and generative learning.

3
00:00:11.400 --> 00:00:14.070
So we'll say that a,

4
00:00:14.100 --> 00:00:18.600
a learning algorithm corresponds to a discriminative learning algorithm.

5
00:00:18.870 --> 00:00:23.870
If it's trying to optimize a conditional likelier that is if it's trying to

6
00:00:24.640 --> 00:00:26.160
optimize the last function,

7
00:00:26.161 --> 00:00:31.161
which looks at a minus the log of the probability of the true target given x.

8
00:00:32.910 --> 00:00:33.661
So for instance,

9
00:00:33.661 --> 00:00:37.840
conditional random fields are usually train in the conditional way and the

10
00:00:37.870 --> 00:00:42.030
actually always trained conditionally because a conditional random fields only

11
00:00:42.031 --> 00:00:45.350
defines a conditional distribution.
It actually,
uh,

12
00:00:45.630 --> 00:00:50.630
doesn't really look at defining a distribution just over x or John Distribution

13
00:00:51.320 --> 00:00:53.690
for y and X.
Um,

14
00:00:54.870 --> 00:00:59.310
and other alternative actually would be to do generative learning.

15
00:00:59.670 --> 00:01:02.820
That is to optimize the joint log likelihood.

16
00:01:03.120 --> 00:01:07.860
So in other ways optimize minus the log of the joint probability of observing

17
00:01:07.861 --> 00:01:12.300
why and x as opposed to the probability of observing why,

18
00:01:12.480 --> 00:01:17.330
given that I'm observing x.
So for instance,
Hmms,

19
00:01:17.590 --> 00:01:20.670
uh,
are usually trained generatively.
So if you know about Hmms,

20
00:01:20.671 --> 00:01:23.880
what you've probably seen is,
uh,
the learning algorithm for training.

21
00:01:23.881 --> 00:01:27.140
Then generatively,
uh,
though we could also train it.

22
00:01:27.141 --> 00:01:31.290
It's in this way because if I give you a joint distribution of our y and x,

23
00:01:31.320 --> 00:01:35.790
I can then,
uh,
do the inference of what is p of y given x.

24
00:01:35.791 --> 00:01:40.530
And then I could try to chain train it a discriminative Lee.
But,
uh,

25
00:01:40.560 --> 00:01:43.560
but initially hmms I've mostly been developed and,

26
00:01:43.561 --> 00:01:48.060
and propose in the context of generative training and a,

27
00:01:48.061 --> 00:01:51.320
So let's exploit that formula a little bit.
So we have that,

28
00:01:51.321 --> 00:01:53.540
the joint distribution,
it's just a,

29
00:01:53.580 --> 00:01:58.440
it can be written down as the product of the conditional p of y given x times

30
00:01:58.710 --> 00:02:01.500
the,
uh,
marginal p of x.

31
00:02:01.830 --> 00:02:05.330
And because the log of a sum is the sum of logs,
then,
uh,

32
00:02:05.500 --> 00:02:10.500
this is also equal to minus the log of p of y given x minus the log of p of x.

33
00:02:11.040 --> 00:02:13.110
So we see that the difference is this term here.

34
00:02:13.170 --> 00:02:16.170
So we have this here and this here,

35
00:02:16.590 --> 00:02:21.590
but we have this term that is added when we were performing generative learning.

36
00:02:24.270 --> 00:02:25.103
And,
uh,

37
00:02:25.140 --> 00:02:28.710
so one way we're thinking about this is that perhaps this storm is kind of

38
00:02:28.711 --> 00:02:31.020
acting like a regular riser.
That is,

39
00:02:31.021 --> 00:02:35.760
it's we're adding another term which,
uh,
is,

40
00:02:35.790 --> 00:02:37.220
does not involve why.

41
00:02:37.230 --> 00:02:41.790
So it's not a term that encourages the model to assign high probability to the

42
00:02:41.791 --> 00:02:46.230
true target given x.
It's just a model that requires to do something else.

43
00:02:46.231 --> 00:02:50.700
It's going to favor models that also explained well the marginal,

44
00:02:50.701 --> 00:02:55.340
so I assigned high probability to observing true data.
So the data,

45
00:02:55.470 --> 00:02:59.980
just the inputs that we see from the training set.
And for that reason,

46
00:03:00.370 --> 00:03:04.690
a good intuition is to think that does go to the learning will tend to fit the

47
00:03:04.691 --> 00:03:09.160
data better while generative learning will be more regularized because of this

48
00:03:09.161 --> 00:03:09.994
term here.

49
00:03:11.710 --> 00:03:12.543
<v 1>Yeah.</v>

50
00:03:13.240 --> 00:03:17.920
<v 0>Actually we can show that,
uh,
there are two scenarios,</v>

51
00:03:18.010 --> 00:03:22.660
uh,
that we can observe in terms of how genitive versus of learning will compare

52
00:03:22.661 --> 00:03:26.470
ugly,
uh,
uh,
comparatively come,
uh,
compare.

53
00:03:27.100 --> 00:03:30.400
So if a model is well specified,

54
00:03:30.550 --> 00:03:34.790
so that means that the data in our training set is that it is,

55
00:03:34.791 --> 00:03:39.791
is a actually was actually generated from the model from the same class as our

56
00:03:40.001 --> 00:03:40.341
model.

57
00:03:40.341 --> 00:03:44.530
So there's a sending of our parameters for our model that corresponds exactly to

58
00:03:44.531 --> 00:03:48.910
the true model that January,
that the data,
so if the mother as well specified,

59
00:03:49.270 --> 00:03:52.690
then we can show that generative learning essentially always better.

60
00:03:52.990 --> 00:03:56.110
That is for any size of training sets that we use,

61
00:03:56.320 --> 00:04:01.270
there's always going to be a gap between what generative learning,
uh,
uh,

62
00:04:01.300 --> 00:04:05.540
gets in terms of generalization performance compared to a descriptive training.

63
00:04:05.541 --> 00:04:09.430
So generative training is it's going to perform better.
They'll,
uh,

64
00:04:09.460 --> 00:04:12.280
they should both eventually converged to the,
uh,

65
00:04:12.430 --> 00:04:16.180
to essentially an error Zeros or,
uh,
a perfect performance.

66
00:04:16.360 --> 00:04:19.840
But genitive learning is,
uh,
going to get there much faster.

67
00:04:22.800 --> 00:04:27.600
However,
if the model is not well specified,
which is what he most of the time.

68
00:04:27.601 --> 00:04:31.800
So if data was generated by some experts that labeled some data,

69
00:04:32.360 --> 00:04:35.580
then this is a process that that is almost most,
uh,

70
00:04:35.640 --> 00:04:40.560
almost certainly more complicated than the model we've written down in math.

71
00:04:41.100 --> 00:04:45.570
Uh,
so,
so really this is essentially for all this interesting problem.

72
00:04:45.900 --> 00:04:50.520
Uh,
the,
uh,
model will tend to be,
uh,
uh,
will not be well specified.

73
00:04:51.330 --> 00:04:55.500
Then,
uh,
the picture is not as clear.
So,
uh,

74
00:04:55.501 --> 00:04:59.690
what we'll see is a picture that is consistent with the view that genitive

75
00:04:59.700 --> 00:05:02.850
learning is just more corresponds to more regularization.

76
00:05:03.360 --> 00:05:07.220
So if the training set is small,
then,
uh,

77
00:05:07.230 --> 00:05:11.640
we should observe that generative learning.
We'll get a better performance,

78
00:05:11.940 --> 00:05:15.960
uh,
in terms of generalization.
But as we increase the size of it,

79
00:05:15.961 --> 00:05:16.711
our training set,

80
00:05:16.711 --> 00:05:20.820
eventually we should reach a point where this learning we'll catch up with

81
00:05:20.821 --> 00:05:24.680
generative learning and,
uh,
we'll start being actually better in terms of,

82
00:05:24.740 --> 00:05:28.500
of generalization performance.
I get lower generalization,
uh,

83
00:05:28.590 --> 00:05:30.690
error then a generative learning.

84
00:05:31.440 --> 00:05:31.870
<v 1>Okay.</v>

85
00:05:31.870 --> 00:05:35.700
<v 0>Um,
and in fact,
more specifically,
uh,</v>

86
00:05:35.740 --> 00:05:40.740
what we get is that discreted of learning will converge to a smaller syntonic

87
00:05:42.400 --> 00:05:44.920
error,
a synthetic error being the error.

88
00:05:44.980 --> 00:05:49.060
Your model would get if you had infinite amount of training data.
So we'll,
uh,

89
00:05:49.090 --> 00:05:52.540
it has a smaller I synthetic error,
then generative learning,

90
00:05:52.570 --> 00:05:54.820
which won't be able to do as well,

91
00:05:54.821 --> 00:05:59.480
even with an infinite amount of training data.
Okay.
So again,
here,

92
00:05:59.760 --> 00:06:01.310
uh,
a nice way of,

93
00:06:01.360 --> 00:06:05.510
a simpler way of interpreting this is that generative learning means more

94
00:06:05.511 --> 00:06:06.470
regularization.

95
00:06:06.560 --> 00:06:10.540
So if there's a small amount of training data generative learning,
uh,

96
00:06:10.640 --> 00:06:14.540
we'll probably do better.
But if I have quite a bit of training data,

97
00:06:14.541 --> 00:06:18.220
then now we can expect this kind of active learning to eventually catch up and

98
00:06:18.230 --> 00:06:19.063
do better.

99
00:06:19.630 --> 00:06:23.690
So that's actually a good thing to know about if you're tackling a problem for

100
00:06:23.691 --> 00:06:27.440
which you either have a lot of data or not a lot of data.
So you know,

101
00:06:27.620 --> 00:06:30.140
which kind of learning you should prefer.

102
00:06:30.830 --> 00:06:35.830
And a far more theoretical details on this invite you to consult this paper on

103
00:06:35.931 --> 00:06:39.110
descriptive,
Asus generative classifiers.
Uh,

104
00:06:39.111 --> 00:06:42.500
so this is a paper by enjoying and Michael Jordan in 2001.

