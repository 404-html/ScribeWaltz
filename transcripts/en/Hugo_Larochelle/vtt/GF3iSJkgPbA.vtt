WEBVTT

1
00:00:00.870 --> 00:00:05.580
In this video will introduce a motivation for using conditional random fields.

2
00:00:07.830 --> 00:00:11.570
So what we've seen so far with neural networks is that,
uh,

3
00:00:11.630 --> 00:00:16.630
they were a model that could take a single input and then through the

4
00:00:18.031 --> 00:00:20.280
computation of different hidden layers,

5
00:00:20.520 --> 00:00:25.090
we'd get at the output of the neural network,
a vector of pre activations.

6
00:00:25.200 --> 00:00:28.950
And then by using the softmax a nonlinearity,

7
00:00:29.070 --> 00:00:34.070
we could get a full distribution over what the potential label label of the,

8
00:00:35.700 --> 00:00:40.080
uh,
actual inputs in this case this image could be.

9
00:00:40.140 --> 00:00:44.730
So then based on this distribution,
which we can think of as what is the,

10
00:00:44.790 --> 00:00:49.290
uh,
beliefs of the neural network about what is the most likely,

11
00:00:49.350 --> 00:00:54.090
what is the likeliness of a assigning x to any different labelled?

12
00:00:54.720 --> 00:00:55.920
By using this distribution,

13
00:00:55.921 --> 00:01:00.630
we could make a prediction by a classifying the input to the class that is the

14
00:01:00.631 --> 00:01:02.490
most likely according to the neural network.

15
00:01:02.880 --> 00:01:07.800
And then if we gave it a new or different inputs,
uh,
here,

16
00:01:07.801 --> 00:01:11.910
then it would again compute all the hidden layers at the output layer,

17
00:01:11.911 --> 00:01:13.470
get a p activation vector,

18
00:01:13.680 --> 00:01:18.360
and after the softmax get a distribution over what the label could be.
Now,

19
00:01:18.380 --> 00:01:20.970
uh,
imagine that,
uh,

20
00:01:20.971 --> 00:01:25.770
we are actually facing with a problem where the examples are organized in a

21
00:01:25.771 --> 00:01:27.660
sequence.
So in particular,

22
00:01:27.720 --> 00:01:31.700
if we're thinking of classifying images that corresponds to handwritten

23
00:01:31.710 --> 00:01:35.640
characters,
these characters probably belonged to at work.

24
00:01:36.000 --> 00:01:40.470
And then actually knowing what,
uh,
the previous character was,

25
00:01:40.471 --> 00:01:44.440
might be helpful in identifying what is the,
uh,

26
00:01:44.670 --> 00:01:48.800
next character.
Uh,
so for instance,
we don't necessarily expect,
uh,

27
00:01:49.030 --> 00:01:52.650
to have a one,
a two eyes in a,
in a row.

28
00:01:52.651 --> 00:01:56.970
So I have a word with one eye than another,
I or then three eyes.
So a,

29
00:01:56.971 --> 00:01:58.950
we know the sequence is very unlikely.

30
00:01:59.040 --> 00:02:02.730
So if we know that before the previous step that was an eye,

31
00:02:02.731 --> 00:02:07.670
then maybe we don't want to discourage making a prediction for there to be an

32
00:02:07.671 --> 00:02:12.420
eye for the second time step in the sequence.
And we can think of other,

33
00:02:12.690 --> 00:02:17.550
uh,
uh,
such sequences of,
of strings,
which would be quite unlikely.

34
00:02:17.670 --> 00:02:22.410
And just in general,
the fact that,
uh,
you know,
certain,
uh,
sort of,

35
00:02:22.440 --> 00:02:27.330
uh,
um,
characters tend to be more likely after,
before other characters.

36
00:02:27.331 --> 00:02:31.350
So we'd like to exploit this,
uh,
in our prediction.
And,
uh,

37
00:02:31.380 --> 00:02:35.250
get some gains in terms of accuracy when we use these neural networks.

38
00:02:36.270 --> 00:02:40.350
So that's the idea behind conditional random fields.
In a,

39
00:02:40.351 --> 00:02:42.570
if we have a sequence of observations,
in this case,

40
00:02:42.571 --> 00:02:46.770
a sequence of images that corresponds to all the characters in the word.

41
00:02:47.010 --> 00:02:52.010
Then what we would like to do is to actually model the joint distribution over

42
00:02:52.501 --> 00:02:55.530
the whole sequence.
So I have a prediction,

43
00:02:55.531 --> 00:02:58.410
given the sequence of images as input,

44
00:02:58.710 --> 00:03:02.560
get a distribution over what the sequence of labels.

45
00:03:02.561 --> 00:03:07.290
So the sequence of classes for each character in the input.
Uh,

46
00:03:07.440 --> 00:03:11.220
so I have a distribution of the whole sequence and then be able to reason,
uh,

47
00:03:11.290 --> 00:03:15.190
based on,
on,
uh,
uh,
that distribution.
So for instance,

48
00:03:15.191 --> 00:03:20.191
find what is the most likely sequence of labels as opposed to making an element

49
00:03:20.291 --> 00:03:25.270
wise classification and they're trying to get a,
a,
the full sequence and a,

50
00:03:25.271 --> 00:03:28.960
so this is what conditional random fields are going to allow us to,
uh,
to do.

51
00:03:31.570 --> 00:03:36.570
So let's first introduce some notation now in this context of classifying

52
00:03:37.001 --> 00:03:37.870
sequences.

53
00:03:38.050 --> 00:03:42.340
Then our examples would be pairs of sequences of the same length where we have

54
00:03:42.370 --> 00:03:46.330
capital,
Aleks t,
which is going to be our sequence of images.

55
00:03:46.600 --> 00:03:49.420
And then why d,
which is going to be our sequence of labels.

56
00:03:49.421 --> 00:03:54.421
So notice that why now is a in bold because it's a vector and XD is going to be,

57
00:03:55.150 --> 00:03:55.930
uh,

58
00:03:55.930 --> 00:04:00.840
our sequence of vector x one tee up to x,
uh,

59
00:04:01.000 --> 00:04:03.070
Katie,
uh,
team.

60
00:04:03.100 --> 00:04:08.100
So here the Katie is going to be the length of the TF sequence in my training

61
00:04:09.461 --> 00:04:11.170
set.
So a,

62
00:04:11.171 --> 00:04:15.720
so now the inputs XD is going to be a list of vectors and the targets,

63
00:04:15.750 --> 00:04:19.930
why tea is going to be a list of individual scalar labels.

64
00:04:20.260 --> 00:04:25.120
And another way of thinking like stairs could also be as a matrix where either

65
00:04:25.121 --> 00:04:30.010
the rows are the vectors or the columns are the different factors in the

66
00:04:30.100 --> 00:04:32.020
sequence depending on,
uh,

67
00:04:32.200 --> 00:04:36.490
what we favor is in our notation or in an actual implementation.

68
00:04:38.980 --> 00:04:40.240
So with this notation,

69
00:04:40.870 --> 00:04:45.470
what we'll end up with the conditional random field is the,
uh,
uh,

70
00:04:45.520 --> 00:04:50.050
full distribution p of y bold for all.

71
00:04:50.230 --> 00:04:51.640
Why one,
up to y,

72
00:04:51.700 --> 00:04:56.700
k where k is the length of the sequence given capital x,

73
00:04:57.340 --> 00:05:01.220
where x is the,
uh,
capitol Xis the sequence of x one,

74
00:05:01.280 --> 00:05:06.130
two x k input vectors.
And the sequence that I'm,
that I'm modeling,

75
00:05:06.730 --> 00:05:09.440
so what we'll see is how we can,
uh,

76
00:05:09.460 --> 00:05:12.880
incorporate the notion of a conditional random fields to obtain a,

77
00:05:13.180 --> 00:05:17.140
a forum for this full,
a conditional distribution.

