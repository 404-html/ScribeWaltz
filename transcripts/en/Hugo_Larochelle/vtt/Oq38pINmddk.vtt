WEBVTT

1
00:00:00.130 --> 00:00:00.680
Yeah.

2
00:00:00.680 --> 00:00:05.390
<v 1>In this video we'll see the approach known as unsupervised retraining for</v>

3
00:00:05.420 --> 00:00:08.600
improving the training of deep neural networks.

4
00:00:10.990 --> 00:00:11.200
<v 0>Okay.</v>

5
00:00:11.200 --> 00:00:14.260
<v 1>So we talked,
uh,
before that Dart,</v>

6
00:00:14.320 --> 00:00:17.230
two reasons why we can,
uh,

7
00:00:17.290 --> 00:00:22.030
observe that a training deep neural networks is hard.
One,
which is,
uh,

8
00:00:22.060 --> 00:00:26.470
related to under fitting,
which might require to use better optimization methods.

9
00:00:26.860 --> 00:00:30.190
And,
uh,
if I need a second,
uh,
possibility,

10
00:00:30.400 --> 00:00:34.390
which is more associated with ora fitting,
where would use better regularization.

11
00:00:34.630 --> 00:00:36.940
And so here,
uh,
what we'll see,

12
00:00:36.941 --> 00:00:41.890
it was one approach based on the corporation of unsupervised learning as a one

13
00:00:41.891 --> 00:00:46.270
approach for addressing this.
A second hypothesis for why deep learning is hard.

14
00:00:49.950 --> 00:00:54.720
So,
um,
the solution is actually fairly simple given what we've seen so far.

15
00:00:55.120 --> 00:00:59.400
Uh,
the idea is just to change how we initialize the parameters of the hidden

16
00:00:59.401 --> 00:01:00.210
layers.

17
00:01:00.210 --> 00:01:04.140
And specifically we're going to use unsupervised learning to initialize the

18
00:01:04.141 --> 00:01:08.250
parameters of the hidden units in the,
uh,
hidden.

19
00:01:08.260 --> 00:01:12.260
There's d idea here is that,
uh,

20
00:01:12.330 --> 00:01:16.980
we want the neural network to not just extract features or representation,

21
00:01:16.981 --> 00:01:21.030
which is used for classification,
but also extracted in units that,
uh,

22
00:01:21.031 --> 00:01:25.470
also extract a extract more information about what's,
uh,

23
00:01:25.610 --> 00:01:29.400
the specificities of the input distribution.
And in particular,

24
00:01:29.401 --> 00:01:33.920
we want to encourage neural networks that can represent this late instructor

25
00:01:34.040 --> 00:01:37.740
that's specific to the input distribution on which we're training.

26
00:01:38.400 --> 00:01:42.210
So for instance,
if we're training on images of characters,
uh,
Dan,
uh,

27
00:01:42.211 --> 00:01:46.770
we'd like it in units to contain information for why this is a character image

28
00:01:46.950 --> 00:01:51.090
and that this is different from data would have been generated from any other

29
00:01:51.091 --> 00:01:55.470
distribution such as a random distribution where each pixel is uniforms.

30
00:01:55.560 --> 00:01:58.710
So which would generate this something like this,
like a random image.

31
00:01:59.550 --> 00:02:03.390
So we want to add this,
um,

32
00:02:03.540 --> 00:02:07.140
regularization that says amongst all neural networks that do well in

33
00:02:07.141 --> 00:02:08.070
classification,

34
00:02:08.370 --> 00:02:13.290
I'd like to favor a neural networks that also understand why my input data is,

35
00:02:13.291 --> 00:02:15.870
is special.
And it's different from say,
random data.

36
00:02:17.100 --> 00:02:19.980
And that's essentially what unsupervised learning is trying to do.

37
00:02:20.070 --> 00:02:25.070
We've justified restricted Boltzmann machines or throwing quarters as a training

38
00:02:25.501 --> 00:02:29.430
to sort of make that distinction in some way.
And,
uh,

39
00:02:29.431 --> 00:02:31.380
and so now we'll try to,
uh,

40
00:02:31.410 --> 00:02:35.400
encouraged the mar deep neural network that Duke classification and also try to

41
00:02:35.401 --> 00:02:36.660
do this to some extent.

42
00:02:40.280 --> 00:02:43.820
And so now we asking a bit more about,
uh,

43
00:02:43.821 --> 00:02:46.520
our neural network and we're not just asking you to be good at supervised

44
00:02:46.521 --> 00:02:47.750
learning classification.

45
00:02:48.330 --> 00:02:52.130
Also asking you to learn what makes the input Datas and special.

46
00:02:52.160 --> 00:02:57.160
So this should reduce the variance of our estimating a procedure.

47
00:02:58.040 --> 00:03:02.920
Uh,
and,
and because of that,
we,
we should reduce the variance and dust,

48
00:03:03.250 --> 00:03:07.380
uh,
uh,
uh,
reduced over fitting and by doing this well,

49
00:03:07.381 --> 00:03:11.290
so hope that we're not increasing the bias too much and it needed practice often

50
00:03:11.291 --> 00:03:13.900
that's what we observe in the sense that we do observe,

51
00:03:13.901 --> 00:03:16.420
improve generalization performance,
uh,

52
00:03:16.421 --> 00:03:19.000
by incorporating this unsupervised learning idea.

53
00:03:20.790 --> 00:03:21.450
<v 0>Okay.</v>

54
00:03:21.450 --> 00:03:25.570
<v 1>The specific procedure we'll use for doing this,
uh,
which is very common.
Uh,</v>

55
00:03:25.571 --> 00:03:28.350
you could use,
incorporate unsupervised learning in some other way,

56
00:03:28.351 --> 00:03:30.780
but the procedure that's,
uh,

57
00:03:31.050 --> 00:03:34.800
perhaps most popular and quite simple is known as a greedy,

58
00:03:34.801 --> 00:03:37.830
Leora wise pre-training.
Um,

59
00:03:37.890 --> 00:03:42.890
and so the way this is going to work is that will effectively add hidden a

60
00:03:43.080 --> 00:03:47.250
hidden layers one at a time.
And then whenever we're adding a new hidden there,

61
00:03:47.251 --> 00:03:51.960
we're going to train it's parameters locally using an unsupervised learning

62
00:03:51.961 --> 00:03:55.830
algorithm for a single layer.
And so for instance,

63
00:03:55.831 --> 00:03:59.340
we could start with this neural network here where we just added our first

64
00:03:59.341 --> 00:04:01.110
hidden unit,
a hidden layer,

65
00:04:01.380 --> 00:04:06.380
and then we'll pretrained or train these connections here based on some

66
00:04:06.751 --> 00:04:10.830
unsupervised learning algorithms like an Rpm,
uh,
contrasted divergence,

67
00:04:10.831 --> 00:04:14.430
argue with them or the noising autoencoder regular to quarter and so on.

68
00:04:15.480 --> 00:04:17.460
Then we do this from some iterations.

69
00:04:18.150 --> 00:04:20.760
Then we stop and then we fix these parameters.

70
00:04:20.761 --> 00:04:24.900
So this is illustrated by having these grayed out.
And now we add a,

71
00:04:25.050 --> 00:04:26.370
say a second hidden layer,

72
00:04:26.610 --> 00:04:30.510
and then we train this hidden layer using again and unsupervised learning

73
00:04:30.511 --> 00:04:34.920
algorithm like RBM learning algorithm,
um,
to,
uh,

74
00:04:34.950 --> 00:04:39.950
but we train only that hidden layer on top of the data represented through the

75
00:04:41.550 --> 00:04:43.590
representation of the first hidden layer,

76
00:04:44.160 --> 00:04:48.090
which is extracted by using these parameters that we've pretrained before.

77
00:04:48.630 --> 00:04:53.400
So in other words,
we're training say an RBM just on the same training said,

78
00:04:53.401 --> 00:04:56.220
but that's been preprocessed by this first hidden layer.

79
00:04:56.940 --> 00:05:01.320
And we do this for some number of iterations and then we continue like this for,

80
00:05:01.680 --> 00:05:03.840
uh,
as many hidden.
There's as we want to,

81
00:05:03.841 --> 00:05:05.970
we could pretrained then a third hidden layer,

82
00:05:06.180 --> 00:05:10.440
which is trained on the representation extracted by these,
uh,

83
00:05:10.740 --> 00:05:15.120
two subsequent hidden there.
So we trained on data represented,
uh,

84
00:05:15.150 --> 00:05:20.040
by this hidden layer and so on.
Paul,
uh,
as many a hidden,
there's we want,

85
00:05:21.150 --> 00:05:22.250
so now not,

86
00:05:22.410 --> 00:05:26.850
we train layers one at a time using someone supervise objective.

87
00:05:27.440 --> 00:05:28.710
Whenever we're training a hidden,

88
00:05:28.711 --> 00:05:30.660
there were fixing the parameters of the previous,

89
00:05:30.661 --> 00:05:34.740
and then there's that have been retrained.
And so we can view these other hidden,

90
00:05:34.741 --> 00:05:38.800
there's these previous hidden,
there's as just feature extractors,
uh,

91
00:05:38.940 --> 00:05:42.540
that provide an input representation for the unsupervised learning algorithm

92
00:05:42.930 --> 00:05:44.820
that we run for the current hidden layer.

93
00:05:49.470 --> 00:05:52.260
Oh yeah.
This procedure is known as unsupervised pre-training.

94
00:05:52.430 --> 00:05:53.570
If you read that into the literature,

95
00:05:53.571 --> 00:05:55.790
that's what river we're making a reference to.

96
00:05:56.660 --> 00:06:01.660
So the first lien hidden there will learn hidden unit features that are a

97
00:06:01.850 --> 00:06:05.930
characteristic or more common in the inputs in our training said,

98
00:06:05.931 --> 00:06:09.620
then say in random inputs or any other types of inputs.

99
00:06:10.190 --> 00:06:15.190
And then the second layer would learn combinations of firsthand layer unit

100
00:06:15.561 --> 00:06:20.300
features that are more common in the trainings,
uh,
sets,
uh,

101
00:06:20.330 --> 00:06:23.600
in the training set examples,
then say in random,

102
00:06:24.090 --> 00:06:27.320
a hidden first hidden layer,
a unit features.

103
00:06:27.800 --> 00:06:30.950
And then third layer would learn combinations of combinations and so on.

104
00:06:32.440 --> 00:06:35.890
And now the hope is that this initialization procedure,

105
00:06:35.891 --> 00:06:39.580
this pre training procedure will initialize the neural network in their region.

106
00:06:39.581 --> 00:06:41.870
The parameter space where,
uh,

107
00:06:41.950 --> 00:06:46.950
the local Optima we will reach when we're doing subsequent backpropagation,

108
00:06:48.060 --> 00:06:52.900
uh,
will over fit less,
would correspond to a congregation that,
uh,
is,

109
00:06:52.930 --> 00:06:56.500
uh,
uh,
it will,
uh,
els overfitting.

110
00:06:56.501 --> 00:06:58.690
Then if we add an initialized in the random,

111
00:06:59.140 --> 00:07:02.560
a neural network parameter configuration.

112
00:07:06.130 --> 00:07:10.570
So like I just said,
after this,
uh,
after,
uh,
onset by speech training,

113
00:07:10.571 --> 00:07:15.010
we just do regular backpropagation.
This is often referred to as fine tuning.

114
00:07:15.550 --> 00:07:20.010
So,
uh,
once we've trained say our first and in there,

115
00:07:20.020 --> 00:07:21.820
and then the second and the third one,

116
00:07:22.000 --> 00:07:27.000
we add an output layer and we train their whole narrow network using supervised

117
00:07:27.731 --> 00:07:32.620
learning bike backpropagation.
So regular backpropagation,
uh,

118
00:07:32.680 --> 00:07:36.610
in the feet forward and roll network.
Um,
so again,

119
00:07:36.611 --> 00:07:40.660
we call this procedure often refer to it as fine tuning,
uh,

120
00:07:40.690 --> 00:07:41.620
the idea.

121
00:07:41.650 --> 00:07:46.650
So the reason for that being that the parameters are most of them are pretty

122
00:07:46.811 --> 00:07:50.620
much all train really.
Uh,
they're not going to move that much.

123
00:07:50.621 --> 00:07:52.630
They've got to move somewhat,
but not as much as safe.

124
00:07:52.631 --> 00:07:56.860
We started from a random initialization,
so really the parameters are tuned,

125
00:07:57.490 --> 00:08:02.080
uh,
four to supervise,
supervise task at hand.
And so in other words,

126
00:08:02.081 --> 00:08:05.460
the representation is going to be adjusted to be more descriptive for the

127
00:08:05.470 --> 00:08:07.840
particular classification problem.
I'm interested in,

128
00:08:08.860 --> 00:08:13.000
one nice feature of this pretrade unsurprised p training followed by fine tuning

129
00:08:13.001 --> 00:08:16.900
is that if we have a lot of unlabeled data,
we could do pre-training,

130
00:08:16.990 --> 00:08:21.780
a more data than we do find tuning or backpropagation where we need labeled

131
00:08:21.781 --> 00:08:25.570
data.
And so,
and other sort of,
uh,
uh,

132
00:08:25.571 --> 00:08:30.130
advantage we get from this procedure is that if we have not a lot of supervised

133
00:08:30.131 --> 00:08:34.840
data labeled data would a lot of unlabeled data,
then we can get a,
uh,

134
00:08:34.841 --> 00:08:39.841
improvements by doing pre training on the big amount of unlabeled data and then

135
00:08:41.291 --> 00:08:45.820
fine tuning on the,
on the small training set with supervised data.

136
00:08:49.150 --> 00:08:51.610
So here's just a pseudo code to pass,

137
00:08:51.611 --> 00:08:55.320
make everything a bit more precise about what I just said.

138
00:08:55.560 --> 00:09:00.560
So the pre training parts iterates from the first hidden layer to the last we

139
00:09:02.251 --> 00:09:04.740
build and unsupervised training set.
That is,

140
00:09:04.741 --> 00:09:09.210
we take all of our training inputs and we ignored the labels if,

141
00:09:09.360 --> 00:09:10.470
if it was available.

142
00:09:10.830 --> 00:09:15.090
And then we compute the representation that the previous a hidden layer,

143
00:09:15.270 --> 00:09:19.320
so four l equals one h zero is just going to beat the actual input,

144
00:09:19.321 --> 00:09:23.270
the right input.
And then a four l equals a source.

145
00:09:23.290 --> 00:09:26.640
That's four l equals one four l equals to a,

146
00:09:26.830 --> 00:09:30.420
then we'd be a computing h,
one of XD here.

147
00:09:30.810 --> 00:09:32.730
So we construct this unsupervised training set.

148
00:09:32.910 --> 00:09:37.180
We feed it to what we sometimes call the greedy modules.
So that's just uh,

149
00:09:37.510 --> 00:09:40.620
unsupervised learning algorithm that can be trained,

150
00:09:40.740 --> 00:09:44.940
that trains a single hidden layer based on a representation that we compute
here.

151
00:09:45.090 --> 00:09:47.670
So it could be an RBM when it was one quarter learning off of them.

152
00:09:49.230 --> 00:09:54.060
And then we use the hidden layer of weights and biases of that really module,

153
00:09:54.061 --> 00:09:55.800
that RBM or that auto encoder.

154
00:09:55.801 --> 00:10:00.801
We train on that Dataset to initialize the parameters for the,

155
00:10:01.410 --> 00:10:02.390
uh,
lef.

156
00:10:02.820 --> 00:10:07.020
A hidden layer in our deep neural network will want to find too,

157
00:10:07.590 --> 00:10:12.240
once we retrain it all hidden layers.
Then once all hidden,

158
00:10:12.290 --> 00:10:15.330
there's our pretrained,
we add connections to an output layer.

159
00:10:15.420 --> 00:10:19.620
So that's w capital l plus one and B capitol.
That was plus one.

160
00:10:19.890 --> 00:10:21.990
We can internalize them randomly as usual.

161
00:10:22.950 --> 00:10:27.950
And then fine tuning is just a supervised learning using backpropagation with

162
00:10:29.161 --> 00:10:32.100
ses to Cassie gradient descent.
All right,

163
00:10:32.101 --> 00:10:36.210
so that's unsupervised pre training for learning deep neural networks.

164
00:10:39.150 --> 00:10:39.983
<v 0>MMM.</v>

165
00:10:40.500 --> 00:10:44.810
<v 1>Now you might wonder,
uh,
what kind of a unsupervised learning I'll go to them.</v>

166
00:10:44.820 --> 00:10:46.920
You could be using,
uh,
it's during quede.

167
00:10:46.921 --> 00:10:51.570
This unsupervised pre training procedure was invented and they're proposing

168
00:10:51.571 --> 00:10:55.560
these papers by Geoff Hinton and his colleagues.
Um,

169
00:10:55.650 --> 00:10:59.880
and that's where the suggested using rbms there was a whole rationale for why

170
00:10:59.881 --> 00:11:02.790
rbms were at the right algorithm to use for doing free training.

171
00:11:03.390 --> 00:11:07.350
But then later on,
uh,
different people,
including myself,
uh,

172
00:11:07.410 --> 00:11:11.790
proposed to in stent stack or throwing quarters instead of stacking rbms.

173
00:11:12.000 --> 00:11:15.060
So,
uh,
and,
uh,
the paper I was involved in,

174
00:11:15.061 --> 00:11:19.380
we proposal to in quarters and in this other paper,
but yeah,
people that Nyu,

175
00:11:19.740 --> 00:11:23.190
uh,
uh,
liquid in a macro hairdo,
hands,

176
00:11:23.191 --> 00:11:26.880
Eto and a young liquor sparser twin quarters,

177
00:11:26.940 --> 00:11:30.890
we're instead proposed.
But essentially that the idea of us was very similar.

178
00:11:30.910 --> 00:11:31.920
The algorithms were different,

179
00:11:31.921 --> 00:11:35.880
but we were in both cases suggesting that we use another algorithms,

180
00:11:35.881 --> 00:11:39.160
then a algorithm then restricted Boltzmann machine for initializing yeah.

181
00:11:39.330 --> 00:11:40.260
Deep neural network.

182
00:11:42.130 --> 00:11:42.790
<v 0>Yeah.</v>

183
00:11:42.790 --> 00:11:46.330
<v 1>Um,
then later on d nosing or doing callers were proposed.</v>

184
00:11:46.360 --> 00:11:48.280
They were actually proposing that context,

185
00:11:48.281 --> 00:11:51.400
the context of training deep neural networks,
uh,

186
00:11:51.401 --> 00:11:54.430
and not just for extracting in general.

187
00:11:55.180 --> 00:11:56.740
And there's been a,

188
00:11:56.741 --> 00:12:01.741
also a lot of other papers suggesting other types of unsupervised learning for p

189
00:12:03.141 --> 00:12:04.540
training.
Uh,

190
00:12:04.541 --> 00:12:08.980
so there's an example here using the idea of a semi-supervised embeddings or

191
00:12:08.981 --> 00:12:13.510
Colonel Pca or independent subspace analysis.
Again,

192
00:12:13.511 --> 00:12:17.230
in all cases showing that there were some gains from doing this,

193
00:12:17.231 --> 00:12:20.560
from stacking these modules of unsupervised learning.

194
00:12:20.980 --> 00:12:24.940
And so there's been a lot of research and pretty much every time we'd see that

195
00:12:25.000 --> 00:12:28.450
unsupervised learning was,
was being helpful.
So this,
uh,

196
00:12:28.810 --> 00:12:33.040
essentially confirms that it's really the general idea of unsupervised learning

197
00:12:33.220 --> 00:12:37.990
for initializing the parameters of a deep neural network that works well.

198
00:12:37.991 --> 00:12:42.560
And the choice of the learning algorithm will make some difference.
But,
uh,

199
00:12:42.950 --> 00:12:44.140
uh,
but,

200
00:12:44.230 --> 00:12:48.320
but really the main characteristic that they all share a is that they're

201
00:12:48.340 --> 00:12:49.630
unsupervised learning algorithms.

