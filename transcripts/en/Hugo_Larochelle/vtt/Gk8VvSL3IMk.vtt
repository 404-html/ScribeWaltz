WEBVTT

1
00:00:00.390 --> 00:00:00.780
Okay.

2
00:00:00.780 --> 00:00:04.460
<v 1>In this video we'll see an example of the results one can get with a commercial</v>

3
00:00:04.461 --> 00:00:07.500
loan network on an actual object recognition dataset.

4
00:00:10.100 --> 00:00:13.860
So the experiments we're going to look at are taken from this paper by Jared,

5
00:00:13.861 --> 00:00:17.990
that all in 2009,
uh,
there researchers at Nyu.
Uh,

6
00:00:17.991 --> 00:00:21.770
and uh,
what's really interesting is that they're,
uh,

7
00:00:21.850 --> 00:00:26.420
they tested different variations on the commercial loan network.
Uh,

8
00:00:26.750 --> 00:00:30.710
one thing that it tested is the actual impact of using or not using the

9
00:00:30.711 --> 00:00:35.420
rectification layer or using or not the contrast normalization.
There

10
00:00:37.900 --> 00:00:41.790
other thing that they looked at is the impact of initialization of the

11
00:00:41.791 --> 00:00:45.420
parameters in the convolutional neural network.
Uh,

12
00:00:45.510 --> 00:00:48.480
so that's something we haven't discussed thus far.
Um,

13
00:00:48.540 --> 00:00:51.450
but like in the regular kids wore a neural network,

14
00:00:51.451 --> 00:00:55.170
we could have initialized to parameters before training randomly.

15
00:00:56.540 --> 00:00:59.160
Uh,
but given what we know about unsupervised speed training,

16
00:00:59.161 --> 00:01:01.880
that's also an approach we might want to try,
uh,
uh,

17
00:01:02.220 --> 00:01:06.810
for the conditional neural network case.
Uh,
now notice that in this case,

18
00:01:06.840 --> 00:01:11.610
the hidden units in each of the feature maps are only connected to a small

19
00:01:11.611 --> 00:01:15.180
receptive field within the whole image.
And,
uh,

20
00:01:15.181 --> 00:01:19.830
so now we need to initialize this sort of smaller matrix,
which is not of size,

21
00:01:20.250 --> 00:01:24.420
size of the image,
uh,
height of the image by with the image.

22
00:01:24.900 --> 00:01:28.410
And so a procedure for performing unsupervised,
uh,

23
00:01:28.510 --> 00:01:33.510
pre training of couple additional neural network is to generate these data sets

24
00:01:33.870 --> 00:01:38.750
of small patches that fit the size of these,

25
00:01:38.770 --> 00:01:43.090
uh,
receptive fields that are hidden units are going to have.
So,
uh,

26
00:01:43.110 --> 00:01:45.480
essentially the,
uh,

27
00:01:45.510 --> 00:01:50.400
we need to convert pre-training into a patch patch wise learning problem.

28
00:01:50.910 --> 00:01:53.110
And we would do that is that we take some,
uh,

29
00:01:53.310 --> 00:01:58.080
images and then we extract patches of the same size as the receptive fields of

30
00:01:58.081 --> 00:02:02.550
the hidden units in our first layer,
uh,
in nonconventional neural network.

31
00:02:02.990 --> 00:02:05.640
And we extract a bunch of patches at random positions.

32
00:02:06.150 --> 00:02:07.830
So we collected a big data set like this.

33
00:02:07.831 --> 00:02:11.430
Then we trained some unsupervised neural network and RBM and not throwing

34
00:02:11.431 --> 00:02:15.990
quarter or as far as the,
as far as scoring model on those patches.

35
00:02:16.500 --> 00:02:18.150
And then we use the weights,

36
00:02:18.210 --> 00:02:23.210
the connections between the input patch and each hidden units to get the initial

37
00:02:23.971 --> 00:02:28.430
value of uh,
the uh,
filter or the,

38
00:02:28.431 --> 00:02:33.431
a matrix of connections for a different feature maps in our neural network.

39
00:02:33.691 --> 00:02:37.320
So the number of hidden units of say the RBM should be the same as the number of

40
00:02:37.321 --> 00:02:41.040
feature maps in the conventional neural network and its first hidden layer.

41
00:02:42.030 --> 00:02:46.710
And then to get initialization of subsequent hidden layer,
uh,
hidden layers,

42
00:02:46.711 --> 00:02:51.660
we would need to map the images through the feature maps and a and perform

43
00:02:51.661 --> 00:02:55.320
pulling if want to perform bullying.
And so get up to the uh,

44
00:02:55.410 --> 00:03:00.410
feature maps right before the next convolutional layer and then repeat this

45
00:03:02.051 --> 00:03:07.051
procedure of extracting patches and doing free training using St Rbm and a to

46
00:03:07.211 --> 00:03:12.211
get an initial value of the feature maps for parameters for that convolutional

47
00:03:12.971 --> 00:03:14.350
layer.
Okay.

48
00:03:14.351 --> 00:03:19.280
So that's all we would perform on speed training in this case and also in the

49
00:03:19.281 --> 00:03:23.890
eye experiments from Jared at all.
We'll also look at,
uh,

50
00:03:23.891 --> 00:03:28.450
so we'll look at the impact of using a random initialization versus a using

51
00:03:28.451 --> 00:03:29.620
unsupervised pre training.

52
00:03:29.800 --> 00:03:33.730
And we'll also look at what's the impact of using some fine tuning.

53
00:03:33.731 --> 00:03:36.710
So performing gradient descent,
uh,

54
00:03:36.850 --> 00:03:39.960
to train all the parameters of,
uh,

55
00:03:40.050 --> 00:03:41.950
the neural network after initialization.

56
00:03:42.370 --> 00:03:46.900
So that's going to be noted with a little plus or a only training,

57
00:03:46.901 --> 00:03:50.800
the output layer.
So without any plus in the table that we'll see a,

58
00:03:50.801 --> 00:03:54.090
it means that all the units,
uh,

59
00:03:54.170 --> 00:03:58.750
the feature maps are left with their initial values of their parameters and only

60
00:03:58.751 --> 00:04:02.770
the output layer is being trained with it's fully connected,
uh,
connections.

61
00:04:03.820 --> 00:04:05.110
And so for random initialization,

62
00:04:05.111 --> 00:04:08.230
we'll have the later letter r and foreign supplies be training.

63
00:04:08.260 --> 00:04:10.270
We'll have the letter u and the tables.

64
00:04:13.130 --> 00:04:13.430
<v 0>Okay,</v>

65
00:04:13.430 --> 00:04:16.610
<v 1>so here's the table summarizing all of the results that they perform on the</v>

66
00:04:16.611 --> 00:04:18.980
Caltech one on one data set.
Um,

67
00:04:19.010 --> 00:04:22.310
so when they're writing f a c g,

68
00:04:22.370 --> 00:04:27.370
that means the convolution layer r is for the rectification there and is for the

69
00:04:28.131 --> 00:04:30.110
local contrast normalization layer.

70
00:04:30.710 --> 00:04:34.580
P M is for the Max pooling layer and p a four,
the average pooling layer.

71
00:04:35.960 --> 00:04:40.220
And so here we have a network with a single,
uh,

72
00:04:40.490 --> 00:04:41.840
what they call stage.

73
00:04:41.841 --> 00:04:46.841
So that means a single sequence of convolutional layer,

74
00:04:47.750 --> 00:04:52.130
potentially some rectification,
potentially some,
uh,
local contrast,
normalization.

75
00:04:52.310 --> 00:04:55.930
And then finally some pooling.
And then the two stage,
uh,

76
00:04:56.060 --> 00:04:59.860
sequence two stage system has,
uh,
uh,

77
00:04:59.990 --> 00:05:04.990
twice this sort of alternation between a convolution rectification,

78
00:05:06.021 --> 00:05:07.460
contrast,
normalization and pooling.

79
00:05:07.461 --> 00:05:11.550
So we have essentially a total to pooling layers.
Uh,

80
00:05:11.660 --> 00:05:15.530
and then the last pooling layer is fed to the output layer.

81
00:05:16.310 --> 00:05:21.050
So that's essentially the sort of equivalent of a single layer if though it's

82
00:05:21.080 --> 00:05:24.470
multiple layers.
But if you want to consider all of these,

83
00:05:24.540 --> 00:05:29.000
the sequence of the latest layers and one actual layer and we,
uh,

84
00:05:29.060 --> 00:05:31.160
sort of a shallow version of a convolutional net.

85
00:05:31.190 --> 00:05:33.470
And so this would be a deeper version of it.

86
00:05:35.440 --> 00:05:38.860
So if we just use convolutional with average pooling,

87
00:05:38.890 --> 00:05:43.450
their results are actually really,
really bad.
So this is,
uh,
accuracy.

88
00:05:43.480 --> 00:05:47.560
So 14% accuracy for,
uh,
here.
Uh,

89
00:05:47.561 --> 00:05:50.770
so for each line we have unsupervised learning plus fine tuning,

90
00:05:51.190 --> 00:05:55.670
random plus fine tuning,
unsupervised learning without fine tuning.

91
00:05:55.671 --> 00:05:57.510
So just training the top connections up,

92
00:05:57.511 --> 00:05:59.810
the connections and just random and we can't ignore,

93
00:05:59.960 --> 00:06:02.810
we won't talk about this one here or,
or this one.

94
00:06:02.811 --> 00:06:04.990
You can look at the original paper,
uh,

95
00:06:05.070 --> 00:06:06.860
if you want more details about these experiments.

96
00:06:07.430 --> 00:06:11.060
But so just conversion on average pooling roles,
results are really bad.

97
00:06:11.870 --> 00:06:15.260
If we introduce normalization,
local contrast,
normalization,

98
00:06:15.290 --> 00:06:17.990
then we get a little bit of a boost in terms of performance.

99
00:06:18.950 --> 00:06:23.600
But the biggest boost comes from just adding rectification.
Absolute.

100
00:06:23.601 --> 00:06:28.130
So computing adding a layer that computes just the absolute value of the

101
00:06:28.131 --> 00:06:30.320
feature.
The previous layer feature maps,

102
00:06:31.180 --> 00:06:35.330
we see that we go from something like 15% to 50,

103
00:06:35.331 --> 00:06:38.720
47% and so on.
So really much better results.

104
00:06:39.680 --> 00:06:44.360
Part of the reason for this is that when we computing average bullying,

105
00:06:44.361 --> 00:06:49.130
we're summing together,
uh,
all the activations,
uh,

106
00:06:49.131 --> 00:06:51.620
within some,
uh,
local neighborhood.

107
00:06:52.310 --> 00:06:56.310
Now if we have a,
um,
if say a,

108
00:06:56.311 --> 00:06:59.690
a feature is detecting a particular edge,

109
00:07:00.110 --> 00:07:04.370
if we move across the neighborhood that we might actually,

110
00:07:04.371 --> 00:07:08.570
because the edge is sort of gonna change a polarity as we move,
uh,

111
00:07:08.600 --> 00:07:13.550
we might actually see that the activations will cancel out if we sum them

112
00:07:13.551 --> 00:07:18.230
together because we'll have some negative activations canceling the positive

113
00:07:18.231 --> 00:07:21.620
activations just because we have,
as we move in the image,

114
00:07:21.650 --> 00:07:26.330
a sort of a change of the polarity of have an edge.
If we have an edge detector,

115
00:07:27.330 --> 00:07:30.980
if we take the absolute value,
we don't get this problem anymore.
And,
and that's,

116
00:07:31.070 --> 00:07:33.710
uh,
the,
uh,
suggestion,

117
00:07:33.830 --> 00:07:37.700
the explanation that's suggested by Jared at all for why we get a big boost
here.

118
00:07:38.390 --> 00:07:42.830
This is partly confirmed by the fact that if we actually use Max pooling
instead,

119
00:07:43.130 --> 00:07:48.130
we get a actually a pretty good result with a contrast formalization much better

120
00:07:49.191 --> 00:07:52.460
than if we use average.
Uh,
pulling has we see here.

121
00:07:53.450 --> 00:07:57.080
So that's part of the explanation for why average pooling can really fail if we

122
00:07:57.081 --> 00:08:02.000
don't,
uh,
make sure that,
uh,
we don't get a cancellation.
In fact,

123
00:08:02.450 --> 00:08:07.040
by averaging,
uh,
where,
uh,
pull the polarity changes in the,

124
00:08:07.120 --> 00:08:11.870
uh,
contrast or the edges in the image would provoke these cancellations.

125
00:08:12.770 --> 00:08:17.690
And if we combine both a absolutes,
a rectification,
uh,

126
00:08:17.720 --> 00:08:20.510
with absolute value and,
uh,
uh,

127
00:08:20.540 --> 00:08:23.720
local contrast normalization that we get even better results,

128
00:08:23.721 --> 00:08:25.760
that's the best result with a single stage system.

129
00:08:26.990 --> 00:08:30.710
The picture is more or less the same for the two stage system,

130
00:08:30.740 --> 00:08:33.980
the sort of two hidden layer system.
Uh,
but the results are better.

131
00:08:33.980 --> 00:08:37.580
So we get an improvement by using a deeper competitional of neural network,

132
00:08:37.581 --> 00:08:39.780
which again,
uh,

133
00:08:39.870 --> 00:08:43.070
goes to say that a deep neural network tends to a,

134
00:08:43.071 --> 00:08:46.920
is able to extract more useful features.
Um,

135
00:08:47.450 --> 00:08:52.250
and uh,
we actually also know there's something perhaps a bit strange.

136
00:08:53.080 --> 00:08:53.600
Uh,

137
00:08:53.600 --> 00:08:58.600
so this means that convolutional layers I've been initialized.

138
00:08:58.650 --> 00:09:02.430
The filter maps have been initialized with random parameters and because there's

139
00:09:02.431 --> 00:09:05.760
no plus,
it means that we're training only the output layer.

140
00:09:06.060 --> 00:09:10.800
So really this is using random features.
They are not trained at all.

141
00:09:10.950 --> 00:09:13.050
The only thing that is training is the top most layer.

142
00:09:13.110 --> 00:09:15.690
And yet the results I actually really,
really good.

143
00:09:16.530 --> 00:09:20.070
So that seems counter intuitive.
And,
uh,

144
00:09:20.100 --> 00:09:24.450
I believe that this was even discovered by the authors sort of by chance.
They,

145
00:09:24.630 --> 00:09:27.810
they thought they had regularized correctly the neural network,

146
00:09:27.811 --> 00:09:29.250
but it turns out that,
uh,

147
00:09:29.460 --> 00:09:32.160
was not training the other layers and yet they were getting good results.

148
00:09:32.161 --> 00:09:35.880
So they started investigating this,
uh,
particular effect here.

149
00:09:38.670 --> 00:09:42.900
And so random filters are a working surprisingly good.

150
00:09:42.901 --> 00:09:47.810
And the reason for this is that they actually do tend to,
uh,

151
00:09:47.880 --> 00:09:50.650
extract something like edge and formation.

152
00:09:51.270 --> 00:09:55.830
So if we have,
so the experiment that they did is as follows,

153
00:09:55.831 --> 00:10:00.660
the a initialize,
a layer,
a convolutional layer using some random filters.

154
00:10:01.050 --> 00:10:04.680
And then what they did is that it took a hidden unit and they perform great in

155
00:10:04.681 --> 00:10:09.360
the sense on the,
uh,
original,
uh,
input,
uh,

156
00:10:09.361 --> 00:10:12.510
so as to maximize the activity of that hidden unit.

157
00:10:12.570 --> 00:10:17.570
So the idea is to see what's the input pattern that activates to sit in unit as

158
00:10:17.611 --> 00:10:20.280
much as possible.
And so for these,

159
00:10:20.730 --> 00:10:24.780
all of these different squares here correspond to different feature maps with

160
00:10:24.781 --> 00:10:27.060
different filters of parameters.

161
00:10:28.350 --> 00:10:32.520
And here is the associated best input pattern.

162
00:10:33.360 --> 00:10:37.110
And we see that for a lot of them.
Say for instance,
this one or this one,

163
00:10:37.410 --> 00:10:41.340
you actually get an input pattern,
which as a particular,
uh,
uh,

164
00:10:41.380 --> 00:10:44.590
spatial orientation.
So,
uh,

165
00:10:44.640 --> 00:10:49.170
it kind of looks like a sequence of edges in a particular orientation.
They,

166
00:10:49.260 --> 00:10:54.260
it seems like they can actually be tuned to particular orientation of edges and

167
00:10:55.651 --> 00:10:59.550
for extracting and being able to detect certain objects.
Uh,

168
00:10:59.850 --> 00:11:03.450
useful information for doing that is actually orientation of the edges.

169
00:11:03.930 --> 00:11:08.310
So our random initialization within the convolutional neural network,

170
00:11:08.820 --> 00:11:12.900
a yields these hidden units that are not doing something necessarily totally

171
00:11:12.901 --> 00:11:16.770
random,
it's actually extracting,
um,
and usually get to hit.

172
00:11:16.771 --> 00:11:21.520
And Nene and star are tuned to particular,
uh,
spatial orientations of,

173
00:11:21.570 --> 00:11:22.403
of edges.

174
00:11:22.900 --> 00:11:27.480
Whereas if you learn the filters use and use an unsupervised pre-training,

175
00:11:27.720 --> 00:11:31.570
we get things that really looked like edge detectors and the maximum,

176
00:11:31.720 --> 00:11:35.700
the input pattern step,
maximize the activation,
uh,

177
00:11:35.730 --> 00:11:38.790
are not that different from the ones we get.
Uh,

178
00:11:38.820 --> 00:11:41.190
if you use a random initialization,

179
00:11:41.191 --> 00:11:44.400
they're a bit more localized and it's more obvious that they're tuned to

180
00:11:44.401 --> 00:11:49.380
particular spacial orientation.
Uh,
but they're in a sense somewhat similar.

181
00:11:49.890 --> 00:11:53.580
So this would be the explanation for why random initialization,
uh,

182
00:11:53.590 --> 00:11:54.670
actually works well.

183
00:11:59.120 --> 00:12:04.010
And so this really shows that choosing the right architecture is really

184
00:12:04.011 --> 00:12:05.990
important in this specific setting.

185
00:12:06.020 --> 00:12:09.590
If a almost more than the learning algorithm unsupervised and utilization,

186
00:12:09.860 --> 00:12:12.860
it's not still the,
uh,
helping that much.
It's helping a little bit,

187
00:12:12.861 --> 00:12:14.660
but not that much.
Uh,

188
00:12:14.661 --> 00:12:18.290
but using rectification or not using it is very important.

189
00:12:18.320 --> 00:12:20.330
Same thing with local gunshots,
normalization,

190
00:12:20.331 --> 00:12:25.331
which will give some boosts and actually the how important this is is depending

191
00:12:25.821 --> 00:12:26.990
on the how much training data,

192
00:12:27.240 --> 00:12:29.510
if you have very little training data per category,

193
00:12:29.511 --> 00:12:33.020
which is true for Caltech one on one,
uh,
we see this effect quite a bit,

194
00:12:33.560 --> 00:12:38.560
but did this experiment on the NORC data set where we have a much fewer training

195
00:12:39.081 --> 00:12:43.940
classes,
there are actually a little toys,
a film,
uh,
that film.
But,
um,

196
00:12:44.180 --> 00:12:44.980
uh,
that,
uh,

197
00:12:44.980 --> 00:12:49.520
for which we've taken pictures of different orientations eliminations.
And,
um,

198
00:12:50.240 --> 00:12:53.990
in this case we have many more samples per class.

199
00:12:54.230 --> 00:12:57.620
And so on this data that said that were able to increase the number of training

200
00:12:57.621 --> 00:12:59.870
examples per class and see,
uh,

201
00:12:59.871 --> 00:13:03.860
what's the relative performance of the different architecture choices.

202
00:13:04.130 --> 00:13:08.570
So this is the one that would work really badly without any the rectification or

203
00:13:08.630 --> 00:13:12.740
local a normalization.
So that's the Blue Line here.

204
00:13:12.800 --> 00:13:17.000
And we see that with little training data,
it does indeed perform really badly.

205
00:13:17.270 --> 00:13:21.110
But as we increase the amount of training data,
it actually starts performing,

206
00:13:21.490 --> 00:13:22.430
uh,

207
00:13:22.460 --> 00:13:26.660
fairly similarly to say the best choice which would,

208
00:13:26.710 --> 00:13:27.920
which would be here,

209
00:13:28.070 --> 00:13:33.070
which corresponds to rectification and a normalization and using some fine

210
00:13:34.341 --> 00:13:37.430
tuning.
So using backpropagation to train all the parameters,

211
00:13:37.730 --> 00:13:41.690
whereas the random purely random filters their work well initially.

212
00:13:41.691 --> 00:13:45.320
But eventually when you have a lot of training data that they don't perform

213
00:13:45.321 --> 00:13:50.321
nearly as well as the case where we actually train these filters using some

214
00:13:50.721 --> 00:13:55.610
backpropagation.
So with the plus here,
okay.
So part of that,

215
00:13:55.611 --> 00:13:56.444
also,

216
00:13:56.540 --> 00:14:00.560
the random filters I worked really well is sort of an artifact of the fact that

217
00:14:00.561 --> 00:14:03.970
we don't have a lot of training data for each objects that we want to,

218
00:14:04.220 --> 00:14:05.053
we want to detect.

219
00:14:05.680 --> 00:14:06.280
<v 0>Okay.</v>

220
00:14:06.280 --> 00:14:09.700
<v 1>All right.
So,
uh,
for more information about these experiments are really,</v>

221
00:14:09.701 --> 00:14:11.950
really enlightening and helpful.

222
00:14:12.010 --> 00:14:15.910
See this paper by Jared at all in 2009 so that you can see the reference on the

223
00:14:15.911 --> 00:14:17.680
website for this course.

224
00:14:18.920 --> 00:14:18.980
<v 0>Okay.</v>

