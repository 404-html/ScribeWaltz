WEBVTT

1
00:00:00.170 --> 00:00:00.870
Yeah.

2
00:00:00.870 --> 00:00:04.740
<v 1>In this video we'll finally see how we can train a conditional random fields.</v>

3
00:00:04.770 --> 00:00:09.770
And so we'll first discuss the loss function that we'll use to train and learn

4
00:00:10.051 --> 00:00:11.170
our conditional rendering fields.

5
00:00:14.170 --> 00:00:18.300
So in the previous videos we've defined what we meant by a linear chain

6
00:00:18.301 --> 00:00:19.260
conditional random fields.

7
00:00:19.261 --> 00:00:22.290
So that's the specific conditional random fields we'll consider.

8
00:00:22.470 --> 00:00:24.570
So p of y given x square,

9
00:00:24.571 --> 00:00:28.890
why is a sequence of label and exit sequence of inputs is going to be the

10
00:00:28.891 --> 00:00:33.390
exponential of a,
some of you and there re uh,

11
00:00:33.450 --> 00:00:34.850
potential Oh,

12
00:00:34.860 --> 00:00:39.860
a lot factors and a pairwise luck factors divided by normalization constant that

13
00:00:41.070 --> 00:00:44.190
we have to compute with some dynamic programming music,

14
00:00:44.191 --> 00:00:48.960
the forward backward a algorithm.
And uh,
so,

15
00:00:48.961 --> 00:00:53.580
uh,
thus far in the previous videos I always assumed that are conditional.

16
00:00:53.581 --> 00:00:57.180
Random fields was already trained in and I've talked about how we can perform

17
00:00:57.181 --> 00:01:01.020
inference compete,
the partition function,
performed classification and so on.

18
00:01:01.080 --> 00:01:04.650
So now we'll finally look at how we can train a conditional random fields on

19
00:01:04.651 --> 00:01:05.880
some available data.

20
00:01:06.950 --> 00:01:07.783
<v 0>Okay.</v>

21
00:01:07.870 --> 00:01:08.321
<v 1>So again,</v>

22
00:01:08.321 --> 00:01:13.240
we'll use empirical risk minimization as our guiding principle for the writing

23
00:01:13.241 --> 00:01:14.140
and learning algorithm.

24
00:01:14.620 --> 00:01:19.620
That is we are going to choose a loss function and to train on conditional and

25
00:01:19.691 --> 00:01:23.590
them feel on a training day,
a training set that is uh,

26
00:01:23.910 --> 00:01:28.910
a set of pairs of input sequences and target sequences or label sequences.

27
00:01:29.470 --> 00:01:34.300
We'll minimize the average up to some of the loss function where we compare to

28
00:01:34.301 --> 00:01:38.740
what extent our model fits well with the,
uh,
target,
uh,

29
00:01:38.741 --> 00:01:40.360
sequence information we have.

30
00:01:41.080 --> 00:01:46.080
So we'll optimize this average loss alone sold plus the a way the regular riser

31
00:01:47.591 --> 00:01:52.591
to a control to what extent we want our model to fit well the data and to be

32
00:01:53.591 --> 00:01:56.470
able to control for overfitting.
So again,

33
00:01:56.471 --> 00:01:59.630
we're casting learning as optimization like we've done in the regular neural

34
00:01:59.631 --> 00:02:00.464
network.

35
00:02:00.640 --> 00:02:04.780
And a though ideally would like to optimize the castigation error.

36
00:02:04.781 --> 00:02:05.614
So in this case,

37
00:02:05.830 --> 00:02:10.210
perhaps you'd like to optimize us some of per label classification there.
Uh,

38
00:02:10.211 --> 00:02:11.710
this error again is non smooth,

39
00:02:11.711 --> 00:02:14.400
so we have to use what we usually call a surrogate loss.

40
00:02:14.401 --> 00:02:17.560
So an alternative laws that sort of looks like what we want to optimize,

41
00:02:17.860 --> 00:02:20.620
but that is a better behaved.
Uh,

42
00:02:20.621 --> 00:02:24.190
and in specifically in this case is going to be differentiable so we can perform

43
00:02:24.191 --> 00:02:25.240
great in descent training

44
00:02:27.830 --> 00:02:32.150
and again we'll use a stochastic gradient descent that is uh,

45
00:02:32.151 --> 00:02:35.180
that we were training is going to proceed is that we'll first initialize the

46
00:02:35.181 --> 00:02:38.240
parameters of our model.
If we're sending a number of iterations,

47
00:02:38.260 --> 00:02:43.260
will cycle through our pairs of input sequences and target sequences will

48
00:02:43.701 --> 00:02:46.160
compute the uh,
update direction,

49
00:02:46.161 --> 00:02:51.161
which is going to be minus the gradient of the loss for my current pair of uh,

50
00:02:53.030 --> 00:02:58.030
input and target sequences minus also the gradient doctor regular riser.

51
00:02:58.250 --> 00:03:00.610
And then I'll take a step.
Uh,

52
00:03:00.611 --> 00:03:05.500
so alpha times the update direction to change my,

53
00:03:05.530 --> 00:03:09.520
uh,
vector from is my set of parameters.
And so again,

54
00:03:09.521 --> 00:03:13.300
I have to specify a loss function as specified procedure for computing.

55
00:03:13.301 --> 00:03:16.920
The gradient with respect to the loss function also have to specify the

56
00:03:16.921 --> 00:03:20.410
irregular riser and its gradient as well as the initialization method.

57
00:03:20.740 --> 00:03:23.350
So these two parts are not really going to change or we're not really going to

58
00:03:23.560 --> 00:03:28.350
discuss them.
We'll focus on choosing a loss function and also,
uh,

59
00:03:28.410 --> 00:03:30.940
specifying what we compute the perimeter great hymns.

60
00:03:31.240 --> 00:03:34.960
And is this particular video we'll look at.
The last function would optimize.

61
00:03:38.290 --> 00:03:39.010
So again,

62
00:03:39.010 --> 00:03:43.000
because a conditional random feel estimates the conditional probability of the

63
00:03:43.001 --> 00:03:44.590
target given the input,

64
00:03:45.040 --> 00:03:50.040
what is a natural thing to do is to maximize the probability assigned to the

65
00:03:50.891 --> 00:03:55.810
true target.
The sequence given some input sequence,
uh,

66
00:03:55.811 --> 00:03:59.000
as extracted from our training set and a,

67
00:03:59.020 --> 00:04:02.470
as we've seen in the case of irregular neural network,

68
00:04:02.590 --> 00:04:04.540
it's of maximizing probabilities.

69
00:04:04.541 --> 00:04:07.990
We can minimize the negative log of the probability,

70
00:04:08.020 --> 00:04:12.790
which is more a better behaved and yield simpler math.
And so,

71
00:04:12.820 --> 00:04:15.400
uh,
to form it has a minimization problem.

72
00:04:15.550 --> 00:04:17.470
We minimize the negative log likelihood.

73
00:04:17.471 --> 00:04:22.471
So our loss function will be minus the log of the probability of the true

74
00:04:22.570 --> 00:04:27.460
target.
So normally this would be the true target given the inputs sequence.

75
00:04:28.510 --> 00:04:32.530
So this is really the same that we've done in irregular neural network where we

76
00:04:32.531 --> 00:04:37.390
have an input vector and we're trying to classify it into a set of a CE classes.

77
00:04:37.570 --> 00:04:41.350
The only difference is that now this is a sequence and this is also a sequence.

78
00:04:42.010 --> 00:04:45.820
One sort of subtle difference also is that,
uh,

79
00:04:45.821 --> 00:04:49.780
while in a non sequential concert vacation,
if you have 10 classes,

80
00:04:49.781 --> 00:04:54.370
we might actually explicitly compute the full conditional distribution p of y

81
00:04:54.371 --> 00:04:56.530
given x by iterating over all classes.

82
00:04:56.770 --> 00:05:01.480
And so obtaining the full vector of probability.
Now with a sequence,

83
00:05:01.481 --> 00:05:05.080
we can do this because the number of sequences is exponential.

84
00:05:05.200 --> 00:05:09.640
So really when we're computing,
this will,
uh,

85
00:05:09.850 --> 00:05:13.150
use,
uh,
forward,
backward to get our outdoor,

86
00:05:13.151 --> 00:05:18.040
our bed a table and to compute the partition function and we'll use them

87
00:05:18.041 --> 00:05:22.900
explicitly the partition function into the expression for p of y given x.

88
00:05:22.930 --> 00:05:27.790
And then we'll take minus the log of that.
So we won't actually compute a,

89
00:05:27.791 --> 00:05:32.320
we don't have the full distribution of all sequences that just because that's

90
00:05:32.620 --> 00:05:35.800
too big of an object.
It's something that's,
uh,

91
00:05:35.830 --> 00:05:37.300
there's an exponential well sequences.

92
00:05:37.301 --> 00:05:39.520
So that would be an exponential number of probabilities.

93
00:05:40.120 --> 00:05:43.360
So that's the last function we'll use for training or conditional random fields.

