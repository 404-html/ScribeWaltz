WEBVTT

1
00:00:00.550 --> 00:00:02.340
In this video,
we'll discuss the,
uh,

2
00:00:02.440 --> 00:00:05.890
idea of local connectivity in convolutional neural networks.

3
00:00:07.200 --> 00:00:08.033
<v 1>Yeah.</v>

4
00:00:08.390 --> 00:00:12.050
<v 0>So,
uh,
we mentioned that we would,
uh,
in a previous video that we would,
uh,</v>

5
00:00:12.080 --> 00:00:15.860
talk about conventional known networks as a type of neural network that are

6
00:00:15.861 --> 00:00:18.560
specifically adapted for a computer vision problems.

7
00:00:18.800 --> 00:00:23.530
And that we'll first talk about a first property of conversional known networks,

8
00:00:23.610 --> 00:00:25.940
uh,
which is a local connectivity of its hidden units.

9
00:00:28.920 --> 00:00:30.300
So the idea is very simple.

10
00:00:30.450 --> 00:00:35.150
If you have the hidden layer in these convolutional neural networks,
uh,

11
00:00:35.250 --> 00:00:40.250
then each unit in the hidden layer is actually going to be connected not to all

12
00:00:40.651 --> 00:00:45.250
the,
uh,
units in the previous hidden there in the previous layer.
So in,

13
00:00:45.630 --> 00:00:50.470
for the first layer,
this would be the,
the visible layer of the input layer.
Uh,

14
00:00:50.520 --> 00:00:54.630
it's actually instead going to be connected only to a small number of units.

15
00:00:54.720 --> 00:00:58.860
And specifically it's going to be connected to a unit in the locally,

16
00:00:59.250 --> 00:01:03.000
a spatially localized region of a dean.

17
00:01:03.001 --> 00:01:07.890
But so fun since we could have this hidden units here be connected only to the

18
00:01:08.130 --> 00:01:13.080
inputs that are present in that region of the image in this unit could be

19
00:01:13.081 --> 00:01:17.970
written,
uh,
connected to only these inputs here in that region and so on.
Uh,

20
00:01:18.090 --> 00:01:18.740
in fact,

21
00:01:18.740 --> 00:01:23.740
what we will have is that we have a first in our first unit that will connected

22
00:01:24.600 --> 00:01:28.170
only to these,
uh,
inputs here.

23
00:01:28.230 --> 00:01:30.430
And then another one connected to a,

24
00:01:30.750 --> 00:01:35.010
a region that is right next to it.
This will be the next thing in the unit.

25
00:01:35.011 --> 00:01:39.840
And you have like your hidden units for all of these small windows in the,

26
00:01:39.900 --> 00:01:43.200
uh,
original input.
Um,

27
00:01:43.520 --> 00:01:48.050
it hidden unit will be connected to all the,
uh,
so called channels.
So,

28
00:01:48.080 --> 00:01:51.450
uh,
if we have agreed scale image,
uh,

29
00:01:51.500 --> 00:01:53.750
then you're connected to just one channel,

30
00:01:53.751 --> 00:01:58.280
which is the intensity of the green scale intensity of the pixels.

31
00:01:58.550 --> 00:02:01.730
And then we have a color image,
then we'll connect it to all three channels.

32
00:02:01.731 --> 00:02:06.530
There are the g and the beach handle.
So that is the density of the red channel,

33
00:02:06.531 --> 00:02:10.780
the green channel,
the Blue Chattel,
and the image.
And so this,
uh,

34
00:02:10.790 --> 00:02:14.630
local connectivity property,
we'll solve the two following problem.

35
00:02:15.170 --> 00:02:18.710
The first one is that if we had fully connected hidden there,
uh,

36
00:02:18.740 --> 00:02:22.190
we would actually have an unmanageable number of parameters and,
uh,

37
00:02:22.360 --> 00:02:25.310
if we increased the number of hitting this we could have on these so many hidden

38
00:02:25.311 --> 00:02:26.720
units,
uh,

39
00:02:26.870 --> 00:02:30.860
without having memory problems and started with just in terms of just storing

40
00:02:31.070 --> 00:02:33.530
the providence of that,
uh,
no one network.

41
00:02:34.460 --> 00:02:38.570
The other thing is that computing the,
uh,
linear activation.

42
00:02:38.571 --> 00:02:42.950
So I mean the pre activations of the hidden units,
we actually be very expensive.

43
00:02:42.951 --> 00:02:47.930
If the hidden there was fully connected,
uh,
it would scale and the size of the,

44
00:02:48.260 --> 00:02:51.890
uh,
of the input and the input size is very big.

45
00:02:51.891 --> 00:02:56.010
So that would be very expensive by adding heat should in unit be connected to

46
00:02:56.020 --> 00:03:00.230
only a small subset of the inputs.
Then we reduced number of,
of uh,

47
00:03:00.350 --> 00:03:03.250
multiplications we have to do between weights and inputs.

48
00:03:03.251 --> 00:03:06.730
So that's gonna give us a procedure which is a much more efficient.

49
00:03:08.020 --> 00:03:12.110
When we're talking about the,
uh,
uh,
part of the image that a,

50
00:03:12.170 --> 00:03:14.080
a unit is connected to,

51
00:03:14.081 --> 00:03:17.730
we're going to refer to that as a receptive field.
So the,

52
00:03:17.731 --> 00:03:22.660
the grayish area here are the receptive fields of the different units.
Uh,

53
00:03:22.661 --> 00:03:23.200
and uh,

54
00:03:23.200 --> 00:03:28.200
sometimes I'm going to refer to our as being the height or the width if they're

55
00:03:28.361 --> 00:03:31.330
square of the receptive fields.

56
00:03:33.780 --> 00:03:34.613
<v 1>Okay.</v>

57
00:03:34.650 --> 00:03:38.520
<v 0>And,
um,
so this slide is just to emphasize that if you have many channels,</v>

58
00:03:38.521 --> 00:03:43.050
for instance,
three channels in RGB images.
So we have the,
our channel here,

59
00:03:43.350 --> 00:03:46.170
the B channel in the,
uh,
sorry,

60
00:03:46.171 --> 00:03:49.920
the g channel here and the beach channel here.
Uh,

61
00:03:49.950 --> 00:03:53.970
then we would actually get a heating unit would normally be connected to all

62
00:03:53.971 --> 00:03:56.700
channels.
So to simplify those,

63
00:03:56.910 --> 00:04:01.080
this receptive field corresponded to four units,
uh,
which is clearly not true,

64
00:04:01.081 --> 00:04:05.430
but just to simplify things.
Then we have four units here and four units here.
Uh,

65
00:04:05.431 --> 00:04:09.570
then this hidden unit here would have,
uh,

66
00:04:09.600 --> 00:04:13.920
12 connections for here,
plus the four here,
plus the four here.

67
00:04:14.760 --> 00:04:18.210
Okay.
So that's the idea of local connectivity and convolutional neural networks.

