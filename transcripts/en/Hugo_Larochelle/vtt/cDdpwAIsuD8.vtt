WEBVTT

1
00:00:00.290 --> 00:00:00.940
Okay.

2
00:00:00.940 --> 00:00:01.451
<v 1>In this video,</v>

3
00:00:01.451 --> 00:00:05.920
we're ready to put all the previous pieces together and they find a full,

4
00:00:06.250 --> 00:00:08.050
a convolutional neural network.

5
00:00:10.080 --> 00:00:15.030
So having now these three ideas together and how we should implement it in

6
00:00:15.031 --> 00:00:18.420
computing the hidden layers of a commercial neural network,

7
00:00:18.750 --> 00:00:23.540
we are now ready to see what the full forward propagation in a conversional

8
00:00:23.610 --> 00:00:26.690
neural network corresponds to.
And then we'll talk about,
uh,

9
00:00:26.820 --> 00:00:30.660
what are the missing pieces for training a convolutional neural network.

10
00:00:33.240 --> 00:00:33.810
<v 0>Okay.</v>

11
00:00:33.810 --> 00:00:35.850
<v 1>So to get the convolutional neural network,</v>

12
00:00:35.851 --> 00:00:40.851
we'll just alternate between these conversional operations and pooling

13
00:00:40.980 --> 00:00:44.910
operations or,
uh,
if you want to call them instead layers.

14
00:00:45.420 --> 00:00:48.630
Um,
so when I say convolutional layer,

15
00:00:48.631 --> 00:00:50.960
I'm referring to the,
uh,

16
00:00:51.000 --> 00:00:55.410
performing all the convolutions for each input channel,
uh,

17
00:00:55.411 --> 00:00:57.300
and then aggregating them.

18
00:00:57.301 --> 00:00:59.910
So summing them together and applying the nonlinearity.

19
00:01:00.060 --> 00:01:04.920
So this convolutional plus nonlinearity operation will be the convolutional

20
00:01:04.921 --> 00:01:05.670
layer,

21
00:01:05.670 --> 00:01:10.410
and then the pooling operation is the pooling followed by these sub sampling.

22
00:01:11.270 --> 00:01:13.950
Uh,
so that's what I mean here by pooling layer.

23
00:01:14.610 --> 00:01:17.040
And what we do is that we alternate between the two.

24
00:01:17.070 --> 00:01:22.070
So I mentioned we had an input image which had just in this example a single

25
00:01:22.111 --> 00:01:22.890
channel.

26
00:01:22.890 --> 00:01:27.870
Then we could compute a hidden layer that corresponds to 60 for a feature maps

27
00:01:28.290 --> 00:01:29.190
and uh,

28
00:01:29.220 --> 00:01:33.360
that where each of the unit would have a nine by nine receptive field.

29
00:01:33.840 --> 00:01:38.590
So that would give us 64 75 by 70 75,
uh,

30
00:01:38.640 --> 00:01:39.450
feature maps.

31
00:01:39.450 --> 00:01:44.450
So there are 64 feature maps and each are Maitri matrices of 75 by 75.

32
00:01:46.170 --> 00:01:50.610
Uh,
so this is just the result for each of taking a single convolution because we

33
00:01:50.611 --> 00:01:53.580
have just one input channel and then applying some nonlinearity.

34
00:01:54.660 --> 00:01:56.190
Then we could apply some pooling.

35
00:01:56.191 --> 00:02:00.180
So we could for each feature map each 64 feature map,

36
00:02:00.210 --> 00:02:04.030
take some say Max pooling and uh,
uh,
uh,

37
00:02:04.110 --> 00:02:09.000
none overlapping regions and then doing the sub sampling by retaining all leader

38
00:02:09.030 --> 00:02:12.630
maximizing values.
So then we'd get a,
again,

39
00:02:12.631 --> 00:02:16.680
64 feature maps on art that are smaller.
That is that our 14 by 14,

40
00:02:17.760 --> 00:02:21.930
and then we could apply a new convolutional layer.
Now in this case,

41
00:02:21.931 --> 00:02:26.931
the 64 feature maps at layer two are going to act as the input channels for

42
00:02:27.721 --> 00:02:30.870
layer three and a.
In this example,

43
00:02:31.350 --> 00:02:36.030
they use again nine by nine receptive fields on these,
uh,

44
00:02:36.210 --> 00:02:39.150
14 by 14,
uh,
input channels,

45
00:02:39.151 --> 00:02:41.670
which are the 14 by 14 feature maps of layer two.

46
00:02:42.480 --> 00:02:44.940
And so notice that in this case computing,

47
00:02:44.941 --> 00:02:49.941
say that feature map requires performing 64 convolutions on all of the,

48
00:02:51.710 --> 00:02:54.450
uh,
14 by 14,
uh,
feature maps here,

49
00:02:54.451 --> 00:02:58.500
which are acting as it put channels for that convolutional layer here and that

50
00:02:58.800 --> 00:02:59.633
feature here.

51
00:03:00.790 --> 00:03:04.570
So that's means I'm going to have a nine by nine by,

52
00:03:05.150 --> 00:03:06.730
I'm going to have 64,

53
00:03:06.880 --> 00:03:11.830
nine by nine matrix cs for each feature map here.
So that can be a lot.

54
00:03:12.250 --> 00:03:15.700
Uh,
and,
uh,
actually,
uh,
uh,
in some work,

55
00:03:15.701 --> 00:03:20.701
people instead randomly choose between the 60 for just a subset of a,

56
00:03:21.670 --> 00:03:25.900
a few feature maps that say each feature map is going to be connected through.

57
00:03:25.901 --> 00:03:30.370
So you sort of randomly assign to each of the feature maps in the three,

58
00:03:30.371 --> 00:03:34.960
a third hidden layer,
a subset of the 64.
And,
uh,
uh,

59
00:03:35.170 --> 00:03:38.320
pull the and subsample second hidden layer here.

60
00:03:38.410 --> 00:03:41.350
So that's one way of reducing the number of parameters in the number of

61
00:03:41.351 --> 00:03:46.120
computations,
uh,
that can be used.
So after we've done this,
we get,

62
00:03:46.480 --> 00:03:47.250
uh,

63
00:03:47.250 --> 00:03:52.250
so in this case was chosen to have 256 feature maps at this level and performing

64
00:03:53.231 --> 00:03:58.090
the convolutions.
Uh,
we were left with six by six,
uh,
uh,

65
00:03:58.120 --> 00:03:59.800
it made me sees for these feature maps.

66
00:04:00.190 --> 00:04:04.210
And then after some a six by six pooling,
uh,

67
00:04:04.230 --> 00:04:07.830
then we get 256,
uh,
uh,
feature maps,

68
00:04:07.840 --> 00:04:10.360
although actually one by one that are just a single that,

69
00:04:11.380 --> 00:04:16.330
and to obtain a final output,
we would just treat this as a factor and connected,

70
00:04:16.630 --> 00:04:20.980
fully connected to some output layer,
uh,
where the nonlinearity at.

71
00:04:20.981 --> 00:04:23.590
The output would be a softmax if we're doing,
say,

72
00:04:23.591 --> 00:04:27.890
object recognition or a character classification.
So a hundred,
a hundred,

73
00:04:28.110 --> 00:04:30.340
101 is taken for the Caltech,

74
00:04:30.341 --> 00:04:34.060
one o one dataset where we'd have 101 different categories,

75
00:04:34.270 --> 00:04:37.030
so 101 different classes for the classification problem.

76
00:04:37.390 --> 00:04:39.040
So here it wouldn't have any convolutions.

77
00:04:39.041 --> 00:04:43.090
It would just be this factor of 10 56 multiply by Matrix of weights,

78
00:04:43.150 --> 00:04:47.420
less some bias passed into a soft Max.
Uh,

79
00:04:47.540 --> 00:04:51.550
you might have noticed also that here,
uh,
for the pooling layers,

80
00:04:51.790 --> 00:04:56.560
they actually used a bigger pooling,
uh,
neighborhood,
uh,

81
00:04:56.590 --> 00:05:01.300
Dan d a sub sampling neighborhood.
Um,

82
00:05:01.390 --> 00:05:05.500
and so we can actually use different sizes for either the pooling of sub

83
00:05:05.501 --> 00:05:07.780
sampling if we want a,

84
00:05:07.781 --> 00:05:10.960
but often people actually use the same neighborhood size,

85
00:05:10.961 --> 00:05:15.580
so they have the same uh,
uh,
sub sampling and pulling neighborhoods for,

86
00:05:15.640 --> 00:05:17.510
uh,
in computing a pooling layer.

87
00:05:20.650 --> 00:05:21.483
<v 0>Okay.</v>

88
00:05:21.660 --> 00:05:23.420
<v 1>So like I said,
the output layer is irregular,</v>

89
00:05:23.520 --> 00:05:28.520
fully connected softmax layer and the output does,

90
00:05:29.070 --> 00:05:32.310
gives us an estimate of the conditional probability of each class of each

91
00:05:32.370 --> 00:05:34.890
category of object.
You want to detect,

92
00:05:35.670 --> 00:05:38.670
a one to recognize and then to train it.

93
00:05:38.700 --> 00:05:41.790
We can do as we did for regular feed forward Nola at work.

94
00:05:41.791 --> 00:05:46.110
That is use the negative log likelihood conditional love.
Like if the class,

95
00:05:46.111 --> 00:05:50.880
given the input image as our training loss and perform stochastic gradient

96
00:05:50.881 --> 00:05:52.470
descent,
um,

97
00:05:52.600 --> 00:05:57.470
backpropagation is similar as the fully connected network is.

98
00:05:57.650 --> 00:05:59.570
We do first a Ford pass,

99
00:05:59.870 --> 00:06:02.870
which computes the output and we get a loss from this.

100
00:06:02.990 --> 00:06:05.690
And then we get a great end of the output layer and then we have to back

101
00:06:05.691 --> 00:06:10.130
propagate the gradient towards the input,
uh,

102
00:06:10.160 --> 00:06:13.970
in a way that we can thus get the,
with respect to all the parameters.

103
00:06:14.150 --> 00:06:17.540
And in this case,
the parameters are going to be,
uh,
the biases.

104
00:06:17.541 --> 00:06:20.330
If you have some biases.
We have the output connection Matrix,

105
00:06:20.331 --> 00:06:24.270
but we also have the,
uh,
colonels or the,
the,
uh,

106
00:06:24.470 --> 00:06:29.390
Maitri sees a fill or filters that we have for each feature map.

107
00:06:29.830 --> 00:06:33.890
Uh,
so we have to train these parameters now,
uh,
when we're doing backpropagation.

108
00:06:34.720 --> 00:06:34.980
<v 0>Okay,</v>

109
00:06:34.980 --> 00:06:37.680
<v 1>so we only know how to pass gradients through,
say,</v>

110
00:06:37.681 --> 00:06:42.660
Ellenton wise activation function.
For instance,
when we're computing the,
uh,

111
00:06:42.690 --> 00:06:45.330
once we've computed the reactivation using the convolutions,

112
00:06:45.331 --> 00:06:48.340
we do an element wise activation function.
Uh,

113
00:06:48.450 --> 00:06:50.340
we all need to know how to pass a gradient through that.

114
00:06:50.341 --> 00:06:51.780
So we'll go to that part.

115
00:06:51.781 --> 00:06:56.030
We just look at the missing pieces for implementing,
uh,

116
00:06:56.100 --> 00:06:57.090
backpropagation,

117
00:06:57.480 --> 00:07:01.740
which is how to back propagate through the convolution operation and,
uh,

118
00:07:01.780 --> 00:07:03.660
the pooling and sub sampling operation.

119
00:07:06.690 --> 00:07:07.340
<v 0>Okay.</v>

120
00:07:07.340 --> 00:07:10.000
<v 1>Now let l B,
uh,</v>

121
00:07:10.040 --> 00:07:14.450
my short notation for the loss function for some given a training example.

122
00:07:15.410 --> 00:07:20.000
And just to simplify now why Jay is going to be the pre activation,

123
00:07:20.410 --> 00:07:25.020
uh,
as computer based on the IAFF,
uh,
input channel,
uh,

124
00:07:25.220 --> 00:07:26.360
for some given layer.

125
00:07:26.390 --> 00:07:30.920
And kig is going to be their conventional colonel associated with my matrix of

126
00:07:30.921 --> 00:07:35.570
connections for my JF,
uh,
JFE feature map.

127
00:07:36.320 --> 00:07:36.750
<v 0>Okay.</v>

128
00:07:36.750 --> 00:07:39.040
<v 1>Now,
if I have,
uh,</v>

129
00:07:39.080 --> 00:07:44.080
the gradient of the full a result of the convolution,

130
00:07:45.290 --> 00:07:46.770
uh,
for the GF feature map,

131
00:07:46.800 --> 00:07:50.820
so I'm calling this a Nebula Yj l.

132
00:07:50.821 --> 00:07:53.880
So that's the greening of the last week,
respected the full feature map,

133
00:07:54.310 --> 00:07:58.710
a computed based on some input channel.
I,

134
00:07:58.740 --> 00:08:01.540
so I'm actually here assuming there's only one,
uh,

135
00:08:01.590 --> 00:08:03.390
essentially there's only one input channel.

136
00:08:04.720 --> 00:08:04.950
<v 0>Okay?</v>

137
00:08:04.950 --> 00:08:07.640
<v 1>Uh,
generalizing to multiple in per channel would be simple.</v>

138
00:08:08.150 --> 00:08:11.360
So I just take that gradient,
which is itself an image.

139
00:08:11.630 --> 00:08:16.050
And then I do deconvolution,
uh,

140
00:08:16.080 --> 00:08:18.000
over,
uh,
uh,

141
00:08:18.050 --> 00:08:22.710
I do the convolution with zero panning with the weight matrix.

142
00:08:22.740 --> 00:08:25.440
W Ij.
So no,
this,
now I'm doing the convolution,

143
00:08:25.441 --> 00:08:27.780
not using the kernel,

144
00:08:27.990 --> 00:08:32.280
which is just the flipped rows and columns version of Wij.

145
00:08:32.281 --> 00:08:37.281
I'm actually doing the convolution with WWE Ij and I'm also doing it with zero

146
00:08:37.411 --> 00:08:40.800
padding,
so I won't go over the details for why I have to do this,

147
00:08:40.801 --> 00:08:41.940
why this is the correct thing.

148
00:08:41.941 --> 00:08:46.200
You might want to sit down and do a little toy example to figure out why this is

149
00:08:46.201 --> 00:08:47.034
the right thing to do.

150
00:08:48.300 --> 00:08:48.570
<v 0>Okay.</v>

151
00:08:48.570 --> 00:08:49.381
<v 1>And now,</v>

152
00:08:49.381 --> 00:08:54.180
because the IIF input channel is used by all feature maps,

153
00:08:54.780 --> 00:08:58.340
uh,
I have to sum over all of these,
uh,

154
00:08:58.530 --> 00:09:01.690
gradients that come from Eaton Bgf,
uh,

155
00:09:01.710 --> 00:09:04.170
feature map and sum them together.
So I have a,

156
00:09:04.171 --> 00:09:09.171
some over j of the BACP propagated gradient from DGF feature map,

157
00:09:10.500 --> 00:09:15.340
which,
uh,
to back propagate it,
I have to do this convolution with zero patty,

158
00:09:15.690 --> 00:09:20.690
with my Matrix Wij and now I [inaudible] so I want to learn Wij.

159
00:09:21.961 --> 00:09:26.940
So at this point I can also compute the gradients with respect to wag my matrix,

160
00:09:27.060 --> 00:09:31.740
my filter matrix.
Uh,
in this case,
what did corresponds to it's again,

161
00:09:31.741 --> 00:09:36.510
taking,
uh,
my,
uh,
great in retrospect too.
I,

162
00:09:36.520 --> 00:09:39.690
it should be,
there should be a j here and,
uh,

163
00:09:39.780 --> 00:09:43.850
doing a convolution with the uh,
version.

164
00:09:43.860 --> 00:09:47.760
So my input channel x Xy,
but where I flipped the rows and columns.

165
00:09:47.910 --> 00:09:50.040
So that's why I have to tilt here.
Okay.

166
00:09:50.430 --> 00:09:55.430
And that's doing that operation gives me the gradient retrospect to double my

167
00:09:55.771 --> 00:09:58.110
matrix w Ij for the,

168
00:09:58.170 --> 00:10:02.360
the parameters of the feature map that connects the input,
um,

169
00:10:02.790 --> 00:10:07.590
that connects the input channel.
I with the,
uh,
feature,
uh,
fi,
uh,

170
00:10:07.920 --> 00:10:08.753
feature map Jane.

171
00:10:09.630 --> 00:10:14.080
So we see that much like the forward propagation can be,
uh,

172
00:10:14.100 --> 00:10:15.960
when we apply and conclusions,

173
00:10:16.020 --> 00:10:21.020
I can actually invoke some code that does either zero padding or without zero

174
00:10:21.751 --> 00:10:23.670
patty convolution operations.

175
00:10:23.671 --> 00:10:26.990
The green ants can also be computed using these operation,
uh,

176
00:10:27.190 --> 00:10:28.710
these convolutional operations.

177
00:10:31.980 --> 00:10:34.140
And as for the gradients,
uh,

178
00:10:34.170 --> 00:10:38.100
passing through the pooling and sub sampling layer before Max pooling,

179
00:10:39.300 --> 00:10:41.580
um,
well the gradients.

180
00:10:41.581 --> 00:10:46.170
So if I have the gradients with respect to why I J K,

181
00:10:46.500 --> 00:10:49.050
so,
uh,
this,
uh,

182
00:10:49.051 --> 00:10:53.370
this would correspond to a for some given a feature map.

183
00:10:53.460 --> 00:10:58.410
I the uh,
gradients,
uh,
the,
uh,
yes,

184
00:10:58.411 --> 00:11:01.710
the,
the gradient with respect to the element I Jane the,
uh,

185
00:11:01.860 --> 00:11:03.870
that feature map for their a pooling layer.

186
00:11:04.710 --> 00:11:08.390
Now the gradient retrospect to the uh,
uh,

187
00:11:08.580 --> 00:11:12.450
value of the input channel,
which is the previous layer feature map.

188
00:11:13.050 --> 00:11:13.940
Uh,

189
00:11:14.010 --> 00:11:18.600
so the ingredients for each of its elements is actually going to be zero for

190
00:11:18.601 --> 00:11:23.601
everything except the positions which I'm calling p prime in Q prime,

191
00:11:23.820 --> 00:11:28.010
which corresponded to the maximizing of values.
Um,

192
00:11:28.140 --> 00:11:30.710
so visually it means that,
uh,

193
00:11:30.780 --> 00:11:33.720
imagine I had a neighborhood like this,

194
00:11:34.260 --> 00:11:38.880
a three by three over which I did pooling and that the maximizing element was

195
00:11:38.881 --> 00:11:42.450
this one.
And now,
uh,

196
00:11:42.451 --> 00:11:46.890
and so when I did for propagation,
I took this value and put it at the pooling,

197
00:11:47.310 --> 00:11:51.700
uh,
layer,
uh,
physician corresponding to that,
uh,

198
00:11:51.780 --> 00:11:56.290
that computation.
Nice have the gradients with respect to add elements.

199
00:11:56.560 --> 00:12:01.560
Well that green is going to be copied and consider as decree of that element

200
00:12:01.931 --> 00:12:06.310
directly and everything else is going to have a gradient of zero.

201
00:12:07.270 --> 00:12:12.100
Okay.
So that's how we would just back propagate the gradients,
uh,
for uh,

202
00:12:12.160 --> 00:12:15.010
through a,
uh,
uh,
Max pooling operation.

203
00:12:16.110 --> 00:12:19.360
And for the average fooling operation where we're just taking the average over a

204
00:12:19.361 --> 00:12:24.130
local neighborhood,
well we actually just uh,
take,
uh,

205
00:12:24.131 --> 00:12:29.131
we just upped sample the gradients which respect to the pool and sub sample a

206
00:12:30.340 --> 00:12:34.450
layer and a then we multiply by this scalar here,

207
00:12:34.960 --> 00:12:37.270
a one over m squared.

208
00:12:37.600 --> 00:12:40.600
So going again over my example here,

209
00:12:40.601 --> 00:12:44.430
if at this neighborhood here and uh,

210
00:12:44.480 --> 00:12:49.030
I had the gradient of the,
with respect to the pool version of that neighborhood,

211
00:12:49.300 --> 00:12:53.320
which say was,
um,
just to make up an example,
0.5,

212
00:12:53.650 --> 00:12:58.270
then the great end here would all be 0.5 for everything.

213
00:12:58.270 --> 00:13:00.640
So here,
here,
here,
here,
here,

214
00:13:01.090 --> 00:13:04.570
but I would also multiply by one over night.

215
00:13:04.930 --> 00:13:07.150
So he 0.5 divided by nine.

216
00:13:07.390 --> 00:13:11.860
That would be the gradient of the pre pooling and sub sampling,

217
00:13:12.240 --> 00:13:14.050
uh,
values of the layer here.

218
00:13:14.260 --> 00:13:18.820
And that's then what I would back propagate a continuing back propagating

219
00:13:18.821 --> 00:13:21.100
through the network.
All right.

220
00:13:21.101 --> 00:13:24.600
So using these definitions of the gradients,
uh,

221
00:13:24.720 --> 00:13:29.720
of each outputs of either the convolutional operation or conventional layer or

222
00:13:30.341 --> 00:13:32.260
the pooling layer,

223
00:13:32.410 --> 00:13:37.410
then I can implement backpropagation by going in the reverse order to the Ford

224
00:13:37.451 --> 00:13:41.620
propagation and calling these backpropagation,
uh,
sorry,

225
00:13:41.621 --> 00:13:45.310
these great and propagating a operations which I've defined.

226
00:13:46.330 --> 00:13:49.480
And this will give me the parameters with respect to my,
uh,

227
00:13:49.490 --> 00:13:51.040
the green and respect to my parameters.

228
00:13:51.041 --> 00:13:55.810
And then I do a great end up update as in regular stochastic gradient descent.

