WEBVTT

1
00:00:01.530 --> 00:00:04.980
In this video,
we'll talk about the popular model in deep learning,

2
00:00:05.490 --> 00:00:06.810
the deep auto encoder.

3
00:00:09.270 --> 00:00:13.590
So here we're considering the case where a instead of training is single hidden

4
00:00:13.650 --> 00:00:14.960
there or throwing quarter,

5
00:00:14.990 --> 00:00:19.940
we would actually want to train a multilayer or two quarter in particular,

6
00:00:19.941 --> 00:00:24.000
a deal don't quarter will be interested in is in its own corner where the

7
00:00:24.030 --> 00:00:28.950
encoder gradually decreases the dimensionality of the input.

8
00:00:28.951 --> 00:00:31.830
So it extracts a 2000 hidden units,

9
00:00:32.160 --> 00:00:35.730
a vector representation.
So 2000,
and that's just an example,

10
00:00:35.770 --> 00:00:37.350
could have chosen other numbers,

11
00:00:37.351 --> 00:00:42.351
but then we will gradually decrease the representation as we move along in the

12
00:00:43.051 --> 00:00:45.510
computations made by Dee and quarter.

13
00:00:46.830 --> 00:00:50.490
And so this would allow us to reach eventually a very small dimensionality,

14
00:00:50.491 --> 00:00:54.330
even like a a two,
a two d representation,

15
00:00:54.331 --> 00:00:59.220
which we could for instance,
visualize,
uh,
uh,
in,
uh,

16
00:00:59.310 --> 00:01:02.220
as an image to do some data visualization.

17
00:01:02.820 --> 00:01:07.820
And then the decoder will not just be a single linear plus nonlinearity

18
00:01:08.160 --> 00:01:10.650
reconstruction operation.
They will actually again,

19
00:01:10.651 --> 00:01:14.450
gradually increased the dimentionality,
uh,

20
00:01:14.560 --> 00:01:18.810
from the encoded representation into a 500 like here.

21
00:01:18.811 --> 00:01:20.880
And then a thousand like here.

22
00:01:21.330 --> 00:01:24.660
And then eventually 2000 like here,

23
00:01:25.020 --> 00:01:30.020
and then ultimately go back into the input space and do a full reconstruction of

24
00:01:30.541 --> 00:01:32.190
the original input here.

25
00:01:33.600 --> 00:01:37.460
So training such a deeper tone quarter is actually very hard.
And in this case,

26
00:01:37.470 --> 00:01:39.900
the problem we face is not really under a,

27
00:01:39.901 --> 00:01:42.300
it's not really overfitting but more under fitting.

28
00:01:42.301 --> 00:01:46.290
It's actually a hard to reach very good training error,
uh,

29
00:01:46.291 --> 00:01:50.210
especially if you have a large training sample.
And,
uh,

30
00:01:50.410 --> 00:01:55.050
it turns out that if we use the pre-training with unsupervised learning

31
00:01:55.051 --> 00:01:57.980
procedure where we really greedily trained,

32
00:01:57.981 --> 00:02:02.580
say a first RBM and then a second RBM and so on,

33
00:02:03.620 --> 00:02:07.260
uh,
so actually this would be the first RBM then get another RBM here.

34
00:02:07.550 --> 00:02:10.950
Now they're RBM and so on,
until we reach a hidden,
there are 30 hidden units.

35
00:02:10.951 --> 00:02:15.320
If you used this to initialize a deeper tone quarter,
well actually get some,
uh,

36
00:02:15.360 --> 00:02:16.330
quite good results.

37
00:02:16.331 --> 00:02:20.190
So this was a result presented by Jeff Hinton and Russell is adequate enough in

38
00:02:20.490 --> 00:02:23.100
science in 2006.
Um,

39
00:02:23.250 --> 00:02:27.600
and that's somewhat surprising because I've argued thus far that pre training

40
00:02:27.601 --> 00:02:32.601
with unsupervised learning was useful to obtain a good regularization.

41
00:02:32.761 --> 00:02:36.780
So to fight over fitting and here it seems we actually fighting under fitting.

42
00:02:37.310 --> 00:02:39.870
So that's somewhat surprising.
Um,

43
00:02:39.990 --> 00:02:44.940
so this perhaps means that unsupervised speech training doesn't play only a role

44
00:02:45.180 --> 00:02:49.420
of regularization,
but perhaps it can be sometimes useful for uh,
uh,

45
00:02:49.590 --> 00:02:52.950
improving the quality of the optimization here.

46
00:02:52.951 --> 00:02:56.880
It could be due to the fact that we're actually not performing classification.

47
00:02:56.881 --> 00:02:59.040
We all are doing unsupervised learning training.

48
00:02:59.110 --> 00:03:02.760
Deep auto encoder is unsupervised learning where that pretty thing a labeled.

49
00:03:02.760 --> 00:03:04.540
We really reconstructing the input.

50
00:03:04.900 --> 00:03:08.860
So perhaps there's a relationship between unsupervised pre-training and just

51
00:03:08.861 --> 00:03:13.750
doing better at global joint unsupervised,
uh,
uh,

52
00:03:13.780 --> 00:03:18.160
learning.
So that's perhaps part of the reason.
Uh,

53
00:03:18.190 --> 00:03:19.420
but that being said,

54
00:03:19.421 --> 00:03:23.170
this is a procedure in the case of the depot tone quarter were on spy speed

55
00:03:23.171 --> 00:03:26.890
training actually helps fighting,
uh,
uh,
under fitting,

56
00:03:27.550 --> 00:03:31.900
uh,
also better optimization algorithms will help.
Uh,

57
00:03:31.901 --> 00:03:36.280
there's this paper deep learning via Estrin free optimization by James Martin's

58
00:03:36.281 --> 00:03:39.110
in 2010 that,
uh,
show that,
uh,

59
00:03:39.310 --> 00:03:41.710
instead of using stochastic gradient descent,

60
00:03:41.711 --> 00:03:46.000
if you use a more sophisticated second order optimization,

61
00:03:46.420 --> 00:03:51.100
uh,
we could actually get really good results even without pre-training with

62
00:03:51.101 --> 00:03:55.450
unsupervised learning.
Um,
so I mentioning this because this is a model,

63
00:03:55.451 --> 00:04:00.010
the deeper tone color,
which is often used for evaluating better,
uh,

64
00:04:00.011 --> 00:04:03.040
optimization algorithms in the context of,
uh,

65
00:04:03.070 --> 00:04:05.320
of neural networks in the context of deep learning.

66
00:04:07.350 --> 00:04:10.410
So one application for it is,
uh,
to uh,

67
00:04:10.560 --> 00:04:14.650
reduce the dimensionality of some data set.
If we wanted to,
uh,
for instance,
uh,

68
00:04:14.670 --> 00:04:18.000
take these big images and actually store them in a much smaller dimensional

69
00:04:18.180 --> 00:04:21.600
vector so that we can actually store this on this more easily.

70
00:04:21.870 --> 00:04:23.760
And then if we keep the decoder around them,

71
00:04:23.761 --> 00:04:27.180
we can easily reconstruct the original input.
Uh,

72
00:04:27.240 --> 00:04:31.170
and if we compare that with the,
uh,
type of,
uh,

73
00:04:31.200 --> 00:04:35.400
representation would get if we did PCA and then from PC we provided a

74
00:04:35.401 --> 00:04:37.950
reconstruction.
We see that the reconstruction,

75
00:04:37.951 --> 00:04:42.420
we get as much better with the deeper tone quarter then with PCA where this is

76
00:04:42.421 --> 00:04:43.920
the original data,
uh,

77
00:04:44.430 --> 00:04:47.370
notice that here because we're talking about a depot in quarter.

78
00:04:47.640 --> 00:04:51.750
So we showed that PCA was in some sense optimal,

79
00:04:52.040 --> 00:04:56.640
uh,
compared to,
uh,
training a single hidden layer,

80
00:04:56.641 --> 00:04:59.700
auto encoder with some nonlinearity.

81
00:04:59.710 --> 00:05:03.990
So essentially we a re a PCA will do better than any auto encoder,

82
00:05:03.991 --> 00:05:08.520
a single hidden layer and the squared output a reconstruction error.

83
00:05:09.200 --> 00:05:11.340
But because here in the deep auto encoder,

84
00:05:11.610 --> 00:05:14.250
we have a nonlinear decoder,

85
00:05:14.610 --> 00:05:19.610
a then essentially the demonstration for PC being optimal does not apply for the

86
00:05:20.431 --> 00:05:22.350
case of a depot twin quarter.

87
00:05:22.351 --> 00:05:26.160
So a deeper tone color could improve over a PCA because it has a more

88
00:05:26.161 --> 00:05:26.994
complicated,

89
00:05:27.330 --> 00:05:32.330
a nonlinear decoder multilayers that sequentially reconstruct or increased the

90
00:05:32.671 --> 00:05:36.990
dimensionality until we reached the,
uh,
original input space.
Okay.

91
00:05:36.991 --> 00:05:40.880
So this is why this result is not really a contradiction with,
uh,

92
00:05:41.130 --> 00:05:45.300
with respect to what we've seen before in the single layer.
A auto encoder case.

93
00:05:47.570 --> 00:05:48.220
<v 1>Okay.</v>

94
00:05:48.220 --> 00:05:50.540
<v 0>And other application is just visualization.</v>

95
00:05:50.541 --> 00:05:53.930
So if we make the middle hidden there,

96
00:05:54.280 --> 00:05:58.790
a two dimensional vector,
then we can plot this by,
uh,

97
00:05:59.270 --> 00:06:00.600
taking all of our data and,

98
00:06:00.640 --> 00:06:05.540
and placing a point in two d at the position given by the two dimensional
vector.

99
00:06:05.960 --> 00:06:10.580
And so this is an example from a,
again,
the same science paper where,

100
00:06:10.610 --> 00:06:11.540
uh,
uh,

101
00:06:11.670 --> 00:06:16.520
[inaudible] was trained on documents where a document is represented by a deer

102
00:06:16.521 --> 00:06:21.521
frequency of each possible word or each possible word that we can recognize in a

103
00:06:21.951 --> 00:06:25.970
document is a different dimension.
So words are pixels.

104
00:06:25.971 --> 00:06:29.150
Essentially if we want to make a curse funds with images,

105
00:06:29.151 --> 00:06:32.500
and then the intensity of Pixel is just how many times we see the,
the,

106
00:06:32.560 --> 00:06:36.800
the given the word,
uh,
in that document.
Um,

107
00:06:37.100 --> 00:06:41.330
and uh,
and we see that it seems to learn,
uh,
to,
uh,
essentially,

108
00:06:41.660 --> 00:06:44.810
uh,
put together,
uh,

109
00:06:44.840 --> 00:06:48.260
documents that talk about similar things.

110
00:06:48.260 --> 00:06:51.940
So say disasters and accidents,
legals,
DCO,

111
00:06:52.190 --> 00:06:54.830
government borrowings,
account earnings,
and so on.

112
00:06:54.831 --> 00:06:58.250
So the color represents these different classes and we see that they are close

113
00:06:58.251 --> 00:07:02.450
together in this two d visualization of the original data.
Okay.

114
00:07:03.170 --> 00:07:04.940
So that's a deeper tone quarter.

115
00:07:04.941 --> 00:07:08.420
It can help us get a very small dimensional representation.
Um,

116
00:07:08.540 --> 00:07:12.680
some data that still maintains quite a bit.
The original information from the,

117
00:07:12.860 --> 00:07:17.390
from the original data can help us with two d dimensions,
uh,
to the,

118
00:07:17.450 --> 00:07:21.110
uh,
visualization of the data.
And,
uh,

119
00:07:21.140 --> 00:07:23.320
there's quite a bit of research,
uh,

120
00:07:23.360 --> 00:07:27.110
research being done over these models because they're a good benchmark in

121
00:07:27.111 --> 00:07:31.700
particular for evaluating a better optimization algorithms for neural networks.

