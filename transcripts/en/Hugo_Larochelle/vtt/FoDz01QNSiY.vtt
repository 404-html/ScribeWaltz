WEBVTT

1
00:00:00.200 --> 00:00:00.810
Yeah.

2
00:00:00.810 --> 00:00:03.840
<v 1>In this video will introduce the neural network language model.</v>

3
00:00:06.650 --> 00:00:10.640
The previous video we discussed,
the end Graham models and particular,

4
00:00:10.641 --> 00:00:15.641
we talked about the fact that a while in an application to get a,

5
00:00:16.090 --> 00:00:19.340
a good model we want and to be large,
if n is large,

6
00:00:19.341 --> 00:00:24.170
then we started suffering from the problem data sparsity which means that it's

7
00:00:24.171 --> 00:00:28.590
quite likely at test time that will be faced with a context corresponding to

8
00:00:28.591 --> 00:00:30.740
words that we've never observed that training time.

9
00:00:31.130 --> 00:00:35.780
And while there are some smoothing approaches for improving in grand models,
uh,

10
00:00:36.020 --> 00:00:40.550
uh,
these can help them model before much better.
But,
uh,

11
00:00:40.580 --> 00:00:42.020
data sparsity is still an issue.

12
00:00:42.470 --> 00:00:47.210
And so what we'll see in this video is a neural network language model approach

13
00:00:47.211 --> 00:00:50.520
to this problem,
which is I actually going to do,
uh,

14
00:00:50.750 --> 00:00:53.570
perform better than a regular and grand model.

15
00:00:55.010 --> 00:00:55.843
<v 0>Yeah.</v>

16
00:00:57.230 --> 00:01:01.790
<v 1>So the idea is actually exactly what I've explained before in the context of</v>

17
00:01:01.880 --> 00:01:03.770
learn a word representations.

18
00:01:03.800 --> 00:01:08.030
We're going to train a neural network that will take a representation of the

19
00:01:08.031 --> 00:01:13.031
context and at its output is going to produce the conditional distribution of

20
00:01:13.270 --> 00:01:13.640
uh,

21
00:01:13.640 --> 00:01:18.640
the next word to the word appearing to the right of the context and add the

22
00:01:19.281 --> 00:01:20.270
input of their neural net.

23
00:01:20.271 --> 00:01:25.271
We'll just put the concatenation of the vectorial word representation of each of

24
00:01:25.491 --> 00:01:26.780
the words in the context.

25
00:01:27.560 --> 00:01:30.860
So in more detail in the neural network,

26
00:01:30.861 --> 00:01:35.861
the first thing that we do is that we take the ideas of each words in the

27
00:01:37.101 --> 00:01:41.440
context and then we look at,
uh,

28
00:01:41.510 --> 00:01:46.510
our lookup table to extract the vectorial were representations of each of these

29
00:01:48.441 --> 00:01:50.270
word ids.
So in other words,

30
00:01:50.660 --> 00:01:54.920
we take the ID here and we look at the corresponding row in the Matrix C and we

31
00:01:54.921 --> 00:01:57.740
put that as the first vector in the context.

32
00:01:58.010 --> 00:02:02.750
And we do this for all words in the context concatenating than each of the word

33
00:02:02.751 --> 00:02:05.420
representation to get our x vector,

34
00:02:05.421 --> 00:02:07.520
which is going to be the input of our neural network.

35
00:02:08.510 --> 00:02:12.260
And then the rest of the computations is the,

36
00:02:12.261 --> 00:02:15.240
are the regular computations we do in a feed forward neural network.

37
00:02:15.241 --> 00:02:19.940
So we take the factor x consisting in all the concatenated word representations,

38
00:02:20.090 --> 00:02:23.630
multiply by the Matrix,
uh,
say w here,

39
00:02:23.780 --> 00:02:28.250
plus the bias than some nonlinearity.
So in this work,
tench was used.

40
00:02:28.850 --> 00:02:31.880
And then,
uh,
to get an output,

41
00:02:31.910 --> 00:02:36.750
we just multiply that hidden layer by the connections between the hidden layer,

42
00:02:36.820 --> 00:02:37.880
the output layer,

43
00:02:38.150 --> 00:02:43.150
and then apply a softmax nonlinearity to get at the outputs,

44
00:02:43.460 --> 00:02:47.090
the distribution for the next word.
So the eff output.

45
00:02:47.330 --> 00:02:52.310
So the earth element and that output layer here is going to be the probability

46
00:02:52.311 --> 00:02:56.830
that the next word,
wt is the word with Id.
I.

47
00:02:58.280 --> 00:03:03.070
So,
um,
this model was proposed by Yoshua Bengio and his colleagues in 2003.

48
00:03:03.071 --> 00:03:07.840
It's also in this work that the idea of learning word representations was first

49
00:03:07.841 --> 00:03:11.380
presented.
And,
uh,
and again,

50
00:03:11.381 --> 00:03:13.000
in this model then when we train,

51
00:03:13.001 --> 00:03:17.020
it will actually train not just the parameters of the neural network,

52
00:03:17.021 --> 00:03:19.420
but also the parameters inside the matrix.

53
00:03:19.430 --> 00:03:23.260
CP word representation of the different words.

54
00:03:26.560 --> 00:03:27.393
<v 0>Okay.</v>

55
00:03:27.430 --> 00:03:32.430
<v 1>Now the reason why this model is better at finding the data sparsity problem for</v>

56
00:03:32.650 --> 00:03:36.670
large n four large contract sizes is that,
uh,

57
00:03:36.700 --> 00:03:40.690
dinero electric is actually potentially able to generalize to context that have

58
00:03:40.691 --> 00:03:45.160
not been seen in the training set.
Uh,
so as an example to illustrate why,

59
00:03:45.280 --> 00:03:48.460
so imagine that we wanted to,
uh,

60
00:03:49.030 --> 00:03:53.110
evaluate the probability of seeing the word eating after seeing the words the

61
00:03:53.140 --> 00:03:58.140
cat is now imagine that the four gram the cat is eating,

62
00:03:59.291 --> 00:04:04.040
which we would need in their regular and Graham model in order to assign a

63
00:04:04.270 --> 00:04:07.070
positive nonzero probability,
uh,

64
00:04:07.360 --> 00:04:09.850
for the property you've seen eating after the cap is.

65
00:04:09.990 --> 00:04:13.450
So imagine that this foreground is actually not in the training corpus,

66
00:04:14.260 --> 00:04:19.260
but imagining that the dog is eating is actually in the corpus.

67
00:04:20.680 --> 00:04:21.513
Now,

68
00:04:21.520 --> 00:04:26.520
if the word representations cat and dog are similar and if the neural network

69
00:04:27.581 --> 00:04:31.050
has learned to predict a good probability of,
uh,

70
00:04:31.180 --> 00:04:36.180
seeing eating after the dog is then if the road representation of Canon dog is

71
00:04:37.091 --> 00:04:41.890
similar,
then it should also give a good probability,
a similar probability,

72
00:04:42.130 --> 00:04:46.120
uh,
in the context where instead of dog,
we have cap.

73
00:04:46.240 --> 00:04:50.110
And that's because since the part of the input that corresponds to cat

74
00:04:50.111 --> 00:04:53.020
dissimilar to the part of the input corresponds to dog.

75
00:04:53.021 --> 00:04:56.380
And since for the other words in the context then is they're exactly the same

76
00:04:56.381 --> 00:04:57.760
words,
then they shouldn't,

77
00:04:57.880 --> 00:05:01.330
the neural network should have a similar output and does generalize way well to

78
00:05:01.331 --> 00:05:04.810
that case.
Now then you might ask,
well,

79
00:05:04.840 --> 00:05:09.490
if the neural network in its training data as in seen the cat is eating,

80
00:05:09.491 --> 00:05:11.380
but as seen the dog is eating,

81
00:05:11.410 --> 00:05:14.650
why you should it actually learned that cat and does should have similar

82
00:05:14.651 --> 00:05:15.670
representations.

83
00:05:16.000 --> 00:05:20.920
Well maybe in the training corpus a while it hasn't seen the cat is eating,

84
00:05:20.980 --> 00:05:25.980
it actually has seen the cat was sleeping and then maybe it also has seen the

85
00:05:26.501 --> 00:05:29.860
dog was sleeping.
So from these two observations,

86
00:05:29.861 --> 00:05:34.540
and it as learned that for both the cat was sleeping and the dog was sleeping,

87
00:05:34.960 --> 00:05:38.110
it should,
uh,
have some,
uh,

88
00:05:38.170 --> 00:05:41.110
meaningful probability of seeing sleeping,
uh,

89
00:05:41.111 --> 00:05:43.630
after the cap was and after the dog was.

90
00:05:43.630 --> 00:05:48.250
So that's a signal that perhaps the representation of cat and dog should be

91
00:05:48.251 --> 00:05:52.150
similar.
So because of this,
because cat and dog,

92
00:05:52.151 --> 00:05:55.890
maybe in the training set in other contexts,
uh,

93
00:05:55.930 --> 00:05:57.830
Kevin and Doug would appear in similar context,

94
00:05:57.831 --> 00:06:01.430
then it should learn similar representations for cat and dog.

95
00:06:01.880 --> 00:06:03.170
And so for that reason,

96
00:06:03.171 --> 00:06:07.250
we can expect that we'd be able to generalize well for this example,

97
00:06:07.280 --> 00:06:11.990
even if the specific context is not,
this exact context has not been observed.

98
00:06:12.560 --> 00:06:16.820
So in that sense is not as much rely.
We're relying on,
uh,

99
00:06:16.850 --> 00:06:21.320
explicitly seeing exactly the given context for which we have to value the

100
00:06:21.321 --> 00:06:23.970
probability for that reason.
Uh,

101
00:06:24.020 --> 00:06:26.810
it should be able to generalize to these new context.

102
00:06:29.870 --> 00:06:33.230
Now a few words about actually performing,
uh,

103
00:06:33.260 --> 00:06:37.310
backpropagation and in this model now,
uh,

104
00:06:37.340 --> 00:06:42.340
we already know how to compute the gradients of in particular the linner pre

105
00:06:42.741 --> 00:06:46.550
activation of the hidden layer,
which is just this.

106
00:06:46.551 --> 00:06:48.650
So l will be the last,
we're optimizing,

107
00:06:49.100 --> 00:06:51.050
we're optimizing the negative log likelihood.

108
00:06:51.051 --> 00:06:54.650
So minus the log of the probability of the next word.

109
00:06:55.550 --> 00:06:59.420
And so with regular backpropagation,
we are both to compute that gradient.

110
00:07:00.410 --> 00:07:05.060
Um,
so,
um,
now what's next is that we have to take that Graydon and back,

111
00:07:05.061 --> 00:07:09.320
propagate it to,
uh,
the,
uh,
word representations.

112
00:07:09.920 --> 00:07:11.270
Now in my notation,

113
00:07:11.271 --> 00:07:16.271
I'm going to know w I capital w I as the sub matrix that connects the a word

114
00:07:18.771 --> 00:07:22.460
with relative position.
I with respect to the word,

115
00:07:22.461 --> 00:07:24.110
we're trying to predict that when your team,

116
00:07:25.040 --> 00:07:30.040
so w capital w I is the matrix connecting wt minus I with the hidden layer.

117
00:07:32.870 --> 00:07:37.700
So we can show that the gradient of the,
of any word representation,
CW.

118
00:07:37.701 --> 00:07:42.470
So for any word w is going to be,
so this expression,

119
00:07:43.030 --> 00:07:43.640
uh,

120
00:07:43.640 --> 00:07:48.640
where we sum over all position relative positions in the context from the Ipos

121
00:07:49.221 --> 00:07:49.561
one.

122
00:07:49.561 --> 00:07:54.230
So that's this word here to I calls and minus one.

123
00:07:54.231 --> 00:07:56.510
So that's this word here.

124
00:07:57.050 --> 00:07:59.270
So we some over all relative positions.

125
00:07:59.510 --> 00:08:04.240
And then if the word at that relative position is the word w.

126
00:08:04.510 --> 00:08:05.780
So this w here,

127
00:08:06.500 --> 00:08:11.500
then we add the gradient at the hidden there multiplied by the transpose of the

128
00:08:13.491 --> 00:08:18.260
part of the Matrix,
the part of the uh,
uh,
connection Matrix,

129
00:08:18.570 --> 00:08:20.870
uh,
that connects this,
uh,

130
00:08:20.900 --> 00:08:25.760
the representation of the word at relative position.
I with the hidden layer.

131
00:08:25.790 --> 00:08:27.900
So that's w hi.

132
00:08:28.880 --> 00:08:32.840
So that expression should not be so surprising.
We know that if we,
uh,

133
00:08:32.870 --> 00:08:36.110
we've seen in the regular neural network that if we compute the matrix

134
00:08:36.111 --> 00:08:39.520
multiplication,
uh,
so for instance,
we take that hit in there,

135
00:08:39.550 --> 00:08:44.030
multiply by a matrix,
then the gradient with respect to,
uh,

136
00:08:44.160 --> 00:08:48.890
the hidden there is going to involve multiplying the gradient at the layer

137
00:08:48.891 --> 00:08:52.940
above,
uh,
by the transpose of the connection.
So for the same reason,

138
00:08:52.941 --> 00:08:56.730
that's why we see these terms appearing.
Now the reason why,

139
00:08:56.790 --> 00:08:57.690
and I remove some mink.

140
00:08:58.080 --> 00:09:01.980
The reason why we have this sum here and we have to some over all position is

141
00:09:01.981 --> 00:09:06.320
that the word w might actually appear in more than one position in the,

142
00:09:06.380 --> 00:09:07.200
in the context.

143
00:09:07.200 --> 00:09:11.880
And also it might actually not appear in some given context for some,
uh,

144
00:09:12.000 --> 00:09:16.650
given training example,
a corresponding to one context and the following word.

145
00:09:17.220 --> 00:09:21.300
So for each time it appears we get this great income contribution and then we

146
00:09:21.301 --> 00:09:24.480
accumulate those to get the gradient of the word representation.

147
00:09:25.520 --> 00:09:28.890
So that's for computing the great end of the word representations and of course

148
00:09:28.891 --> 00:09:32.100
the gradient for all the other parameters in the neural networks.

149
00:09:32.101 --> 00:09:36.990
So the matrix w here than matrix here and so on,
uh,
all of these,

150
00:09:37.110 --> 00:09:41.900
uh,
uh,
gradients that,
uh,
we need to compute,
uh,

151
00:09:41.940 --> 00:09:44.700
they were computed as usual in a neural network.

152
00:09:45.890 --> 00:09:47.750
And I'll just add some notes.
Um,

153
00:09:47.790 --> 00:09:51.550
so these dotted lines here is just to note that,
uh,

154
00:09:51.560 --> 00:09:53.790
and that's something that they considered in that paper.

155
00:09:53.940 --> 00:09:58.940
We might have also direct or direct connections between the input layer,

156
00:09:59.400 --> 00:10:01.680
the output layer.
That's a potential,

157
00:10:02.040 --> 00:10:05.530
a modification to the neural net there that one might consider.

158
00:10:05.531 --> 00:10:09.570
And if you want more details about,
about that,
I suggest you go look at the,

159
00:10:09.571 --> 00:10:11.250
at the corresponding paper.

160
00:10:14.360 --> 00:10:18.110
So let's do an example of,
uh,
the gradients,
uh,

161
00:10:18.450 --> 00:10:23.100
computing the gradients for a given context.
And it's,
uh,
next word.

162
00:10:23.550 --> 00:10:27.600
So imagine we're training the neural network and now we're,
uh,

163
00:10:27.630 --> 00:10:32.630
we're trying to infer the probability of observing cat after seeing the context,

164
00:10:34.140 --> 00:10:38.100
the dog and that.
So a,

165
00:10:38.101 --> 00:10:39.420
for this training examples.

166
00:10:39.421 --> 00:10:43.230
So this whole sequence of techs corresponds to one training example,

167
00:10:43.231 --> 00:10:44.190
far neural network.

168
00:10:44.490 --> 00:10:49.380
We are trying to optimize the loss of a minus the log of the probability of

169
00:10:49.381 --> 00:10:54.150
seeing cat given all the people's words.
And so this is,

170
00:10:54.190 --> 00:10:58.350
we see here that this is a five gram model because we're a concerning the four

171
00:10:58.351 --> 00:11:02.280
previous words,
um,
not imagine that the,

172
00:11:02.460 --> 00:11:06.480
which is w three here as an ID of 21 Doug,

173
00:11:06.481 --> 00:11:11.040
which is w four idea of three and w five idea of 14,

174
00:11:11.280 --> 00:11:15.900
and the which is also present that the pus the absolute position six.

175
00:11:16.350 --> 00:11:20.610
Uh,
so that's w six also again,
has the ID 21.

176
00:11:21.960 --> 00:11:26.960
Now if we're interested in what is the gradients for the word with ID three.

177
00:11:28.081 --> 00:11:30.670
So that's dog.
Uh,

178
00:11:30.750 --> 00:11:35.310
then it's going to be the gradient with respect to the reactivation of the

179
00:11:35.311 --> 00:11:40.311
neural network multiplied by the transpose of part of the input connections that

180
00:11:40.831 --> 00:11:44.550
connects the a word at relative position three.

181
00:11:44.850 --> 00:11:46.020
We take the transpose of that.

182
00:11:46.021 --> 00:11:49.860
Then that's what we multiply with the grading of the pea activation.

183
00:11:50.100 --> 00:11:54.640
So indeed dog is that relative position three.
So that's three to one,

184
00:11:54.850 --> 00:11:55.683
that would be four.

185
00:11:56.060 --> 00:11:59.620
So that's why I'm taking that gradient of the activation of the hidden.

186
00:11:59.621 --> 00:12:04.621
They're multiplying by a w three transpose for 14,

187
00:12:05.110 --> 00:12:07.630
which is end.
It's that relative position too.

188
00:12:07.631 --> 00:12:12.631
So we take the same reactivation gradient and multiply by the transpose of w two

189
00:12:13.990 --> 00:12:18.610
and now four 21 it actually appears that two positions that is at relative

190
00:12:18.611 --> 00:12:19.444
position,

191
00:12:19.880 --> 00:12:24.880
a one so right here and four right here.

192
00:12:26.140 --> 00:12:30.060
So we have to add these two uh,
back propagated gradients.

193
00:12:30.100 --> 00:12:34.990
The reactivation graded multiply by w transpose and then preoccupation Graydon

194
00:12:35.320 --> 00:12:37.930
again,
multiply by W for Trans folks.

195
00:12:38.650 --> 00:12:43.570
And then for all the other words,
w all the other words in our vocabulary,

196
00:12:43.571 --> 00:12:46.960
then they're gradient for this particular training example would be zero.

197
00:12:48.190 --> 00:12:53.190
And so it means that for the vast majority of word representations,

198
00:12:53.410 --> 00:12:57.280
um,
there gradient for that training example is going to be zero,

199
00:12:57.281 --> 00:13:01.510
so we won't actually have to update them.
So in our implementation,

200
00:13:01.511 --> 00:13:06.511
we should only update the representations c three c 14 and see 21.

201
00:13:07.480 --> 00:13:12.220
So a,
instead of updating the whole Matrix C,
we should just update these role,
uh,

202
00:13:12.250 --> 00:13:15.160
rows in the Matrix c,
uh,
so the third row,

203
00:13:15.460 --> 00:13:20.460
14th row and 21st row based on these gradients that we have here.

204
00:13:21.520 --> 00:13:25.060
So that's important to do this to get an efficient implementation.

205
00:13:28.510 --> 00:13:32.080
So when we're evaluating a language models,

206
00:13:32.140 --> 00:13:36.430
a common evaluation metric that's uses the perplexity,
so it's,

207
00:13:36.520 --> 00:13:40.540
uh,
an information theoretic measure.
I won't describe it.

208
00:13:40.570 --> 00:13:45.520
I'll just say that it's essentially the exponential of the average negative log

209
00:13:45.521 --> 00:13:50.050
likelihood of some given dataset.
So we,
you know,

210
00:13:50.051 --> 00:13:52.480
what's the average negative log likelihood?
So it just take the hands,

211
00:13:52.481 --> 00:13:57.200
financial turns out it's deep perplexity and um,
um,

212
00:13:57.250 --> 00:14:00.790
so again,
the lower the,
the number of the,
uh,
the better,

213
00:14:00.791 --> 00:14:02.950
much like the average negative log likelihood.

214
00:14:03.700 --> 00:14:08.600
And so in these experiments reported in that paper on the Brown Corpus,
uh,

215
00:14:08.650 --> 00:14:12.670
which is a well known data set for uh,
for language modeling.

216
00:14:13.470 --> 00:14:17.680
If we take a engram model,
which is smooth using a state of the art,

217
00:14:17.681 --> 00:14:20.410
smoothing a method known as niece,
her name,

218
00:14:20.800 --> 00:14:23.470
we got the perplexity of 321.

219
00:14:24.190 --> 00:14:26.170
And with the neural network language model,

220
00:14:26.171 --> 00:14:30.250
we get a much lower perplexity of 276.

221
00:14:30.251 --> 00:14:32.230
So a gap of 50,

222
00:14:32.231 --> 00:14:37.231
like this almost 50 is actually quite big in the realm of language modeling,

223
00:14:37.330 --> 00:14:40.180
uh,
performances.
And then,
um,

224
00:14:40.210 --> 00:14:43.270
in language modeling also usually combining models,
for instance,

225
00:14:43.271 --> 00:14:47.050
creating an assemble of the neural network and the engram model by just taking

226
00:14:47.051 --> 00:14:52.051
the average of the probability of each model also gives an even better

227
00:14:52.641 --> 00:14:56.790
performance.
So we see that by just with the neural network we already do,

228
00:14:56.791 --> 00:15:00.710
we do much better.
And then if we already had an inquiry model,

229
00:15:00.770 --> 00:15:03.680
combining it with the neural network actually adds even more

230
00:15:07.350 --> 00:15:12.350
now a more interesting but a bit more difficult to do in practice and less

231
00:15:12.701 --> 00:15:17.070
straight forward way of evaluating language model is really do validated in the

232
00:15:17.071 --> 00:15:21.030
particular application that uses a language model such as a machine translation

233
00:15:21.031 --> 00:15:25.130
system or speech recognition system and,
uh,

234
00:15:25.230 --> 00:15:30.030
later work.
So the initial work on neural net language modeling was in 2003,

235
00:15:30.031 --> 00:15:34.320
but,
uh,
uh,
actually much later,
it's been shown that,

236
00:15:35.070 --> 00:15:39.780
uh,
you can get a really,
really good performance of,

237
00:15:39.930 --> 00:15:44.070
uh,
with a neural network language models that incorporate into some standard,

238
00:15:44.380 --> 00:15:47.320
a model that does either,
uh,

239
00:15:47.340 --> 00:15:52.340
speech recognition as in this paper or statistical machine translation as in

240
00:15:53.581 --> 00:15:54.414
this paper.

241
00:15:54.600 --> 00:15:59.600
So I encourage you to look at these references if you want to see examples of

242
00:16:00.240 --> 00:16:04.080
the gains you can get from using a known neural network language model and in

243
00:16:04.081 --> 00:16:06.390
some given NLP system.

244
00:16:07.650 --> 00:16:07.710
<v 0>Okay.</v>

