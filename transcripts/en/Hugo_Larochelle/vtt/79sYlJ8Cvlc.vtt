WEBVTT

1
00:00:00.190 --> 00:00:00.780
Okay.

2
00:00:00.780 --> 00:00:02.250
<v 1>In this video will introduce it.</v>

3
00:00:02.251 --> 00:00:06.110
Another alternative to the regular auto encoder and the dinos in auto encoder,

4
00:00:06.150 --> 00:00:08.400
no one as the contractive or twin quarter.

5
00:00:11.230 --> 00:00:12.070
So again,

6
00:00:12.071 --> 00:00:16.840
here we're trying to fix this problem with a regular auto encoders with over

7
00:00:16.841 --> 00:00:19.090
complete hidden layers,
uh,
uh,

8
00:00:19.091 --> 00:00:22.360
which can actually do a perfect job by just copying,

9
00:00:22.720 --> 00:00:25.600
copying individual units in the input factor.

10
00:00:28.800 --> 00:00:32.100
So when did the nausea on top quarter of the way we fix that is by adding noise

11
00:00:32.101 --> 00:00:36.810
before feeding an input,
uh,
to deal to a quarter with instead,
uh,

12
00:00:37.080 --> 00:00:40.110
uh,
feed it a noisy version of the original input,

13
00:00:40.140 --> 00:00:44.940
but we'd asked you to recover the original input and sort of remove the noise,

14
00:00:44.941 --> 00:00:49.860
the noise from the original input.
Now in the contractive autoencoder.
Instead,

15
00:00:49.861 --> 00:00:54.861
what we'll do is that we're actually going to try to explicitly avoid this

16
00:00:56.790 --> 00:01:01.790
uninteresting a solution by essentially explicitly,

17
00:01:02.070 --> 00:01:06.060
uh,
by adding an explicit term that penalizes that solution.

18
00:01:06.960 --> 00:01:11.640
Um,
the way we're going to do that is based on this motivation.

19
00:01:12.270 --> 00:01:16.350
So the types of features would like to extract are really features that are,

20
00:01:16.380 --> 00:01:21.380
that reflect only the types of variations that we observe in our training set

21
00:01:22.560 --> 00:01:26.130
and from the training distribution in general.
Uh,

22
00:01:26.160 --> 00:01:30.150
and otherwise we actually like hidden you install are invariant to other types

23
00:01:30.151 --> 00:01:34.620
of variations,
variations that are not meaningful for that type of data.

24
00:01:35.030 --> 00:01:39.960
Uh,
so if we have a solution which corresponds to copying each of the inputs

25
00:01:39.961 --> 00:01:40.800
individually,

26
00:01:40.950 --> 00:01:45.090
then this is really reflecting any possible variation.

27
00:01:45.150 --> 00:01:49.590
So any type of very variation here will be perfectly reflected in the hidden

28
00:01:49.591 --> 00:01:52.410
layer.
Now here what,
that's something we'll try to avoid.

29
00:01:52.411 --> 00:01:57.411
We try to encourage the neural network to only have hidden units that reflect

30
00:01:57.901 --> 00:02:02.490
variations that are specific to our training set and others that are otherwise,

31
00:02:02.550 --> 00:02:06.330
uh,
as much as possible in barrier to any other types of variation.

32
00:02:09.110 --> 00:02:09.890
<v 0>Okay.</v>

33
00:02:09.890 --> 00:02:14.890
<v 1>So the way we'll do this is design this new loss function where in addition to</v>

34
00:02:15.381 --> 00:02:17.900
the reconstruction era from the auto quarters,

35
00:02:17.901 --> 00:02:22.220
such as this one for Barnaby observations,
the uh,
cross entropy one,

36
00:02:22.700 --> 00:02:23.990
we'll add this term,

37
00:02:24.320 --> 00:02:29.320
which is going to be weighted by some hyper parameter lambda and uh,

38
00:02:30.740 --> 00:02:35.120
which multiplies the Cobian of the in quarter four,

39
00:02:35.121 --> 00:02:38.200
which we take the square of.
It's a Furby this north.

40
00:02:38.840 --> 00:02:43.840
So the new term we add is a way to a squared for Venus norm of the Jacoby Ian of

41
00:02:45.770 --> 00:02:49.150
the quarter,
which is this thing here.
So,
uh,

42
00:02:49.190 --> 00:02:54.110
we remember that teacher Cobian,
uh,
of uh,
functions.
In this case,

43
00:02:54.111 --> 00:02:58.580
the function is the encoder function is going to be the matrix of all partial

44
00:02:58.581 --> 00:02:59.200
Dale lives,

45
00:02:59.200 --> 00:03:03.950
abandonment of that factor with respect to all of the minutes of um,

46
00:03:04.480 --> 00:03:08.780
the uh,
variable with respect to which we're competing to Jacoby in.

47
00:03:08.781 --> 00:03:11.020
And in this case it's the input vector x.

48
00:03:11.620 --> 00:03:15.160
And so if we want the square of the venous norm,

49
00:03:15.370 --> 00:03:18.910
then it means that,
uh,
we would be,
uh,

50
00:03:19.000 --> 00:03:24.000
this here would correspond to the sum over all elements of the,

51
00:03:24.670 --> 00:03:27.400
at the output of that function and the hidden there,

52
00:03:28.930 --> 00:03:33.790
a nested with the,
some of overall inputs.

53
00:03:33.910 --> 00:03:38.910
So that's the variable that's which I'm computing the joke Cobian of the squared

54
00:03:39.161 --> 00:03:44.080
of the partial derivative of that element in the in quarter.
Uh,

55
00:03:44.081 --> 00:03:47.950
with respect to that element k in the input vector.

56
00:03:49.210 --> 00:03:50.890
So what do we have here?

57
00:03:51.670 --> 00:03:56.670
We have first a term which says I want the encoder to keep all the good

58
00:03:57.251 --> 00:04:00.940
information that's necessary for me to have a good reconstruction of the

59
00:04:00.941 --> 00:04:01.774
original input.

60
00:04:03.430 --> 00:04:07.400
And then I have another term which says,
well,
try to actually do,

61
00:04:07.810 --> 00:04:11.230
I want an encoder that throws away all information.
So why is that?

62
00:04:11.231 --> 00:04:15.520
That's because a way to minimizing the squared of the partial derivative live is

63
00:04:15.521 --> 00:04:20.020
to have a partial derivative of zero.
If the partial derivative is zero,

64
00:04:20.021 --> 00:04:24.070
it means that if I change the value of x,

65
00:04:24.130 --> 00:04:28.660
it actually won't change the value of the hidden unit.
So,
in other words,

66
00:04:28.661 --> 00:04:33.370
it doesn't contain information about this particular,
uh,
uh,

67
00:04:33.400 --> 00:04:36.580
value of,
uh,
this particular element from the input.

68
00:04:37.480 --> 00:04:42.460
And so if I wanted this to be zero for all hidden units,
it means I would have a,

69
00:04:42.530 --> 00:04:47.260
uh,
a hidden vector,
which does not vary.
Uh,

70
00:04:47.530 --> 00:04:52.450
if I change the input x,
now there's a caveat to this.

71
00:04:52.540 --> 00:04:54.730
It's,
it doesn't vary.
Uh,

72
00:04:54.731 --> 00:04:58.480
so the partial data is really what happens around,
uh,

73
00:04:58.481 --> 00:05:02.920
points in this case at a particular value of x of x t.
So for,

74
00:05:02.921 --> 00:05:04.090
at that training example,

75
00:05:04.091 --> 00:05:07.030
what we're saying is that we don't want the hidden units to change.

76
00:05:07.031 --> 00:05:10.570
If I add a little bit of Santa's little bit of random noise to it,

77
00:05:10.571 --> 00:05:15.550
I want the hidden layer to stay the same.
Now I can't have a hidden there,

78
00:05:15.551 --> 00:05:20.380
which is in variants to what the input value is.
And that also reconstruct,

79
00:05:20.381 --> 00:05:21.160
well,

80
00:05:21.160 --> 00:05:25.270
I can't have an encoder that keeps information and throws information away at

81
00:05:25.271 --> 00:05:29.170
the same time.
But if I combine both,
then we'll,

82
00:05:29.171 --> 00:05:31.050
I'll get is that an encoder,

83
00:05:31.060 --> 00:05:36.060
which we'll try to have hidden units that only keep the quote unquote good

84
00:05:36.401 --> 00:05:37.750
information about the input,

85
00:05:37.780 --> 00:05:42.460
the sufficient information for reconstructing the input while being as much as

86
00:05:42.461 --> 00:05:46.630
possible in variant,
disaffected,
the input for any other directions,

87
00:05:46.631 --> 00:05:50.830
any other types of information in the input layer that that's not useful at all.

88
00:05:51.970 --> 00:05:56.200
So to gain a little bit of a intuition for how this can work,

89
00:05:56.350 --> 00:05:59.690
we'll do a little bit a cartoon illustration of,
uh,
of this.

90
00:06:02.060 --> 00:06:05.360
So again,
imagine that,
um,
uh,
actually in two D,

91
00:06:05.390 --> 00:06:10.160
well actually I'll be in the very high dimensions,
but I project back the,
uh,

92
00:06:10.220 --> 00:06:12.310
assume I'm projecting back the,
uh,

93
00:06:12.380 --> 00:06:17.380
inputs into a two dimensional plane here and the imagine I have data which lies

94
00:06:19.131 --> 00:06:21.770
on this one dimensional manifold like this.

95
00:06:22.700 --> 00:06:27.530
So it actually imagined that my data is this training example here,

96
00:06:27.531 --> 00:06:32.300
this too,
and any rotation of that too.
So this would be a rotation,

97
00:06:32.640 --> 00:06:35.480
uh,
counter clockwise and or rotations clockwise.

98
00:06:35.510 --> 00:06:38.390
And so any of these xs is just a rotation of that.

99
00:06:40.340 --> 00:06:45.140
Now,
um,
our objective,
what kind of hidden units would it prefer?

100
00:06:46.190 --> 00:06:47.023
Well,

101
00:06:47.570 --> 00:06:52.460
if we had a hidden unit which was such that it's value changed in that
direction.

102
00:06:52.490 --> 00:06:56.150
So if we went from that example to that example,
that example,

103
00:06:57.060 --> 00:06:59.870
imagine a hidden unit which would take larger and larger value.

104
00:07:00.330 --> 00:07:03.950
Well that hidden unit is useful for reconstruction for instance.

105
00:07:03.951 --> 00:07:08.570
It would allow us to distinguish between that input,
that input in Davenport.

106
00:07:08.571 --> 00:07:09.620
So I feel it this,

107
00:07:09.770 --> 00:07:14.060
then there presumably there'd be a way of from the value of that hidden unit to

108
00:07:14.180 --> 00:07:17.360
say,
I should reconstruct something like this and not something like that or

109
00:07:17.361 --> 00:07:21.960
something like that.
However,
this hidden unit will be,
um,

110
00:07:22.500 --> 00:07:26.270
uh,
we'll have ego colbian which is a,
or a partial derivatives,

111
00:07:26.300 --> 00:07:28.490
which is not zero because it does vary.

112
00:07:28.491 --> 00:07:30.470
It doesn't have a partial dividends of zero.

113
00:07:30.471 --> 00:07:35.180
If it varies its value by changing the value of the input.
However,

114
00:07:35.270 --> 00:07:36.470
if,
uh,

115
00:07:36.500 --> 00:07:41.150
the lambda that waits to Jacoby and is not too big,
then,

116
00:07:41.420 --> 00:07:44.780
um,
if this hidden unit is really useful for reconstruction,

117
00:07:44.781 --> 00:07:47.840
then the loss in terms of,
uh,

118
00:07:47.900 --> 00:07:50.720
so the fact that we're doing worse in terms of the Jacoby,

119
00:07:50.721 --> 00:07:54.980
Ian should be compensated by the fact that we were doing much better in terms of

120
00:07:54.981 --> 00:07:58.580
reconstruction.
So that would be a hidden unit,
uh,
which would be useful.

121
00:08:00.080 --> 00:08:03.770
However,
that hidden unit would not be useful because for one thing,

122
00:08:04.190 --> 00:08:08.960
if it's value changes like this and uh,
well,
you know,

123
00:08:08.961 --> 00:08:12.140
a variation in this direction,
this local variation here,

124
00:08:12.141 --> 00:08:15.320
it means that this hidden unit and Decision Unit and decision unit would

125
00:08:15.321 --> 00:08:19.700
essentially have the same value.
Like,
you know,
a value that if,
uh,

126
00:08:19.930 --> 00:08:23.110
if I project all values of that hidden unit on this slide there,

127
00:08:23.111 --> 00:08:25.010
we'll all essentially have this value here.

128
00:08:25.490 --> 00:08:30.440
So this is one unit would not help me to,
uh,
distinguish between that input,

129
00:08:30.470 --> 00:08:33.190
that input and that input.
However,

130
00:08:33.210 --> 00:08:35.990
also this hidden unit is,
uh,

131
00:08:36.170 --> 00:08:40.220
has partial dividends that are not zero because it does value its output.

132
00:08:40.250 --> 00:08:43.130
It's value.
If I changed the value of the input.

133
00:08:43.820 --> 00:08:46.220
And so we've,
uh,

134
00:08:46.250 --> 00:08:49.940
made the Jacoby and terminal the loss worse,
it's increased,

135
00:08:50.180 --> 00:08:55.180
but also this union that does not help us actually getting better there.

136
00:08:56.100 --> 00:09:00.360
So I hit in unit that behaves like this is a,

137
00:09:00.361 --> 00:09:04.400
would actually be discouraged when we optimizing it and the optimization would

138
00:09:04.401 --> 00:09:07.530
tend to not find hidden units like this and instead tried to discover these

139
00:09:07.531 --> 00:09:10.840
hidden units.
And so again,
we have a,

140
00:09:10.860 --> 00:09:15.020
we should have hidden units that learn something about the,
uh,

141
00:09:15.300 --> 00:09:18.480
lower dimensional manifold,
uh,
behind our data.

142
00:09:18.510 --> 00:09:20.430
So learns about the structure,

143
00:09:20.431 --> 00:09:24.750
the meaningful structure that characterizes our training distribution.

144
00:09:25.320 --> 00:09:28.200
Uh,
so I'm not being very specific in my details.

145
00:09:28.201 --> 00:09:31.500
I'm sort of trying to give intuition here and I encourage you to look at the

146
00:09:31.501 --> 00:09:35.340
paper for getting perhaps a more,
uh,
mathematically,

147
00:09:35.370 --> 00:09:37.800
mathematically grounded intuition for how this is,

148
00:09:37.801 --> 00:09:41.070
but that's essentially what contractive two in quarters,
uh,
do.

149
00:09:43.960 --> 00:09:44.793
<v 0>Okay.</v>

150
00:09:44.890 --> 00:09:48.310
<v 1>Now,
uh,
in practice we find that the nosing auto encoders are contractive.</v>

151
00:09:48.311 --> 00:09:52.120
Autumn quarter,
both performed well.
They outperform regular until quarter.

152
00:09:52.300 --> 00:09:57.210
In terms of the quality features they extract,
uh,
they have different advantages.

153
00:09:57.240 --> 00:10:01.670
Disadvantages.
The advantage of a denosumab don't quarter,
uh,

154
00:10:01.720 --> 00:10:04.540
is that,
well one could argue it's simpler to implement.

155
00:10:04.570 --> 00:10:05.810
Really if you have auto encoder,

156
00:10:05.860 --> 00:10:10.660
call code merely corresponds to adding one or two lines to your code.
The lines,

157
00:10:10.661 --> 00:10:14.770
I would add the noise before feeding it to the,
uh,
before computing.

158
00:10:14.771 --> 00:10:19.530
The headquarter function
does not require a computing,
uh,

159
00:10:19.720 --> 00:10:23.520
the Jill Cooper,
the Chekhovian of the hidden layer.
Uh,
so the,

160
00:10:23.700 --> 00:10:27.460
and that this is this number of hidden units by number of inputs matrix.

161
00:10:27.461 --> 00:10:32.461
So this has some implications in terms of how slow the,

162
00:10:32.770 --> 00:10:35.650
uh,
uh,
how still the method could be.

163
00:10:35.810 --> 00:10:40.730
And that's the last function.
So now we have to take the phone a degree in,

164
00:10:40.731 --> 00:10:43.340
in the center.
That objective,
we have to do,
uh,

165
00:10:43.360 --> 00:10:46.810
compute the gradients of the parameters with respect to the Jacobian Matrix,

166
00:10:46.840 --> 00:10:49.330
which is itself a matrix of derivatives.

167
00:10:49.570 --> 00:10:53.470
So that's the math can easily quickly become hairy.

168
00:10:53.710 --> 00:10:57.220
Though there are some libraries that facilitate this with automatic

169
00:10:57.221 --> 00:11:01.030
differentiation.
The advantage of contractive,
uh,

170
00:11:01.031 --> 00:11:04.540
auto encoder is that we actually have a gradient which is deterministic.

171
00:11:04.541 --> 00:11:06.130
There's no sampling involved.

172
00:11:06.370 --> 00:11:09.640
When we're training a contractive or towing quarter,

173
00:11:10.210 --> 00:11:11.590
so we can more easily,

174
00:11:11.591 --> 00:11:15.060
indirectly use second order optimizers like country get graded and there will be

175
00:11:15.070 --> 00:11:16.950
fts and so on.
Um,

176
00:11:17.410 --> 00:11:21.970
there could be ways of sort of playing with the nosing or towing quarter and

177
00:11:21.971 --> 00:11:24.820
still trying to,
um,
uh,

178
00:11:24.821 --> 00:11:29.560
use country good gradient and that'll bfgs uh,
but just generally speaking,

179
00:11:29.950 --> 00:11:33.750
it might be more stable if we do this in the context of contractive autoencoder

180
00:11:33.850 --> 00:11:36.910
because the grading is deterministic.
So,
uh,

181
00:11:37.270 --> 00:11:40.330
then it means that we don't need any sampling for getting the grading.

182
00:11:40.600 --> 00:11:45.070
And so we're getting an exact rating for what we're optimizing so we can expect

183
00:11:45.071 --> 00:11:48.070
perhaps a more stable performance.
Um,

184
00:11:48.220 --> 00:11:51.850
so really it's just a matter of choice which one you prefer new one implement.

185
00:11:52.270 --> 00:11:54.790
Um,
and again,
so I refer,

186
00:11:54.791 --> 00:11:59.020
anyone wants to learn more about contractive auto encoders to look at this paper

187
00:11:59.021 --> 00:12:03.280
here by,
uh,
these,
uh,
researchers at the University of Montreal.

188
00:12:04.270 --> 00:12:06.850
And that's about it for a contractive or throwing quarters.

