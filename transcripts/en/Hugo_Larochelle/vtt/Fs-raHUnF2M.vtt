WEBVTT

1
00:00:00.600 --> 00:00:01.261
In this video,

2
00:00:01.261 --> 00:00:05.190
we'll discuss the problem of performing models selection with neural networks.

3
00:00:07.020 --> 00:00:08.190
Okay.
So,
uh,

4
00:00:08.191 --> 00:00:12.180
we've seen how to train a neural network with a specific number of hidden units,

5
00:00:12.181 --> 00:00:14.310
specific number of hidden layers.

6
00:00:14.460 --> 00:00:17.580
We saw that stochastic gradient descent could be used.
Uh,

7
00:00:17.581 --> 00:00:21.840
in stochastic gradient descent we have to specify a number of iterations or

8
00:00:21.841 --> 00:00:26.490
epochs or the number of times we go over all the training examples,
uh,

9
00:00:26.491 --> 00:00:30.020
and update the weights number of times.
We'll repeat that procedure.
Uh,

10
00:00:30.180 --> 00:00:33.960
we have to specify the learning rates.
So the Alpha,

11
00:00:35.230 --> 00:00:38.880
the Alpha hyper parameter that we had to specify,

12
00:00:39.720 --> 00:00:42.540
I'm sorry,
a lot of these,
uh,

13
00:00:42.590 --> 00:00:46.500
met up parameters or we call them hyper parameters that are required.

14
00:00:46.530 --> 00:00:49.710
If you want to train a specific neural network,
we need alpha,

15
00:00:49.890 --> 00:00:53.520
we need the number of iterations.

16
00:00:53.790 --> 00:00:58.080
We need a number of hidden units,
uh,

17
00:00:58.081 --> 00:01:02.340
and the number of hidden layers as well.
Uh,
and uh,
uh,

18
00:01:02.400 --> 00:01:07.260
we need also the lambda regular riser for the virtualization.
Um,

19
00:01:07.320 --> 00:01:10.090
so other,
we set these values,
uh,

20
00:01:10.091 --> 00:01:13.740
what's the most common procedure for,
for performing this.

21
00:01:14.610 --> 00:01:19.500
So what we do is typically a and a,
so this is a perhaps the most,

22
00:01:19.630 --> 00:01:22.800
the simplest procedure.
Uh,
and I'll only described this one.

23
00:01:23.160 --> 00:01:27.810
You can also see other procedures for performing model selection in the machine

24
00:01:27.811 --> 00:01:28.644
learning nurture.

25
00:01:28.960 --> 00:01:33.960
But the most common procedure is to take your whole data set d and split it into

26
00:01:34.561 --> 00:01:39.270
three subsets.
It'd be a first subset,
which will come call the training set.

27
00:01:39.750 --> 00:01:41.100
Uh,
and uh,

28
00:01:41.220 --> 00:01:45.810
so that could be about 70% of your data and that's going to be used to train

29
00:01:45.811 --> 00:01:46.740
your model.
In our case,

30
00:01:46.741 --> 00:01:51.600
to performance the CAHSEE grade in descent on a foreign neural network.

31
00:01:52.350 --> 00:01:56.970
Then we're going to have perhaps 15% of the data of all of our data,

32
00:01:56.971 --> 00:01:59.190
which we'll put in something,
we call it a validation set.

33
00:01:59.670 --> 00:02:03.990
And the validation set is going to be used to determine which hyper parameters,

34
00:02:03.991 --> 00:02:07.710
which values for number of iterations,
Alpha and number of hidden units,

35
00:02:07.770 --> 00:02:10.500
the regularization wait,
which values four doors,

36
00:02:10.501 --> 00:02:12.930
hyper parameters are good values,

37
00:02:12.960 --> 00:02:16.560
correspond to a good performance on that set of data.

38
00:02:17.520 --> 00:02:18.121
And then finally,

39
00:02:18.121 --> 00:02:22.080
once we've determined the hyper parameters and we've trained a neural network

40
00:02:22.081 --> 00:02:25.560
with these hyper parameters that we chose as being the best hyper parameters we

41
00:02:25.561 --> 00:02:29.820
can find,
we'll have a test set,
which in this case we'd be the rest of our data,

42
00:02:29.821 --> 00:02:31.650
the 15% remaining,

43
00:02:32.100 --> 00:02:36.680
which is only gonna need to be used to measure the performance of our models.

44
00:02:36.681 --> 00:02:39.540
So in classification,
which is the main,

45
00:02:39.570 --> 00:02:41.910
the main problem we're looking at here,

46
00:02:42.150 --> 00:02:45.900
we'd measure our classification error on the test set of the neural net.

47
00:02:45.901 --> 00:02:48.930
We've trained with the best hyper parameters based on the validation set.

48
00:02:50.190 --> 00:02:54.030
So one thing that's extremely important in machine learning that's really,

49
00:02:54.031 --> 00:02:58.140
really key is that what we care about is not whether our neural net is good on

50
00:02:58.141 --> 00:02:59.740
the training set.

51
00:02:59.970 --> 00:03:03.490
Where we care about is how well it generalizes to new examples.

52
00:03:03.491 --> 00:03:06.670
So how well does it perform an unseen examples?

53
00:03:07.090 --> 00:03:09.070
And that's the role of the test set.

54
00:03:09.310 --> 00:03:13.840
It's there to act as a collection of unseen examples that I've not been used

55
00:03:13.870 --> 00:03:18.700
either to fit the weights of our neural net or to choose the configuration,
uh,

56
00:03:18.910 --> 00:03:22.270
of,
uh,
during that work,
how many hidden units,
how many layers,

57
00:03:22.271 --> 00:03:26.140
and to what extent we would actually train in on the training set.
Okay.

58
00:03:26.560 --> 00:03:29.090
So this is what we care about,
uh,

59
00:03:29.230 --> 00:03:31.330
in machine learning where are mainly interested.

60
00:03:31.510 --> 00:03:35.890
So the success of a machine learning algorithm is how well it performs on a test

61
00:03:35.891 --> 00:03:39.610
set,
on new examples.
So we'll look at,

62
00:03:39.670 --> 00:03:43.750
so we've seen that stochastic green in this sense is an algorithm for training

63
00:03:43.840 --> 00:03:46.360
and their own network with some given hyper parameters.

64
00:03:46.660 --> 00:03:51.430
Now let's see how we can actually choose,
uh,
the hyper parameters.

65
00:03:51.431 --> 00:03:55.960
So what's the method we should use for searching for good values of the hyper

66
00:03:55.961 --> 00:03:56.794
parameters?

67
00:03:58.710 --> 00:03:59.543
<v 1>Okay.</v>

68
00:04:00.720 --> 00:04:03.030
<v 0>Um,
so one of the most,
uh,</v>

69
00:04:03.031 --> 00:04:06.630
so this is the problem is selecting these hyper parameters known as model

70
00:04:06.631 --> 00:04:09.650
selection.
Um,
and,
uh,

71
00:04:09.660 --> 00:04:13.740
here's a few different ways of performing.
The most popular one is,

72
00:04:13.760 --> 00:04:15.750
is grid search,
which I had described here.

73
00:04:16.590 --> 00:04:21.090
So I imagine though to simplify that we have to hyper parameters.
Uh,

74
00:04:21.091 --> 00:04:26.091
we'll have the number of the number of hidden units in a single hidden layer

75
00:04:27.811 --> 00:04:31.740
neural network.
So it wasn't just a one hidden layer and we'll have the Alpha,

76
00:04:31.741 --> 00:04:34.770
which is the learning rate.
So in grid search,

77
00:04:34.771 --> 00:04:37.770
what we do is that for each hyper parameter we are interested in,

78
00:04:37.950 --> 00:04:40.020
are we going to list a set of values?

79
00:04:40.021 --> 00:04:45.021
Would like to try out for a number of vignettes could be say 10 a hundred and

80
00:04:45.151 --> 00:04:48.070
maybe a thousand and for the learning rate,

81
00:04:48.090 --> 00:04:53.090
maybe 0.05 and 0.005 and that will will do is that we'll try all possible

82
00:04:57.990 --> 00:05:02.990
configurations are combinations of these values will try 10 with 0.05 a hundred

83
00:05:03.640 --> 00:05:08.640
zero 0.0 5,000 0.05 and 10 with 0.005 and so what?

84
00:05:09.680 --> 00:05:14.640
So because we have a three here,
three values and we have,

85
00:05:14.970 --> 00:05:19.470
sorry,
three values.
And we have two values here.

86
00:05:19.610 --> 00:05:22.530
That means that all configuration I'll combinations correct,

87
00:05:22.740 --> 00:05:27.500
will give us six pairs of number of hidden units.
And,
uh,
learning,

88
00:05:28.820 --> 00:05:33.150
learning,
learning,
learning rates.
And so,
so that's grid search.

89
00:05:33.151 --> 00:05:37.080
We try all of these six values and then we'll look at the resulting neural

90
00:05:37.081 --> 00:05:41.310
network that was trained for either 10 hidden units and 0.5,
uh,

91
00:05:41.370 --> 00:05:44.910
learning right or 100,000 units and 0.05,
sorry,

92
00:05:44.911 --> 00:05:49.020
0.05 learning rate.
And so on,
which we look at all of these six neural nets,

93
00:05:49.021 --> 00:05:53.670
which one performs better on the validation set and the one that performs
better?

94
00:05:53.671 --> 00:05:57.080
We remember what were the hyper parameters we used.

95
00:05:57.110 --> 00:06:01.520
And these will be our selected a hyper parameters,
uh,

96
00:06:01.580 --> 00:06:06.140
that we'll consider that as the best options.
Um,
and other alternative,

97
00:06:06.141 --> 00:06:10.970
which works surprisingly well is to perform something like a random search.

98
00:06:11.270 --> 00:06:14.290
So the idea is that instead of specifying your list,

99
00:06:14.620 --> 00:06:18.230
like here are a list of values for each hyper parameter will specify a

100
00:06:18.231 --> 00:06:21.290
distribution of values would like to try out.

101
00:06:21.320 --> 00:06:24.530
So for the number of hidden units,

102
00:06:24.830 --> 00:06:26.930
we might be saying that,
well,

103
00:06:26.931 --> 00:06:31.931
we'd like a uniform distribution on the set of integers from one to a thousand.

104
00:06:35.240 --> 00:06:38.350
And for the learning rate,
uh,

105
00:06:38.390 --> 00:06:42.930
we could have a uniform distribution,
but,
uh,

106
00:06:42.980 --> 00:06:45.410
between,
uh,
between one and zero,

107
00:06:45.411 --> 00:06:48.590
and maybe we'll have a lug uniform distribution.

108
00:06:48.591 --> 00:06:52.830
So a uniform distribution on the luck scale of,
of Alpha [inaudible].

109
00:06:53.660 --> 00:06:54.021
And then,

110
00:06:54.021 --> 00:06:58.700
so what we do here is that instead of trying all combination,

111
00:06:58.701 --> 00:07:02.480
so here we have distribution.
So instead what we do is that we specify,
okay,

112
00:07:02.481 --> 00:07:04.760
I want to try maybe,
uh,

113
00:07:04.790 --> 00:07:09.350
10 different configurations of the hyper parameters and to get each

114
00:07:09.351 --> 00:07:14.030
configuration,
I'll sample independently from these distribution.
So maybe,

115
00:07:14.300 --> 00:07:18.380
uh,
you know,
the first trial that I want to do,

116
00:07:18.381 --> 00:07:22.550
we'll generate a sample for a number of vignettes of like 59,

117
00:07:22.820 --> 00:07:26.240
and then maybe the learning rate would be 0.01,

118
00:07:26.241 --> 00:07:29.180
two five a.
So that will be one trial.

119
00:07:29.960 --> 00:07:34.400
And then maybe another trial might be try to a hundred,

120
00:07:34.401 --> 00:07:39.401
then a one hidden units and the learning rate of 0.13 a.

121
00:07:40.200 --> 00:07:41.033
So I'll get,
you know,

122
00:07:41.100 --> 00:07:46.100
10 configurations like this because I specify that I had enough time to do 10

123
00:07:46.250 --> 00:07:51.170
experiments.
And again,
what we do is that we picked a neural net,
uh,

124
00:07:51.200 --> 00:07:55.190
with it's a and the hyper parameters of the neural net that performs best on the

125
00:07:55.191 --> 00:07:59.950
validation set,
uh,
as the selected hyper parameter.
Again,

126
00:07:59.960 --> 00:08:04.960
we always use the validation set to select the best configuration and,

127
00:08:04.971 --> 00:08:06.350
uh,
it's you,

128
00:08:06.380 --> 00:08:10.460
we can always decide to actually go back and refine either our distribution if

129
00:08:10.461 --> 00:08:15.260
you're using random search or the,
uh,
uh,
the values in the grid.

130
00:08:15.290 --> 00:08:18.650
So in the list of values for each hyper parameter,
uh,

131
00:08:18.680 --> 00:08:22.670
based on the performance that we got from a first grid search or first random

132
00:08:22.671 --> 00:08:26.930
search.
So we can always go back and forth between specifying a grid,

133
00:08:27.050 --> 00:08:31.010
evaluating all points on that grid,
and then re specifying a new grid,

134
00:08:31.250 --> 00:08:33.890
trying all the values on that grid,

135
00:08:34.250 --> 00:08:38.060
and tried to improve as best as possible,
the performance and the validation set.

136
00:08:38.810 --> 00:08:42.560
Okay.
But the important thing is that at the very end,

137
00:08:42.650 --> 00:08:45.800
once we've found our best pair of hyper parameters,

138
00:08:46.100 --> 00:08:50.270
we now evaluate the neural network with these hyper parameters on the test set.

139
00:08:50.540 --> 00:08:53.320
And that's the performance of a,

140
00:08:53.360 --> 00:08:57.540
that we can expect from that neural net on new examples because the test set was

141
00:08:57.541 --> 00:09:02.541
not used either to train the neural net or to choose these values of a hyper

142
00:09:03.601 --> 00:09:07.920
parameters.
Uh,

143
00:09:07.950 --> 00:09:12.950
finally if a for the number of epochs or sometimes I'll say number of the

144
00:09:14.730 --> 00:09:16.860
training iterations.
Uh,

145
00:09:16.861 --> 00:09:21.470
so that's the number of times we loop over all training examples.
Uh,

146
00:09:21.471 --> 00:09:25.020
so for the number of epochs,
uh,
instead of saying,
okay,

147
00:09:25.021 --> 00:09:28.860
well I'll try 10 iterations and then run the simulation and see what's a

148
00:09:28.861 --> 00:09:30.870
performance and validation set.
Then they'll say,
okay,

149
00:09:30.871 --> 00:09:34.710
well maybe I'll try a hundred iterations instead.
And so on,
uh,

150
00:09:34.711 --> 00:09:38.520
so trained from scratch,
a new neural network with a hundred iterations.

151
00:09:39.600 --> 00:09:44.600
What we can do is instead is to actually track the validation set error,

152
00:09:46.280 --> 00:09:48.540
um,
as training progressive's.

153
00:09:49.050 --> 00:09:52.890
And what we'll typically do if you also track the training error,

154
00:09:53.670 --> 00:09:56.370
what we typically see is a curve like this.

155
00:09:56.371 --> 00:10:00.030
We'll see that the training error will always get better and better and better.

156
00:10:00.060 --> 00:10:02.820
That's because,
uh,
we're,
uh,

157
00:10:03.060 --> 00:10:07.110
more or less directly optimizing the training error by doing stochastic gradient

158
00:10:07.111 --> 00:10:10.140
descent.
So we expect it to always decrease.
However,

159
00:10:10.141 --> 00:10:15.141
the validation set era will start being better and better and then eventually

160
00:10:15.211 --> 00:10:17.550
we'll become higher.
Actually,
the,

161
00:10:17.830 --> 00:10:22.350
the real thing that's happening is that the gap between the training and the

162
00:10:22.351 --> 00:10:26.520
validation set error is expected to always increase.

163
00:10:26.910 --> 00:10:28.500
And so for that reason,
at one point,

164
00:10:28.501 --> 00:10:33.030
the training or won't increase a one decrease enough for the validation set to

165
00:10:33.031 --> 00:10:37.140
also decrease,
they'll start increasing in absolute terms.

166
00:10:37.920 --> 00:10:42.570
And so what we want to find is the number of the box here that corresponds to

167
00:10:42.571 --> 00:10:44.310
the best validation set error.

168
00:10:44.940 --> 00:10:48.420
And so what we can do is that we maybe train the neural network for one

169
00:10:48.421 --> 00:10:52.440
iteration and maybe that's the performance.
And then for one more iteration,

170
00:10:52.441 --> 00:10:55.320
we start from that neural net and do one more iteration.

171
00:10:55.321 --> 00:10:59.910
So we get these values here and then do one more iteration and get these values.

172
00:11:00.240 --> 00:11:03.450
And once we reach here,
we'll see that now the validation set has increased.

173
00:11:03.451 --> 00:11:08.400
So that's an indication that we're starting to over fit.
And so,
uh,

174
00:11:08.460 --> 00:11:11.750
because you know,
there might be some,
uh,

175
00:11:11.850 --> 00:11:16.350
some variation across iterations.
It could go back down a little bit.

176
00:11:16.830 --> 00:11:21.420
Uh,
and so for that reason,
instead of stopping right away,
often we use something,

177
00:11:21.750 --> 00:11:22.890
I call a look ahead.

178
00:11:22.891 --> 00:11:27.891
So we'll instead allow for training a few more iterations and once say,

179
00:11:28.991 --> 00:11:33.510
I will use a look at a five once the a validation set or as not improved,

180
00:11:33.930 --> 00:11:34.591
uh,
four,

181
00:11:34.591 --> 00:11:39.240
five iterations that I'll just stop and I'll go back to the neural net that had

182
00:11:39.241 --> 00:11:42.030
the best validation cetera.
So I'll remember that.

183
00:11:42.670 --> 00:11:46.170
I'll maybe save the weights of my neural net for that number of iterations.

184
00:11:46.800 --> 00:11:51.030
And that's the,
that,
so that's a,
the procedure known as early stopping.

185
00:11:51.030 --> 00:11:54.970
And that's the procedure that we often use to specifically for determining the

186
00:11:54.971 --> 00:11:58.840
number of training our POCs or training iterations for the neural network.

187
00:11:59.960 --> 00:12:04.420
All right.
And I should say that this curve here,

188
00:12:04.750 --> 00:12:08.120
uh,
we expect to find this group not just for the number of epochs,

189
00:12:08.140 --> 00:12:10.810
but also for the number of hidden units.

190
00:12:11.020 --> 00:12:13.180
So if we train a neural net with say,

191
00:12:13.210 --> 00:12:16.810
10 hidden units and then we'll train another neural net with 20 in a hidden

192
00:12:16.811 --> 00:12:20.140
units and other one at 30,
the one with 40 and so on.

193
00:12:20.141 --> 00:12:25.141
We also expect that the gap between the training and validation set there isn't

194
00:12:25.180 --> 00:12:30.040
going to increase.
And so if I have a neural net that's too small,

195
00:12:30.041 --> 00:12:30.341
that is,

196
00:12:30.341 --> 00:12:34.180
it could may be made bigger and we'd obtain a better validation set there.

197
00:12:34.270 --> 00:12:38.290
We say that we're under fitting and if it's,
if it's the other way around,

198
00:12:38.291 --> 00:12:41.050
if now our neural nets that we're trying is actually too big and we should

199
00:12:41.051 --> 00:12:44.320
decrease it's size,
it's size,
we'll say it's overfitting.
So it's,

200
00:12:44.560 --> 00:12:48.640
it's essentially starting to learn by heart what's in the valid training set and

201
00:12:48.641 --> 00:12:49.630
that's not helping it,

202
00:12:49.730 --> 00:12:53.740
that's actually hurting it in terms of how it performs on examples that it's

203
00:12:53.800 --> 00:12:55.990
hasn't seen the examples in the validation set.

204
00:12:57.070 --> 00:12:59.170
And just in general here,

205
00:12:59.200 --> 00:13:04.200
this label could be replaced by any hyper parameter that increases capacity.

206
00:13:05.710 --> 00:13:10.020
So the more capacity I give to my neural network,
uh,

207
00:13:10.160 --> 00:13:13.990
the more the gap between the training set and validation set there is going to

208
00:13:13.991 --> 00:13:16.090
increase.
All right?

209
00:13:16.091 --> 00:13:19.930
So that covers the different notions for doing model selection.

210
00:13:20.020 --> 00:13:24.820
We can use grid search or randomize search,
uh,
to select,
uh,

211
00:13:24.821 --> 00:13:27.790
the number of hidden units and the learning rates and so on.

212
00:13:27.970 --> 00:13:31.300
And for the number of iterations we can actually use early stopping.

213
00:13:31.450 --> 00:13:34.210
So given the number of immune,
it's in a,
say,
a learning rate,

214
00:13:34.270 --> 00:13:38.900
I can use early stopping for these two hyper parameters to select a,

215
00:13:39.070 --> 00:13:42.400
what's the right number of iterations for that number of hidden units and that,

216
00:13:42.760 --> 00:13:46.300
that learning,
right?
Right.
So that covers model selection.

