WEBVTT

1
00:00:00.620 --> 00:00:00.790
Okay.

2
00:00:00.790 --> 00:00:05.680
<v 1>In this video we'll discuss a convolutional network approach for addressing the</v>

3
00:00:05.681 --> 00:00:07.330
problem of a word tagging.

4
00:00:09.320 --> 00:00:09.880
<v 0>Okay.</v>

5
00:00:09.880 --> 00:00:14.030
<v 1>So as we mentioned in the previous video,
we will discuss a,
uh,</v>

6
00:00:14.031 --> 00:00:19.031
an approach proposed by [inaudible] and in Western for are trying to do a word

7
00:00:20.451 --> 00:00:22.550
tagging in natural language processing.

8
00:00:24.950 --> 00:00:25.783
<v 0>Okay.</v>

9
00:00:26.220 --> 00:00:29.730
<v 1>So,
um,
like I mentioned before,</v>

10
00:00:29.790 --> 00:00:33.880
one way of tag tackling a sequential prediction problem.

11
00:00:33.881 --> 00:00:36.510
So we're tagging really is the sequential prediction problem,

12
00:00:36.530 --> 00:00:39.540
whoever sequence of words and each word is associated with a class,

13
00:00:39.541 --> 00:00:40.380
which is a tag.

14
00:00:40.620 --> 00:00:44.490
And so we could use and conditional random fields approach with the linear chain

15
00:00:44.520 --> 00:00:48.930
much like we've seen before.
And uh,
and,
and then we would use funds,

16
00:00:48.931 --> 00:00:50.970
it's a neural network,
uh,

17
00:00:51.000 --> 00:00:55.320
to model the urinary potentials of that conditional random fields.

18
00:00:55.350 --> 00:00:58.140
And each neural network could,
uh,
take a,

19
00:00:58.141 --> 00:01:00.360
are a window as input,

20
00:01:00.361 --> 00:01:04.140
which would correspond to a context of a certain number of words centered at

21
00:01:04.380 --> 00:01:08.610
some given position to model the potential value at that position.

22
00:01:09.120 --> 00:01:12.660
And then we could use where representations for that neuro network and so on.

23
00:01:13.050 --> 00:01:16.350
So that would be sort of the most straightforward thing of addressing the

24
00:01:16.351 --> 00:01:19.230
problem of work tagging.
Given what we've seen so far.

25
00:01:20.040 --> 00:01:23.370
One problem with that approach is that for and tasks,

26
00:01:23.371 --> 00:01:27.380
much like semantic role labeling,
um,

27
00:01:27.780 --> 00:01:32.780
it might be that the words that we need to make a particular prediction of what

28
00:01:33.211 --> 00:01:37.560
the word,
uh,
idea,
sorry,
the word tax should be for this given word.

29
00:01:37.710 --> 00:01:39.730
It might be that,
um,

30
00:01:39.810 --> 00:01:44.460
there's information that's useful for determining for performing that prediction

31
00:01:44.760 --> 00:01:46.650
pretty much anywhere in the sentence.

32
00:01:46.651 --> 00:01:50.760
That is we cannot really make the assumption that using the neighboring words in

33
00:01:50.761 --> 00:01:55.761
the sentence wearable able to make a good prediction of what is the label of

34
00:01:56.071 --> 00:02:00.570
that word at the center.
And semantic role labeling is one example of that.

35
00:02:00.571 --> 00:02:05.190
The relevant context might be very far away from the word at some given position

36
00:02:05.370 --> 00:02:08.870
that we're trying to label.
So for that reason,
uh,

37
00:02:09.000 --> 00:02:13.450
Colabella and western suggested a convolutional network approach where,
uh,

38
00:02:13.590 --> 00:02:16.770
the output of the neural network,
which would give,

39
00:02:16.771 --> 00:02:19.410
will be used as the urinary potential in the CRF.

40
00:02:19.650 --> 00:02:21.960
But now because it's convolutional neural network,

41
00:02:22.350 --> 00:02:25.620
the way it's going to be a constructed,
the output,

42
00:02:25.621 --> 00:02:30.540
we actually potentially depend on any word in the input sentence thanks to the

43
00:02:30.541 --> 00:02:35.220
convolution plus some Max pooling,
uh,
that will be applied in the architecture.

44
00:02:36.430 --> 00:02:38.080
All right,
so that's um,

45
00:02:38.430 --> 00:02:41.910
I'm going to give you a very brief description and,

46
00:02:41.930 --> 00:02:45.930
and generic description of what the network does and then we'll look at in more

47
00:02:45.931 --> 00:02:48.420
details at the different stages of the neural net.

48
00:02:49.260 --> 00:02:50.460
So the way they've structured things,

49
00:02:50.461 --> 00:02:55.350
instead of having the input size here and going up there in their paper,
they go,

50
00:02:55.500 --> 00:02:58.350
uh,
from the,
uh,
the top to the bottom.

51
00:02:58.970 --> 00:02:59.260
<v 0>Okay.</v>

52
00:02:59.260 --> 00:03:02.320
<v 1>Uh,
which is different from what I've used to just be aware of that.</v>

53
00:03:02.680 --> 00:03:05.410
So at d input we have the sentence,

54
00:03:05.411 --> 00:03:08.770
so the full sentence with all the words and then each words are going to be

55
00:03:08.771 --> 00:03:12.260
mapped into a word representation space.
Uh,

56
00:03:12.340 --> 00:03:14.950
quite similarly to what we've seen so far.

57
00:03:14.951 --> 00:03:17.920
That is the words are going to be associated with the lookup table that gives

58
00:03:17.921 --> 00:03:21.340
you the representation for that word.
And in this way,

59
00:03:21.341 --> 00:03:25.690
we are able to represent all of the words in our sequence as a vector.

60
00:03:26.280 --> 00:03:26.990
<v 0>Okay?</v>

61
00:03:26.990 --> 00:03:31.310
<v 1>So this is done by going through some lookup table,
which gives us these vectors.</v>

62
00:03:31.790 --> 00:03:36.790
And then by concatenating the vectors which we treat here as column vectors,

63
00:03:37.761 --> 00:03:41.840
if he couldn't cat donated,
uh,
uh,
um,

64
00:03:41.990 --> 00:03:46.070
horizontally.
So this would be the representation that the first word,

65
00:03:46.550 --> 00:03:49.190
the other word,
the second word,
the third word,
and so on.

66
00:03:49.250 --> 00:03:53.570
Then we will get a matrix and on that matrix will perform a convolution.

67
00:03:55.070 --> 00:03:57.020
Convolution is,
uh,

68
00:03:57.050 --> 00:04:01.970
the same kind of operation that we've seen for a computer vision.
Uh,

69
00:04:02.030 --> 00:04:07.030
and it's going to be a convolution but in one dimension across positions of the

70
00:04:07.950 --> 00:04:10.310
sentence here.
So in a sense,

71
00:04:10.311 --> 00:04:14.090
you can think of it as a two d convolution,
but where the,

72
00:04:14.390 --> 00:04:19.040
um,
the height of the filter matches exactly the,

73
00:04:19.290 --> 00:04:22.100
a number of elements in the word representation,

74
00:04:22.190 --> 00:04:27.020
in which case we can just do a convolution,
uh,
in this direction.

75
00:04:27.540 --> 00:04:28.000
<v 0>Okay.</v>

76
00:04:28.000 --> 00:04:30.580
<v 1>Because there's a conclusion to also apply some padding.</v>

77
00:04:30.820 --> 00:04:33.340
I won't go too much into the details of that,

78
00:04:33.370 --> 00:04:36.160
but you can look at it in the original paper.

79
00:04:36.760 --> 00:04:38.290
And once you've done this convolution,

80
00:04:38.291 --> 00:04:42.040
then to get a fixed size representation for the whole sentence,

81
00:04:42.190 --> 00:04:46.480
we'll apply a max pooling operation,
doing a Max over,

82
00:04:46.720 --> 00:04:50.440
uh,
all of the,
uh,
difference,
uh,

83
00:04:50.470 --> 00:04:54.520
all of the different roles of that matrix that is the result of the convolution

84
00:04:54.521 --> 00:04:55.354
with perform.

85
00:04:56.260 --> 00:04:59.770
And then for the rest of the network will use regular kind of operations.

86
00:04:59.771 --> 00:05:04.560
So Matrix,
multiplication,
multiplication.
So then your mom,
uh,
linear,
uh,

87
00:05:04.740 --> 00:05:09.350
formation all by nonlinearity,
all by another linear,
uh,

88
00:05:09.970 --> 00:05:13.270
uh,
an inner transformation which gives us the output,

89
00:05:13.510 --> 00:05:15.790
which is going to be used as the and every potential.

90
00:05:16.000 --> 00:05:19.090
So we're going to get a vector of size,
number of possible tags,

91
00:05:19.091 --> 00:05:21.850
which we'll introduce in a conditional random fields.

92
00:05:22.710 --> 00:05:26.380
So now let's look at the specific details of each of these different operations.

93
00:05:30.100 --> 00:05:32.950
So,
uh,
to represent the words,
uh,

94
00:05:32.951 --> 00:05:37.780
we will use a word representations that w that is,
we'll have some,
uh,

95
00:05:37.781 --> 00:05:42.580
way.
Uh,
we will train a,
a word representation Matrix,
a look up table,

96
00:05:42.880 --> 00:05:46.480
which we'll learn a real valid vector representations of the words.

97
00:05:46.910 --> 00:05:47.380
<v 0>Okay.</v>

98
00:05:47.380 --> 00:05:51.700
<v 1>One thing that's interesting in this work is that instead of having a word</v>

99
00:05:51.701 --> 00:05:55.150
representation,
just for the word itself,
the word id,

100
00:05:55.151 --> 00:05:58.820
we'll also have a work representations for other types of,
of features.

101
00:05:58.821 --> 00:06:02.720
So we'll treat the word itself as a feature and we'll map that to a word

102
00:06:02.721 --> 00:06:05.150
representation of some given size.

103
00:06:05.450 --> 00:06:08.330
But then we'll also extract from the original word,

104
00:06:08.360 --> 00:06:10.790
other types of features like substring features.

105
00:06:10.791 --> 00:06:13.760
So we could look at the prefix or suffix,

106
00:06:13.910 --> 00:06:18.730
constructive vocabulary out of these prefixes or suffixes.
And then,
uh,

107
00:06:18.740 --> 00:06:21.380
we would have now a word representation,

108
00:06:21.381 --> 00:06:25.730
the vector representation for all possible prefixes and suffixes.

109
00:06:25.731 --> 00:06:30.731
So we would also learn this matrix that maps prefixes and suffixes to where a

110
00:06:31.041 --> 00:06:36.041
word vector representation other features that they use our gas at tier

111
00:06:36.351 --> 00:06:40.550
features.
So whether or not,
uh,
a word belongs to a list of,

112
00:06:40.580 --> 00:06:41.810
of known locations,

113
00:06:42.110 --> 00:06:45.890
because these are things that are useful in particular for a named entity

114
00:06:45.891 --> 00:06:49.670
recognition.
So again,
you can look at the original paper for more information.

115
00:06:50.210 --> 00:06:51.043
And um,

116
00:06:51.620 --> 00:06:55.130
so by using all these features and treat them them like essentially like words,

117
00:06:55.131 --> 00:06:59.230
they were like each have their own lookup tables.
And,
uh,

118
00:06:59.270 --> 00:07:04.100
ultimately what we'll do is that for a given word and it's suffix and prefix and

119
00:07:04.101 --> 00:07:04.551
so on,

120
00:07:04.551 --> 00:07:08.600
we look struck all the vector for all of these features and concatenate it them

121
00:07:08.601 --> 00:07:12.080
to get a more,
a richer,
uh,
con,
uh,

122
00:07:12.230 --> 00:07:14.420
vectorial representation of that word.

123
00:07:18.110 --> 00:07:18.780
Um,

124
00:07:18.780 --> 00:07:23.780
and other thing that we must do is that we need to encode somehow a what word in

125
00:07:26.041 --> 00:07:30.480
the input sentence at which position we're trying to make a prediction.
So,

126
00:07:30.810 --> 00:07:35.370
uh,
because we're doing a conclusion for the whole sentence and then we do a Max,

127
00:07:35.400 --> 00:07:38.700
it means that,
uh,
based on,
on that computation,

128
00:07:38.701 --> 00:07:42.720
there's nowhere it information about based on this,
uh,

129
00:07:42.780 --> 00:07:44.880
Max pooled representation,

130
00:07:44.881 --> 00:07:48.810
which word in the input sentence do we actually want to make a prediction for?

131
00:07:49.290 --> 00:07:53.490
So I have to provide that information somehow.
So that influences the way that,

132
00:07:53.520 --> 00:07:53.971
uh,
the,

133
00:07:53.971 --> 00:07:58.971
the actual representation after Max pooling and a one way they've done this is

134
00:07:59.101 --> 00:08:02.490
that they would add another word feature,
uh,

135
00:08:02.491 --> 00:08:05.790
which would correspond to the relative position of the word.

136
00:08:06.210 --> 00:08:11.210
So if I is the position of some word in the sentence and pause w is the position

137
00:08:13.531 --> 00:08:15.090
of the current word,

138
00:08:15.091 --> 00:08:19.200
that is the word that for which we're computing the urinary potential.

139
00:08:19.620 --> 00:08:24.150
Um,
uh,
sorry.
Um,
uh,
the,
yes,
that's right.

140
00:08:24.151 --> 00:08:28.200
The word for which we're trying to compute the uh,
uh,

141
00:08:28.230 --> 00:08:32.610
the urinary potential for getting a preference over what is labeled could be.

142
00:08:32.910 --> 00:08:37.910
Then we would just add I minus pause w as another feature along with the prefix

143
00:08:38.641 --> 00:08:43.290
and the suffix and we'll learn a vector representation with its own lookup table

144
00:08:43.291 --> 00:08:46.290
for that number.
So for instance,
if,
uh,

145
00:08:46.440 --> 00:08:51.440
we were taking this sentence and we wanted the unit potential for the word sat,

146
00:08:51.800 --> 00:08:55.590
uh,
then we would also upenn amongst the different features with W's.

147
00:08:55.591 --> 00:08:56.550
Here are the features,

148
00:08:56.551 --> 00:09:01.230
but it would be another feature which would be what is the relative position of

149
00:09:01.231 --> 00:09:05.850
all of the words with respect to sat.
So this one would be zero.
So it'd be one,

150
00:09:05.880 --> 00:09:09.330
two,
three in here.
What happened?
Minus one,
minus two.

151
00:09:09.840 --> 00:09:14.070
And now each of these integers would be associated with a,
uh,

152
00:09:14.090 --> 00:09:16.800
a word representation or a vector we're representation.

153
00:09:16.800 --> 00:09:18.030
That would also be alarmed.

154
00:09:18.720 --> 00:09:21.890
And so taking all of this feeding data from the competitional net,

155
00:09:21.930 --> 00:09:26.560
that would give us an output vector,
which is the,
uh,
you never,

156
00:09:26.620 --> 00:09:31.620
potentials and knife wanted the union potentials for the word on.

157
00:09:32.010 --> 00:09:36.750
Then instead of using these values would actually use all the same other

158
00:09:36.751 --> 00:09:41.130
features.
But for the uh,
position feature,
it would be zero here.

159
00:09:41.131 --> 00:09:45.900
Now one,
two here minus one minus two minus three here.

160
00:09:45.901 --> 00:09:48.930
So we see that the input is changed or the output will also change.

161
00:09:49.050 --> 00:09:53.340
So we get a different vector for the year and every potential prediction of the

162
00:09:53.370 --> 00:09:54.203
neural network.

163
00:09:55.450 --> 00:09:55.700
<v 0>Okay.</v>

164
00:09:55.700 --> 00:09:58.640
<v 1>And for semantic role labeling,
uh,</v>

165
00:09:58.750 --> 00:10:02.870
not only don't we need to know which word where,
uh,

166
00:10:02.900 --> 00:10:07.010
currency computing,
the a which position we want the,
you never potentials,

167
00:10:07.011 --> 00:10:09.020
but we also need to know,
uh,

168
00:10:09.050 --> 00:10:12.830
we're trying to predict the roles for which verb in the sentence.

169
00:10:13.490 --> 00:10:16.870
And so what we would add another feature,
uh,

170
00:10:16.910 --> 00:10:21.140
which would be do you relative position of that verb.

171
00:10:21.170 --> 00:10:24.890
So Pause v would be the position that verb and I did work for,

172
00:10:24.891 --> 00:10:29.260
which we are computing the,
uh,
unit potentials a.
For instance,
uh,

173
00:10:29.270 --> 00:10:33.110
in this case we only have sat and if you want it to get the urinary potential

174
00:10:33.111 --> 00:10:37.500
for what role them,
I'd be playing,
uh,
well for one thing we'd have the,

175
00:10:37.760 --> 00:10:39.200
this feature here.

176
00:10:39.201 --> 00:10:44.201
So we'd have zero one minus one minus two minus three minus four.

177
00:10:45.230 --> 00:10:49.700
And then we would also need to add the relative position of a given work with

178
00:10:49.701 --> 00:10:54.290
respect to what verb,
uh,
from what we're,
we're identifying the roles.

179
00:10:54.560 --> 00:10:58.760
So here the,
uh,
there's only one verb,
so it's zero.

180
00:10:58.761 --> 00:11:03.050
So we'd have another feature which is zero,
uh,
one,
two,

181
00:11:03.080 --> 00:11:06.140
three and then minus one minus two.

182
00:11:06.560 --> 00:11:08.240
So now for semantic grow labeling,

183
00:11:08.241 --> 00:11:12.350
we need to add this set of features and this set of features to get,

184
00:11:12.860 --> 00:11:14.170
uh,
to,

185
00:11:14.180 --> 00:11:18.950
to as the input of the conversion lower net to get the you and every potential

186
00:11:18.951 --> 00:11:23.060
vector for the a word there in that sentence.

187
00:11:26.630 --> 00:11:30.290
Okay.
So now we have all of these different type of features and uh,

188
00:11:30.350 --> 00:11:32.360
we have get a mapping those features,

189
00:11:32.361 --> 00:11:36.350
these ideas into a vector representation.
Uh,

190
00:11:36.351 --> 00:11:40.490
so for each lookup table we get this factor and then we can just concatenate

191
00:11:40.491 --> 00:11:41.770
them for,
uh,

192
00:11:41.780 --> 00:11:46.780
in order to obtain a single vector for each a word in the sentence.

193
00:11:48.280 --> 00:11:52.670
Now if we'd just stack them horizontally,
we get a matrix.

194
00:11:53.230 --> 00:11:56.980
And then next to get some,
uh,
we apply linear transformation,

195
00:11:56.981 --> 00:12:01.070
specifically a conversional,
uh,
transformation,
uh,

196
00:12:01.330 --> 00:12:04.870
on this matrix of word representation vectors.

197
00:12:05.620 --> 00:12:05.970
<v 0>Okay.</v>

198
00:12:05.970 --> 00:12:10.970
<v 1>Specifically at every position will compute some linear activations,</v>

199
00:12:11.820 --> 00:12:16.800
uh,
based on a window of word representations and their features.

200
00:12:16.950 --> 00:12:21.420
So for instance,
we would apply a,
we would take that vector,

201
00:12:21.600 --> 00:12:25.590
that vector,
and that vector.
So this whole vector here,
this whole vector here,

202
00:12:25.680 --> 00:12:28.010
this whole vector here,
we,
uh,

203
00:12:28.020 --> 00:12:32.460
would concatenate these factors and we will apply some tens formation by

204
00:12:32.461 --> 00:12:35.610
multiplying by some matrix say that we call m one.

205
00:12:35.970 --> 00:12:40.710
And this will give us effector,
which would be this factor.
Now,
uh,

206
00:12:40.711 --> 00:12:44.910
to get this vector here,
we'd perform the same transformation,

207
00:12:44.911 --> 00:12:49.890
the same type of operation.
But now we take that vector,
that vector,

208
00:12:50.190 --> 00:12:54.090
that vector,
concatenate it and multiply it again by m one.

209
00:12:54.091 --> 00:12:55.590
And that would give us this vector.

210
00:12:56.250 --> 00:12:59.890
So we see that we applied the same dinner transformation but on all a

211
00:13:00.010 --> 00:13:05.010
overlapping windows of a word representations in the,

212
00:13:05.070 --> 00:13:06.870
uh,
the,
uh,
sentence.

213
00:13:07.520 --> 00:13:07.820
<v 0>Okay.</v>

214
00:13:07.820 --> 00:13:10.590
<v 1>So this,
uh,
this thing,
uh,</v>

215
00:13:10.650 --> 00:13:15.260
that I just described can as easily be understood as taking that matrix and

216
00:13:15.261 --> 00:13:20.261
performing a convolution with a vector where the summary with a filter where the

217
00:13:21.441 --> 00:13:26.170
filter has,
uh,
it's uh,
height,
uh,
where the,
the height of the,

218
00:13:26.390 --> 00:13:31.390
the filter matches the size of the joint vector fall features for each word.

219
00:13:32.060 --> 00:13:36.590
And then the width of that filter would be the size of their context window.

220
00:13:37.190 --> 00:13:39.500
Okay.
So if that's not obvious,

221
00:13:39.501 --> 00:13:42.920
maybe we want to sit down and think about that a little bit and see how this is

222
00:13:42.921 --> 00:13:44.330
actually equivalent.

223
00:13:44.660 --> 00:13:48.260
But effectively what we're doing is that we're taking all context sizes and we

224
00:13:48.261 --> 00:13:50.840
computing this linear transformation.

225
00:13:51.690 --> 00:13:52.210
<v 0>Okay.</v>

226
00:13:52.210 --> 00:13:57.190
<v 1>And then finally to get a fixed size representation,
uh,</v>

227
00:13:57.220 --> 00:13:58.120
we'll just do it.

228
00:13:58.250 --> 00:14:03.250
And Max pooling operation where we doing the Max over all positions of the uh,

229
00:14:04.901 --> 00:14:08.110
of the sentence.
So each vector we have here,

230
00:14:08.111 --> 00:14:13.111
we can think of each of them as being deep reactivation in some hidden layer for

231
00:14:13.510 --> 00:14:14.530
each position.

232
00:14:14.950 --> 00:14:19.840
And then we do max pooling across position for each individually for each hidden

233
00:14:19.841 --> 00:14:21.040
units two.

234
00:14:21.041 --> 00:14:25.120
And by performing the Max we get the nonlinear representation.

235
00:14:25.130 --> 00:14:27.290
So an activation,
uh,

236
00:14:27.340 --> 00:14:31.390
for the whole sentence and activation vector that represents the whole sentence.

237
00:14:34.950 --> 00:14:37.980
And then finally,
now that we have this fixed size representation,

238
00:14:37.981 --> 00:14:41.730
so it's fixed size because no matter what was the size of the original sentence,

239
00:14:41.760 --> 00:14:43.650
now the size of this vector we'd gotten,

240
00:14:43.651 --> 00:14:48.390
the Pv step is the number of hidden units that we wanted to have at that point

241
00:14:48.391 --> 00:14:52.610
in the neural network.
So we can take that fixed size,
uh,
uh,

242
00:14:52.640 --> 00:14:53.480
representation.

243
00:14:53.481 --> 00:14:58.481
Then then connected with some regular nonlinear hidden there by performing the

244
00:14:58.590 --> 00:15:01.910
inner transformation.
So multiplying by some other matrix and two,

245
00:15:02.090 --> 00:15:06.410
and then applying some squashing function on top of that vector and their paper.

246
00:15:06.411 --> 00:15:08.840
They propose a hard tense.
It's a,

247
00:15:09.140 --> 00:15:11.780
instead of having a 10 age that is smoother like this,

248
00:15:11.840 --> 00:15:16.840
it's a tangent that is a piecewise linear function where it's zero here,

249
00:15:17.660 --> 00:15:21.290
one here,
exactly,
sorry,
minus one here,
one here.
And here.

250
00:15:21.291 --> 00:15:24.920
It's exactly an inner function.
So that's just another alternative.

251
00:15:24.980 --> 00:15:27.440
You can look at the paper to get more details about that.

252
00:15:28.160 --> 00:15:31.040
And then you take that final,
that top most hidden layer,

253
00:15:31.041 --> 00:15:35.600
and then you apply a linear transformation using same matrix m three,

254
00:15:35.780 --> 00:15:39.050
and then you use the result of that transformation,

255
00:15:39.260 --> 00:15:44.260
which has a size of a B that SQL to the number of tags that we're,

256
00:15:45.130 --> 00:15:48.500
that the different tax that the words can take.
And uh,

257
00:15:48.501 --> 00:15:51.620
we use that as the urinary potential in the linear chain,

258
00:15:51.621 --> 00:15:56.180
conditional random fields.
And so for the linear chain conditional in the field,

259
00:15:56.181 --> 00:15:58.520
we know how that works.
We've seen that previously.

260
00:15:59.330 --> 00:16:03.650
So we could use that convolutional neural network architecture for,
uh,

261
00:16:03.710 --> 00:16:07.400
an a different neural net with this convolutional architecture for each of the

262
00:16:07.401 --> 00:16:11.990
different,
uh,
we're tagging task we'd like to solve and then we could solve them,

263
00:16:12.080 --> 00:16:13.190
uh,
separately.

264
00:16:14.150 --> 00:16:19.150
And though would already give us a fairly good performance and,

265
00:16:19.311 --> 00:16:20.490
uh,
uh,

266
00:16:20.510 --> 00:16:23.570
are the one problem with this is that there are actually no connections between

267
00:16:23.571 --> 00:16:24.740
the conditional and then feels,

268
00:16:24.741 --> 00:16:28.940
even though they might each benefit from each other if they're somewhat related.

269
00:16:29.450 --> 00:16:30.620
And,
uh,

270
00:16:30.680 --> 00:16:35.680
and what we'll see in the next video is a way of actually combining the

271
00:16:36.171 --> 00:16:40.340
information that are available between the different tasks,

272
00:16:40.370 --> 00:16:42.230
between the different board tagging tasks.

