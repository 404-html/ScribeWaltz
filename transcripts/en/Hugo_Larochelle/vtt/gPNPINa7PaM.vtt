WEBVTT

1
00:00:00.270 --> 00:00:00.830
Yeah.

2
00:00:00.830 --> 00:00:01.371
<v 1>In this video,</v>

3
00:00:01.371 --> 00:00:05.870
we'll look at how representations are merged in a recursive neural network.

4
00:00:07.930 --> 00:00:12.880
So we briefly described the recursive neural network in the previous video where

5
00:00:13.060 --> 00:00:15.340
after we've extracted a sink tactic,

6
00:00:15.730 --> 00:00:18.100
a parse tree of a sentence,

7
00:00:18.310 --> 00:00:23.310
then we essentially merge representations by following the structure of this

8
00:00:23.670 --> 00:00:26.010
syntactics parse tree.
And,
uh,

9
00:00:26.020 --> 00:00:28.660
these representations are merged by a neural network.

10
00:00:28.661 --> 00:00:32.830
It's so now we'll see specifically what kind of structure wood was used by which

11
00:00:32.831 --> 00:00:37.180
are social and his coauthors for extract,
for merging the,

12
00:00:37.510 --> 00:00:41.350
uh,
vector representations of the words and the phrases in the sentence.

13
00:00:44.980 --> 00:00:49.390
So the neural network that they use is actually quite simple.
Uh,

14
00:00:49.420 --> 00:00:53.910
it essentially corresponds to a neural network where we have,
uh,

15
00:00:54.030 --> 00:00:58.810
just a hidden layer,
uh,
which is called P,

16
00:00:59.110 --> 00:01:04.110
which is connected to the concatenation of the two input vector representations,

17
00:01:04.811 --> 00:01:08.720
c one and c two c one and c two might be either the representation of,

18
00:01:08.750 --> 00:01:11.290
of some words or have some,

19
00:01:11.380 --> 00:01:15.340
a phrase which is covered by an internal node in the syntactic tree.

20
00:01:15.940 --> 00:01:20.940
And P is going to correspond to the new vector representation for a d note,

21
00:01:22.010 --> 00:01:26.800
uh,
that has c one and c two as as children in the syntactic a treat.

22
00:01:27.700 --> 00:01:29.140
And so specifically,

23
00:01:29.141 --> 00:01:33.460
this representation will be obtained by taking the concatenation of the vector c

24
00:01:33.461 --> 00:01:38.461
one and c two multiplied by a matrix w adding a bias and then passing that

25
00:01:38.711 --> 00:01:42.160
through a nonlinearity,
for instance,
a Sigmoidal nonlinearity.

26
00:01:42.940 --> 00:01:47.940
And so p will then become either see one of c to have a,

27
00:01:48.540 --> 00:01:53.530
a new call to the same neural net with the same weights,
W and B,

28
00:01:53.710 --> 00:01:57.410
which will be merging this p and then perhaps,
uh,

29
00:01:57.460 --> 00:02:02.460
some other vector representation in this intact if we as we move along towards

30
00:02:02.531 --> 00:02:05.710
the root of,
uh,
the syntactic tree.

31
00:02:06.760 --> 00:02:08.830
So that's the representation,

32
00:02:08.831 --> 00:02:13.330
that's the parametrization of the neural net that does the merging of the vector

33
00:02:13.331 --> 00:02:14.170
representation.

34
00:02:15.430 --> 00:02:20.290
And another thing that this neural network will do is actually compute a score

35
00:02:20.291 --> 00:02:24.430
of,
uh,
how the,
the essentially the quality of the merge.

36
00:02:24.431 --> 00:02:29.431
How appropriate is it to take c one and C two and merge it into a vector,

37
00:02:30.650 --> 00:02:32.810
uh,
which corresponds to pee.

38
00:02:33.640 --> 00:02:38.640
And to get this cord essentially only model it as a linear transformation of the

39
00:02:38.681 --> 00:02:40.060
merge representation piece.

40
00:02:40.061 --> 00:02:44.650
So you take the vector of B multiplied by w scored at different,
uh,

41
00:02:44.651 --> 00:02:45.700
in this case,
not a matrix,

42
00:02:45.701 --> 00:02:49.990
but in factor that's multiplied by P and gives a real value add score.

43
00:02:50.560 --> 00:02:55.560
And this core is actually going to use that as a sort of estimate of the quality

44
00:02:56.591 --> 00:02:58.660
of the merge.
And,
uh,

45
00:02:58.810 --> 00:03:03.810
essentially it's going to be used to decide which pairs of representations,

46
00:03:05.090 --> 00:03:06.670
uh,
uh,

47
00:03:06.730 --> 00:03:11.230
should we merge first as we construct is syntactic trees.

48
00:03:11.260 --> 00:03:12.160
So in other words,

49
00:03:13.090 --> 00:03:17.950
it's going to be used to determine what is the best syntactic tree to use,
uh,

50
00:03:17.951 --> 00:03:22.951
in order to extract the vector representation of the different phrases and then

51
00:03:23.681 --> 00:03:26.430
ultimately the sentence.
Uh,

52
00:03:26.440 --> 00:03:31.440
so I had to get this vector representation and so that part would be useful,

53
00:03:31.990 --> 00:03:32.530
uh,

54
00:03:32.530 --> 00:03:36.670
in the next video when we'd start talking about how to extract the same tactic

55
00:03:36.671 --> 00:03:38.200
tree from it given sentence.

