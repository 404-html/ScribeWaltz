1
00:00:00,580 --> 00:00:01,181
In this video,

2
00:00:01,181 --> 00:00:05,440
we'll discuss the relationship between
the linear chain conditional random

3
00:00:05,441 --> 00:00:07,390
fields with the hidden Markov model.

4
00:00:09,190 --> 00:00:09,890
Okay.

5
00:00:09,890 --> 00:00:11,710
So, uh, we've, uh,

6
00:00:11,711 --> 00:00:15,230
in a previous video talked about whether
there were alternatives to the linear

7
00:00:15,231 --> 00:00:16,940
chain conditional then field for sequence.

8
00:00:16,941 --> 00:00:21,941
So classification a of alternatives that
might be more appropriate when we have

9
00:00:22,071 --> 00:00:23,120
a lot of training data.

10
00:00:23,780 --> 00:00:24,410
Yeah.

11
00:00:24,410 --> 00:00:28,870
This has led us to talk about the
maximum entropy Markov model. And A,

12
00:00:28,880 --> 00:00:31,460
I've mentioned why,
uh,

13
00:00:31,490 --> 00:00:35,660
it suffered from a problem known as the
labeled bias problem where observation

14
00:00:35,661 --> 00:00:36,250
for the wind,

15
00:00:36,250 --> 00:00:40,700
the sequence would actually not have
any impact on the prediction and the

16
00:00:40,701 --> 00:00:45,020
marginal probability for the label
at positions early in the sequence.

17
00:00:48,060 --> 00:00:52,180
So in other alternative to the MDM
EMM, which is well known that you, uh,

18
00:00:52,260 --> 00:00:53,660
and we've actually,
uh,

19
00:00:53,790 --> 00:00:57,840
meant mentioned in passing in previous
videos is the hidden Markov model.

20
00:00:58,410 --> 00:01:00,390
So a,
in the hidden Markov model,

21
00:01:00,690 --> 00:01:05,690
instead of assuming a generative started
where we are given some inputs and

22
00:01:06,090 --> 00:01:10,350
those inputs in the sequence
and then generated the labels,

23
00:01:10,560 --> 00:01:14,490
we have an inverted
generative story where we, uh,

24
00:01:14,520 --> 00:01:18,660
the labels at first been produced.
And then from the labels,

25
00:01:18,661 --> 00:01:23,000
the sequence of observations, I've
actually been produced from this soul.

26
00:01:23,001 --> 00:01:26,760
And instead of of producing
the inputs and then the labels,

27
00:01:26,790 --> 00:01:30,290
it's the other way around.
And actually in Hmms,

28
00:01:30,450 --> 00:01:34,950
we usually define a, uh, uh,
something know that as the,

29
00:01:35,010 --> 00:01:37,770
uh, uh, uh, emission distribution,

30
00:01:37,771 --> 00:01:42,060
which tells us how the inputs were
generated from the labels. So p of x,

31
00:01:42,061 --> 00:01:44,910
given why. So, uh, from this,

32
00:01:45,090 --> 00:01:50,090
we actually get a full joint
distribution over both y and x,

33
00:01:50,550 --> 00:01:55,040
both the sequence of labels
and the sequence of, uh, uh,

34
00:01:55,200 --> 00:01:58,950
of inputs. And if we have a
joint probability from this,

35
00:01:59,070 --> 00:02:02,970
then we can just compute
p of y given x by, uh,

36
00:02:02,971 --> 00:02:07,890
just dividing by the
marginal probability of x,

37
00:02:08,250 --> 00:02:11,850
which means that we can have an expression
for minus the log of the conditional

38
00:02:11,851 --> 00:02:15,120
probability of y given x.
And we could try to optimize that.

39
00:02:15,450 --> 00:02:19,770
And so I essentially optimize our model
to assign high conditional probability

40
00:02:19,771 --> 00:02:24,270
on the labels, uh, given
the inputs. So, uh,

41
00:02:24,510 --> 00:02:28,050
uh,
this training of Hmms is a,

42
00:02:28,051 --> 00:02:29,940
actually used a lot in speech recognition.

43
00:02:29,970 --> 00:02:34,830
It's known as maximum Mitchell
information training. Uh, and, uh,

44
00:02:34,831 --> 00:02:38,880
we can show that we don't have the
same label bias problem anymore.

45
00:02:39,600 --> 00:02:41,610
So that's an advantage.
However,

46
00:02:41,611 --> 00:02:46,611
the director nature of the hmm can make
it a bit more complicated to train.

47
00:02:46,891 --> 00:02:50,990
So optimization the optimization
problem associated with training, uh,

48
00:02:50,991 --> 00:02:54,510
it can become a bit more complicated
because if you write down the,

49
00:02:54,511 --> 00:02:58,950
hmm as a factor,
a model where it's a product of factors,

50
00:02:59,330 --> 00:03:03,850
as we've discussed before, the factors
now need to be normalized probabilities.

51
00:03:04,210 --> 00:03:09,160
And so this has certain consequences in
the optimization of the model. And a,

52
00:03:09,161 --> 00:03:14,161
it can also be a somewhat difficult
to define a good distribution over the

53
00:03:14,951 --> 00:03:19,540
sequence of inputs x if the
vectors x one x two. And so on.

54
00:03:19,780 --> 00:03:24,340
Our real valued vectors,
defining distribution of a
rail real value vector is,

55
00:03:24,341 --> 00:03:28,000
is actually not a trivial thing to do.
Whereas in crs,

56
00:03:28,001 --> 00:03:29,410
because their conditional,

57
00:03:29,411 --> 00:03:34,411
we totally sidestep this problem and a
yet we don't actually suffer from the

58
00:03:35,110 --> 00:03:39,610
label pious problem, much like
in a descriptive. Hmm. So, uh,

59
00:03:39,640 --> 00:03:43,810
one can think of a conditional random
fields and interchain conditional random

60
00:03:43,811 --> 00:03:48,811
feel as the sort of best of both worlds
solution between a discriminative hidden

61
00:03:49,301 --> 00:03:53,590
Markov model and the maximum
entropy markup model. So,

62
00:03:53,591 --> 00:03:57,580
um,
so to sum up a while,

63
00:03:57,581 --> 00:04:02,380
the maximum entropy Markov model can be
perhaps easier to train than the hidden

64
00:04:02,381 --> 00:04:03,161
Markov model.

65
00:04:03,161 --> 00:04:06,730
It suffers from the label bias problem
and the end of the mark of model,

66
00:04:06,731 --> 00:04:09,220
if which words you're training
descriptively though it's possible,

67
00:04:09,221 --> 00:04:11,680
and people do it. And
speech recognition, uh,

68
00:04:11,681 --> 00:04:14,830
it can be harder than the linear
chain conditional and the Va..

