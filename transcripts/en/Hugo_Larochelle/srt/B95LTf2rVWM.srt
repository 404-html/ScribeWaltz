1
00:00:00,350 --> 00:00:01,010
Okay.

2
00:00:01,010 --> 00:00:03,830
In this video we'll discuss the
different type of output layer,

3
00:00:03,831 --> 00:00:07,700
which is going to be more efficient to
use in the context of language modeling.

4
00:00:08,020 --> 00:00:12,200
Uh, and we will call that out type of
output layer, a hierarchical output layer.

5
00:00:14,610 --> 00:00:18,960
So we've seen a previous video how to
define the neural network language model,

6
00:00:18,990 --> 00:00:23,340
which is essentially a neural net
that takes as input all of the word

7
00:00:23,341 --> 00:00:27,720
representations of the words in
some given conditioning context.

8
00:00:27,960 --> 00:00:31,770
And then we concatenate
those representation and
feed that to regular neural

9
00:00:31,771 --> 00:00:35,700
network where at the output
we get a softmax layer,

10
00:00:35,820 --> 00:00:40,820
which will define the and give us the
probability of observing each possible

11
00:00:41,671 --> 00:00:44,520
word in our vocabulary
following our context.

12
00:00:45,390 --> 00:00:50,390
Now one problem with this approach is
that the softmax layer here is actually

13
00:00:51,391 --> 00:00:52,590
really,
really big.

14
00:00:52,620 --> 00:00:57,120
If we have say a 250,000
words in our vocabulary,

15
00:00:57,121 --> 00:01:02,121
that it means that this layer
has to 250,000 different units,

16
00:01:02,161 --> 00:01:06,430
one for each possible word.
And when performing the Softmax,

17
00:01:06,460 --> 00:01:08,910
we does have to compute
all the reactivation.

18
00:01:09,020 --> 00:01:14,020
So these 250,000 activations and then
we have to perform the softmax known in

19
00:01:15,461 --> 00:01:15,870
the charity.

20
00:01:15,870 --> 00:01:18,840
So that's going to be very expensive
to do if we have to do this for every

21
00:01:18,841 --> 00:01:19,651
training example,

22
00:01:19,651 --> 00:01:23,820
every example of context and the
word that follows it in the text.

23
00:01:24,360 --> 00:01:27,720
So what we'll talk about here is
a way of getting a much better,

24
00:01:27,810 --> 00:01:32,340
much more efficient, uh, have
a defining a distribution.

25
00:01:32,341 --> 00:01:36,030
The distribution offered the next
word, given the previous word, uh,

26
00:01:36,031 --> 00:01:39,780
which in practice is very efficient
and gives us good performance as well.

27
00:01:42,030 --> 00:01:42,840
Okay.

28
00:01:42,840 --> 00:01:47,340
The main idea is to, instead of modeling
the probability of the next word,

29
00:01:47,341 --> 00:01:48,480
using a flat layer,

30
00:01:48,481 --> 00:01:53,481
we use a hierarchical layer where our
wheel decompose their probability of

31
00:01:53,671 --> 00:01:58,020
observing the word into a
sequence of probabilities,

32
00:01:58,021 --> 00:02:01,900
which corresponds to choices
as to uh, to which group, uh,

33
00:02:01,950 --> 00:02:06,510
does the next word is more likely
to, to belong, to, to organize.

34
00:02:06,511 --> 00:02:10,320
These groups will actually take
all of the words in our vocabulary.

35
00:02:10,470 --> 00:02:12,810
And let's work with an example
to make this more concrete.

36
00:02:12,811 --> 00:02:17,370
So imagine we have eight words in
our vocabulary and the old fee work.

37
00:02:18,570 --> 00:02:22,770
Now we will do is that will place all
of these words into a treat and sensei

38
00:02:22,771 --> 00:02:25,260
complete tree like here, uh, which is uh,

39
00:02:25,261 --> 00:02:30,261
which is balanced and
essentially the probability of a,

40
00:02:30,741 --> 00:02:35,741
of a given word following the context
is going to be given by the probability

41
00:02:36,721 --> 00:02:41,721
that from the route we would actually
follow the path to the word cat in the

42
00:02:42,091 --> 00:02:46,860
tree. That is, we would go left
here, right here and then right here,

43
00:02:47,010 --> 00:02:51,270
so left at the note labeled one,
right that the node label too.

44
00:02:51,570 --> 00:02:56,040
And then right at the node level five.
So if we model probabilities this way,

45
00:02:56,041 --> 00:03:01,041
we noticed that we actually only need
the probabilities for these three nodes

46
00:03:01,240 --> 00:03:05,700
and we don't need the probabilities
for say the nodes here or this note.

47
00:03:05,710 --> 00:03:09,220
So if we look at the probability
of one specific word,

48
00:03:09,460 --> 00:03:13,760
then actually computing
probabilities is going to be a,

49
00:03:13,780 --> 00:03:15,010
it's going to be a,

50
00:03:15,011 --> 00:03:20,011
I have a computational burden proportional
to the depth of the word in the tree.

51
00:03:20,800 --> 00:03:23,290
And if we have a balanced tree
is going to be logarithmic.

52
00:03:23,291 --> 00:03:27,280
So it's going to be much more efficient
to compute one specific probability.

53
00:03:28,300 --> 00:03:32,630
So more specifically what
we'll do is that we will, uh,

54
00:03:32,740 --> 00:03:36,130
essentially have all of
these nodes in the tree,

55
00:03:36,280 --> 00:03:41,110
correspond to a sigmoidal. Uh, I'll
put you in, it's in the neural nets,

56
00:03:41,111 --> 00:03:43,870
will have connections
from the hidden layer.

57
00:03:43,871 --> 00:03:47,020
So imagine we had this context
and then from this context,

58
00:03:47,320 --> 00:03:50,530
the neural net would extract the word
representations, concatenate them,

59
00:03:50,560 --> 00:03:53,590
and compute the hidden
layer off the neural net.

60
00:03:53,591 --> 00:03:56,890
So that's what this corresponds to here.
And now this hidden layer,

61
00:03:56,891 --> 00:04:01,240
we'll connect it to all of the nodes in
the internal notes in our trees and not

62
00:04:01,241 --> 00:04:03,880
the leaf nodes, which aren't the
words, but the internal notes.

63
00:04:04,150 --> 00:04:07,930
We'll connect them with the matrix of
parameters, which I'm calling v here.

64
00:04:09,250 --> 00:04:10,480
And so like I said before,

65
00:04:10,481 --> 00:04:15,370
now say we take this example where we
want the probability that cat would follow

66
00:04:15,371 --> 00:04:18,160
the context,
which is made up the word that dog.

67
00:04:18,310 --> 00:04:20,950
And that in this case,

68
00:04:21,160 --> 00:04:25,080
all I need is really the probability of,
uh,

69
00:04:25,660 --> 00:04:29,020
taking the right decision,
the right, uh, branches, uh,

70
00:04:29,021 --> 00:04:32,920
at each of the internal
notes, one, two, and five.

71
00:04:33,040 --> 00:04:36,370
So at one specifically I want the
probability of branching left.

72
00:04:36,720 --> 00:04:40,090
And then at two I want the probability
of branching, right? And then five,

73
00:04:40,091 --> 00:04:45,010
the probability of a branch,
Shane, uh, right. So this,

74
00:04:45,011 --> 00:04:48,140
uh, this shouldn't be three. They
should be, they should be fine.

75
00:04:49,450 --> 00:04:51,310
And so, uh, no,

76
00:04:51,311 --> 00:04:56,311
what we need is for the neural net to
output us this probability to provide us a

77
00:04:56,951 --> 00:04:59,230
model for these probabilities of,
of branching.

78
00:05:00,900 --> 00:05:05,670
So like I said,
we could use sigmoidal units and then,

79
00:05:05,730 --> 00:05:08,130
uh,
so essentially for this note,

80
00:05:08,131 --> 00:05:13,131
we would take the product of the row
in Matrix v that corresponds to the

81
00:05:13,501 --> 00:05:14,940
parameters of this note here,

82
00:05:14,941 --> 00:05:18,420
when multiply that by the hidden
layer and then with applied a sigmoid.

83
00:05:18,480 --> 00:05:23,480
And then we could interpret this as
the probability of say a branching,

84
00:05:23,600 --> 00:05:26,670
a right, it could be branching
left, it doesn't matter.

85
00:05:26,671 --> 00:05:27,840
This is an arbitrary choice.

86
00:05:27,841 --> 00:05:31,140
As long as we're consistent with the
way we interpret the probabilities.

87
00:05:31,440 --> 00:05:32,251
So for this example,

88
00:05:32,251 --> 00:05:36,990
we're assuming that taking v multiply by
the hidden layer and applying to sigma,

89
00:05:36,991 --> 00:05:40,950
it gives us the probability of
branching right at a given a note.

90
00:05:41,280 --> 00:05:46,280
So it means that while
initially the probability of
cat of reaching cap would be

91
00:05:47,310 --> 00:05:50,970
to, they're probably the brunching
right? A left then, right then, right

92
00:05:52,450 --> 00:05:54,940
then if I want to use my
model to validate that,

93
00:05:54,941 --> 00:05:59,300
probably I'll convert the probability of
branching as one minus the probability

94
00:05:59,301 --> 00:06:03,540
of branching. Right? And then
like I said before, I'll, uh,

95
00:06:03,620 --> 00:06:08,570
use my model to compute these
probabilities and they're
going to be computed as

96
00:06:08,750 --> 00:06:13,750
the hidden layer multiplied by the
associated row in my matrix of connections

97
00:06:14,181 --> 00:06:19,130
feet. So we see here that will be taking
the first row for the first note here,

98
00:06:19,220 --> 00:06:23,930
taking the second row for the second
node here and in the fifth row for the

99
00:06:23,931 --> 00:06:26,430
fifth, a node here in my, in my tree.

100
00:06:27,350 --> 00:06:29,960
And we were doing the same thing
for the [inaudible], the first bias.

101
00:06:29,980 --> 00:06:33,120
Second bias if fifth
bias in my bias vector B.

102
00:06:33,830 --> 00:06:35,690
So by taking the product of all of this,

103
00:06:35,720 --> 00:06:38,960
then that would give me the
probability of reaching cap.

104
00:06:38,990 --> 00:06:43,360
And I've been able to compute
it in this model using only, uh,

105
00:06:43,460 --> 00:06:45,440
uh,
using computation that scale,

106
00:06:45,441 --> 00:06:48,320
not linearly in the number
of words in my vocabulary,

107
00:06:48,321 --> 00:06:52,640
but logarithmically and that's because
I'm using a balanced tree where the depth

108
00:06:52,700 --> 00:06:57,500
of the tree is, uh, uh, in
the order of the logarithm of,

109
00:06:57,680 --> 00:07:02,660
uh, the number of words in the
vocabulary. Okay. So that's the article.

110
00:07:02,720 --> 00:07:06,220
That's the general idea for
the article output layer. Uh,

111
00:07:06,500 --> 00:07:09,920
that is very useful for, uh, in
the neural network language model.

112
00:07:14,180 --> 00:07:17,920
So now how do we actually define
this? This tree, this word Harkey, uh,

113
00:07:17,970 --> 00:07:21,950
that organizes the words and two
different leaves of a, of a treat.

114
00:07:22,910 --> 00:07:27,740
Uh, the simplest thing to do probably
is to just generate this trees randomly.

115
00:07:27,741 --> 00:07:31,460
So we'd read, we'd a essentially
generate a complete tree,

116
00:07:32,170 --> 00:07:36,050
so make it balanced so that at least
the computations and the tree is

117
00:07:36,051 --> 00:07:39,410
logarithmic. And, um, uh,

118
00:07:39,411 --> 00:07:43,160
and then we just assigned the leaves of
that tree randomly, two different words.

119
00:07:43,161 --> 00:07:46,890
So we take each word and we assigned
randomly to a different leaf in industry.

120
00:07:47,490 --> 00:07:51,920
Uh, this is likely to be suboptimal for
language modeling. It, it actually is.

121
00:07:51,921 --> 00:07:55,410
We get a worse performance
by doing this then, uh,

122
00:07:55,490 --> 00:07:59,630
by using the softmax layer. Uh,
that being said, it's, it's,

123
00:07:59,631 --> 00:08:04,380
it's not an entirely terrible, so,
um, uh, it's, uh, it against it.

124
00:08:04,440 --> 00:08:08,930
It'll give a decent results and other
approaches to actually use something good

125
00:08:08,931 --> 00:08:13,810
stick resources, say a we're net,
which is, uh, uh, essentially a,

126
00:08:13,870 --> 00:08:18,230
a data structure. Four words were
the words organized in some tree.

127
00:08:18,470 --> 00:08:23,170
And, uh, essentially the, uh, words
that are semantically related, uh,

128
00:08:23,220 --> 00:08:26,540
share certain sensors are,
are closer in, in that tree.

129
00:08:26,990 --> 00:08:29,360
So I won't go into detail
of how we're networks.

130
00:08:29,361 --> 00:08:34,361
You can look at this paper for how in
particular you can use where net to derive

131
00:08:34,941 --> 00:08:39,140
a tree for organizing all the
words and the, uh, into a tree.

132
00:08:40,010 --> 00:08:41,360
But in this paper,

133
00:08:41,510 --> 00:08:45,440
they actually reported really good speed
ups that you'd expect from using a,

134
00:08:45,441 --> 00:08:46,274
a tree,

135
00:08:46,400 --> 00:08:50,980
and they only saw a slight decrease in
performance compared to using a softmax

136
00:08:50,990 --> 00:08:54,490
layer. Uh, and then
finally, more recently, uh,

137
00:08:54,630 --> 00:08:57,420
in this paper by me and Hinton,
uh,

138
00:08:57,490 --> 00:09:02,070
a learning approach and machine learning
approach for trying to derive what this

139
00:09:02,071 --> 00:09:04,620
word Harkey should be, uh, was proposed.

140
00:09:04,770 --> 00:09:09,180
It was based on a recursive
partitioning strategy. Uh,

141
00:09:09,210 --> 00:09:13,290
so the way they would actually learn
the trees that they would first learn

142
00:09:13,320 --> 00:09:16,530
irregular model. So a,
if I wanted that models,

143
00:09:16,560 --> 00:09:18,930
if you trained it with a softmax layer,
that would be slow.

144
00:09:18,931 --> 00:09:23,931
So perhaps you could use instead a a
random tree and train a first model.

145
00:09:25,170 --> 00:09:25,830
Uh,

146
00:09:25,830 --> 00:09:30,150
that model will have trained some word
representations and then we'll try to use

147
00:09:30,151 --> 00:09:33,120
these word representations in order
to infer what the tree should be.

148
00:09:33,600 --> 00:09:34,830
And the way we'll do that,
this,

149
00:09:34,831 --> 00:09:39,831
that we'll use some
hierarchical clustering
approach where first weed at the

150
00:09:40,230 --> 00:09:41,340
root note of our tree,

151
00:09:41,341 --> 00:09:46,050
that would correspond to a cluster
which contains all of the words, uh, in,

152
00:09:46,140 --> 00:09:47,430
uh,
in our vocabulary.

153
00:09:48,180 --> 00:09:53,180
And then I'd use the word representations
a and run the a clustering algorithm

154
00:09:53,550 --> 00:09:56,640
where I would extract all the
two clusters. So for instance,

155
00:09:56,790 --> 00:10:00,300
my clustering algorithm could
identify that this forms a cluster.

156
00:10:00,301 --> 00:10:03,210
And this forms and other cluster.
Uh,

157
00:10:03,270 --> 00:10:07,950
and then I would recursively continue
doing these binary clusterings,

158
00:10:08,270 --> 00:10:11,910
uh, but over the partitioning is that
I discovered that the previous stage.

159
00:10:11,910 --> 00:10:15,390
So in this case I would rerun and
other clustering algorithm, uh,

160
00:10:15,420 --> 00:10:19,410
within this Gloucester and then
within also discussed or separately.

161
00:10:19,800 --> 00:10:23,550
So maybe I'd get these two clusters here
and then here I'd get these two other

162
00:10:23,551 --> 00:10:27,930
clusters. And then I continue like
this until in, uh, my clusters,

163
00:10:27,931 --> 00:10:31,950
our only have at most a two words.

164
00:10:33,060 --> 00:10:35,340
Then for cluster,
it has two words in it.

165
00:10:35,400 --> 00:10:40,400
Then that defines the two leaves for
that particular part of the tree and to

166
00:10:41,550 --> 00:10:46,440
extract the tree. Then I just go from
the general clusters or this one here.

167
00:10:46,470 --> 00:10:49,470
That would be the leaf. Uh,
sorry, the root of the tree,

168
00:10:49,590 --> 00:10:52,980
it's two leaves would be this
cluster and this cluster. And then,

169
00:10:52,981 --> 00:10:57,360
so for this cluster it's two leaves
would be this cluster and this cluster.

170
00:10:57,361 --> 00:11:02,160
So all of the words here would be
saying on the left, uh, part of, uh,

171
00:11:02,190 --> 00:11:06,540
that, uh, that tree accorded corresponding
to that cluster here. And this would,

172
00:11:06,570 --> 00:11:09,900
these two words would be on the right.
And then I kicked, did you like this?

173
00:11:09,901 --> 00:11:14,901
So this node in the tree would have one
of its a leaf be this node here and in

174
00:11:15,761 --> 00:11:19,770
another leaf would be an internal node
of crisp corresponding to that cluster,

175
00:11:20,280 --> 00:11:22,140
which has two leaves,

176
00:11:22,141 --> 00:11:26,320
which are these two words here and so
on for the other part of the treat.

177
00:11:26,321 --> 00:11:30,150
So we see that from this, we can extract,
uh, our Harkey, uh, for the words,

178
00:11:30,240 --> 00:11:33,780
and then we could use that
Harkey, uh, for prediction.

179
00:11:33,990 --> 00:11:37,740
And so the advantage of doing this is
that if we have good word representations

180
00:11:37,741 --> 00:11:42,510
that were pretrained using a random
tree or some other, uh, procedure,

181
00:11:42,750 --> 00:11:47,750
then this tree will more directly reflect
the similarity between these words

182
00:11:48,180 --> 00:11:52,470
putting clothes together and the
tree words to have close a word

183
00:11:52,471 --> 00:11:55,420
representations.
And then I could retrain a new model,

184
00:11:55,660 --> 00:12:00,070
a new neural network language model
that uses this tree now instead of c of

185
00:12:00,071 --> 00:12:04,090
random tree in order to perform a
neural network language modeling.

186
00:12:04,900 --> 00:12:08,050
So for more details,
I encourage you to look at this paper,

187
00:12:08,051 --> 00:12:12,040
but they show that they get similar kind
of speed ups you would get with other

188
00:12:12,041 --> 00:12:16,210
types of trees and uh, uh, but
without a performance decrease.

189
00:12:16,360 --> 00:12:19,690
So that's very encouraging. Uh,
it's a, it's a very good result.

190
00:12:20,890 --> 00:12:21,820
And before I end,

191
00:12:21,821 --> 00:12:25,720
I just want to say that this hardcore
output layer is useful where for given

192
00:12:25,721 --> 00:12:30,100
contexts, we want to evaluate
the probability of just
one word or a few words.

193
00:12:30,430 --> 00:12:34,590
If we actually wanted the full
distribution over all the words, uh,

194
00:12:34,810 --> 00:12:38,560
we wouldn't gain anything because we
would still need to get to the probability

195
00:12:38,561 --> 00:12:42,820
of branching left and right for every
node within the tree and the number of

196
00:12:42,821 --> 00:12:43,241
nodes,

197
00:12:43,241 --> 00:12:46,570
the total number of internal nodes in
the tree is going to be linear in the

198
00:12:46,810 --> 00:12:49,180
number of words.
Uh,

199
00:12:49,210 --> 00:12:53,680
and so we won't actually have gained
anything if far giving context.

200
00:12:53,680 --> 00:12:57,150
We have to compute the probability of
all the words. So it's really when for,

201
00:12:57,151 --> 00:12:59,320
since we were training,
we know what is the next word.

202
00:12:59,321 --> 00:13:02,410
Then we only need to evaluate the
probability of that next word.

203
00:13:02,620 --> 00:13:05,470
Then in this case, we get
a speed up. But otherwise,

204
00:13:05,700 --> 00:13:09,720
if we need the full distribution,
then, uh, we don't sell it.

205
00:13:09,721 --> 00:13:11,050
Get a speed up in that case,

206
00:13:11,051 --> 00:13:15,100
and we'll still scale linearly in the
number of words in the vocabulary.

207
00:13:16,030 --> 00:13:19,190
All right, so that's it about
how I call output layer.

208
00:13:19,191 --> 00:13:24,100
And now most modern no network language
models use some sort of hierarchical

209
00:13:24,101 --> 00:13:24,934
output layer.

210
00:13:25,090 --> 00:13:29,950
I've described the binary layer where
at every node you make a binary decision

211
00:13:29,951 --> 00:13:30,940
benching left and right.

212
00:13:30,941 --> 00:13:34,810
You could have other versions of that
where you would actually branch mall

213
00:13:34,950 --> 00:13:39,160
multi-way not just left and
right. Uh, but in general,

214
00:13:39,161 --> 00:13:42,460
the idea is that by doing
a hierarchical approach,

215
00:13:42,490 --> 00:13:46,460
we can get good speed ups in and
be able to apply neural nets, uh,

216
00:13:46,461 --> 00:13:48,640
for language modeling
unfair logical capillaries.

