1
00:00:00,260 --> 00:00:00,900
Okay.

2
00:00:00,900 --> 00:00:04,560
In this video we'll see
different potential choices
for the activation function

3
00:00:04,561 --> 00:00:05,394
of the neuron.

4
00:00:07,220 --> 00:00:11,240
So we've seen previously the
computation that is made in the neuron,

5
00:00:11,241 --> 00:00:13,430
and we started to
involve this Gi function,

6
00:00:13,431 --> 00:00:15,080
which we call the activation function.

7
00:00:15,350 --> 00:00:20,030
So here we'll just see different popular
choices for activation functions in the

8
00:00:20,031 --> 00:00:20,870
neural networks

9
00:00:24,340 --> 00:00:29,030
at first choice is to simply use
a linear activation function.

10
00:00:29,560 --> 00:00:30,980
Uh,
so in this case,

11
00:00:31,220 --> 00:00:36,220
the activation function g takes it's
pre activation and simply outputs it.

12
00:00:37,370 --> 00:00:40,070
So if we were to plot the
this activation function,

13
00:00:40,071 --> 00:00:42,340
you'd get a straight line like this.
Uh,

14
00:00:42,390 --> 00:00:46,940
so it doesn't perform any
squashing of the input that is, uh,

15
00:00:46,960 --> 00:00:49,980
it takes the input and
reproduces it. So, uh,

16
00:00:50,030 --> 00:00:54,440
it won't be upper bounded or lower
bounded. And partly for that reason,

17
00:00:54,441 --> 00:00:56,300
it's not a very interesting,

18
00:00:56,510 --> 00:01:00,380
it doesn't introduce any nonlinearities
in the computation of the neuron,

19
00:01:00,381 --> 00:01:04,790
which might be a useful for performing
more complicated computations

20
00:01:07,080 --> 00:01:11,750
and more interesting choices. The
sigmoid activation function. Um,

21
00:01:12,010 --> 00:01:16,560
so it takes this form a, which,
so we'll sometime note it as a,

22
00:01:16,630 --> 00:01:21,040
with the acronym, a, s,
I, g, m and a for short.

23
00:01:21,580 --> 00:01:26,580
And it's simply takes deep reactivation
and it computes this formula here.

24
00:01:26,680 --> 00:01:31,680
So it's one over one plus the
exponential of minus the pre activation.

25
00:01:32,890 --> 00:01:35,980
And so if we draw this function,

26
00:01:36,130 --> 00:01:40,540
what we see is that it will squash the
neurons a pre activation between zero and

27
00:01:40,541 --> 00:01:44,770
one. We see that all output values
are between zero and one here.

28
00:01:45,250 --> 00:01:47,650
And so the bigger the p activation,

29
00:01:47,770 --> 00:01:52,770
the more it will saturate towards one
and the smallest activation towards minus

30
00:01:53,591 --> 00:01:57,250
infinite, the more it will
saturate towards zero instead.

31
00:01:58,090 --> 00:02:01,390
So this neuron is always positive,
uh,

32
00:02:01,420 --> 00:02:05,380
because it's always strictly
greater than zero. It's a bounded,

33
00:02:05,440 --> 00:02:09,880
that is the, uh, uh, uh,
output of the neuron.

34
00:02:09,881 --> 00:02:14,350
The activation of the neuron cannot be
smaller end zero nor greater than one.

35
00:02:14,800 --> 00:02:18,310
And uh, this activation function
is a strictly increasing.

36
00:02:18,311 --> 00:02:20,650
That is the bigger the activation,

37
00:02:20,920 --> 00:02:24,490
the higher the activation
of the neuron will be.

38
00:02:26,950 --> 00:02:28,590
And other populations choices.

39
00:02:28,591 --> 00:02:32,850
The hyperbolic tangent a or tench.

40
00:02:33,180 --> 00:02:36,510
So it's a t, a n, h for short,

41
00:02:37,080 --> 00:02:41,010
and it's a slightly more complicated.
It also involves some exponential.

42
00:02:41,220 --> 00:02:43,770
So one form is this form here,

43
00:02:43,771 --> 00:02:48,560
why it's the exponential reactivation
minus the exponential of minus EPA

44
00:02:48,570 --> 00:02:53,570
activation divided by the sum of
the exponential of the reactivation,

45
00:02:54,030 --> 00:02:58,020
plus the exponential of
minus the pre activation.

46
00:02:58,500 --> 00:03:00,790
And a,
it can also be written in this form,

47
00:03:00,791 --> 00:03:04,450
which is more convenient because
it computes just one exponential.

48
00:03:04,660 --> 00:03:09,470
So a which is we used here, so it might
be more computationally efficient. And uh,

49
00:03:09,500 --> 00:03:14,110
this is just obtained by multiplying by
the exponential of the pre activation,

50
00:03:14,690 --> 00:03:19,490
the numerator and the denominator
of this formula here. Uh,

51
00:03:19,700 --> 00:03:21,100
so if you plot this function,

52
00:03:21,101 --> 00:03:26,101
we see that now we get an output which
is strained between minus one and one.

53
00:03:26,590 --> 00:03:30,580
So this neuron will say that
it squashes the neurons, uh,

54
00:03:30,640 --> 00:03:31,720
pre activation.

55
00:03:32,080 --> 00:03:35,740
So this should be pre activation
here between minus one and one,

56
00:03:36,520 --> 00:03:39,310
so it can be positive or negative.
Uh,

57
00:03:39,311 --> 00:03:42,550
it's also bounded similarly like the uh,

58
00:03:42,580 --> 00:03:46,750
sigmoid activation function and
it has also strictly increasing.

59
00:03:49,780 --> 00:03:54,160
And a final popular choice for the
activation function is what is called the

60
00:03:54,161 --> 00:03:56,230
rectified linear activation function.

61
00:03:56,680 --> 00:04:01,680
So we know that Recklin for short and
it's simply the maximum between zero and

62
00:04:04,211 --> 00:04:09,160
the pre activation. And so if we
plotted, we get a straight line here,

63
00:04:09,161 --> 00:04:13,600
like a linear function. If the input
is positive is greater than zero,

64
00:04:13,810 --> 00:04:18,670
and otherwise it's just zero. So it's
a straight line, a horizontal line,

65
00:04:19,030 --> 00:04:23,680
uh, and if the input is
negative, it always outputs zero.

66
00:04:24,520 --> 00:04:28,510
So here it's only valid by
below, uh, below by zero.

67
00:04:28,660 --> 00:04:32,230
So it's always non negative. Uh, it's uh,

68
00:04:32,231 --> 00:04:36,190
not upper bounded because the
greater the p activation here,

69
00:04:36,340 --> 00:04:40,360
the greater the output will
be. So this can grow, uh, uh,

70
00:04:40,390 --> 00:04:44,870
this always grows again, uh, converge
to the output, can converse to, uh,

71
00:04:44,980 --> 00:04:49,630
or diverged towards infinite.
If the input goes towards the,

72
00:04:49,631 --> 00:04:53,880
uh, towards, uh, the infinity and, um,

73
00:04:54,120 --> 00:04:58,450
and in practice we see that it tends
to give neurons that are sparse,

74
00:04:58,451 --> 00:05:00,520
that have sparse activity activities.

75
00:05:00,550 --> 00:05:05,380
What we mean by that is that it tends to
get neurons that are often exactly zero,

76
00:05:05,381 --> 00:05:09,820
which was not the case for the
sigmoid or the tach. The similar,

77
00:05:09,850 --> 00:05:14,320
the attention need to have
a pre activations that are,

78
00:05:15,120 --> 00:05:19,750
uh, exactly a particular value. So in
the sigmoid case, to get zero, you need,

79
00:05:20,020 --> 00:05:22,780
it needs to be minus infinite.
So it doesn't happen really.

80
00:05:23,020 --> 00:05:27,460
And for the times to be equal to zero in
the pre activation needs to be exactly

81
00:05:27,461 --> 00:05:31,130
zero. In this case, there's
this whole rage of [inaudible],

82
00:05:31,260 --> 00:05:35,770
which are the output exactly zero. So
for, for this reason, we often get,

83
00:05:35,990 --> 00:05:36,823
uh,
uh,

84
00:05:36,880 --> 00:05:41,320
a zeros as the activation of the
neuron across many different inputs.

85
00:05:41,590 --> 00:05:43,840
And so we'll say that for that reason,

86
00:05:43,841 --> 00:05:47,800
this neuron will have a sparse
activations or sparse activities.

87
00:05:48,640 --> 00:05:51,070
And so those are the
four different choices,

88
00:05:51,071 --> 00:05:54,400
public choices for the
activation function of a neuron.

