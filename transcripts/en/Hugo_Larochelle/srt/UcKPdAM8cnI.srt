1
00:00:00,500 --> 00:00:00,670
Okay.

2
00:00:00,670 --> 00:00:04,320
In this video we'll see another procedure
for addressing the overfitting issues

3
00:00:04,330 --> 00:00:06,970
in deep neural networks
known as dropout training.

4
00:00:09,540 --> 00:00:10,060
Okay.

5
00:00:10,060 --> 00:00:14,230
So we saw previously that if we are
having over fitting issues in deep neural

6
00:00:14,231 --> 00:00:16,750
networks,
we could use unsupervised retraining.

7
00:00:17,080 --> 00:00:19,950
And now we'll look at the other uh,
uh,

8
00:00:19,990 --> 00:00:24,550
procedure for finding over fitting a
known as to gastric dropout training.

9
00:00:25,820 --> 00:00:26,653
Okay.

10
00:00:27,300 --> 00:00:31,290
So to avoid overfitting, um, again,

11
00:00:31,291 --> 00:00:36,291
the idea is going to be that instead
of directly optimizing what we,

12
00:00:36,760 --> 00:00:41,160
uh, would normally optimize with
backdrop, uh, at our training loss,

13
00:00:41,370 --> 00:00:46,200
we'll try to somehow, uh,
diversity, uh, training procedures.

14
00:00:46,201 --> 00:00:49,770
So I did it does something else.
So for advice, we training,

15
00:00:49,771 --> 00:00:54,771
we had to do some unsupervised learning
as an initialization phase and for

16
00:00:54,901 --> 00:00:59,280
dropout where in a sense going
to cripple the neural network.

17
00:00:59,281 --> 00:01:03,750
Why it's training by removing
stochastically hidden units.

18
00:01:04,420 --> 00:01:07,020
Um, so in the basic
version of dropouts, uh,

19
00:01:07,021 --> 00:01:11,310
what we do is that for each hidden unit,
once we've computed it,

20
00:01:11,640 --> 00:01:15,270
we also independently,
we'll set it to zero.

21
00:01:15,271 --> 00:01:19,110
So essentially a throw
away the actual value,

22
00:01:19,111 --> 00:01:22,590
the hidden units had by say,
multiplying it by zero,

23
00:01:22,591 --> 00:01:26,700
sending it to zero with
a probability of 0.5.

24
00:01:27,420 --> 00:01:32,220
So for instance, if we add this
neural network and uh, imagine that,

25
00:01:32,540 --> 00:01:36,900
uh, we computed the first hidden
there, uh, then once we do that,

26
00:01:36,901 --> 00:01:39,570
we actually for each of the new unit,
uh,

27
00:01:39,600 --> 00:01:44,600
we'll set it to zero with a probability
of 0.5 and we're going to sample this

28
00:01:46,080 --> 00:01:50,580
possibility of sending it
to zero independently for
each hit in unit. And then,

29
00:01:50,970 --> 00:01:54,930
uh, so then some hidden hidden
units will be set to zero.

30
00:01:54,931 --> 00:01:58,170
And then we repeat this procedure for
computing. The second hidden layer,

31
00:01:58,530 --> 00:02:00,950
we compute how hidden units
and then against the Catholic,

32
00:02:00,951 --> 00:02:03,600
we will set some of them
actually to zero instead.

33
00:02:03,750 --> 00:02:07,440
And we continued this until we reached
the output layer where we don't apply

34
00:02:07,441 --> 00:02:11,490
this sort of masking noise.
And so for instance,

35
00:02:11,491 --> 00:02:16,491
one result of this sampling procedure
could be that we would be sending this one

36
00:02:16,711 --> 00:02:20,310
to zero at the first layer and this
one to zero at the second layer,

37
00:02:21,390 --> 00:02:23,600
which means really that we would,
uh,

38
00:02:23,790 --> 00:02:27,120
be computing this subset
of hidden units at the end.

39
00:02:27,121 --> 00:02:32,121
And the connections that are really used
during for propagation would correspond

40
00:02:32,401 --> 00:02:37,401
to those that do not go into or out of
the mass or dropped out hidden units.

41
00:02:39,480 --> 00:02:42,000
As you can see, this is similar to, uh,

42
00:02:42,020 --> 00:02:46,080
the type of noise we would add in
denoising auto in quarters. Uh,

43
00:02:46,081 --> 00:02:51,081
one difference is here that we're doing
this at the hidden a unit level and said

44
00:02:51,601 --> 00:02:53,190
that the input unit level,

45
00:02:53,700 --> 00:02:56,710
and the other difference is that we're
not training on autoencoder here. So what,

46
00:02:56,860 --> 00:02:57,780
when we're doing this,

47
00:02:57,781 --> 00:03:02,781
we're actually doing it while we're doing
backpropagation in supervised learning

48
00:03:02,861 --> 00:03:03,790
with a neural network.

49
00:03:05,050 --> 00:03:10,050
So the impact of this is that now the
hidden units cannot assume that all hidden

50
00:03:11,141 --> 00:03:14,050
units in the same hidden
there are going to be present.

51
00:03:14,440 --> 00:03:17,500
And so they cannot code
that do these other units.

52
00:03:17,740 --> 00:03:22,150
They cannot sort of collaborate to uh,
generate these complicated patterns,

53
00:03:22,470 --> 00:03:26,230
uh, that might be useful for fitting the
training data because now they cannot

54
00:03:26,231 --> 00:03:29,530
expect that all hidden units in the same
hidden there are going to be present.

55
00:03:30,430 --> 00:03:33,790
And so by breaking this a coaptation,
uh,

56
00:03:34,000 --> 00:03:37,720
a behavior that the hidden
units could have, um,

57
00:03:37,930 --> 00:03:42,220
the idea is that now he cheated in
unit will be forced to sort of focus on

58
00:03:42,400 --> 00:03:47,400
extracting a feature which is useful
in general and not just when used in

59
00:03:48,011 --> 00:03:50,650
combination with all hidden units.

60
00:03:50,920 --> 00:03:55,150
They still need to somehow do things that
are different. So there, there is not,

61
00:03:55,330 --> 00:03:57,790
uh, no, uh, uh,

62
00:03:57,880 --> 00:04:00,580
no sort of a collaboration
between the hidden units,

63
00:04:00,760 --> 00:04:02,860
but they cannot develop as complicated,

64
00:04:03,010 --> 00:04:05,740
quite updation patterns between
themselves because they're not,

65
00:04:05,870 --> 00:04:08,860
they're knocking out as soon that aisle
or the other hidden units are going to

66
00:04:08,861 --> 00:04:13,450
be present in some of the other hidden
units will be set to zero stochastically

67
00:04:14,440 --> 00:04:18,670
and a and the basic version, we use 0.5
as the probability of sending two zero.

68
00:04:18,671 --> 00:04:21,580
We could use another probability,
this could be hyper parameter,

69
00:04:21,730 --> 00:04:24,610
but in practice it seems
like 0.5 works really well.

70
00:04:27,980 --> 00:04:32,330
So how does that change the regular
forward and backward propagation?

71
00:04:32,370 --> 00:04:35,810
I'll go to them for training
a neural network. Um,

72
00:04:35,870 --> 00:04:39,170
well let's assume that for
each year than they are.

73
00:04:39,171 --> 00:04:41,320
We've sampled these binary masks,

74
00:04:41,330 --> 00:04:43,850
so we could do this before
doing for propagation.

75
00:04:43,851 --> 00:04:47,980
And I'll call these binary mask.
M a k work gay in Eh,

76
00:04:48,260 --> 00:04:53,260
is an index for the hidden layer will
compute our pre activation like we do bef,

77
00:04:53,450 --> 00:04:54,470
uh,
normally.

78
00:04:54,890 --> 00:04:59,660
But now what we'll do is that will change
the definition of the hidden layer by

79
00:04:59,661 --> 00:05:04,520
incorporating this
multiplication with a zero. If a,

80
00:05:04,521 --> 00:05:09,020
with properties open five decided to set
the head unit to zero. And otherwise,

81
00:05:09,021 --> 00:05:13,130
by multiplying by one, it means we
retain the value of the hidden there.

82
00:05:13,490 --> 00:05:15,410
So this idea of dropping out hidden units,

83
00:05:15,411 --> 00:05:19,460
we can just implement it by multiplying
by disbarring when you factor that we

84
00:05:19,461 --> 00:05:23,480
sample every time we do for propagation.
And next action, you Borden's.

85
00:05:23,510 --> 00:05:27,710
If we put the same
training inputs, uh, here,

86
00:05:28,060 --> 00:05:31,670
uh, uh, at two different
for propagation steps,

87
00:05:31,790 --> 00:05:34,220
we will sample different mass,

88
00:05:34,280 --> 00:05:37,280
or at least we'll sample individual masks,

89
00:05:37,281 --> 00:05:41,690
which will almost certainly be different.
Okay. So that's very important.

90
00:05:41,691 --> 00:05:42,890
The mask always changes.

91
00:05:42,891 --> 00:05:47,090
It is a samples to gasoline and that's
really the only thing that changes in the

92
00:05:47,091 --> 00:05:51,530
algorithm before when we compute the
hidden layer value of hidden unit,

93
00:05:52,220 --> 00:05:56,870
now we compute the activation back there
and then we multiply it by this binary

94
00:05:57,230 --> 00:05:58,063
mass.

95
00:05:59,500 --> 00:06:00,333
Yeah.

96
00:06:00,660 --> 00:06:03,750
And when it comes to doing
backpropagation, again,

97
00:06:03,751 --> 00:06:07,890
the only thing that's going to change
actually are two things that are going to

98
00:06:07,891 --> 00:06:08,724
change.

99
00:06:08,730 --> 00:06:13,730
The first one is that because in the
definition of h came on a swan or Hca,

100
00:06:14,610 --> 00:06:16,590
we were taking the activation,

101
00:06:16,591 --> 00:06:20,280
what used to be the activation
vector and multiplying it by a mask.

102
00:06:20,520 --> 00:06:23,810
Then when we're back propagating,
uh,

103
00:06:23,830 --> 00:06:26,310
the gradient up to the pre activation,
uh,

104
00:06:26,311 --> 00:06:30,990
we also need to multiply by the chain
because of the chain rule by the mask,

105
00:06:31,080 --> 00:06:34,920
a vector.
And so this means that at this level,

106
00:06:35,100 --> 00:06:38,400
many of the gradients,
we'll be actually set to zero.

107
00:06:38,401 --> 00:06:41,050
So there won't be a backpropagation,
uh,

108
00:06:41,080 --> 00:06:44,970
that flows through a, so
like no brainer. We'll file,

109
00:06:45,120 --> 00:06:48,570
we'll go through a hidden unit
that has been dropped out.

110
00:06:48,750 --> 00:06:51,720
So this will be the consequence of this.
And we can derive this.

111
00:06:51,720 --> 00:06:54,780
Just buy regular the
regular chain role for, uh,

112
00:06:54,960 --> 00:06:58,290
in the context of a neural network.
So that's the first thing that changes.

113
00:06:58,291 --> 00:06:59,740
And the other thing,
uh,

114
00:06:59,760 --> 00:07:04,440
which doesn't really change but we should
take into account is that this vector

115
00:07:04,441 --> 00:07:06,090
now incorporates the mask.

116
00:07:06,091 --> 00:07:09,450
So this is the mass as
the activation patterns.

117
00:07:09,900 --> 00:07:12,780
That's what we should not forget
that here, this is not the, uh,

118
00:07:12,781 --> 00:07:15,900
this is a vector that contains
a, a lot of zero. On average,

119
00:07:15,901 --> 00:07:18,360
50% of its entries are going to be zero,

120
00:07:18,361 --> 00:07:23,361
because it's the what used to be the
activation vector multiplied by the mask.

121
00:07:24,260 --> 00:07:25,920
Okay. But that's all
that changes. Otherwise,

122
00:07:25,921 --> 00:07:29,960
the regular backpropagation
algorithm, uh, remains, uh,

123
00:07:30,000 --> 00:07:33,450
essentially the same. It's
just the incorporation of
these masks that drops out.

124
00:07:33,480 --> 00:07:34,313
He didn't units.

125
00:07:38,200 --> 00:07:41,080
So that's for training.
When we're training, uh,

126
00:07:41,170 --> 00:07:46,170
we have to sample these mass for every
training example and a yet a slightly

127
00:07:46,571 --> 00:07:50,770
adapt the forward and
backward, uh, equations. Uh,

128
00:07:50,771 --> 00:07:53,770
but at test time, instead of
getting something that's to castic,

129
00:07:53,800 --> 00:07:58,390
what we will do is that we will
replace the masks by their expectation.

130
00:07:58,630 --> 00:08:03,160
And so for instance,
if we sampled republic the 0.5,

131
00:08:03,370 --> 00:08:07,850
we will actually set the hidden unit to
zero and drop it out. Then this, the,

132
00:08:07,851 --> 00:08:12,010
the mask at test time would be a vector
filled with the value of 0.5 because

133
00:08:12,011 --> 00:08:15,930
that's the expectation of
that binary vector. Um,

134
00:08:16,600 --> 00:08:19,540
so actually in the
single hidden layer case,

135
00:08:19,750 --> 00:08:24,370
we can show that this is equivalent
to taking a decision at test time,

136
00:08:24,610 --> 00:08:29,530
which is based on the geometric
average of all neural networks,

137
00:08:30,040 --> 00:08:34,990
single hidden layer neural networks
with uh, all these, uh, plus possible.

138
00:08:35,260 --> 00:08:36,670
Uh, Byron, we masks.

139
00:08:36,730 --> 00:08:41,730
So it's as if we're building this hand
symbol of an exponential number of neural

140
00:08:42,341 --> 00:08:46,360
networks that way combining together
through each or geometric average. So,

141
00:08:46,361 --> 00:08:50,470
just as a reminder, if we have
a number, a and a number be,

142
00:08:50,471 --> 00:08:52,180
if we're taking the geometric average,

143
00:08:52,360 --> 00:08:56,490
it would just be described route up
the product. And just in general,

144
00:08:56,760 --> 00:09:01,340
if you have say three
numbers, then it would be, uh,

145
00:09:01,380 --> 00:09:03,840
the product to the one over three.

146
00:09:03,990 --> 00:09:06,420
So that's the geometric average.

147
00:09:07,350 --> 00:09:11,190
And so we can think of it as
an average in the log space.

148
00:09:11,191 --> 00:09:14,940
Or if I took the log of this would
just be a one over three of the,

149
00:09:14,941 --> 00:09:16,620
some of the lug of these values.

150
00:09:17,670 --> 00:09:21,450
And then we take the exponential back
and that gives us our geometric average.

151
00:09:22,100 --> 00:09:25,910
And so, uh, intuitively
we in machine learning,

152
00:09:25,911 --> 00:09:29,950
we know that the best systems
that rely on machine learning, uh,

153
00:09:29,990 --> 00:09:33,700
or if you want to improve the, the
qualities of the system, one fairly raw,

154
00:09:33,740 --> 00:09:37,270
reliable way of doing that is to
construct and then assemble of men.

155
00:09:37,270 --> 00:09:39,410
We train models.
Well,

156
00:09:39,411 --> 00:09:43,260
we can think of dropout
training as doing this, uh,

157
00:09:43,370 --> 00:09:44,630
in a very efficient way.

158
00:09:44,631 --> 00:09:47,660
We don't actually have to train
an exponential number of models.

159
00:09:47,780 --> 00:09:52,470
We just get them as a byproduct of the
training procedure. Uh, at test time,

160
00:09:53,150 --> 00:09:55,130
uh,
by just taking a,

161
00:09:55,140 --> 00:09:59,780
replacing the mask by this 0.5
probable that this factor of,

162
00:09:59,840 --> 00:10:02,970
of value with the value
of 0.5 in every entry.

163
00:10:03,510 --> 00:10:07,320
That's only true in the single hidden
layer case for multiple hidden there.

164
00:10:07,321 --> 00:10:12,070
This story doesn't really follow
through, but we still see in, uh,

165
00:10:12,340 --> 00:10:13,080
that it,
uh,

166
00:10:13,080 --> 00:10:17,460
we get much better generalization with
this we could combining with combined

167
00:10:17,461 --> 00:10:20,790
this with unsupervised training so we
could do unsupervised for training and

168
00:10:20,791 --> 00:10:25,550
then instead of using regular backup
proper would use dropout backpropagation.

169
00:10:26,670 --> 00:10:30,090
And, um, in general, this procedure, uh,

170
00:10:30,180 --> 00:10:34,950
either with unsurprised retraining of
also alone, uh, allows us to get very,

171
00:10:34,951 --> 00:10:38,400
very good data results in many datasets.
I won't go over them,

172
00:10:38,401 --> 00:10:42,980
but you should consult this paper
by Jeff Hinton and colleagues, uh,

173
00:10:43,020 --> 00:10:47,520
that provide a lot of evidence that this
is a very good, better way of training,

174
00:10:47,640 --> 00:10:51,750
uh, of, uh, training a neural
network to get good a generalization.

175
00:10:52,260 --> 00:10:54,900
And, um, uh,

176
00:10:54,901 --> 00:10:58,590
and I would also add that one disadvantage
of dropout is that training can be

177
00:10:58,591 --> 00:11:02,610
slower. Uh, but it again, it as, uh,

178
00:11:02,640 --> 00:11:06,570
if you're willing to put more
computational time in training,

179
00:11:06,780 --> 00:11:07,730
then Eh,

180
00:11:07,760 --> 00:11:12,480
what we find is that often it will indeed
yield with much better results at a,

181
00:11:12,490 --> 00:11:15,820
at test time. Alright. So
that's it for dropout training.

