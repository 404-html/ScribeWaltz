1
00:00:00,900 --> 00:00:04,750
In this video, we're going to see
another type of deep neural network, uh,

2
00:00:04,950 --> 00:00:08,610
which is at the origin of
the recent advances in, uh,

3
00:00:08,640 --> 00:00:10,610
training deep neural networks.
Uh,

4
00:00:10,750 --> 00:00:13,620
this deep neural network is
known as the deep belief network.

5
00:00:15,900 --> 00:00:20,650
So the deep belief network is I
think, particularly important to, uh,

6
00:00:20,670 --> 00:00:21,810
look at,
uh,

7
00:00:21,840 --> 00:00:26,250
because it's actually at the origin of
the unsupervised layer wise be training

8
00:00:26,251 --> 00:00:30,570
procedure that we've seen for a
deep feet forward, no networks.

9
00:00:31,470 --> 00:00:35,760
Um, and so let's look at what
it is. It's going to, uh,

10
00:00:35,761 --> 00:00:40,761
have a see a lot of important concepts
in a training deep neural networks that

11
00:00:40,950 --> 00:00:42,990
are probabilistic in nature.

12
00:00:44,070 --> 00:00:49,050
So a deep belief network or
DBN is a generative model.

13
00:00:49,800 --> 00:00:53,340
Uh, it can be an unsupervised, entirely
on supervised generative model,

14
00:00:53,670 --> 00:00:58,670
which is going to mix undirected
and directed interactions
between the variables

15
00:01:00,571 --> 00:01:05,040
that constitute either the input
layer or visible layer, uh,

16
00:01:05,250 --> 00:01:06,930
or all the hidden layer.

17
00:01:06,930 --> 00:01:10,920
So here we have an example of
a DBM with three hidden layer.

18
00:01:11,870 --> 00:01:14,280
And as we see,
we have undirected connections here,

19
00:01:14,281 --> 00:01:17,820
but we have directed connections
or interactions here.

20
00:01:19,140 --> 00:01:20,640
So in a DBN,

21
00:01:20,970 --> 00:01:24,780
the top two layers will
always form an RBM.

22
00:01:25,140 --> 00:01:27,270
So in other words, uh, in this case,

23
00:01:27,720 --> 00:01:32,720
the distribution over h two and h three
is going to be an RBM with undirected

24
00:01:33,990 --> 00:01:34,920
interactions.

25
00:01:36,090 --> 00:01:41,090
And now the other layers are going to
form a Bayesian network instead with

26
00:01:41,881 --> 00:01:43,470
directed interactions,

27
00:01:43,920 --> 00:01:48,840
specifically the conditional distributions
of the units in these layers,

28
00:01:48,841 --> 00:01:52,470
given the layer above is
going to take this form.

29
00:01:53,070 --> 00:01:57,870
Uh, so this is going to correspond to the
probabilistic model associated with the

30
00:01:57,871 --> 00:02:02,340
logistic regression model.
That is the probability of say,

31
00:02:02,341 --> 00:02:07,110
a hidden unit in the first hidden
layer in this case or a visible layer,

32
00:02:07,290 --> 00:02:10,560
uh, visible unit. And the
visible layer is going.

33
00:02:10,620 --> 00:02:14,820
So the probability of either of these
units to be equal to one is going to be

34
00:02:14,821 --> 00:02:19,140
the sigmoid applied on a linear
transformation of the layer above.

35
00:02:19,200 --> 00:02:24,120
So for each one that's going to be a lane
and transformation of h two and four x,

36
00:02:24,121 --> 00:02:27,510
the visible layer, it's going to be
a linear transformation of a h one.

37
00:02:28,860 --> 00:02:33,240
So, uh, and when we have units
that interact in this way, uh,

38
00:02:33,241 --> 00:02:36,030
we call such a model as
sigma it belief networks.

39
00:02:36,031 --> 00:02:41,031
So that's a model that was known before
dbs and a which was called a similar

40
00:02:41,790 --> 00:02:45,810
belief network.
So to sum up in this model,

41
00:02:45,811 --> 00:02:48,600
the top two layers are
going to form an RBM.

42
00:02:49,320 --> 00:02:53,730
While the bottom layers are going
to form a sigma it belief network.

43
00:02:54,360 --> 00:02:58,050
And think about how this model works.
For instance,

44
00:02:58,140 --> 00:03:00,870
generate data to generate data from a DBN,

45
00:03:00,880 --> 00:03:05,290
you will need to do gift sampling
between the top two layers, uh,

46
00:03:05,291 --> 00:03:09,770
for a very long time. Uh, and
then, uh, once you've converted it,

47
00:03:09,800 --> 00:03:14,620
equilibrium distribution and h two is
sample from, uh, it's uh, the actual,

48
00:03:14,980 --> 00:03:18,880
uh, prior, uh, p of h two and
h three. Then from age two,

49
00:03:18,881 --> 00:03:23,620
you would directly generate h one
using this, the gastric process,

50
00:03:23,621 --> 00:03:24,610
these equations.

51
00:03:24,880 --> 00:03:29,080
And then from h one you would generate
directly x and that would give us an

52
00:03:29,081 --> 00:03:33,780
observation or a sample
of the input layer x, uh,

53
00:03:33,880 --> 00:03:35,650
from a deep belief network.

54
00:03:36,260 --> 00:03:36,710
Okay.

55
00:03:36,710 --> 00:03:40,340
Okay. So, so sampling from this model
requires a lot of good sampling.

56
00:03:40,490 --> 00:03:44,630
Add the top two layers. And then once
we've reached equivalent and distribution,

57
00:03:44,631 --> 00:03:49,370
then we can a sample down, then obtain
a sample from the visible layer.

58
00:03:50,480 --> 00:03:54,170
So that's generative process
associated with a deep belief networks.

59
00:03:54,710 --> 00:03:58,910
So notice that a DBF is
not a feed forward network.

60
00:03:58,911 --> 00:04:02,690
So sometimes in the literature you
see some people calling, uh, you know,

61
00:04:02,691 --> 00:04:06,650
stacking different rbms,
uh, a deep belief networks.

62
00:04:06,800 --> 00:04:08,700
I prefer to, uh, and, and,

63
00:04:08,710 --> 00:04:11,630
and if you just do that to initialize
a feed forward neural network,

64
00:04:11,631 --> 00:04:13,910
I would not personally called
this a deep belief network.

65
00:04:13,911 --> 00:04:17,570
I would just call this unsupervised free
trading four feet forward, no network.

66
00:04:17,571 --> 00:04:22,571
So a DBN is really a specific model
where the hidden units are stochastic,

67
00:04:22,641 --> 00:04:26,280
there are random variables.
They're binary and uh,

68
00:04:26,330 --> 00:04:30,550
they have this special structure where
the top two layers are an RBM formula in

69
00:04:30,590 --> 00:04:34,760
RBM and the bottom layers, uh,
for may submit belief network.

70
00:04:38,310 --> 00:04:43,310
So more specifically then the joint
distribution over the input layer and our

71
00:04:43,921 --> 00:04:45,800
three hidden layers for three hidden,

72
00:04:45,850 --> 00:04:50,850
they were a case is going to be a prior
over h two and h three which is going to

73
00:04:51,271 --> 00:04:52,860
take this form here.
So that's,

74
00:04:52,861 --> 00:04:57,240
we recognize here the
probability distribution of a
restricted Boltzmann machine

75
00:04:57,241 --> 00:05:01,710
where we have our weights between h two
and h three and then the biases for the

76
00:05:01,800 --> 00:05:03,780
um, uh, two layers.

77
00:05:04,020 --> 00:05:08,070
And then we have this marginalization
as this a normalization constant.

78
00:05:08,970 --> 00:05:11,000
So we have this prior and then uh,

79
00:05:11,040 --> 00:05:15,690
we have a times p of h one given h two.

80
00:05:16,280 --> 00:05:16,570
Okay.

81
00:05:16,570 --> 00:05:20,340
So that's the part of the
sigma belief network that uh,

82
00:05:20,490 --> 00:05:24,220
corresponds did it conditional of h one
given HD. So we notice here for instance,

83
00:05:24,221 --> 00:05:28,570
that each one is independent of h three,

84
00:05:28,600 --> 00:05:30,130
given h two in this model.

85
00:05:30,940 --> 00:05:33,910
And then we multiply
by p of x given h one.

86
00:05:33,911 --> 00:05:38,020
So x is a sample from each one using
the same weight belief network,

87
00:05:38,140 --> 00:05:41,830
conditional distribution we've seen
in the previous slide. And so again,

88
00:05:41,860 --> 00:05:46,090
x is only dependent on each
one. Uh, if, if we know h one,

89
00:05:46,091 --> 00:05:48,340
it does not depend on
either h two or h three.

90
00:05:49,100 --> 00:05:49,460
Okay.

91
00:05:49,460 --> 00:05:53,790
Okay. And also in the, uh,
as sig my belief network, uh,

92
00:05:53,870 --> 00:05:58,000
the full distribution of h
one given each to, uh, um,

93
00:05:58,070 --> 00:06:02,390
in fact rises into individual conditional.
So in other words, given age, do,

94
00:06:02,570 --> 00:06:05,870
each of the hidden units in the
first hidden layer are independent.

95
00:06:06,050 --> 00:06:09,470
And similarly for a
divisible layer given h one,

96
00:06:10,820 --> 00:06:15,450
if one of to look to get perhaps a more
intuitive idea of how a DBM models the

97
00:06:15,451 --> 00:06:16,160
data.

98
00:06:16,160 --> 00:06:21,160
You can look at this demo here online
where a deep belief networks was trained

99
00:06:21,320 --> 00:06:26,300
on emptiness digits. It's a special
type of deep belief network where the,

100
00:06:26,390 --> 00:06:31,130
um,
prior here actually also involves a label.

101
00:06:31,131 --> 00:06:34,970
Why? So in this case, instead
of just being unsupervised, uh,

102
00:06:34,971 --> 00:06:36,290
where we only model x,

103
00:06:36,291 --> 00:06:41,291
we would also model a labeled y and
the labeled why would be introduced by

104
00:06:41,481 --> 00:06:45,980
connecting it to the top most hidden,
they are age three.

105
00:06:46,520 --> 00:06:48,620
So you can look at that at this link here.

106
00:06:48,800 --> 00:06:52,880
It's pretty interesting and actually
quite impressive how good the samples can

107
00:06:52,881 --> 00:06:57,290
be from this. Uh, DBM a
model train on feminist.

108
00:06:58,790 --> 00:06:59,840
And,
um,

109
00:07:00,290 --> 00:07:05,290
what makes this model interesting is
that much like four deep p four node

110
00:07:05,381 --> 00:07:09,680
network, uh, training at
DBN is a hard problem.

111
00:07:10,340 --> 00:07:13,420
And A, actually, it turns out that, uh,

112
00:07:13,560 --> 00:07:17,780
a good initialization can play a really
crucial role in the quality of results

113
00:07:17,781 --> 00:07:22,080
we can get. And this
is in fact how the, uh,

114
00:07:22,130 --> 00:07:26,210
greedy layer wise be training procedure
was developed as a way of initializing

115
00:07:26,420 --> 00:07:28,760
better the parameters of
a deep belief network.

116
00:07:29,120 --> 00:07:31,670
And then after that it was noticed
that we could just use the same

117
00:07:31,671 --> 00:07:33,890
initialization four feet
for neural networks.

118
00:07:33,950 --> 00:07:37,090
And that was also working
really well for, uh, on, uh,

119
00:07:37,130 --> 00:07:39,650
initializing a feed
forward neural networks.

120
00:07:40,610 --> 00:07:43,890
So that's why we're going to talk
about deep belief networks mostly in,

121
00:07:43,891 --> 00:07:47,560
in this course so that we get an
idea of where this came about. Uh,

122
00:07:47,720 --> 00:07:48,471
how this came about,

123
00:07:48,471 --> 00:07:53,060
this procedure of stacking restrictive
bolsa machines for a pre training,

124
00:07:53,061 --> 00:07:53,900
a neural network.

125
00:07:57,470 --> 00:08:02,470
So the idea for this
initialization procedure was that,

126
00:08:03,380 --> 00:08:05,030
uh,
in order to train,

127
00:08:05,031 --> 00:08:10,031
say a three hidden there DBN we would
actually go from a one hidden layer DPN.

128
00:08:12,380 --> 00:08:13,400
So I want him later.

129
00:08:13,430 --> 00:08:18,430
DBN would just be an RBM because if we
take the top two layers and make them an

130
00:08:18,501 --> 00:08:22,370
RBM that there are no other layers
left to form a sigma belief networks.

131
00:08:22,371 --> 00:08:25,370
So this would be just
the RBM part of the DBN.

132
00:08:25,430 --> 00:08:30,430
So that's a one hidden layer VPN and then
we train that for a while and then we

133
00:08:30,831 --> 00:08:35,720
would use it to initialize a two
hidden layer DBM where would take the

134
00:08:35,721 --> 00:08:36,590
parameters here,

135
00:08:36,591 --> 00:08:41,591
would use those to initialize
the parameters here in
the two hidden layer deep

136
00:08:42,321 --> 00:08:43,190
belief network.

137
00:08:43,640 --> 00:08:48,640
And then we would be left at a
training this top part of the DPN,

138
00:08:49,491 --> 00:08:54,110
the RBM part of the two hidden layer BBN.
Um,

139
00:08:54,350 --> 00:08:55,910
so,
uh,

140
00:08:56,020 --> 00:09:00,180
and by keeping these weights fixed and
that's how we would sort of initialize

141
00:09:00,181 --> 00:09:04,230
the parameters of this layer.
And then to go from a to B,

142
00:09:04,420 --> 00:09:06,420
a hidden layer,
two or three hidden there,

143
00:09:06,421 --> 00:09:11,421
DBN would again use these weights to
initialize that part of the three hidden

144
00:09:12,061 --> 00:09:12,930
there at DBN.

145
00:09:13,260 --> 00:09:17,580
These weights to initialize
that part of the,

146
00:09:18,010 --> 00:09:18,960
uh,

147
00:09:18,990 --> 00:09:23,850
300 layer DBN and then we would be left
with a training these weights here,

148
00:09:24,240 --> 00:09:27,810
um, uh, to get a good
initialization for these weights.

149
00:09:28,260 --> 00:09:31,500
And it would repeat that for a little
while for as many hidden layers as well.

150
00:09:31,530 --> 00:09:32,430
So if there was three,

151
00:09:32,431 --> 00:09:35,520
would just stop there and then there
would be a fine tuning procedure,

152
00:09:35,521 --> 00:09:38,760
which is not backpropagation.
It's known as the up down Algorithm,

153
00:09:38,761 --> 00:09:43,050
which we won't see, but you can
consult in the original publication.

154
00:09:44,340 --> 00:09:49,320
So that's just the high level, uh,
description of the pre training procedure,

155
00:09:49,500 --> 00:09:53,940
uh, how it applies in the context
of a deep belief network. And,

156
00:09:54,390 --> 00:09:56,400
uh, before we look at the
details of this procedure,

157
00:09:56,401 --> 00:10:00,420
I just want to give a
quick intuition, uh, of,

158
00:10:00,480 --> 00:10:04,440
of what we're actually going to
try to achieve with this procedure.

159
00:10:05,160 --> 00:10:10,160
So the idea is that if we've
trained a one a hidden,

160
00:10:10,861 --> 00:10:13,380
they are [inaudible] or
in other words in Rbm,

161
00:10:13,680 --> 00:10:17,410
then we have a model on p of x,
which uh,

162
00:10:17,550 --> 00:10:22,550
it's just the marginalization
of the hidden layer for
the joint p of x and h one

163
00:10:22,770 --> 00:10:24,300
which is defined by an RBM.

164
00:10:25,710 --> 00:10:30,630
Now one thing we could do at this
point is to notice that, well, okay,

165
00:10:30,631 --> 00:10:35,631
this p of x and h one that's really p
we can write it as p of x given h one

166
00:10:36,870 --> 00:10:41,130
times p of h one and in an RBM,

167
00:10:41,131 --> 00:10:46,131
the parameters that are involved in the
formula for this conditional are also

168
00:10:46,501 --> 00:10:51,501
involved in the prior over h one they're
both present in both this conditional

169
00:10:51,961 --> 00:10:53,240
and the prior over h one.

170
00:10:54,120 --> 00:10:57,600
And so the idea when we were passing
from one hidden layer to 200 layers is to

171
00:10:57,601 --> 00:11:00,300
say, well, let's separate these two parts.

172
00:11:00,301 --> 00:11:05,301
Let's make the parameters here and the
parameters here different and actually

173
00:11:05,551 --> 00:11:09,510
let's keep those parameters fixed to
the values we found by training a one

174
00:11:09,511 --> 00:11:11,040
hidden layer RBM.

175
00:11:11,490 --> 00:11:16,490
And then that's only adapt the parameters
dot are now and p of h one and four P

176
00:11:17,011 --> 00:11:20,650
of h one we'll use an RBM,
uh, which corresponds.

177
00:11:20,700 --> 00:11:22,440
So this would be this part here.

178
00:11:22,830 --> 00:11:27,830
This would be a new RBM which with its
own weights and a to get p of h one and

179
00:11:28,231 --> 00:11:30,270
it would need to marginalize over age two.

180
00:11:30,510 --> 00:11:35,510
So that here would be p of h one and
but it would be when we're at the two

181
00:11:38,371 --> 00:11:39,390
hidden layer DBN,

182
00:11:39,391 --> 00:11:43,260
it would be an RBM with its own parameters
that are separate from the parameters

183
00:11:43,261 --> 00:11:48,261
that RP and p of x given h one unlike
in the RBM where the parameters between

184
00:11:48,901 --> 00:11:50,580
the two parts are actually tied.

185
00:11:51,870 --> 00:11:56,870
And then we repeat this procedure for
p h one and h two we also noticed that

186
00:11:58,241 --> 00:12:02,530
while Pfh one given h two that's p
of h one, sorry poh one Nah Dude,

187
00:12:02,531 --> 00:12:07,531
that's poh one given h
two times a prior of HD.

188
00:12:07,960 --> 00:12:12,430
If this as in this two hidden layer RBM,
if the DPN,

189
00:12:12,431 --> 00:12:15,850
if this corresponds to an RBM, then
these parameters would be tied. Well,

190
00:12:15,851 --> 00:12:20,410
once we fall into the three hidden layer
DBN we actually untied those parameters.

191
00:12:20,411 --> 00:12:23,010
We're going to have different
parameters here. And, uh,

192
00:12:23,040 --> 00:12:27,580
these are going to be different parameters
which are going to occur as fun to

193
00:12:27,581 --> 00:12:30,460
the, um, parameters of an RBM,

194
00:12:30,461 --> 00:12:35,080
which has its own third hidden there
and where we marginalize it out.

195
00:12:35,290 --> 00:12:37,810
That's how I would define
pure h two in this model here.

196
00:12:38,200 --> 00:12:41,410
And it's going to have its own parameters
that are different from those here.

197
00:12:42,070 --> 00:12:42,903
Okay.

198
00:12:43,120 --> 00:12:48,120
So that's sort of the high level idea
behind stacking rbms in the context of the

199
00:12:48,231 --> 00:12:49,150
belief networks.

200
00:12:49,750 --> 00:12:54,170
And then the following Dubios we'll
look at more of the details for, uh,

201
00:12:54,250 --> 00:12:58,690
how we can actually perform this and
guarantee that we're doing effective

202
00:12:58,691 --> 00:13:00,760
training.
We are effectively improving the model.

203
00:13:01,300 --> 00:13:02,650
And so in the next video specifically,

204
00:13:02,651 --> 00:13:06,070
we'll look at the general
tool for training, uh,

205
00:13:06,100 --> 00:13:10,720
graphical models with a hidden variables
known as the variational bound,

206
00:13:10,721 --> 00:13:15,670
which is going to be a tool very useful
for, uh, designing this and, and, uh,

207
00:13:15,730 --> 00:13:20,490
properly, uh, designing this greedy
layer wise pre-training with rbms.

