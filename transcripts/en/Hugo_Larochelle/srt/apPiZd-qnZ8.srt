1
00:00:00,810 --> 00:00:04,410
In this video, we'll formerly introduced
the multi layer neural network.

2
00:00:06,030 --> 00:00:10,290
We seen previously that there are
certain problems that a single artificial

3
00:00:10,291 --> 00:00:13,680
neuron can not model.
Well, uh, because, uh,

4
00:00:13,681 --> 00:00:17,250
it's a problem that which is saying
a classification setting that is not

5
00:00:17,251 --> 00:00:19,680
linearly separable.
However,

6
00:00:19,740 --> 00:00:23,240
however we've seen that
we can actually take a,

7
00:00:23,241 --> 00:00:26,690
our input representation
and try to instead, uh,

8
00:00:26,730 --> 00:00:31,140
transform it in some way by applying
some perhaps simple transformation like

9
00:00:31,141 --> 00:00:36,120
applying he and in this case
here, the end function over, uh,

10
00:00:36,150 --> 00:00:41,150
to transform version of the input to
obtain the new representation of our input

11
00:00:41,641 --> 00:00:44,190
vector,
which is now linearly separable.

12
00:00:45,330 --> 00:00:46,980
And if this,

13
00:00:47,070 --> 00:00:52,070
the computation of this representation
can be made by artificial neurons,

14
00:00:52,320 --> 00:00:56,790
then this is suggesting a more complicated
model for more complicated problems,

15
00:00:56,970 --> 00:01:01,970
which would consistent first
repor first computing a set or a,

16
00:01:02,950 --> 00:01:07,890
a number of artificial neurons that
will compute this new representation.

17
00:01:08,340 --> 00:01:11,220
And then connecting those
neurons to an output neuron,

18
00:01:11,221 --> 00:01:13,540
which would do the rest of the job and,
uh,

19
00:01:13,560 --> 00:01:16,860
finished a computation of are
more complicated function.

20
00:01:17,400 --> 00:01:20,120
And so this will be the
inspiration behind, uh,

21
00:01:20,190 --> 00:01:23,550
the use and development of
multi layer neural networks.

22
00:01:25,560 --> 00:01:30,560
So let's look at this idea more formally
in the case of a single hidden layer.

23
00:01:31,830 --> 00:01:32,970
Uh, so, uh,

24
00:01:32,971 --> 00:01:37,140
what we mean by the hidden layer is
the computation of this representation,

25
00:01:37,141 --> 00:01:40,590
which will make the problem
more, uh, linearly separable.

26
00:01:41,460 --> 00:01:45,090
And so in this single layer
neural network will, we'll have a,

27
00:01:45,091 --> 00:01:48,690
the first part of the computation
which will compute the representation.

28
00:01:49,080 --> 00:01:53,730
And then finally the computation
of our output unit, which will, uh,

29
00:01:53,790 --> 00:01:58,230
perform the, um, the computation of
our, say, our binary classification.

30
00:01:59,150 --> 00:02:03,420
The more formally when we're computing
the hidden layer, we will compute again,

31
00:02:03,450 --> 00:02:04,980
a pre activation function.

32
00:02:05,550 --> 00:02:10,260
And not in this hidden layer we see
here we might have several, uh, neurons,

33
00:02:10,290 --> 00:02:12,870
not just a single neuron. And so, uh,

34
00:02:12,900 --> 00:02:15,990
each of the IAF neuron,

35
00:02:16,050 --> 00:02:19,230
which will I will call a
WHO's activation function,

36
00:02:19,231 --> 00:02:21,540
the outfit activation will be h of X.

37
00:02:21,600 --> 00:02:26,010
I will be connected to
all potential inputs.

38
00:02:26,730 --> 00:02:30,750
And so the connection with between the,
uh,

39
00:02:30,840 --> 00:02:35,840
I have hidden a neuron with the JF input
will be a contained in some matrix,

40
00:02:40,080 --> 00:02:44,430
which we will call w and tickets
opponent in parenthesis one.

41
00:02:44,700 --> 00:02:49,330
So this is to refer to the case where
we have just a single, a hidden layer.

42
00:02:49,331 --> 00:02:53,190
So doubly one is the weight
Matrix, uh, for the same, uh,

43
00:02:53,220 --> 00:02:54,420
the first hidden layer.

44
00:02:55,660 --> 00:03:00,660
And similarly we will have biases and so
bias of the iff hidden unit will be B I

45
00:03:06,940 --> 00:03:08,350
and uh,
uh,

46
00:03:08,351 --> 00:03:12,070
the one here in the exponent is to
refer to the first hidden layer again.

47
00:03:12,910 --> 00:03:16,330
So whenever we are computing
the pre activation,

48
00:03:16,331 --> 00:03:19,810
we will actually have a
vector of reactivation, uh,

49
00:03:19,811 --> 00:03:24,580
where the IFL element of that
vector will be the IFP, uh,

50
00:03:24,600 --> 00:03:28,690
activation for the IMF neuron
and that a hidden layer.

51
00:03:29,590 --> 00:03:32,620
And so for the I of effector,
sorry,

52
00:03:32,670 --> 00:03:37,180
dif pre activation will get the IMF,
uh,

53
00:03:37,300 --> 00:03:37,960
uh,

54
00:03:37,960 --> 00:03:42,960
bias plus the weighted combination
of all inputs j and where x j will be

55
00:03:46,181 --> 00:03:51,181
multiplied by its connection with the
of a neuron and a if we want to perform

56
00:03:53,141 --> 00:03:57,130
this computation to want to perform
this computation for each hidden unit.

57
00:03:57,250 --> 00:03:57,761
So in fact,

58
00:03:57,761 --> 00:04:02,761
we can write it into matrix form to
obtain directly deep reactivation vector.

59
00:04:03,580 --> 00:04:08,410
And, uh, this would only essentially
correspond to taking the vector x,

60
00:04:08,620 --> 00:04:11,260
multiplying it by our matrix w one,

61
00:04:11,950 --> 00:04:16,150
and then adding the whole vector
of biases B one. So indeed,

62
00:04:16,151 --> 00:04:21,100
if we take x and multiply
it by W, um, we'll, uh,

63
00:04:21,101 --> 00:04:23,510
we'll, we'll, we'll get
this, that for the iff role,

64
00:04:23,530 --> 00:04:28,530
we'll get a term which corresponds to the
sum of all the elements over that role

65
00:04:31,300 --> 00:04:36,300
of matrix w one multiplied by the
corresponding elements in the vector X.

66
00:04:36,700 --> 00:04:39,880
Okay. So we see that this, and
this is actually equivalent,

67
00:04:41,480 --> 00:04:46,060
uh, then to compute the
hidden layer activation, uh,

68
00:04:46,061 --> 00:04:51,061
we simply apply our activation function
over all the elements in the vector.

69
00:04:51,991 --> 00:04:54,220
So this, uh, here in my notation,

70
00:04:54,221 --> 00:04:59,221
I'm assuming we are doing an element
wise application of the activation

71
00:04:59,531 --> 00:05:00,171
function,
Jay,

72
00:05:00,171 --> 00:05:04,740
that we've chosen chosen either the
sigmoid or the Tange or rectified,

73
00:05:04,741 --> 00:05:05,800
linear and so on.

74
00:05:06,820 --> 00:05:10,210
And then finally we get
an output activation,

75
00:05:11,140 --> 00:05:16,140
which I will call or I will note
f of x by taking the hidden layer.

76
00:05:17,950 --> 00:05:22,010
So this hidden layer, so does a
parenthesis missing missing here, a,

77
00:05:22,060 --> 00:05:26,890
we'll call it h one of x again one,
because we have a single hidden layer.

78
00:05:27,310 --> 00:05:32,310
So we'll take that vector of hidden units
and then we'll multiply it by double u

79
00:05:33,161 --> 00:05:33,940
two,

80
00:05:33,940 --> 00:05:38,940
which is going to be the vector of
activations between the output and all the

81
00:05:39,431 --> 00:05:40,264
hidden units.

82
00:05:40,390 --> 00:05:45,390
So w to y here is the weight of the
connection between the output unit and the

83
00:05:50,380 --> 00:05:54,040
hidden units.
So we compute that,

84
00:05:54,041 --> 00:05:58,790
then we add the bias of the opportunity
and then we will also have a uh,

85
00:05:58,791 --> 00:05:59,930
activation function.

86
00:06:00,170 --> 00:06:04,490
And I'm going to know that o
for output activation function.

87
00:06:04,820 --> 00:06:09,170
And this is because we will typically
make different choices for the output

88
00:06:09,171 --> 00:06:13,390
activation function. Then, uh,
for the hidden units in the neuron

89
00:06:13,440 --> 00:06:14,273
network.

90
00:06:18,150 --> 00:06:22,350
So, uh, if we want to do
binary classification,

91
00:06:22,351 --> 00:06:26,940
we can just use as the output activation,
the sigmoid activation function.

92
00:06:27,210 --> 00:06:29,130
And so this would correspond to the,

93
00:06:29,370 --> 00:06:33,270
this means that at the output
we would get the probability of,

94
00:06:33,690 --> 00:06:37,230
uh,
the input belonging to class one.

95
00:06:37,860 --> 00:06:42,750
But if we have multiple classes, if I
have more than two classes, uh, then, uh,

96
00:06:42,751 --> 00:06:45,120
what we'll need to do is
have multiple outputs.

97
00:06:45,240 --> 00:06:49,590
And specifically we want to have
one output unit at the output layer,

98
00:06:49,990 --> 00:06:52,860
uh, per class. And, uh,

99
00:06:52,920 --> 00:06:57,920
now we'll still want to keep this idea
that our neural network will estimate the

100
00:06:59,160 --> 00:07:00,540
a conditional probability.

101
00:07:00,541 --> 00:07:05,541
So we'll like the output layer
to give us the probability of a,

102
00:07:06,220 --> 00:07:11,010
the input belonging to each potential
class, which I note seats. So,

103
00:07:11,250 --> 00:07:11,820
um,

104
00:07:11,820 --> 00:07:16,820
see what a typically belong in one
up to a certain number of classes.

105
00:07:19,490 --> 00:07:22,920
Capitol C. Okay. So if we had 10 classes,

106
00:07:22,921 --> 00:07:26,010
then capitol c would be equal to 10 and a,

107
00:07:26,011 --> 00:07:29,460
so we would have indexed
all of our classes, uh,

108
00:07:29,490 --> 00:07:34,440
from one to capitol seat one to 10.
And so,

109
00:07:34,500 --> 00:07:36,540
uh,
to get an output,

110
00:07:36,810 --> 00:07:40,350
which corresponds to a valid
conditional probability.

111
00:07:40,530 --> 00:07:45,000
What we'll use at as the output
activation function is what is called the

112
00:07:45,050 --> 00:07:47,340
softmax activation function.

113
00:07:48,060 --> 00:07:53,060
And what it takes is that what it does
is that it takes the pre activation and

114
00:07:53,281 --> 00:07:58,281
it simply takes the exponential of all
M and elements of that reactivation

115
00:07:58,591 --> 00:08:02,550
vector. So it's a vector now
because we have multiple classes.

116
00:08:02,551 --> 00:08:05,820
So we have multiple, uh, output neurons.

117
00:08:06,060 --> 00:08:10,110
And so we are multiple
reactivation, uh, values.

118
00:08:10,680 --> 00:08:15,090
So the soft Max takes the exponential
of all of those numbers and then divides

119
00:08:15,180 --> 00:08:18,480
each of these numbers by the
sum of the vector, by the,

120
00:08:18,481 --> 00:08:23,481
some of the exponential of the pre
activation for all possible classes.

121
00:08:24,271 --> 00:08:27,660
See, that's why we have this
some old receipt here. Okay.

122
00:08:28,110 --> 00:08:31,980
So because we are normalizing our vector
like this because we're dividing by the

123
00:08:31,981 --> 00:08:34,080
sum of the numerators here,

124
00:08:34,290 --> 00:08:37,200
then we are guaranteed
that it will sum to one.

125
00:08:38,400 --> 00:08:42,270
And uh, that's because we're
divided by the sun, the,

126
00:08:42,271 --> 00:08:43,560
some of the numerators.

127
00:08:43,830 --> 00:08:46,680
And also we are guaranteed that
it's going to be strictly positive.

128
00:08:46,681 --> 00:08:50,850
And that's because the exponential of
any number is necessarily greater than

129
00:08:51,060 --> 00:08:55,950
zero. So we will have a valid, uh,
probably the distribution, the numbers,

130
00:08:55,951 --> 00:08:57,990
the probabilities are positive
and there's some two more.

131
00:08:59,100 --> 00:09:02,130
And then if you want to
predict an actual class,

132
00:09:02,131 --> 00:09:07,131
so if he get an input x and
one to actually assign it
to a particular class a,

133
00:09:07,290 --> 00:09:12,210
then the natural thing to do is to use
the class with the highest estimated

134
00:09:12,211 --> 00:09:14,900
probability. That is again, uh,

135
00:09:15,090 --> 00:09:20,090
we just look at all the output units and
we output the class associated with the

136
00:09:21,350 --> 00:09:23,910
upper cabinet that has the largest value.

137
00:09:25,990 --> 00:09:26,823
Yeah.

138
00:09:27,550 --> 00:09:32,440
And finally, uh, in the previous example,
we had only a single hidden layer,

139
00:09:32,470 --> 00:09:37,130
but there's no reason for not using
multiple hidden layers. And, uh,

140
00:09:37,240 --> 00:09:41,950
we will discuss in other videos,
uh, why we might want to do that.

141
00:09:42,250 --> 00:09:45,670
But to get this generalization
to multiple hidden layers,

142
00:09:45,671 --> 00:09:49,110
say l hidden layers,
uh,

143
00:09:49,120 --> 00:09:52,540
we'll have now a activation function as,
sorry,

144
00:09:52,541 --> 00:09:56,800
a p activation value at any hidden layer.
Kay.

145
00:09:57,190 --> 00:09:59,560
So we'll call that a k of x.

146
00:10:00,100 --> 00:10:05,100
And as this will just correspond
to the bias of that hidden layer.

147
00:10:06,250 --> 00:10:10,030
Plus the Matrix of
connections for that key.

148
00:10:10,031 --> 00:10:15,031
If hidden layer multiply by the activation
values at the previous hidden there.

149
00:10:16,450 --> 00:10:18,490
So at the hidden they are k minus one.

150
00:10:19,150 --> 00:10:22,390
And if we use a for our definition,
uh,

151
00:10:22,420 --> 00:10:26,020
if we assume that h zero of x is just x,

152
00:10:26,320 --> 00:10:29,090
so age zero would be the
zero with in an layer.

153
00:10:29,090 --> 00:10:34,090
So this will correspond to the input
layer here in our definition then that

154
00:10:34,331 --> 00:10:35,710
this,
uh,

155
00:10:35,740 --> 00:10:39,640
correctly defines the pre
activation value at all.

156
00:10:39,760 --> 00:10:43,810
Then there and then to get
the hidden layer activation.

157
00:10:44,020 --> 00:10:48,010
So the output of the hidden
units at each hidden layer,

158
00:10:48,340 --> 00:10:50,440
what we do is that we simply take,

159
00:10:50,500 --> 00:10:55,500
apply again the activation function on
the reactivation of the curse finding

160
00:10:55,851 --> 00:11:00,370
hidden there. And then to get
our output layer activation,

161
00:11:00,610 --> 00:11:05,170
which corresponds to the
index of the layer cake, uh,

162
00:11:05,260 --> 00:11:09,250
equal to l plus one where l is
the number of hidden layers.

163
00:11:09,940 --> 00:11:12,220
Then, uh, what we could,

164
00:11:12,430 --> 00:11:17,020
if we use these definitions here
for the activation function, uh,

165
00:11:17,021 --> 00:11:22,021
so for the layer l plus one h l plus
four on the x is going to be again the

166
00:11:24,880 --> 00:11:26,860
activation function.
Uh,

167
00:11:26,890 --> 00:11:31,090
but pass through an activation
function for the case. Uh,

168
00:11:31,450 --> 00:11:35,490
we have the, uh, output
layers. So the, uh, uh,

169
00:11:35,620 --> 00:11:38,200
index alk plus one,
we'll use our ever,

170
00:11:38,260 --> 00:11:40,840
not the g activation function,

171
00:11:40,841 --> 00:11:45,580
but the output activation function
that we're noting. Oh, and, uh,

172
00:11:45,610 --> 00:11:49,960
typically instead of using this for
the notation of the output layer, uh,

173
00:11:49,961 --> 00:11:52,420
often referred to it instead as f of x,

174
00:11:52,690 --> 00:11:55,300
which is to be the output
of the neural network.

175
00:11:56,430 --> 00:12:01,430
Now if you're wondering why we say that
these layers here are hidden and this is

176
00:12:02,140 --> 00:12:02,621
the output.

177
00:12:02,621 --> 00:12:06,430
So this is the output for the obvious
reason that this is the actual value that

178
00:12:06,431 --> 00:12:11,320
is you say by our classifier,
which is trying to classify some input.

179
00:12:12,070 --> 00:12:17,070
And these are hidden because we don't
know what's the actual right value for

180
00:12:17,320 --> 00:12:21,460
these hidden units. So the
neural network, uh, when we talk,

181
00:12:21,750 --> 00:12:25,000
when we talk about learning and
the parameters of a neural network,

182
00:12:25,060 --> 00:12:29,620
it will try to find what's the correct
behavior for these neurons here.

183
00:12:30,100 --> 00:12:33,730
However, for the output, we know what's
the right answer. So we're normally,

184
00:12:33,731 --> 00:12:36,610
we'll have some label data
and for some given input,

185
00:12:36,820 --> 00:12:39,610
we will actually know which
output we want to see,

186
00:12:39,611 --> 00:12:43,360
have it's value be the
biggest amongst all output.

187
00:12:43,960 --> 00:12:48,040
So that's why these guys are actually,
these hidden units are actually hidden.

188
00:12:48,340 --> 00:12:52,780
They're hidden in the sense that we don't
know what's the actual best value for

189
00:12:52,781 --> 00:12:57,730
them to take for some given input.
We only know what value would like to see,

190
00:12:58,090 --> 00:13:01,740
uh, for the output. If
we actually know, uh,

191
00:13:02,020 --> 00:13:06,850
say in some data, if we actually know that
certain inputs are classified in some,

192
00:13:06,960 --> 00:13:08,020
uh,
certain classes.

