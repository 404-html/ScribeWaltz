1
00:00:00,540 --> 00:00:01,171
In this video,

2
00:00:01,171 --> 00:00:05,340
we'll briefly overview some concepts
that we often see in the literature on

3
00:00:05,341 --> 00:00:08,670
conditional random fields.
That is the concept of factor,

4
00:00:08,910 --> 00:00:11,100
a sufficient statistics and uh,

5
00:00:11,170 --> 00:00:14,280
the specific case of a linear
conditional random fields.

6
00:00:16,770 --> 00:00:19,740
Now, if you look at the literature
and conditional random fields, uh,

7
00:00:19,741 --> 00:00:23,080
often you'll find that uh, the, uh,

8
00:00:23,130 --> 00:00:26,490
different CRF models that
are being developed, uh,

9
00:00:26,491 --> 00:00:31,110
will be written in the form
that's a bit different. So one,

10
00:00:31,140 --> 00:00:34,530
a more standard format you'll see
in the literature is as follows,

11
00:00:34,860 --> 00:00:39,860
I will just write p of y given x as
being one over a normalization constant.

12
00:00:40,261 --> 00:00:42,330
So again,
we have our partition function here,

13
00:00:42,810 --> 00:00:47,730
but times the product of
some factors index by F,

14
00:00:48,570 --> 00:00:51,300
which uh,
so we called them factors up.

15
00:00:51,301 --> 00:00:55,660
Sometimes we call them also
compatible with the functions that uh,

16
00:00:55,680 --> 00:00:58,920
look at, uh, part of the, uh,

17
00:00:58,980 --> 00:01:01,800
target why and the input sequence X.

18
00:01:01,980 --> 00:01:04,620
And I'll put a value that is not negative.

19
00:01:04,620 --> 00:01:09,620
So between zero and infinity and the
job of each factor or each compatibility

20
00:01:10,021 --> 00:01:15,021
function is to look at a particular part
of why an ex and determine whether it

21
00:01:15,810 --> 00:01:17,100
likes it or not.

22
00:01:17,130 --> 00:01:21,960
Whether it prefers that these
parts of why an x take the value,

23
00:01:22,190 --> 00:01:25,200
uh,
that's why an XR currently taking or not.

24
00:01:25,470 --> 00:01:29,040
So we see that if this factor is,
uh,

25
00:01:29,190 --> 00:01:31,860
for instance smaller than
ones are between zero and one,

26
00:01:32,040 --> 00:01:36,330
then this will essentially decreased
the value of the old expression.

27
00:01:36,331 --> 00:01:40,380
Whereas if it was greater than one,
that would actually increase it.

28
00:01:41,280 --> 00:01:43,440
And so I'll just generally speaking,

29
00:01:43,441 --> 00:01:48,441
the bigger the compatibly function or
factors output is then the more likely,

30
00:01:49,260 --> 00:01:50,280
uh,

31
00:01:50,340 --> 00:01:54,840
seeing why associated with the input
x is going to be in this model.

32
00:01:55,740 --> 00:02:00,120
And if very frequent parameterization
of these factors or compatibility

33
00:02:00,121 --> 00:02:05,121
functions in the literature is to use a
log linear form that is the exponential

34
00:02:06,090 --> 00:02:10,680
of a Lennar some that combines
the model's parameters.

35
00:02:10,980 --> 00:02:11,711
Uh, no, no.

36
00:02:11,711 --> 00:02:16,711
That data as a thera here with a bunch
of features or sometimes we call them

37
00:02:17,520 --> 00:02:21,360
sufficient statistics,
they're called sufficient statistics.

38
00:02:21,361 --> 00:02:26,361
Because if you had a large data
set where we have a first label,

39
00:02:26,730 --> 00:02:31,730
why one associated with an input
sequence x one and then we'd,

40
00:02:31,831 --> 00:02:36,831
so we'd have one pair and then we have
another pair of why to associated with

41
00:02:37,650 --> 00:02:40,590
its input sequence x two and so on.

42
00:02:40,770 --> 00:02:43,920
So if you were to compute a,
in this model,

43
00:02:43,921 --> 00:02:47,520
what is the probability of
seeing all of these sequences,

44
00:02:47,521 --> 00:02:50,580
y one and x and y two and so on,

45
00:02:50,760 --> 00:02:54,190
associated with the sequences,
uh,

46
00:02:54,240 --> 00:02:59,190
x one x two and so on.
Then assuming each pairs are a,

47
00:02:59,290 --> 00:03:00,610
each example is independent.

48
00:03:00,611 --> 00:03:05,611
That would be the product of the P of
XD given a whitey given [inaudible].

49
00:03:06,670 --> 00:03:08,710
So it'd be the product
of these expressions.

50
00:03:09,040 --> 00:03:14,040
Now if we pair it dries are
factors in the non log linear way.

51
00:03:14,740 --> 00:03:18,700
So as the exponential of a, some of
parameters multiplied by some features.

52
00:03:18,970 --> 00:03:23,740
Then so this would get a product of a
bunch of exponentiate it sums so we could

53
00:03:23,741 --> 00:03:28,450
accumulate them all because the product
of exponentials, the exponential of sons.

54
00:03:28,660 --> 00:03:33,610
So we essentially get an expression where
we have parameters multiplied by the

55
00:03:33,611 --> 00:03:38,440
sum of these terms. Over
the whole data set. And uh,

56
00:03:38,441 --> 00:03:39,400
so in other words,

57
00:03:39,401 --> 00:03:44,401
the probability of the whole data set
could be essentially written down as an

58
00:03:44,951 --> 00:03:46,810
expression that only depends on the,

59
00:03:46,811 --> 00:03:51,430
some of these features or
sufficient statistics, uh,

60
00:03:51,431 --> 00:03:55,380
over the whole dataset. And
in fact, uh, technically, uh,

61
00:03:55,510 --> 00:03:58,000
we will say that the sufficient
statistics is actually the,

62
00:03:58,001 --> 00:04:00,880
some of the features and
not the feature themselves.

63
00:04:01,390 --> 00:04:06,130
So I'm going over this very
quickly. You can, uh, look at, uh,

64
00:04:06,220 --> 00:04:09,820
the literature to, uh, learn more
about what a sufficient statistic mean,

65
00:04:09,821 --> 00:04:13,960
but just know that normally when we talk
about sufficiency statistics in a CRF

66
00:04:14,230 --> 00:04:15,280
conditional random fields,

67
00:04:15,281 --> 00:04:19,690
they're essentially the features that
are being multiplied by the parameters of

68
00:04:19,691 --> 00:04:21,880
the model.
And then in this case,

69
00:04:22,060 --> 00:04:27,060
the whole probabilities going to be
the exponential of linear expression,

70
00:04:27,940 --> 00:04:30,340
linear with respect to the parameters.

71
00:04:30,341 --> 00:04:35,230
So they're going to curse fun if
we use this parameterization, uh,

72
00:04:35,260 --> 00:04:38,890
they're going to essentially current
respond to a log linear model in the

73
00:04:38,891 --> 00:04:40,630
features that we've chosen.

74
00:04:43,150 --> 00:04:45,580
So if we are to write our,
uh,

75
00:04:45,610 --> 00:04:49,960
use our notation and put it back
in that form that we just seen,

76
00:04:50,320 --> 00:04:53,950
uh, then the conditional
random fields factors, uh,

77
00:04:53,951 --> 00:04:57,700
so the Psi F, uh, would be as follows.

78
00:04:57,701 --> 00:04:59,860
So there'd be four types of factors.

79
00:04:59,861 --> 00:05:03,030
So this is for the specific
case where we have, uh,

80
00:05:03,070 --> 00:05:07,780
a context window of radius one, and we
have a neural network that maps, uh,

81
00:05:07,781 --> 00:05:11,800
either the left or the middle,
the right part to, uh, the,

82
00:05:11,890 --> 00:05:15,940
uh, current position. So we
have three neural networks.

83
00:05:16,330 --> 00:05:20,260
So in this case we'd have,
uh, three urinary factors.

84
00:05:20,261 --> 00:05:21,670
So these five F's,

85
00:05:22,060 --> 00:05:25,570
there'd be some that would look at the
current label and compare it that would

86
00:05:25,571 --> 00:05:28,900
compare it with the,
uh, input on the left.

87
00:05:29,480 --> 00:05:33,190
There'd be some factors that could look
at their current level and look at the,

88
00:05:33,260 --> 00:05:34,093
uh,

89
00:05:34,150 --> 00:05:38,860
input that's at the same position and
others that look at their current label,

90
00:05:38,890 --> 00:05:41,560
but look at the input at
the position on the right.

91
00:05:42,070 --> 00:05:47,070
And then we'd also have pairwise factors
that just look at adjacent labels and

92
00:05:47,531 --> 00:05:50,470
they would correspond to
the exponential of, uh,

93
00:05:50,540 --> 00:05:54,940
they essentially to take this form, the
exponential of the pre activation value,

94
00:05:55,280 --> 00:05:59,870
uh, computed based on the
input on the left for the,

95
00:05:59,930 --> 00:06:04,430
uh, value of the Kia
label in my sequence. Why?

96
00:06:04,640 --> 00:06:06,500
So this would be for,
for this factor,

97
00:06:06,501 --> 00:06:10,670
he would be the exponentiate it
be activation function of x gay,

98
00:06:11,000 --> 00:06:13,970
uh, for the label Ika, uh, and so on.

99
00:06:14,060 --> 00:06:19,060
So we now see that in this notation
where we decompose p of y given x as a

100
00:06:19,131 --> 00:06:20,570
product of factors.

101
00:06:20,840 --> 00:06:25,220
Then the a terms as well as the,

102
00:06:25,280 --> 00:06:28,130
uh, pairwise Matrix v, uh,

103
00:06:28,131 --> 00:06:32,840
really do correspond to the exponential
of a log factors essentially.

104
00:06:32,841 --> 00:06:36,980
So this is where the luck factor,
uh, uh, expression comes from.

105
00:06:38,360 --> 00:06:42,290
Unfortunately, in this form,
there's no simple expression, uh,

106
00:06:42,291 --> 00:06:46,160
for the sufficient statistics that is
we can't ride these terms as just a,

107
00:06:46,161 --> 00:06:50,210
some of parameters time,
some, uh, simple, uh,

108
00:06:50,230 --> 00:06:53,180
deterministic features
extracted from x and y.

109
00:06:54,760 --> 00:06:55,180
Okay.

110
00:06:55,180 --> 00:06:55,461
And that's,

111
00:06:55,461 --> 00:06:59,620
so that's because essentially these
expressions here with hidden units are

112
00:06:59,621 --> 00:07:00,950
essentially non linear.

113
00:07:04,070 --> 00:07:08,150
If you look at the specific case of a
linear conditional random fields in here,

114
00:07:08,151 --> 00:07:10,140
linear is,
uh,

115
00:07:10,160 --> 00:07:15,050
refers to the parametrization
and it doesn't refer to
the linear chain structure

116
00:07:15,200 --> 00:07:18,880
of the interaction between,
uh, the pair pairwise. Uh,

117
00:07:18,890 --> 00:07:23,460
so the adjacent labels. So linner
means essentially that, uh,

118
00:07:23,510 --> 00:07:27,410
the, uh, neural network
has no hidden units.

119
00:07:27,470 --> 00:07:30,950
So the p activation is a
linear function of the input.

120
00:07:31,940 --> 00:07:36,940
Well in this case we can
write the expression for uh,

121
00:07:37,550 --> 00:07:38,090
yeah,

122
00:07:38,090 --> 00:07:42,740
we can sort of further exploit the a and
they have a more precise expression for

123
00:07:43,100 --> 00:07:47,130
a d a p of y given x in a,
as a product of factors.

124
00:07:47,131 --> 00:07:51,710
So we'd have g five now, uh,
types of factors. We have factors.

125
00:07:51,740 --> 00:07:56,180
You're near your factories that a look
at whether the label takes a particular

126
00:07:56,181 --> 00:07:57,770
value.
And if it does,

127
00:07:57,771 --> 00:08:02,360
then we just take the exponential
of bias at the output layer. Uh,

128
00:08:02,361 --> 00:08:05,880
we'd have factors again that
look at the current, uh,

129
00:08:05,930 --> 00:08:10,130
label and that looks at some given input,

130
00:08:10,160 --> 00:08:13,010
the input at the position on the left.

131
00:08:13,370 --> 00:08:18,200
And it would just take whether a dn dicker
to function of whether why Kate takes

132
00:08:18,201 --> 00:08:23,201
of labels c and multiply that by the IFM
puts for the input vector position came

133
00:08:23,841 --> 00:08:24,620
minus one.

134
00:08:24,620 --> 00:08:29,620
And that would be multiply by the entry
in the Matrix that connects the left

135
00:08:30,051 --> 00:08:32,120
input with the current position.

136
00:08:32,240 --> 00:08:36,250
It would look at the entry c and the uh,

137
00:08:36,251 --> 00:08:37,970
so the row C and d column.

138
00:08:39,010 --> 00:08:42,560
So we have a bunch of factors
like this for each class,

139
00:08:42,890 --> 00:08:47,540
potential class C, each potential
position in the input vectors.

140
00:08:47,600 --> 00:08:51,310
I N uh, each position, Kay. Uh,

141
00:08:51,320 --> 00:08:55,250
so this gay and this case,
each value for that key in the sequence,

142
00:08:55,620 --> 00:08:58,590
but have also a bunch of
other factors that, uh,

143
00:08:58,680 --> 00:09:03,680
would do the same but for the input gay
at the same position or another for the

144
00:09:04,790 --> 00:09:08,440
[inaudible] position.
Cape last one. So, um, uh,

145
00:09:08,450 --> 00:09:12,390
so that's for the case where again,
we have neural nets that uh,

146
00:09:12,480 --> 00:09:15,000
we have three different neural
nets in the context window,

147
00:09:15,090 --> 00:09:19,500
but turns out we could essentially
get a very similar expression if a we

148
00:09:19,501 --> 00:09:22,710
considered the case where we think of
it that's just there being one neural

149
00:09:22,711 --> 00:09:26,610
network that connects a full context
window directly to the label.

150
00:09:26,640 --> 00:09:29,370
And that's because there are no
hidden units in the linear CRF.

151
00:09:30,150 --> 00:09:35,020
So we could essentially get the similar
expression and a similarly the, uh,

152
00:09:35,070 --> 00:09:36,310
factors,
uh,

153
00:09:36,330 --> 00:09:41,330
pairwise factors would be the exponential
of VCC prime if y gay a is equal to c

154
00:09:43,171 --> 00:09:45,660
and if y k plus one is equal to c prime.

155
00:09:45,661 --> 00:09:48,850
So we have such factors in the expression,
uh,

156
00:09:48,870 --> 00:09:53,870
for each value of c c prime
and for each position came.

157
00:09:54,481 --> 00:09:58,150
So this would be, we could
write this down as a, uh,

158
00:09:58,350 --> 00:10:02,130
in the expression that is the product
of all of these types of factors for all

159
00:10:02,131 --> 00:10:06,790
values of y c and K
for, uh, a pair of, uh,

160
00:10:07,140 --> 00:10:09,840
uh, uh, sequence label. Why an X.

161
00:10:10,230 --> 00:10:12,210
And notice that in this expression

162
00:10:13,980 --> 00:10:18,980
now I written down the full model as
a product of sufficient statistics or

163
00:10:19,891 --> 00:10:24,660
features times a scalar parameter.
Okay.

164
00:10:24,661 --> 00:10:25,910
So, uh, now the,

165
00:10:25,911 --> 00:10:29,730
the notation seems more complicated in
this form and this is just to make the

166
00:10:29,731 --> 00:10:32,790
mapping with the case where I have,
uh,

167
00:10:32,880 --> 00:10:35,040
factors that are a lug linear.

168
00:10:35,160 --> 00:10:39,180
And if you remember in my expression
the parameters that are f s there were,

169
00:10:39,181 --> 00:10:43,680
there were scalar parameters. So that's
why I get some, uh, fairly, uh, you know,

170
00:10:43,700 --> 00:10:47,100
the annotation for the sufficient
statistics is a bit more complicated.

171
00:10:48,360 --> 00:10:52,440
So in the case of a linear CRF or
so, for instance, in this case,

172
00:10:52,441 --> 00:10:55,210
a linear linear chain Crf,
uh,

173
00:10:55,220 --> 00:10:57,740
I can actually write down the,
uh,

174
00:10:57,750 --> 00:11:01,530
into the standard form we saw in the
beginning of this video where I can

175
00:11:01,531 --> 00:11:05,910
separate out my parameters and my
sufficient statistics in this way.

176
00:11:05,911 --> 00:11:09,270
So we have a log linear model and, uh, um,

177
00:11:09,900 --> 00:11:12,960
and where all the parameters are scalers
and they're multiply by sufficient

178
00:11:12,961 --> 00:11:17,320
statistics. So, um, you know, I invite
you to look more at the literature.

179
00:11:17,321 --> 00:11:21,120
If you want more details on this, I'm
going fairly quickly over this, but, uh,

180
00:11:21,121 --> 00:11:25,050
I just wanted to mention it. If you ever
see the notion of sufficient statistics,

181
00:11:25,080 --> 00:11:27,110
essentially we were talking about a,

182
00:11:27,120 --> 00:11:32,120
usually a log linear model where we've
defined a bunch of statistics that we

183
00:11:32,731 --> 00:11:35,520
think are useful for predicting the label.

