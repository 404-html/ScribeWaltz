1
00:00:00,520 --> 00:00:00,960
Okay.

2
00:00:00,960 --> 00:00:01,501
In this video,

3
00:00:01,501 --> 00:00:05,550
we'll look at an example of what we get
when we train an auto encoder on some

4
00:00:05,551 --> 00:00:09,380
real data.
All right,

5
00:00:09,440 --> 00:00:12,620
so we have this auto encoder here.
It's a feed forward neural network.

6
00:00:12,650 --> 00:00:17,570
It outputs a reconstruction of the input
and then we've looked at how we could

7
00:00:17,571 --> 00:00:20,780
define a loss that compares the
reconstruction with the input.

8
00:00:21,320 --> 00:00:23,820
And then using that loss we can,
uh,

9
00:00:24,120 --> 00:00:28,340
compute the gradients which respect
to the pre activation and then back

10
00:00:28,341 --> 00:00:33,020
propagate that through the network in
order to do stochastic gradient descent

11
00:00:33,021 --> 00:00:37,400
training of the auto encoder
on some set of unlabeled data.

12
00:00:38,120 --> 00:00:41,510
So let's look at what we get in terms of,
uh,

13
00:00:41,540 --> 00:00:44,390
a later and representation
in terms of features.

14
00:00:44,630 --> 00:00:47,840
If we train such an auto
encoder, uh, on some real data.

15
00:00:50,200 --> 00:00:53,650
So we'll go back to the
MDS data set, which, uh,

16
00:00:53,651 --> 00:00:58,060
we've looked at the phone when we talked
about the restrictive Bolsa machine. So,

17
00:00:58,080 --> 00:00:59,250
uh,
we'll train,

18
00:00:59,360 --> 00:01:02,830
we'll a lot of show is a result of
training and ongoing color on that,

19
00:01:02,920 --> 00:01:05,030
the same data. And, uh,

20
00:01:05,080 --> 00:01:09,730
we'll look at each hidden unit in the
encoder and see what kind of features it

21
00:01:09,731 --> 00:01:12,580
extracts, what kind of representation
it extracts from the input.

22
00:01:15,580 --> 00:01:16,300
So again,

23
00:01:16,300 --> 00:01:21,300
each square here is going to be one
hidden unit in a deal toy quarter and a,

24
00:01:23,261 --> 00:01:28,261
it's a visualization of the weight factor
between all inputs and specific hidden

25
00:01:28,841 --> 00:01:32,860
units.
And so gray means a weight of zero black,

26
00:01:33,130 --> 00:01:34,720
a weight that's negative.

27
00:01:34,721 --> 00:01:39,721
And why a weight that's positive that
we see that duo tone quarters seems to

28
00:01:39,731 --> 00:01:40,660
have learned,
for instance,

29
00:01:40,661 --> 00:01:45,661
that we tend to have either white pixels
close to each other in a spatially

30
00:01:46,871 --> 00:01:49,420
localized region or black pixel.

31
00:01:50,410 --> 00:01:55,410
It doesn't seem Harvard to have learned
a very interesting feature such as

32
00:01:55,840 --> 00:02:00,040
stroke detectors or edge detectors.
Uh, it actually has a lot of them,

33
00:02:00,041 --> 00:02:04,160
which are mainly zero and
they're actually kind of um,

34
00:02:04,920 --> 00:02:09,850
uh, we'd say, uh, uh, they have,
they show high spacial frequency,

35
00:02:10,180 --> 00:02:14,650
uh, but they don't seem to extract a
particular particularly appealing and

36
00:02:14,651 --> 00:02:17,620
meaningful visual pattern.
Um,

37
00:02:17,740 --> 00:02:22,450
it has understood that pixels around that
are in some neighborhoods tend to have

38
00:02:22,451 --> 00:02:23,980
the same value,
but otherwise it does.

39
00:02:24,000 --> 00:02:27,130
It doesn't seem to have understood
a lot of the structure that's got,

40
00:02:27,250 --> 00:02:31,480
characterizes a Henry did characters.
So why is that?

41
00:02:31,510 --> 00:02:32,830
And can we improve on that?

42
00:02:32,860 --> 00:02:36,670
So it turns out this auto encoder can
still be useful in certain contexts,

43
00:02:36,700 --> 00:02:40,840
and we'll be able to talk about this when
we talk about deep learning or we will

44
00:02:40,841 --> 00:02:42,400
also be able to improve it.

45
00:02:42,790 --> 00:02:47,350
And so in the next videos I'll talk
about what's a problem here that we have

46
00:02:47,351 --> 00:02:49,960
with this particular way of
training on auto encoder.

47
00:02:50,020 --> 00:02:52,570
And then we'll talk about
some alternative solutions.

