1
00:00:00,650 --> 00:00:03,840
This video, we'll see a few
concepts related to optimization,

2
00:00:03,870 --> 00:00:07,800
which are going to help us understand
how optimization behaves in training

3
00:00:07,801 --> 00:00:09,060
neural networks and,
uh,

4
00:00:09,120 --> 00:00:13,860
give us ideas for how we could actually
improve optimization of neural networks.

5
00:00:15,270 --> 00:00:15,660
Okay.

6
00:00:15,660 --> 00:00:20,660
So we've seen that neural network training
is essentially based on optimizing a,

7
00:00:20,700 --> 00:00:24,570
an average loss or a regularized
average lost if loss,

8
00:00:24,571 --> 00:00:29,490
if we have a regular riser. And, uh, what
we've done is that, is that we've used a,

9
00:00:29,520 --> 00:00:31,680
uh, known, uh,

10
00:00:31,830 --> 00:00:35,580
a known optimization methods to cast
a great in dissent and optimize that

11
00:00:35,640 --> 00:00:36,900
average loss,
uh,

12
00:00:36,990 --> 00:00:41,520
to perform training to fit on your
own network to our training data. So,

13
00:00:41,521 --> 00:00:46,100
uh, what about optimizing this
optimization problem? Is it a,

14
00:00:46,110 --> 00:00:50,310
is it well behaved? What properties
does it have? Unfortunately,

15
00:00:50,311 --> 00:00:54,900
for neural nets, uh, it's, uh,
the optimization problem is,

16
00:00:54,990 --> 00:00:59,220
uh, not particularly
easy. Uh, for one thing,

17
00:00:59,250 --> 00:01:03,750
there isn't a single global optimum to
the optimization problem with trying to

18
00:01:03,751 --> 00:01:08,250
solve, uh, minimizing the average
training loss or the average, uh,

19
00:01:08,310 --> 00:01:13,140
regularize training loss. Uh, we
can see this because we can compete,

20
00:01:13,350 --> 00:01:16,710
we can permute all the hidden
units along with their connections.

21
00:01:17,070 --> 00:01:20,070
And we actually don't
change the value of f of x,

22
00:01:20,071 --> 00:01:24,480
the output value of the neural
network. It hasn't changed at all. Uh,

23
00:01:24,510 --> 00:01:29,510
and so this means that there isn't a
single unique solution that corresponds to

24
00:01:29,671 --> 00:01:32,760
the best training error we can
achieve with a neural network.

25
00:01:33,080 --> 00:01:36,540
Any permutation of the hidden units
will result in the same function,

26
00:01:36,541 --> 00:01:37,380
the same solution.

27
00:01:38,630 --> 00:01:38,960
Okay.

28
00:01:38,960 --> 00:01:43,700
Uh, so in that sense we can say that
the parameters of a hidden layer,

29
00:01:44,120 --> 00:01:46,370
uh, are not identifiable. Okay.

30
00:01:46,371 --> 00:01:51,200
There we can't identify if their data
set was truly generated from a neural

31
00:01:51,201 --> 00:01:51,591
network.

32
00:01:51,591 --> 00:01:56,591
We can't identify what's the
actual weights of each unit.

33
00:01:57,080 --> 00:02:01,010
We can't identify actually the order in
which these hidden units, uh, were, uh,

34
00:02:01,110 --> 00:02:06,080
organizing the original neural network.
But that's not such a bad thing.

35
00:02:06,081 --> 00:02:10,190
As long as we get the right function
f of x, uh, the right predictions,

36
00:02:10,220 --> 00:02:12,140
then we'd be happy with that.
However,

37
00:02:12,141 --> 00:02:17,141
also the optimization problem is known
as a non convex optimization problem,

38
00:02:18,620 --> 00:02:21,200
uh,
and that even if they were identifiable,

39
00:02:21,201 --> 00:02:25,790
that would still be a lot of
local minimum or local minima. So,

40
00:02:26,080 --> 00:02:28,370
uh,
the optimization problem,

41
00:02:28,371 --> 00:02:32,330
instead of looking something like this
where we have a sort of bullshit convex

42
00:02:32,331 --> 00:02:35,270
function where there's only
a single global optimum,

43
00:02:35,271 --> 00:02:39,210
and if we followed the gradient
steps, then eventually we would, uh,

44
00:02:39,350 --> 00:02:42,310
necessarily converge to
The kobold optimum. Uh,

45
00:02:42,440 --> 00:02:45,230
we actually have a function
which has a lot of local,

46
00:02:45,380 --> 00:02:47,810
potentially a lot of local
minima where for instance,

47
00:02:47,811 --> 00:02:52,280
we have a region here which locally looks
likable. But if we got outside of it,

48
00:02:52,520 --> 00:02:56,690
we could find that there's another
region which has a better solution.

49
00:02:57,680 --> 00:02:59,180
So this region here would correspond.

50
00:02:59,410 --> 00:03:04,410
And actually this value
here that's optimizes the
function locally here is known

51
00:03:04,421 --> 00:03:09,421
as a local minima and also other less
obvious but still fairly annoying,

52
00:03:12,150 --> 00:03:15,650
uh, properties that we can find
and optimization, optimizing,

53
00:03:15,760 --> 00:03:19,030
optimizing neural networks,
a known as plateaus.

54
00:03:19,031 --> 00:03:23,470
So these would be regions where
the function is essentially flat,

55
00:03:23,471 --> 00:03:26,540
is very close to just a flat line.
Uh,

56
00:03:26,860 --> 00:03:29,170
if you think of the stochastic
gradient descent algorithm,

57
00:03:29,200 --> 00:03:34,120
what it would be doing there is, or the
just in general gradient descent a here,

58
00:03:34,240 --> 00:03:37,810
well it would follow the grain and the
grain and because the function is almost

59
00:03:37,811 --> 00:03:40,600
flat is very close to zero.
So we make very,

60
00:03:40,601 --> 00:03:45,190
very small steps here as
it's optimizing the function.

61
00:03:45,790 --> 00:03:46,750
And so,
uh,

62
00:03:46,780 --> 00:03:50,020
because you don't get to see the whole
function where you're optimizing this,

63
00:03:50,021 --> 00:03:53,650
you only see the progress of the training
error because it doesn't change a lot.

64
00:03:53,651 --> 00:03:56,470
You might think, oh, I'm
close to convergence and uh,

65
00:03:56,560 --> 00:03:59,500
maybe you'll stop before you
actually reach this part here,

66
00:03:59,501 --> 00:04:01,900
which will drastically improve the error.

67
00:04:02,770 --> 00:04:05,440
And so these regions here can mean that.

68
00:04:05,441 --> 00:04:09,220
So the plateau here is as long it might
be so long that for the amount of time

69
00:04:09,221 --> 00:04:11,590
you are willing to optimize
your neural net, uh,

70
00:04:11,770 --> 00:04:13,480
you might actually never
get out of the plateau.

71
00:04:13,540 --> 00:04:18,110
So that's another thing that we
sometimes see in neural network and, uh,

72
00:04:18,160 --> 00:04:21,660
that are issues you need to
think about for designing, uh,

73
00:04:21,850 --> 00:04:25,390
different potentially better training
algorithms for neural networks.

74
00:04:26,790 --> 00:04:30,010
Um, so just keep in mind that
we're optimizing neural nets.

75
00:04:30,280 --> 00:04:32,770
We are not guaranteed
to find a global Optima.

76
00:04:32,860 --> 00:04:36,670
We might just find a local optimum.
And uh,

77
00:04:36,790 --> 00:04:40,810
and also it,
even if the training or doesn't move much,

78
00:04:40,840 --> 00:04:43,930
it might just be that you're on the
plateau and that eventually you'll,

79
00:04:44,050 --> 00:04:47,740
you'll be reaching a region where the
training air will drastically improve.

80
00:04:47,741 --> 00:04:52,030
So if you see that your training error
is moving very slowly and then eventually

81
00:04:52,240 --> 00:04:55,690
dropping a lot,
this is something that's normal.

82
00:04:55,691 --> 00:04:58,910
It's necessarily a bug in your code
that just might happen because the,

83
00:04:59,170 --> 00:05:02,550
the function is noncombat sin
has long plateaus like these.

84
00:05:05,340 --> 00:05:07,660
Now let's see these properties in action.

85
00:05:07,870 --> 00:05:12,160
We'll look at a demo that was
provided by Andrei carpathy.

86
00:05:12,700 --> 00:05:17,230
Uh, and it's going to illustrate
this a property of local optimum.

87
00:05:17,231 --> 00:05:22,231
That is the fact that if we take a neural
network and initialize it at different

88
00:05:22,811 --> 00:05:27,790
values, we might converge to different
local Optima. So if we look back at this,

89
00:05:28,020 --> 00:05:31,450
uh, cartoon case here, if we are an
initialized gradient descent here,

90
00:05:31,451 --> 00:05:35,530
it would eventually converge
to that optimum. Whereas here,

91
00:05:35,710 --> 00:05:39,430
it would actually have converged
to slowly move to that optimum.

92
00:05:40,690 --> 00:05:43,810
So let's see this in action. Uh, so here,

93
00:05:43,811 --> 00:05:48,070
this is a classification problem where
we have these dots here that are in one

94
00:05:48,071 --> 00:05:52,780
class. And then these dots here that
are another class and that this is the,

95
00:05:52,800 --> 00:05:57,500
uh, uh, decision, uh, a
decision boundary for,

96
00:05:57,770 --> 00:06:00,680
uh, the castigation that's performed
by eight, this neural network.

97
00:06:01,130 --> 00:06:05,120
And here we have the training error.
We see it's still going down a little bit,

98
00:06:05,121 --> 00:06:07,040
but it's, uh, it's mostly converged.

99
00:06:07,370 --> 00:06:11,960
Now let's see what happens
if I reinitialize the neural
net with different, uh,

100
00:06:11,961 --> 00:06:15,530
random weights. Now let's get to
what it convergence. Does it change?

101
00:06:15,710 --> 00:06:19,490
Does it still have this shape or will
they have a fairly different chain?

102
00:06:20,760 --> 00:06:21,360
Okay,

103
00:06:21,360 --> 00:06:22,480
sorry.
It's retraining.

104
00:06:23,950 --> 00:06:28,320
And we see now that the shape has changed
a little bit. So here we have, uh,

105
00:06:28,630 --> 00:06:31,990
this partier used to be not
associated with that class,

106
00:06:31,991 --> 00:06:36,460
but with this class and that this
part here also is a bit less flat.

107
00:06:37,020 --> 00:06:39,760
Um, interesting. That's initialize. Again,

108
00:06:40,540 --> 00:06:43,700
this looks a little bit more like
before we've seem to have come,

109
00:06:43,990 --> 00:06:48,610
it seems to be converging
to similar region. Uh,

110
00:06:48,730 --> 00:06:52,660
let's do that again, a little bit about,
this is starting to be quite different.

111
00:06:52,661 --> 00:06:56,260
So we get a big region here
associated with that class.

112
00:06:57,460 --> 00:07:02,170
So as we see with different initial,
uh, different initialization,

113
00:07:02,260 --> 00:07:05,610
so different random seeds essentially
that generate the random numbers and

114
00:07:05,611 --> 00:07:08,620
utilize the weights.
We can get fairly different solutions.

115
00:07:08,890 --> 00:07:12,610
But of this is that it always at least
perfectly classify as the training set.

116
00:07:12,670 --> 00:07:16,990
But a new examples which say might
lie here, different neural nets,

117
00:07:16,991 --> 00:07:20,020
my life provide different answers
depending on the initialization.

118
00:07:24,710 --> 00:07:25,320
Okay.

119
00:07:25,320 --> 00:07:29,340
Okay. Uh, I've also talked about
grading the sand and I've, uh, you know,

120
00:07:29,760 --> 00:07:34,320
thus far just said that, you know,
it works fine and it's a, uh,

121
00:07:34,600 --> 00:07:38,910
optimizes the training error. But
what about convergence? So, uh,

122
00:07:38,970 --> 00:07:43,470
what are the conditions that we need to
guarantee that it will eventually, uh,

123
00:07:43,650 --> 00:07:47,970
uh, converge to the true solution
instead of, uh, at home point,

124
00:07:48,720 --> 00:07:50,710
uh, reach, uh,

125
00:07:50,730 --> 00:07:54,750
reach a value in maybe cycle
and never actually zoom,

126
00:07:54,810 --> 00:07:56,640
zoom in on the actual solution?

127
00:07:57,720 --> 00:08:01,530
The two conditions we need is that a,
so alpha t,

128
00:08:01,531 --> 00:08:06,480
which is the learning rate for the
teeth, uh, update of the neural nets, uh,

129
00:08:06,570 --> 00:08:07,403
parameters.

130
00:08:07,920 --> 00:08:12,750
So the sum of all the learning waits for
all iterations needs to divergent needs

131
00:08:12,751 --> 00:08:14,460
to go to infinity,

132
00:08:15,060 --> 00:08:20,040
but the sum of the square of the
learning rates needs to converge.

133
00:08:20,160 --> 00:08:22,370
So that needs to be a converging some.

134
00:08:22,980 --> 00:08:26,190
So notice that in the case
where Alpha is constant,

135
00:08:26,610 --> 00:08:30,830
then this would, uh, this
was actually a diverged.

136
00:08:31,380 --> 00:08:35,220
And so a constant learning rate does
not guarantee that stochastic gradient

137
00:08:35,221 --> 00:08:39,230
descent will eventually converge.
Okay.

138
00:08:39,270 --> 00:08:43,320
And I'll talk about why this
isn't always a bad problem,

139
00:08:43,980 --> 00:08:48,720
but if you do want to satisfy
these conditions, then uh,

140
00:08:48,721 --> 00:08:53,070
what we'll want to, uh, you know,
the few decreasing, uh, strategies.

141
00:08:53,340 --> 00:08:57,150
One that's very common is this one.
So we just take an initial learning rate.

142
00:08:57,690 --> 00:08:59,700
So that's our regular
alphabet hyper parameter.

143
00:08:59,940 --> 00:09:03,620
And then we divided by one plus the
number of updates we've done so far,

144
00:09:03,690 --> 00:09:07,140
times a decreasing constant
or decrease constant, uh,

145
00:09:07,141 --> 00:09:11,240
which is another hyper parameter.
Now that we need to select a,

146
00:09:11,250 --> 00:09:15,630
this is another strategy which has been
proposed in the literature and it has

147
00:09:15,631 --> 00:09:16,950
certain properties.

148
00:09:17,440 --> 00:09:21,660
So we divide just by the number
of either update so far, but uh,

149
00:09:21,990 --> 00:09:25,290
to the, uh, my decrease
constant parameter.

150
00:09:25,620 --> 00:09:30,210
And in this case we have to satisfy
this condition. Uh, and, uh,

151
00:09:30,540 --> 00:09:32,790
looking at the literature,
there are other proposals.

152
00:09:32,820 --> 00:09:36,630
This one is probably one of
the most popular in general.

153
00:09:36,631 --> 00:09:41,100
It's actually a good idea to keep a fixed
learning rate like we've seen in the

154
00:09:41,190 --> 00:09:45,400
originally I saw a fixed
Alpha for uh, a few, uh,

155
00:09:45,450 --> 00:09:49,500
the first few updates with their first
few epochs and then eventually start

156
00:09:49,501 --> 00:09:52,830
decreasing it. Uh, so we make
a lot of, I guess initially,

157
00:09:52,831 --> 00:09:57,180
and then we start decreasing the
learning rate so that it can zoom in and

158
00:09:57,181 --> 00:09:59,670
converge to a good value.
Now,

159
00:09:59,671 --> 00:10:04,671
the reason that we might still use a
constant alpha is that remember that we

160
00:10:04,831 --> 00:10:08,520
don't necessarily care about optimizing
perfectly the training error because

161
00:10:08,521 --> 00:10:10,140
what we care about this generalization,

162
00:10:10,320 --> 00:10:15,140
where we care about this performing
well on the test set. And so, uh,

163
00:10:15,150 --> 00:10:19,670
it might be that before
we get to get, uh, to, uh,

164
00:10:19,710 --> 00:10:23,970
come close to the best training error
configuration of the neural net,

165
00:10:24,300 --> 00:10:28,110
we actually weigh into a region
that has a lot of our fittings.

166
00:10:28,140 --> 00:10:31,950
So in the curve that we've seen where
we have the validation set that's like

167
00:10:31,951 --> 00:10:36,640
this, the generalization performance in
the training set, it might be that, um,

168
00:10:36,750 --> 00:10:39,300
you know, if we get to convergence,
we actually going to get very,

169
00:10:39,301 --> 00:10:43,050
very bad a value on the validation set,
a lot of over fitting.

170
00:10:43,560 --> 00:10:47,340
So because we don't care about optimizing
the training set perfectly and we

171
00:10:47,550 --> 00:10:49,830
mainly care about getting a
good generalization error,

172
00:10:50,310 --> 00:10:53,610
as long as the learning rate
does not too big, then, uh,

173
00:10:53,611 --> 00:10:58,611
we very well might stop learning with
early stopping way before the algorithm,

174
00:10:59,540 --> 00:11:03,420
uh, uh, starts cycling and not
converging to particular lab value.

175
00:11:03,990 --> 00:11:08,820
So in practice, as long as
Alpha is not too big, then,

176
00:11:08,850 --> 00:11:11,760
uh, we will, uh, early stopping.

177
00:11:11,761 --> 00:11:16,140
We'll have a stop training way before
we get any problems in terms of

178
00:11:16,141 --> 00:11:20,940
convergence. Okay. So, um, unless
you actually for some reason really,

179
00:11:20,941 --> 00:11:23,920
really care about getting a
very good training error. Uh,

180
00:11:24,030 --> 00:11:26,460
if you think that actually
you're not going to overfit,

181
00:11:26,520 --> 00:11:30,450
then you tend to almost be
in the overfitting situation.

182
00:11:30,690 --> 00:11:34,500
A constant Alpha is often something
that works well in practice.

183
00:11:37,500 --> 00:11:41,820
Other, uh, variations on a gradient
descent that we can see in the literature.

184
00:11:42,030 --> 00:11:45,360
Um, sometimes people use what's
called minibatch learning.

185
00:11:45,810 --> 00:11:50,810
So a mini batch is just a set of maybe
a hundred examples that we've picked

186
00:11:50,971 --> 00:11:54,980
randomly from the training
set and idea is to, uh,

187
00:11:55,030 --> 00:12:00,030
instead of using the gradients over a
single example to perform our update as in

188
00:12:02,561 --> 00:12:03,790
a stochastic gradient descent,

189
00:12:04,030 --> 00:12:09,030
we just use the average gradient of all
the examples in my mini batch of enemy.

190
00:12:10,510 --> 00:12:14,200
Minibatch will often be about 128
examples, something like that.

191
00:12:15,070 --> 00:12:19,570
So the average gradient is the same
thing as taking the grain of the average

192
00:12:19,600 --> 00:12:22,090
regularized loss or the
average loss on the mini batch.

193
00:12:23,200 --> 00:12:26,740
And the idea is that this is going to
be because we're taking the average of a

194
00:12:26,741 --> 00:12:29,230
bunch of, uh, uh, example,

195
00:12:29,231 --> 00:12:34,060
we're going to get a more accurate
estimate of the true gradient of the,

196
00:12:34,090 --> 00:12:37,810
uh, uh, average, um, uh,

197
00:12:37,900 --> 00:12:40,990
of the average a loss.
I shouldn't say risk here.

198
00:12:40,991 --> 00:12:45,040
It's a more accurate
estimate of the average,

199
00:12:46,830 --> 00:12:51,640
uh, loss gradient. And, uh,

200
00:12:51,850 --> 00:12:56,850
one advantage in terms of computations
is that if we are working with mini

201
00:12:57,251 --> 00:13:00,680
batches, then we can write the uh, uh,

202
00:13:00,740 --> 00:13:04,690
forward prop and backdrop, uh,
operations. So backpropagation,

203
00:13:05,140 --> 00:13:08,980
uh,
using some matrix matrix operations,

204
00:13:08,981 --> 00:13:13,720
matrix multiplications and performing
Matrix Matrix multiplications is going to

205
00:13:13,721 --> 00:13:18,370
be much more efficient with
certain libraries like blast blast,

206
00:13:18,371 --> 00:13:23,371
which are just a very well known low
level library for performing a linear

207
00:13:23,621 --> 00:13:27,560
Algebra operations. So Matrix,
Matrix multiplications,

208
00:13:27,570 --> 00:13:32,330
I'm going to be much faster, uh,
with the library like blast then, uh,

209
00:13:32,440 --> 00:13:37,440
performing 128 forward
propagation and backpropagation.

210
00:13:37,721 --> 00:13:42,721
So performing all 128
grade him a computations
simultaneously with Matrix Matrix

211
00:13:44,021 --> 00:13:47,020
operations will either procedure
which is much more efficient.

212
00:13:47,530 --> 00:13:52,270
And that's because a library is like plus
our cash aware and I try to optimize,

213
00:13:52,680 --> 00:13:57,610
uh, the memory access. Um, so, uh,

214
00:13:57,611 --> 00:14:00,130
so yeah, I'll, I'll, I'll let you
look at the literature for you can,

215
00:14:00,310 --> 00:14:00,891
you can do this.

216
00:14:00,891 --> 00:14:04,960
So you can try to think about how we
can simultaneously compute gradients for

217
00:14:04,961 --> 00:14:09,961
many examples by performing instead of
performing matrix vector operations and

218
00:14:10,181 --> 00:14:14,680
like in regular backdrop performing
matrix, matrix multiplication. Um,

219
00:14:14,740 --> 00:14:18,910
and other approach that's often use is
to instead of using the gradient of a

220
00:14:18,911 --> 00:14:19,750
single example,

221
00:14:19,751 --> 00:14:24,751
use an exponential average of gradients
as we perform a gradient descent.

222
00:14:27,070 --> 00:14:28,810
So the uh,

223
00:14:28,900 --> 00:14:33,190
direction for updating our parameters
for the teeth update could be in the

224
00:14:33,220 --> 00:14:37,390
instead of just, and so this is for
the, uh, non non regularized loss.

225
00:14:37,810 --> 00:14:42,490
Instead of just taking the gradient of
the loss for our current example will

226
00:14:42,491 --> 00:14:45,100
also combine it with better,

227
00:14:45,101 --> 00:14:50,101
which is going to be a hyper parameter
times d exponential average that,

228
00:14:51,410 --> 00:14:54,310
uh,
I had computers at the previous updates.

229
00:14:54,830 --> 00:14:56,720
So if you look at this formula,

230
00:14:56,721 --> 00:15:01,721
you can see that at any time the update
direction is a combination of not just

231
00:15:02,211 --> 00:15:04,640
the green and for the
current training example,

232
00:15:04,641 --> 00:15:09,160
but also all the other previous
training examples which are in this, uh,

233
00:15:09,430 --> 00:15:10,580
uh,
previous term here.

234
00:15:11,690 --> 00:15:16,100
So this can be useful if we have
plateaus by, uh, moving more quickly,

235
00:15:16,101 --> 00:15:19,590
getting a grade in
which is larger. Uh, uh,

236
00:15:19,640 --> 00:15:22,940
if all the Graydon's always
pointed the same direction.

237
00:15:23,030 --> 00:15:28,030
So intuitively it says if the
optimization is gaining momentum when we,

238
00:15:28,220 --> 00:15:32,990
uh, when all the gradients are always
pointing in the same direction.

239
00:15:32,991 --> 00:15:35,960
So it's as if we had a ball
that's moving on a slope,

240
00:15:35,961 --> 00:15:37,760
which is always going straight.

241
00:15:37,910 --> 00:15:42,500
So then the ball keeps going faster
and faster. It's gaining momentum.

242
00:15:42,501 --> 00:15:46,140
So it's the same idea here, which might
allow us to move faster in the factors.

243
00:15:48,800 --> 00:15:52,690
Funnily, I want to described
a different method for, uh,

244
00:15:52,980 --> 00:15:56,600
in optimization for
optimizing the function. Uh,

245
00:15:56,660 --> 00:15:59,180
it's called Newton's method.
And a,

246
00:15:59,190 --> 00:16:02,540
it's an improved way of
optimizing a particular function.

247
00:16:02,541 --> 00:16:07,541
And I want to mention it because we often
see a variance of that basic Newton's

248
00:16:08,001 --> 00:16:12,710
method. In the papers on developing
better neural network training algorithms.

249
00:16:13,520 --> 00:16:17,960
So I'll just give you a gist of the idea
behind Newton's method and this will

250
00:16:18,320 --> 00:16:22,520
help you explore the literature on
more advanced neural network training

251
00:16:22,521 --> 00:16:26,360
algorithms. So, uh, in
Newton Newton's method,

252
00:16:26,630 --> 00:16:30,740
the idea is that if you
want to minimize something,

253
00:16:30,741 --> 00:16:33,980
so imagine we have just a single example.
You want to minimize,

254
00:16:34,190 --> 00:16:38,780
it's a training loss.
So the loss when given x as input,

255
00:16:39,230 --> 00:16:43,280
uh, the neural network with a
particular set of parameters, uh,

256
00:16:43,310 --> 00:16:46,370
predict something f of x and
a that's comparable or why.

257
00:16:46,760 --> 00:16:49,460
So we want to optimize that loss.
Uh,

258
00:16:49,490 --> 00:16:54,310
well one thing we could do is say,
well instead of optimizing this last,

259
00:16:54,311 --> 00:16:56,240
which is non comebacks and complicated,

260
00:16:56,510 --> 00:17:01,510
but just approximated around the point
which corresponds to the current value of

261
00:17:02,301 --> 00:17:05,750
my parameter in my grading and descents,
uh,

262
00:17:06,040 --> 00:17:09,020
migrant gradient descent procedure
in my training procedure.

263
00:17:10,100 --> 00:17:14,640
And so what we could do is use a terrorist
series of expansion up to, uh, the, uh,

264
00:17:14,720 --> 00:17:19,520
second order, a term where we
say that, well, around a t,

265
00:17:19,610 --> 00:17:23,090
which is the current value of
mine, neural nets parameters. Well,

266
00:17:23,091 --> 00:17:28,091
the value of the loss for any a set of
parameters data is going to be close to

267
00:17:31,131 --> 00:17:34,280
that out. So it's going to
be approximately this value,

268
00:17:34,281 --> 00:17:36,560
the value of the loss at
the current parameter,

269
00:17:36,800 --> 00:17:41,800
plus the gradients at my current
parameter times the difference between the

270
00:17:44,120 --> 00:17:48,110
theater, which is any potential value
of the weights of my neural net,

271
00:17:48,320 --> 00:17:52,480
minus the current value of the weights.
And plus another term,

272
00:17:52,500 --> 00:17:57,490
the second order term, which
involves a, uh, product, uh,

273
00:17:57,540 --> 00:18:02,220
left tried product with instead of being
with the gradient, the, which is the,

274
00:18:02,550 --> 00:18:06,420
um, uh, vector of all
first order derivatives,

275
00:18:06,630 --> 00:18:10,710
it's going to be the Eshun,
which is the matrix of all second order,

276
00:18:11,330 --> 00:18:14,730
uh, dairy dips. And, uh, so we get,

277
00:18:14,731 --> 00:18:18,240
and then we get the left and
right product here with, uh,

278
00:18:18,270 --> 00:18:21,870
the difference between the value of the
weights minus the current value of the

279
00:18:21,871 --> 00:18:25,410
weights. And the idea behind
Newton's method is to say, well,

280
00:18:25,470 --> 00:18:28,800
instead of optimizing the loss,
I'll actually optimize this.

281
00:18:28,801 --> 00:18:31,590
Simpler function does
simpler approximation,

282
00:18:32,160 --> 00:18:36,870
which comes from the dealer territories,
series expansion. Uh, and, uh,

283
00:18:36,871 --> 00:18:39,120
what's nice is that the
second order version,

284
00:18:39,121 --> 00:18:41,640
because it's a going to be quadratic,

285
00:18:41,641 --> 00:18:44,160
we get actually optimizing perfectly.

286
00:18:44,670 --> 00:18:48,840
So visually it means that they
mentioned that it had dysfunction here.

287
00:18:49,280 --> 00:18:54,090
So imagine this line was a true function
and imagined that currently my weights.

288
00:18:54,091 --> 00:18:58,620
So this would be a very simple function
with just a single parameter. Uh,

289
00:18:58,621 --> 00:19:02,740
well I would just locally approximate
it with a quadratic function. Uh, sorry,

290
00:19:02,790 --> 00:19:03,900
like this.

291
00:19:04,410 --> 00:19:09,410
So I would just assume that this blue
line is actually well approximated by this

292
00:19:09,960 --> 00:19:13,290
red line. And we see that
in these very close to, uh,

293
00:19:13,410 --> 00:19:15,060
the current value of the parameter.
It's very,

294
00:19:15,210 --> 00:19:19,470
the two lines are essentially on top of
each other, but as we go further away,

295
00:19:19,620 --> 00:19:24,330
it's not as good. Um, but still locally
it would be a good approximation.

296
00:19:24,331 --> 00:19:29,280
So it might be a good idea to just take
that approximation and move directly to,

297
00:19:29,620 --> 00:19:32,610
it's a global minimum,
which would be around here.

298
00:19:33,190 --> 00:19:36,240
And because it's a quadratic
function, we can actually, uh, uh,

299
00:19:36,270 --> 00:19:39,510
find what's analytically,
what's the true solution.

300
00:19:39,540 --> 00:19:44,540
So we can take the derivatives with
resect to our parameter of this a second

301
00:19:45,811 --> 00:19:50,790
order approximation and set it
to zero. Uh, so this, you know,

302
00:19:50,820 --> 00:19:53,320
taking the derivative of a,

303
00:19:53,350 --> 00:19:58,040
with respect to theater of
this whole expression, uh, uh,

304
00:19:58,210 --> 00:20:03,210
it gives me this and we can show that,

305
00:20:05,670 --> 00:20:08,960
uh,
the solution then a curse sponsors.

306
00:20:08,961 --> 00:20:12,780
So the new point would
actually be my current point,

307
00:20:12,781 --> 00:20:15,600
my current parameter value multiply as,
sorry,

308
00:20:15,601 --> 00:20:20,070
minus the inverse of the
session times degrading.

309
00:20:20,220 --> 00:20:23,280
So notice that this is actually
quite similar to grading dissent.

310
00:20:23,400 --> 00:20:26,070
Great in the sense,
instead of having a Hessian here,

311
00:20:26,071 --> 00:20:30,390
just has a single scalar alpha.
Well, with Newton's method,

312
00:20:30,780 --> 00:20:35,780
we get to actually take into account
the curvature of the function,

313
00:20:36,180 --> 00:20:38,610
which is taking into account by the,
by the action.

314
00:20:38,760 --> 00:20:43,380
So in particular from a function
which is very flat like this, um,

315
00:20:43,410 --> 00:20:47,980
this term here, the inverse
session is going to be a quite big,

316
00:20:48,490 --> 00:20:53,290
which means that if I was here, then
it would, um, this term being big,

317
00:20:53,350 --> 00:20:58,230
it means that it would, I, I would,
uh, take a step that would be much, uh,

318
00:20:58,300 --> 00:21:02,110
bigger. So I have a bigger
term times the same gradient.

319
00:21:02,320 --> 00:21:05,290
Then if I had the function
with high curvature,

320
00:21:05,291 --> 00:21:08,050
which would look something like this.
So at this point here,

321
00:21:08,051 --> 00:21:11,230
because I have higher curvature,
this term would be smaller.

322
00:21:11,231 --> 00:21:13,570
It would make a smaller smartest step.

323
00:21:13,900 --> 00:21:17,950
So the nice thing is that if you
have a plateau like function here,

324
00:21:18,280 --> 00:21:23,200
the Hessian allows us to make bigger
steps and so we might be able to move

325
00:21:23,201 --> 00:21:26,590
faster into a plateau where as
if we end up very steep function,

326
00:21:26,680 --> 00:21:30,280
then it's actually going to adapt and
take much smallest steps and updating the

327
00:21:30,281 --> 00:21:34,750
parameters. Now, um, there
are still issues with this.

328
00:21:34,780 --> 00:21:39,280
The first one is that, uh, well
this Hessian is a matrix of size,

329
00:21:39,281 --> 00:21:44,230
number of parameters by a
number of parameters and
then there will network with

330
00:21:44,231 --> 00:21:48,160
a few thousand units that easily
reaches and the millions of parameters.

331
00:21:48,160 --> 00:21:51,820
So this would be 1 million
by 1 million, uh, matrix say,

332
00:21:52,180 --> 00:21:56,170
and this is way too big to
even compute for every update.

333
00:21:56,430 --> 00:21:58,850
And it's also way too big to just,
uh,

334
00:21:58,930 --> 00:22:02,470
inverse as well because inverting the
matrix is also going to be very expensive.

335
00:22:03,130 --> 00:22:05,890
So this is only practical.
They're very small neural network.

336
00:22:06,340 --> 00:22:10,990
And also it's only we can apply
this update only if it's a,

337
00:22:10,991 --> 00:22:14,620
at least locally comebacks. If the
function is locally convex, that is,

338
00:22:14,621 --> 00:22:18,550
if at a point I compute the question,

339
00:22:18,970 --> 00:22:22,510
uh, if it's locally convex
at that point, I mean, uh,

340
00:22:22,540 --> 00:22:26,320
then the action is actually going to be
positive. Definite, which means that this,

341
00:22:26,321 --> 00:22:30,870
this matrix here, this inverted
matrix is going to be a positive, uh,

342
00:22:30,910 --> 00:22:35,620
definite. And, uh, so if I
locally add a function, which is,

343
00:22:35,890 --> 00:22:39,940
uh, like this, then the Newton's
method update. So in this case,

344
00:22:39,941 --> 00:22:44,770
I would have a Hessian which
would be negative, definite.
And, uh, uh, if I was to,

345
00:22:45,320 --> 00:22:49,600
if I was to minimize the quadratic
approximation of that function,

346
00:22:49,601 --> 00:22:53,200
say this was the quadratic approximation.
And we clearly see that in this case,

347
00:22:53,201 --> 00:22:54,300
the optimum would,

348
00:22:54,560 --> 00:22:58,600
would be to take the value and
increase it and an athlete.

349
00:22:59,320 --> 00:23:03,880
So a, so Newton's method is only practical
if we have these two constraints.

350
00:23:04,180 --> 00:23:06,580
Uh,
still there are a lot of albums.

351
00:23:06,581 --> 00:23:11,050
I take inspiration from Newton's
method and, uh, try to, uh,

352
00:23:11,320 --> 00:23:15,910
either not have to rely on computing
the full question and somehow make sure

353
00:23:15,911 --> 00:23:20,911
that the matrix that we have
here is a still positive,

354
00:23:21,041 --> 00:23:22,900
definite.
And so for that,

355
00:23:22,901 --> 00:23:27,430
I encourage you to look at the recommended
readings on the course's website.

356
00:23:27,640 --> 00:23:29,920
Well, you see, uh, some, uh,

357
00:23:29,921 --> 00:23:34,921
examples of methods that try to correct
for these issues and get a naturally

358
00:23:35,920 --> 00:23:38,440
practical logarithm for
large neural networks.

