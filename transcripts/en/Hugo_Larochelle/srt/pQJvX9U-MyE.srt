1
00:00:00,640 --> 00:00:01,211
In this video,

2
00:00:01,211 --> 00:00:05,500
we actually see how we can perform
sequence classification in a linear chain

3
00:00:05,501 --> 00:00:09,360
conditional random fields.
So,

4
00:00:09,361 --> 00:00:13,880
so far what we've seen is how we could
compute the probability, the churn,

5
00:00:13,940 --> 00:00:18,660
probability of a whole a sequence
of labels given an input sequence.

6
00:00:18,960 --> 00:00:19,650
Uh,

7
00:00:19,650 --> 00:00:24,090
so we've seen how we could compute in
particular this nasty partition function

8
00:00:24,330 --> 00:00:25,710
using dynamic programming.

9
00:00:27,570 --> 00:00:27,960
Okay.

10
00:00:27,960 --> 00:00:32,550
And we are also seeing how
we could compute marginal
probabilities. For instance,

11
00:00:32,551 --> 00:00:36,600
the marginal probabilities of
a label like a given position.

12
00:00:36,601 --> 00:00:39,960
K given the full sequence and again,

13
00:00:40,080 --> 00:00:44,910
performing this computation required
the same dynamic program, uh,

14
00:00:44,911 --> 00:00:48,360
that uh, uh, we can use for
computing the partition function,

15
00:00:48,510 --> 00:00:52,800
what would compute these log alpha and
lug better tables and use them in the

16
00:00:52,801 --> 00:00:55,380
computation of the marginal probability.

17
00:00:58,120 --> 00:00:58,440
Yeah.

18
00:00:58,440 --> 00:01:01,980
Now here what we'll see is how we
can actually perform classification.

19
00:01:02,040 --> 00:01:06,680
So if I give you a sequence of
inputs now I ask you to, uh,

20
00:01:06,780 --> 00:01:07,680
provide with,

21
00:01:07,910 --> 00:01:12,910
provide me a sequence of labels that
you think is a good prediction for the

22
00:01:13,121 --> 00:01:17,640
sequence of, uh, the, the actual
true sequence of labels, uh, that,

23
00:01:17,730 --> 00:01:20,940
uh, that a human or an
expert would have predicted.

24
00:01:22,440 --> 00:01:24,660
So, um, Darra two options here.

25
00:01:24,661 --> 00:01:28,950
So we'll see two different procedures for
performing that prediction. The first,

26
00:01:29,040 --> 00:01:32,190
uh, is perhaps those simplest, uh,

27
00:01:32,220 --> 00:01:35,040
what we do is that for each position,
Kay,

28
00:01:35,370 --> 00:01:40,050
we compute the marginal p
of y k given the sequence.

29
00:01:40,110 --> 00:01:43,170
So we compute the probability
for any potential label,

30
00:01:43,171 --> 00:01:46,650
why k of it being the true
key if label in the sequence.

31
00:01:46,950 --> 00:01:50,660
And then as our prediction,
we just say that, uh,

32
00:01:50,670 --> 00:01:55,360
in our predicted sequence, the label
for the prediction, uh, the key,

33
00:01:55,361 --> 00:01:59,080
if the position is going to beat the
label that has the highest marginal

34
00:01:59,100 --> 00:02:02,010
probability.
Now I'm,

35
00:02:02,011 --> 00:02:06,330
so this is very similar to
how for a neural network, uh,

36
00:02:06,360 --> 00:02:10,200
that would take just a single input. So
none in the sequence prediction sending,

37
00:02:10,440 --> 00:02:14,260
we would just feed the input to
the neural net. It would produce a,

38
00:02:14,320 --> 00:02:17,110
a output distribution,
uh,

39
00:02:17,130 --> 00:02:20,100
for what the true label
would be for the given input.

40
00:02:20,101 --> 00:02:24,840
And we just take the label
with the highest probability
and now we can actually

41
00:02:24,841 --> 00:02:29,100
show that this procedure for
constructing a prediction sequence.

42
00:02:29,380 --> 00:02:34,260
So a sequence of labels that we
would provide us an answer, uh, is,

43
00:02:34,350 --> 00:02:34,630
uh,

44
00:02:34,630 --> 00:02:39,630
corresponds to the prediction that
minimizes the sum of classification errors

45
00:02:40,680 --> 00:02:45,570
over the whole sequence if we assume that
our linear conditional random fields.

46
00:02:45,780 --> 00:02:50,610
So our model is actually the true model
is the true distribution that produced

47
00:02:50,611 --> 00:02:55,160
the a sequenced labels in our
dataset. So we actually, uh,

48
00:02:55,440 --> 00:03:00,150
this gives some justification for
using that procedure. And, uh,

49
00:03:00,210 --> 00:03:02,620
uh,
because this is an interesting concept,

50
00:03:02,640 --> 00:03:07,640
I actually will actually do the
demonstration for why maximizing for each

51
00:03:07,871 --> 00:03:08,704
position,

52
00:03:08,800 --> 00:03:13,300
the marginal distribution actually
corresponds to finding this, uh,

53
00:03:13,470 --> 00:03:16,390
minimizing sequence of the,
some of classification there.

54
00:03:16,600 --> 00:03:18,760
If we assume our model is
the treat distribution.

55
00:03:22,540 --> 00:03:25,480
So what do we want to do is,
uh,

56
00:03:25,600 --> 00:03:29,650
find the expression for what
is the predicted sequence?

57
00:03:29,680 --> 00:03:32,920
Why start that minimizes in expectations.

58
00:03:32,921 --> 00:03:35,830
So I should say this is the
minimization in expectation.

59
00:03:35,831 --> 00:03:37,600
According to our model of the,

60
00:03:37,601 --> 00:03:42,240
some of the classification there are for
each position. So where you have these,

61
00:03:42,241 --> 00:03:47,230
some of the identity function of whether
the true label according to our model,

62
00:03:47,231 --> 00:03:52,231
why Cain soars to label in the
sense that it's the label that uh,

63
00:03:52,690 --> 00:03:54,070
based on, uh, uh,

64
00:03:54,100 --> 00:03:59,100
the distribution of our model is different
from the predicted label at position

65
00:03:59,891 --> 00:04:01,270
Kate.
So why star Kay?

66
00:04:01,520 --> 00:04:05,770
So we want to minimize in expectation
that some of castigation there per

67
00:04:05,771 --> 00:04:06,604
position.

68
00:04:09,080 --> 00:04:13,370
So here I'm just writing explicitly
what this expectation here is.

69
00:04:13,550 --> 00:04:18,550
It's the nested some over the first
a label up to the last label of the,

70
00:04:19,611 --> 00:04:24,611
some of Perella a preposition
classification error
weighted by the probability

71
00:04:24,861 --> 00:04:29,580
according to my model that I would
observe this sequence of labels a y.

72
00:04:29,700 --> 00:04:34,230
So these white case here,
yeah,

73
00:04:34,250 --> 00:04:37,400
I'm running the fact that the
minimization over all sequences,

74
00:04:37,401 --> 00:04:39,080
why star is equivalent,

75
00:04:39,081 --> 00:04:43,970
so perform that immunization is the
same as performing minimization, uh, uh,

76
00:04:44,040 --> 00:04:48,710
sequentially over the first
label until the last label.

77
00:04:48,800 --> 00:04:52,810
So it's this minimization over all
sequences is the same as these nested

78
00:04:52,820 --> 00:04:56,540
minimization over first seek
first, uh, uh, position.

79
00:04:56,541 --> 00:04:58,490
Second position until the last position.

80
00:05:01,770 --> 00:05:05,260
Now here in what I'm doing is that
I'm going to take that, uh, first,

81
00:05:05,261 --> 00:05:08,550
I'm going to introduce this here,
uh,

82
00:05:08,560 --> 00:05:11,800
which allows me to just
remove this parent. This is,

83
00:05:11,980 --> 00:05:14,440
and I'm going to change
the order of the songs.

84
00:05:14,470 --> 00:05:19,300
So I'm going to actually some first over
the position k and then some over the

85
00:05:19,301 --> 00:05:24,220
value of the first label up to the last
label. So that's why I have key here.

86
00:05:24,230 --> 00:05:29,120
Then first label up to the
last label. Okay. So, uh, that,

87
00:05:29,130 --> 00:05:29,800
that's all I've done,

88
00:05:29,800 --> 00:05:33,570
introduced this here and change the
order of the sums up the nest itself.

89
00:05:37,670 --> 00:05:39,020
Now,
uh,

90
00:05:39,050 --> 00:05:44,050
I noticed that this term here only
depends on the value of a y k for the

91
00:05:47,361 --> 00:05:51,200
position k that's indexed here.
So I can actually,

92
00:05:51,350 --> 00:05:55,760
this is a constant with
respect to y one Y K minus one.

93
00:05:55,761 --> 00:05:57,680
Why k plus one up two.

94
00:05:57,681 --> 00:06:01,940
So there should be three dots
here up to the last label.

95
00:06:02,230 --> 00:06:06,020
So I can actually push this here,
uh,

96
00:06:06,030 --> 00:06:09,680
up upwards in the nested sums,
writes in from the,

97
00:06:09,681 --> 00:06:13,990
the sum over the value of
the Keith label. And the, uh,

98
00:06:14,030 --> 00:06:16,790
sequence position gay ass,
which I'm measuring the error.

99
00:06:19,670 --> 00:06:20,390
Okay,

100
00:06:20,390 --> 00:06:23,570
next. I know this, that
this expression here,

101
00:06:23,720 --> 00:06:27,410
that's just the marginal p
of y k given this sequence.

102
00:06:27,530 --> 00:06:30,200
So I'm taking the full joint distribution,

103
00:06:30,201 --> 00:06:32,120
well conditioned on the input sequence,

104
00:06:32,121 --> 00:06:34,160
the joint distribution over
the whole sequence. Why?

105
00:06:34,161 --> 00:06:38,230
But I'm summing over all the values
that every label can tech take.

106
00:06:38,240 --> 00:06:40,160
Accept the label.
Why King?

107
00:06:40,280 --> 00:06:45,280
So that's the marginal p of y given X.

108
00:06:47,850 --> 00:06:48,720
Next, I, uh,

109
00:06:48,760 --> 00:06:53,760
realized that this sum here over all
position of what is essentially the

110
00:06:54,930 --> 00:06:58,650
expected error for that position.
Uh,

111
00:06:58,651 --> 00:07:03,651
well that's essentially the sum of the
sum over why one of whether I'm making in

112
00:07:04,381 --> 00:07:09,210
there or by predicting why star one
times weighted by the probability of

113
00:07:09,211 --> 00:07:13,950
actually seeing why one as the
first label plus a. So I do this,

114
00:07:13,951 --> 00:07:17,260
the sum over all, uh, case. So then, uh,

115
00:07:17,280 --> 00:07:21,480
sum up to the sum over y capital gay,
uh,

116
00:07:21,481 --> 00:07:24,720
of whether I'm making an error
by predicting why prime at,

117
00:07:24,721 --> 00:07:28,320
why star capital gay weighted by the
probability that the true Labor would

118
00:07:28,321 --> 00:07:31,980
actually be, why capital gain. Now, uh,

119
00:07:31,981 --> 00:07:36,690
so I have a son and a minimizing
over this sum for each label. Well,

120
00:07:36,691 --> 00:07:40,200
because each term in the, some actually
it depends on this single label.

121
00:07:40,230 --> 00:07:45,230
Well that's the same as minimizing the
first terms are with respect to why a one

122
00:07:46,670 --> 00:07:50,700
then actually, sorry, why
star one only. And uh,

123
00:07:50,840 --> 00:07:54,630
and I don't need to minimize over the
other terms because they're not involved

124
00:07:54,631 --> 00:07:59,631
in that expression plus the minimization
with respect to why start to up to the,

125
00:08:00,570 --> 00:08:00,931
uh,

126
00:08:00,931 --> 00:08:05,931
so plus up to the minimization over
the last label of this term here.

127
00:08:07,670 --> 00:08:08,110
Okay.

128
00:08:08,110 --> 00:08:08,261
Okay.

129
00:08:08,261 --> 00:08:13,100
So that's because the minimization over
all labels for a term that depends on,

130
00:08:13,101 --> 00:08:16,410
on the one label is the same
as minimum as a minimum,

131
00:08:16,480 --> 00:08:19,840
minimizing over a single
position or a single label.

132
00:08:24,100 --> 00:08:27,520
And so, uh, now the, uh, uh,

133
00:08:27,550 --> 00:08:30,580
we noticed that you mean remove some ink.
So we know this,

134
00:08:30,581 --> 00:08:34,990
that this expression here,
which is the sum of a one,

135
00:08:35,020 --> 00:08:36,730
if why one is not,

136
00:08:36,760 --> 00:08:41,760
why star one weighted that by the
probability of observing why one uh,

137
00:08:43,421 --> 00:08:47,980
so that's the sum of the property of
why one for every case, except when,

138
00:08:48,040 --> 00:08:51,430
why one is equal to y star one.
So in other words,

139
00:08:51,670 --> 00:08:56,310
so that's the probability of the
first label not being why star one,

140
00:08:56,700 --> 00:09:01,530
which is equal to one minus
the probability of the
label of being why star one.

141
00:09:02,070 --> 00:09:06,900
And so similarly for this term we also
get one minus the probability of y Star

142
00:09:06,901 --> 00:09:07,734
capital k

143
00:09:10,600 --> 00:09:13,030
and then find a need
performing each minimization,

144
00:09:13,390 --> 00:09:18,390
minimizing one minus this storm is the
same as doing one minus the maximum of

145
00:09:19,721 --> 00:09:23,050
disturbed.
So indeed this is a constant,

146
00:09:23,051 --> 00:09:27,910
so I can't minimize it really the the
choice of why star one won't change the

147
00:09:27,911 --> 00:09:28,750
value of one.

148
00:09:29,230 --> 00:09:33,640
And so I can introduce this here and
the minimization of the negative of

149
00:09:33,641 --> 00:09:38,641
something is the same as minus the
maximization of that's something.

150
00:09:39,550 --> 00:09:40,660
So that's what I have here.

151
00:09:42,010 --> 00:09:45,610
So this is equivalent to this.

152
00:09:46,060 --> 00:09:48,340
And in particular the Arg,
Matt,

153
00:09:48,400 --> 00:09:52,390
the argument here or the
Arg Max is the Arg Max here.

154
00:09:52,660 --> 00:09:53,650
And in other words,

155
00:09:53,680 --> 00:09:58,680
it's the a sequence where the first label
is the one that maximizes the marginal

156
00:10:01,121 --> 00:10:04,330
probability for the first
element in the sequence.

157
00:10:04,810 --> 00:10:08,500
And then same thing for the second level,
third level up to the last label,

158
00:10:08,650 --> 00:10:13,540
which in my prediction should
be the label, why star gay,

159
00:10:13,780 --> 00:10:18,730
which maximizes the marginal
probability of the, uh,

160
00:10:18,880 --> 00:10:23,740
of the, uh, label at the last position
at the position capital came. Okay.

161
00:10:23,741 --> 00:10:28,030
So this ends our demonstration
that if I assume the true, uh,

162
00:10:28,060 --> 00:10:32,680
labels were generated by my model,
my linear, conditional random fields,

163
00:10:32,950 --> 00:10:37,950
then if I wanted to minimize
the expected classification,

164
00:10:38,470 --> 00:10:40,510
there are per position,
uh,

165
00:10:40,511 --> 00:10:44,920
the right thing to do would actually to
be to produce a sequence of all labels

166
00:10:44,921 --> 00:10:49,210
that maximize the probability of being
observed according to my model for each

167
00:10:49,211 --> 00:10:53,830
position and concern
constructing my prediction by
concatenating all of these uh,

168
00:10:53,890 --> 00:10:55,780
sequence, uh, all of these labels.

169
00:10:57,310 --> 00:10:58,143
Okay.

170
00:10:58,600 --> 00:10:59,920
So that's the first option.

171
00:11:00,160 --> 00:11:05,160
And other option would be
to do the equivalent of what
we were doing in the non

172
00:11:05,741 --> 00:11:07,270
sequential,
uh,

173
00:11:07,300 --> 00:11:11,680
prediction part that is I'll just
produce as my sequence output.

174
00:11:12,010 --> 00:11:15,790
The, uh, so actually this
should be a star here.

175
00:11:16,000 --> 00:11:18,730
I'll actually just produce
as the sequence output,

176
00:11:19,030 --> 00:11:21,580
the single sequence with the largest,

177
00:11:21,581 --> 00:11:26,581
probably the largest joint probability
condition on my input sequence.

178
00:11:27,940 --> 00:11:29,290
And so I'll go rapidly over this,

179
00:11:29,291 --> 00:11:34,291
but it turns out that much like
we've seen a way of actually summing,

180
00:11:35,190 --> 00:11:39,970
uh, uh, over the, uh, something
like this stone, the exponential,

181
00:11:40,240 --> 00:11:44,230
uh, an annex exponentiate it,
uh, some of, uh, lug factors.

182
00:11:44,440 --> 00:11:49,330
We can also perform the maximization with
the dynamic program. Uh, so we'll do,

183
00:11:49,430 --> 00:11:50,263
uh,

184
00:11:50,290 --> 00:11:55,290
we get essentially as a baritone guitar
be decoding where in the Ford pass so

185
00:11:56,200 --> 00:12:00,580
much like in the Alpha, uh,
dynamic programming where we,

186
00:12:00,610 --> 00:12:03,850
for the log Alpha, we had
luck, some ex operations,

187
00:12:04,060 --> 00:12:06,430
well in the this bitter
be decoding algorithm,

188
00:12:06,550 --> 00:12:11,550
we replaced those by the maximization
with respect to why prime canes.

189
00:12:11,800 --> 00:12:15,820
That and then we fill the table
with those values. And, uh, in fact,

190
00:12:15,821 --> 00:12:17,070
if you think about it,
luck,

191
00:12:17,071 --> 00:12:22,071
some x is actually can be thought of
as a smooth version of the maximisation

192
00:12:22,631 --> 00:12:24,730
operation.
And that's because,

193
00:12:24,850 --> 00:12:29,650
because of the exponential exponential
here, the term diet is going to dominate.

194
00:12:29,651 --> 00:12:33,850
This sum is going to be the term that
is largest because the exponential

195
00:12:33,851 --> 00:12:37,420
increases very rapidly with
large values of its argument.

196
00:12:37,750 --> 00:12:41,200
And then taking the log,
we go back to the original scale.

197
00:12:41,410 --> 00:12:45,550
And so really the term that's going to
dominate this whole term here is going to

198
00:12:45,551 --> 00:12:48,880
be the maximum. So, uh,
so that's just, you know,

199
00:12:48,881 --> 00:12:52,150
a nice fact to know about the
relationship between the lugs,

200
00:12:52,200 --> 00:12:53,830
Amex and the maximization.

201
00:12:55,130 --> 00:12:55,430
Yeah.

202
00:12:55,430 --> 00:12:59,550
And uh, but so yeah, so we take
the Ford Pass, say in the, uh,

203
00:12:59,600 --> 00:13:03,500
diamond program that confuses the Lug
Alpha table and would you replace the Lug,

204
00:13:03,501 --> 00:13:07,700
some x operations by a maximization
and then in the backward,

205
00:13:07,730 --> 00:13:12,620
then we have to produce it backward pass.
And in the backward fast, well, we start,

206
00:13:13,010 --> 00:13:16,300
we'll essentially compose are decoded.
Uh,

207
00:13:16,340 --> 00:13:21,340
so I predicted sequence by finding the
last label that maximizes the value in

208
00:13:22,281 --> 00:13:25,040
our table.
And then falling backward,

209
00:13:25,460 --> 00:13:28,700
the trace of the Max operations.
So falling backward,

210
00:13:28,701 --> 00:13:33,701
the links that are going to correspond
to what were the Arg Max when I was

211
00:13:33,921 --> 00:13:38,830
performing this maximisation here.
So more specifically,

212
00:13:38,831 --> 00:13:43,080
let's look at the pseudo code
for this, uh, algorithm. We, uh,

213
00:13:43,090 --> 00:13:47,070
first initialized the
value of my Alpha stars.

214
00:13:47,071 --> 00:13:49,390
I'm going to use star
for the dynamic program,

215
00:13:49,391 --> 00:13:54,391
the Viterbi decoding algorithm that is
going to fill this alpha start table as

216
00:13:55,181 --> 00:13:56,530
opposed to the Alpha table.

217
00:13:56,740 --> 00:14:01,360
And notice that here we
have in maximization instead
of a Loxa Mex operation

218
00:14:01,700 --> 00:14:06,700
where we do a maximization over what
would be the argument of the Loxa Mex

219
00:14:07,090 --> 00:14:10,810
operation in the computation
of the log Alpha table.

220
00:14:11,380 --> 00:14:15,850
So initialize the first column of my
Alpha Star table by performing this

221
00:14:15,851 --> 00:14:16,720
maximization.

222
00:14:16,960 --> 00:14:21,960
And I also remember for
each value of y star, uh,

223
00:14:21,970 --> 00:14:23,380
sorry,
why prime too?

224
00:14:23,530 --> 00:14:28,530
What was the actual value of y prime
one that was maximum is maximizing the,

225
00:14:29,470 --> 00:14:32,710
some of the luck factors. So see
here, this is the same thing as here.

226
00:14:32,711 --> 00:14:35,320
So this is just the Arg
Max of this maximisation.

227
00:14:36,600 --> 00:14:37,100
Okay.

228
00:14:37,100 --> 00:14:40,880
And then much like in the log Alpha table,
I'll go from the first,

229
00:14:41,270 --> 00:14:44,630
from k equals two to the
last till the last column.

230
00:14:44,690 --> 00:14:47,630
And then for each value of the label,

231
00:14:47,820 --> 00:14:52,820
I'll perform the similar
similar computation where
I've the Luxor Max by the

232
00:14:53,511 --> 00:14:54,410
maximization.

233
00:14:54,830 --> 00:14:59,830
And here I have you in urinary refactor
or ice factor and the Alpha Star.

234
00:14:59,910 --> 00:15:02,930
So the value at my previous
column in my Alpha Star Table.

235
00:15:03,560 --> 00:15:06,020
And similarly I'll remember
what is the Arg Max.

236
00:15:06,350 --> 00:15:10,520
So I'll put that into this other table,
which I'm uh,

237
00:15:10,550 --> 00:15:14,150
where I put the Arrow to distinguish
it from the Alpha star table.

238
00:15:15,780 --> 00:15:16,471
And then finally,

239
00:15:16,471 --> 00:15:21,471
if I want to know what's the
probability of the most likely sequence,

240
00:15:22,711 --> 00:15:23,790
why star?

241
00:15:23,850 --> 00:15:28,500
So what's the maximum of why start
off of this should be a why star also?

242
00:15:29,670 --> 00:15:30,660
Well,
it's just,

243
00:15:30,720 --> 00:15:35,720
it just corresponds to taking the
maximum over this expression here.

244
00:15:37,120 --> 00:15:37,270
Okay.

245
00:15:37,270 --> 00:15:40,510
And uh, so that's, uh,
actually not exactly taking,

246
00:15:40,511 --> 00:15:43,540
I said the taking the Max over the last
column, but that's not exactly true.

247
00:15:43,541 --> 00:15:48,541
It's the Max over the last column plus
the urinary luck factor for the last

248
00:15:48,731 --> 00:15:49,564
position.

249
00:15:50,540 --> 00:15:51,040
Yeah.

250
00:15:51,040 --> 00:15:55,510
And then to perform decoding that is to
actually construct my predicted sequence.

251
00:15:55,511 --> 00:15:59,320
Why Star? I'll take the,
again, the Arg Max of Dicks.

252
00:15:59,560 --> 00:16:00,820
This maximisation here.

253
00:16:00,821 --> 00:16:05,821
So the Arg Max of the urinary factor
at the last position plus the y at the

254
00:16:06,280 --> 00:16:09,160
Alpha Star, uh, for the last
column of my Alpha Star Table.

255
00:16:09,790 --> 00:16:12,970
So that's going to be the predicted
label for the last position.

256
00:16:13,390 --> 00:16:17,680
And then I'll go backwards
from the previous, the last
position till the first.

257
00:16:18,010 --> 00:16:20,500
And what I'll do is that why star gay,

258
00:16:20,501 --> 00:16:24,610
it's just going to be
what was the, uh, Arg Max.

259
00:16:24,790 --> 00:16:29,170
So you see the Arrow here
was what was the Arg Max, uh,

260
00:16:29,320 --> 00:16:32,710
when I performed the
maximisation for Alpha Star Gay,

261
00:16:33,580 --> 00:16:38,580
assuming that the value for y star
keep last one for y prime plus one is

262
00:16:39,731 --> 00:16:43,180
actually why star keep us one,
which is the value.

263
00:16:43,420 --> 00:16:47,500
Why star in my sequence that I
computed in a previous iteration.

264
00:16:47,590 --> 00:16:51,490
So we see that by initializing,
why star capital key here? Well,

265
00:16:51,500 --> 00:16:53,110
I always have the value for that.

266
00:16:53,350 --> 00:16:58,350
So I'm going from right to left using my
proven Swi star value to construct my,

267
00:16:58,990 --> 00:17:01,740
uh, uh, sorry. I guess the, the,

268
00:17:01,790 --> 00:17:04,860
the value of why star at the position,
uh,

269
00:17:04,900 --> 00:17:09,310
to the right took a constructor,
my prediction for the position,

270
00:17:09,311 --> 00:17:13,390
the current position. Okay. And
then going from right to left. Okay.

271
00:17:13,391 --> 00:17:16,990
So this is how I get a,
this is an algorithm,

272
00:17:16,991 --> 00:17:19,990
but that is also a, that
has, you know, a nice,

273
00:17:20,040 --> 00:17:25,040
a complexity of a that's polynomial in k
and a capital gain and capital c four a

274
00:17:26,621 --> 00:17:29,350
computing, uh, constructing
your prediction.

275
00:17:29,380 --> 00:17:34,240
And so classify a whole sequence of
inputs and producing a sequence of labels

276
00:17:34,241 --> 00:17:38,770
for my sequence of inputs.
So the two types of predictions,

277
00:17:38,830 --> 00:17:42,550
this Viterbi decoding approach and the
other one that maximizers the purpose,

278
00:17:42,560 --> 00:17:46,750
mission marginal are two valid,
uh, predictions to perform.

279
00:17:46,870 --> 00:17:51,720
If we really care about the
sum of the, some of the, uh,

280
00:17:51,990 --> 00:17:56,760
per element, uh, do some of per
element classification errors.

281
00:17:56,761 --> 00:18:00,930
We might want to prefer the
first procedure, but if instead,

282
00:18:00,931 --> 00:18:03,270
our prediction a,
an error,

283
00:18:03,330 --> 00:18:06,450
if the way we evaluate the
performance of our classification,

284
00:18:06,500 --> 00:18:11,040
a sequenced classification
system is different.

285
00:18:11,160 --> 00:18:13,500
And so for different systems,
I can speech, for instance,

286
00:18:13,501 --> 00:18:17,640
we'd be more interested in a phone error
rates, which is more complicated than,

287
00:18:17,700 --> 00:18:22,140
uh, performing. Uh, this Vittoria
be decoding procedure is a,

288
00:18:22,141 --> 00:18:26,220
should be preferred because then we're
not maximizing the sum of per, uh,

289
00:18:26,250 --> 00:18:30,240
position, uh, per position, uh, errors.

