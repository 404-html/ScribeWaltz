1
00:00:00,380 --> 00:00:00,610
Okay.

2
00:00:00,610 --> 00:00:05,010
In this video we'll see how to compute
the pairwise luck factor parameter

3
00:00:05,011 --> 00:00:05,844
gradients.

4
00:00:07,520 --> 00:00:07,920
Okay.

5
00:00:07,920 --> 00:00:10,500
So we've seen before how to uh,

6
00:00:10,530 --> 00:00:13,860
what loss function we would use for
training conditional random fields.

7
00:00:14,100 --> 00:00:15,940
And we also partly saw our,

8
00:00:15,941 --> 00:00:19,680
to compute the perimeter gradients
of the last function. We've seen, uh,

9
00:00:19,681 --> 00:00:24,480
how to compute the perimeter gradients
for the law involved in the log factors,

10
00:00:24,481 --> 00:00:28,110
the urinary luck factors. And I will
see for the pairwise luck factors.

11
00:00:30,620 --> 00:00:35,020
So we let their patients similar to what
we've seen in the previous video. Uh,

12
00:00:35,030 --> 00:00:40,030
we can show that the partial they
live up the negative log of y given x.

13
00:00:40,490 --> 00:00:43,740
Uh, with respect to the
pairwise factor, uh,

14
00:00:44,060 --> 00:00:49,040
positionK for observing
why the value of a label,

15
00:00:49,041 --> 00:00:52,130
why prime gay followed
by a value of a label.

16
00:00:52,131 --> 00:00:56,780
Why Prime Cape Plus one is
actually equal to minus,

17
00:00:57,170 --> 00:00:57,471
uh,

18
00:00:57,471 --> 00:01:02,471
whether the kith label in sequence y is
equal to y prime gay and the k plus one

19
00:01:03,561 --> 00:01:08,561
yet label is equal to y prime k plus one
minus the probability according to the

20
00:01:09,381 --> 00:01:12,200
model that the key of Labor
would be by prime gay.

21
00:01:12,210 --> 00:01:16,100
And they keep plus one year
Labor would be y prime plus one.

22
00:01:16,580 --> 00:01:21,560
So we see that it actually has a form
which is quite similar to uh, the uh,

23
00:01:21,650 --> 00:01:25,730
previous, uh, green and for
the uh, urinary luck factor,

24
00:01:26,210 --> 00:01:30,710
uh, partial derivatives. Again, it's uh,

25
00:01:30,740 --> 00:01:34,130
what is observed in the data
minus what the model thinks.

26
00:01:34,640 --> 00:01:36,560
And also the,
uh,

27
00:01:36,620 --> 00:01:40,610
now the partial data with respect to the
parameters of the pairwise luck factor

28
00:01:40,611 --> 00:01:43,040
is quite simple.
Uh,

29
00:01:43,070 --> 00:01:47,750
because we're modeling directly
the uh, pairwise lug you, uh,

30
00:01:47,810 --> 00:01:52,250
the pairwise luck factors
with the Matrix v then, uh,

31
00:01:52,251 --> 00:01:56,270
so in other words, the,
uh, remove the ink here,

32
00:01:56,390 --> 00:01:59,990
but the pairwise factor value for y prime,

33
00:02:00,130 --> 00:02:04,850
why y y prime keep loss one is
simply the element that role,

34
00:02:04,851 --> 00:02:09,440
why Prime K and a column y
prime plus one in the Matrix v?

35
00:02:09,950 --> 00:02:11,090
Well because of that,

36
00:02:11,180 --> 00:02:16,160
the partial their lives for that entry
in the matrix fee is the similar term

37
00:02:16,310 --> 00:02:17,510
that we saw here.

38
00:02:17,750 --> 00:02:22,750
But now we have to summit
over all position came because
this matrix is used for

39
00:02:23,571 --> 00:02:26,740
all positions. K uh, for uh,

40
00:02:26,870 --> 00:02:30,530
essentially for all the pairwise
log factor across the sequence.

41
00:02:31,490 --> 00:02:36,190
And now we can write it also in vector
form. So get the gradient, uh, by uh,

42
00:02:36,240 --> 00:02:41,120
so the gradient of the loss with
respect to the full matrix fee and uh,

43
00:02:41,121 --> 00:02:44,090
we can show that it occurs funds to this,

44
00:02:44,091 --> 00:02:49,091
some of minus the one on vector with
a one that position why k times the

45
00:02:50,901 --> 00:02:54,770
transpose that the one that vector with
the one that position why k plus one

46
00:02:55,130 --> 00:03:00,130
minus the matrix a that contains all
the pairwise marginal probabilities that

47
00:03:01,781 --> 00:03:06,490
he's all the probabilities of the kid
label being a white cane and they keep us

48
00:03:06,491 --> 00:03:09,010
one year label being why keep plus one.

49
00:03:09,400 --> 00:03:14,400
So essentially this matrix would contain
all of these probabilities here for a

50
00:03:16,501 --> 00:03:21,330
position,K and k plus one
and a.

51
00:03:21,350 --> 00:03:22,030
Um,

52
00:03:22,030 --> 00:03:27,030
another way of writing this is to
perform the sum here over these terms.

53
00:03:27,700 --> 00:03:29,080
So distribute the sum here.

54
00:03:29,081 --> 00:03:34,081
So I have to sum over the pairwise matrix
and then I can just write this here as

55
00:03:35,591 --> 00:03:40,591
just a frequency of observing a given
label followed by another given label.

56
00:03:41,740 --> 00:03:46,070
So I'm writing it, I'm noting
this down as frequent a frack yk.

57
00:03:46,110 --> 00:03:49,960
Why K plus one? Uh, you know,
I could've written just, uh,

58
00:03:49,961 --> 00:03:54,160
you know why and say why
prime theK is not important.

59
00:03:54,400 --> 00:03:58,300
It's just a number of times that a given
level of why was followed by another

60
00:03:58,301 --> 00:04:00,760
label.
Why prime in the sequence.

61
00:04:00,790 --> 00:04:05,320
And I have the full matrix that contains
all of these pay wise label frequencies.

62
00:04:05,470 --> 00:04:09,250
So that's just another way of writing
down the expression for the gradient,

63
00:04:09,750 --> 00:04:10,583
which respective.

64
00:04:13,810 --> 00:04:14,030
Okay.

65
00:04:14,030 --> 00:04:16,520
So that's pretty much all
for the parameter gradients.

66
00:04:16,640 --> 00:04:18,940
A quick word on regularization,
uh,

67
00:04:18,950 --> 00:04:23,690
we can use the same regular riser as we
use in the non sequential neural network.

68
00:04:24,230 --> 00:04:28,520
Um, and, uh, uh, normally we
have a regularizing term for it,

69
00:04:28,521 --> 00:04:31,190
the connection Matrix, the matrix fee, uh,

70
00:04:31,191 --> 00:04:34,990
but we still don't recognize the
biases and the neural network. Um,

71
00:04:35,480 --> 00:04:39,440
we could decide to scale the
amount of regularization, uh,

72
00:04:39,470 --> 00:04:44,290
by the sequence size. Um, so if we, uh,

73
00:04:44,330 --> 00:04:47,780
thought that large sequences don't,
uh,

74
00:04:47,840 --> 00:04:52,670
necessarily have more information about
what the right parameters should be,

75
00:04:52,671 --> 00:04:56,540
then shorter sequences, then that
would allow us, for instance,

76
00:04:56,541 --> 00:05:01,190
to penalize or regularize more or
longer sequences and less on smaller

77
00:05:01,191 --> 00:05:02,690
sequences. So, you know,

78
00:05:03,020 --> 00:05:06,120
that's a veteran that you might
see in the literature that, uh,

79
00:05:06,950 --> 00:05:10,610
people might try to adapt
for some given problem.

80
00:05:11,880 --> 00:05:12,070
Okay.

81
00:05:12,070 --> 00:05:15,370
Uh, but, uh, otherwise the
regularization gradients are,

82
00:05:15,371 --> 00:05:17,650
are essentially the same
as we've seen before.

83
00:05:18,190 --> 00:05:22,540
And so now with the loss and the
regularization gradients and the parameter

84
00:05:22,541 --> 00:05:23,830
gradients,
uh,

85
00:05:23,831 --> 00:05:28,510
we have all the Greens and we need to
apply stochastic gradient descent. And A,

86
00:05:28,511 --> 00:05:29,890
I haven't mentioned the initialization,

87
00:05:29,891 --> 00:05:33,370
but we use exactly the same
as we've used, uh, in the, uh,

88
00:05:33,400 --> 00:05:36,580
non sequential neural networks.
And also,

89
00:05:36,790 --> 00:05:39,510
perhaps I should say that for
initialization, for the Matrix v,

90
00:05:39,640 --> 00:05:42,010
it's fine to initialize it to zero.

91
00:05:42,820 --> 00:05:47,200
So that pretty much gives
us all the pieces we need
to apply stochastic gradient

92
00:05:47,201 --> 00:05:52,000
descent training for conditional
random fields. And, uh, that's it.

93
00:05:53,210 --> 00:05:53,270
Okay.

