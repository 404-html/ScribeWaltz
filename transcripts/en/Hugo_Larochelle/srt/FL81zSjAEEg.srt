1
00:00:00,660 --> 00:00:01,261
In this video,

2
00:00:01,261 --> 00:00:05,760
we'll describe in a bit more detail how
we can use [inaudible] to extract some

3
00:00:05,761 --> 00:00:07,470
features for a given problem.

4
00:00:09,450 --> 00:00:11,880
So one of the most popular,
uh,

5
00:00:11,881 --> 00:00:15,570
usage of sparks coding is
to extract features that is,

6
00:00:15,571 --> 00:00:19,920
say we want to perform a particular
classification problem. Uh,

7
00:00:19,921 --> 00:00:24,270
and we would lie to use our
data and find a representation,

8
00:00:24,271 --> 00:00:25,740
the new representation for our data,

9
00:00:25,920 --> 00:00:29,550
which is going to make the problem
with classification easier.

10
00:00:30,330 --> 00:00:35,010
So a specifically imagine I have
some label training set where I have,

11
00:00:35,040 --> 00:00:39,180
uh, some inputs and each input
is associated to the class label.

12
00:00:39,300 --> 00:00:44,300
So imagine say that the inputs are a
set of images and yt is the class of the

13
00:00:44,791 --> 00:00:49,670
image. Well, to extract some
feature, what we could do is, uh,

14
00:00:49,770 --> 00:00:54,660
we could first train is parse
coding dictionary on the data set.

15
00:00:54,661 --> 00:00:57,330
That only consists of the inputs.

16
00:00:57,330 --> 00:01:01,530
So we've thrown away for now
temporarily the labels yt.

17
00:01:02,310 --> 00:01:06,570
So if we run our sparse coding or a
dictionary learning algorithm on that

18
00:01:06,571 --> 00:01:09,780
Dataset, this will give us
a dictionary matrix deep.

19
00:01:10,260 --> 00:01:12,090
And from that dictionary Matrix,
well,

20
00:01:12,091 --> 00:01:16,870
I could then take all of my inputs
and compute what's the sparse later in

21
00:01:16,871 --> 00:01:21,030
representation, uh, based
on this learn dictionary.

22
00:01:22,110 --> 00:01:27,110
And now what I could do is just take my
original dataset and change it in a way

23
00:01:27,901 --> 00:01:32,901
where I'll remove the original input
and I'll replace it by the sparse

24
00:01:33,481 --> 00:01:37,080
representation.
And so now for any classifier,

25
00:01:37,081 --> 00:01:38,970
I can just use my favorite class fire,

26
00:01:38,971 --> 00:01:43,380
whether it's a neural network or a support
vector machine or a decision tree or

27
00:01:43,381 --> 00:01:44,400
any costs of fire.

28
00:01:44,730 --> 00:01:49,730
I can't just train it on the new dataset
where I've replaced the input by its

29
00:01:49,761 --> 00:01:51,870
parts coding representation.
And from it,

30
00:01:51,871 --> 00:01:56,340
I now have to predict again the label,
but now I have to predict it,

31
00:01:56,700 --> 00:01:58,890
uh, from the, uh,

32
00:01:58,920 --> 00:02:03,750
extracted features from the sparse
representation. And I've, uh,

33
00:02:03,751 --> 00:02:07,560
I had, uh, my system was
confronted with a new input x.

34
00:02:07,650 --> 00:02:12,330
Well then the process of classifying it
would correspond to first inferring it's

35
00:02:12,331 --> 00:02:14,310
parsed coding representation,
h of x,

36
00:02:14,640 --> 00:02:17,790
and then I would feed it to
require to my classifier that I've,

37
00:02:17,850 --> 00:02:22,320
that I've trained here.
So while this part gives me a d matrix,

38
00:02:22,590 --> 00:02:25,170
this parts gives me a classifiers,

39
00:02:25,171 --> 00:02:29,910
say h a f of uh,
of my input.

40
00:02:29,940 --> 00:02:34,900
But now the input that I have to give
it for my cost fire f is actually a spar

41
00:02:34,901 --> 00:02:37,260
squirting representation h of x much like,

42
00:02:37,370 --> 00:02:42,090
and that's because it was trained on
these types of features. Okay. Now,

43
00:02:42,150 --> 00:02:45,840
uh,
for this to be meaningful to make sense,

44
00:02:46,080 --> 00:02:48,900
you essentially have to believe that uh,

45
00:02:49,080 --> 00:02:52,950
these parts coding representation
is going to make the problem easier.

46
00:02:52,951 --> 00:02:56,390
That is my data is such that it's a,

47
00:02:56,400 --> 00:03:01,400
there is a sparse representation which
will identify features that are more

48
00:03:01,661 --> 00:03:06,310
directly related to what's the
class label associated with my data.

49
00:03:08,740 --> 00:03:13,740
So one such type of data is a image
data and in particular let's consider a

50
00:03:14,351 --> 00:03:17,050
image data that corresponds
to handwritten digits.

51
00:03:17,080 --> 00:03:20,930
So I have a very small image
and in the image I have a,

52
00:03:20,950 --> 00:03:25,310
a Henredon wreck characters. So this would
be an image that would be labeled too.

53
00:03:25,520 --> 00:03:30,490
So I have one that would be
one, uh, a pair of example, uh,

54
00:03:30,491 --> 00:03:35,200
with which my, uh, one, the train
my learning of them. Well, uh,

55
00:03:35,230 --> 00:03:40,230
if I run sparse coding on a bunch
of handwritten digits images,

56
00:03:40,860 --> 00:03:41,040
uh,

57
00:03:41,040 --> 00:03:46,040
so I would just scrap temporarily while
I'm running my smart scoring algorithm,

58
00:03:46,151 --> 00:03:50,170
the label and just run on
the images itself. Well,

59
00:03:50,320 --> 00:03:52,840
we have an illustration here,
the types of features we would get.

60
00:03:53,260 --> 00:03:57,280
So now each lil square here,
so that's a square,

61
00:03:57,281 --> 00:03:58,390
that's another square,

62
00:03:58,750 --> 00:04:03,750
that's another square and
so on is a visualization of
one atom in my dictionary.

63
00:04:04,150 --> 00:04:08,890
You Matrix one column,
so I can visualize it because each Adam,

64
00:04:08,960 --> 00:04:10,570
uh,
is a vector of size.

65
00:04:10,571 --> 00:04:15,250
Number of inputs is the same
size as the input size and a,

66
00:04:15,251 --> 00:04:16,570
and because the input is an image,

67
00:04:16,571 --> 00:04:20,560
then I can just reshape my vector
also into the shape of an image.

68
00:04:21,010 --> 00:04:24,040
And then individualization,
if the pixel is great,

69
00:04:24,041 --> 00:04:28,600
it means that the value of the vector
for that Pixel is zero. If it's white,

70
00:04:28,601 --> 00:04:30,580
then it's positive and it's black,
it's negative.

71
00:04:31,720 --> 00:04:36,520
So this essentially
corresponds to, uh, uh, the,

72
00:04:36,550 --> 00:04:38,010
uh,
sort of illustrative,

73
00:04:38,030 --> 00:04:43,000
a visualization of what each
sparse code element, uh, and coats.

74
00:04:43,001 --> 00:04:45,460
So if I,
in my responsibility in representation,

75
00:04:45,650 --> 00:04:50,140
the code associated say with that
feature had the large positive value.

76
00:04:50,141 --> 00:04:53,440
So if this was, uh, uh,
the first feature though,

77
00:04:53,441 --> 00:04:57,100
so x if h of x one was a,

78
00:04:57,130 --> 00:04:59,890
because this will be the first feature,
if it had a large value,

79
00:05:00,130 --> 00:05:03,220
it would mean that if I was to
reconstruct the original input,

80
00:05:03,370 --> 00:05:08,370
I would add one times this little limit
here and say if this would be the other

81
00:05:09,401 --> 00:05:12,390
image, then I add also
this part and so on. Okay.

82
00:05:12,490 --> 00:05:16,810
So that's why this is a meaningful
visualization of the representation.

83
00:05:17,350 --> 00:05:21,310
And now we see that each spar
scope actually has learned, uh,

84
00:05:21,311 --> 00:05:23,950
what looks like essentially
a bunch of pen strokes.

85
00:05:23,951 --> 00:05:28,120
So this will be a pen stroke like this.
This would be a pen stroke like that.

86
00:05:28,540 --> 00:05:29,950
And this one will be like this.

87
00:05:30,310 --> 00:05:34,270
So essentially PowerSchool as parsing
representation would be a big vector that

88
00:05:34,271 --> 00:05:35,990
identifies,
which are this,

89
00:05:36,160 --> 00:05:40,930
the different pen strokes that are
present in some given image. And,

90
00:05:40,960 --> 00:05:42,040
uh,
in fact,

91
00:05:42,041 --> 00:05:47,041
if we were to ask someone to describe
the problem of classifying digits,

92
00:05:47,830 --> 00:05:49,150
uh,
enter the digits,

93
00:05:49,151 --> 00:05:52,890
then it would probably actually use a
description that's based on pen strokes.

94
00:05:52,891 --> 00:05:55,030
So to describe it too, I might say, well,

95
00:05:55,180 --> 00:05:59,450
it has a sort of a bar on as a
loop at the bottom and so on.

96
00:06:00,290 --> 00:06:03,740
So we can see that, uh, intuitively,
this seems to make sense.

97
00:06:03,741 --> 00:06:08,741
This would be a nice way of representing
digit that makes the class information

98
00:06:09,561 --> 00:06:12,650
much more obvious than just
the raw value of the pixels.

99
00:06:13,010 --> 00:06:17,300
So this is why for image data and this
particular case and with the digits

100
00:06:17,390 --> 00:06:20,360
sparse coding features
work well in practice.

101
00:06:22,940 --> 00:06:23,481
In fact,

102
00:06:23,481 --> 00:06:28,190
we could actually train our [inaudible]
coding dictionary on the different type

103
00:06:28,191 --> 00:06:29,870
of data set.
Uh,

104
00:06:29,871 --> 00:06:34,310
so if we had a large collection of
unlabeled images that did not exactly

105
00:06:34,311 --> 00:06:37,700
correspond to the type of images
we wanted to classify a test time,

106
00:06:38,030 --> 00:06:42,890
we could still use the unlabeled images
to extract a better representation and

107
00:06:42,891 --> 00:06:46,480
see whether it transfers
to, uh, uh, are, uh,

108
00:06:46,520 --> 00:06:48,600
my main important problem that,
uh,

109
00:06:48,710 --> 00:06:50,900
I actually want to solve
in terms of classification.

110
00:06:51,740 --> 00:06:55,250
So this is actually known
as self taught learning. Uh,

111
00:06:55,251 --> 00:06:59,960
so it's a combination of unsupervised
learning and transfer learning, uh,

112
00:06:59,961 --> 00:07:02,570
for those that know more
about machine learning.

113
00:07:02,571 --> 00:07:07,070
So these are unsupervised and transfer
learning how to well known types of, uh,

114
00:07:07,071 --> 00:07:11,330
of learning and a self taught learning
can be seen as a combination of both.

115
00:07:11,900 --> 00:07:13,850
So a in south thought learning,

116
00:07:13,880 --> 00:07:18,880
the idea is that we want to train
some features but on a different input

117
00:07:19,041 --> 00:07:20,110
distribution,
uh,

118
00:07:20,360 --> 00:07:24,590
because we might have more data from
that input distribution than the

119
00:07:24,591 --> 00:07:28,610
distribution, uh, for which we actually
want to solve a particular, say,

120
00:07:28,611 --> 00:07:31,940
a classification problem.
So there's a concrete example.

121
00:07:31,941 --> 00:07:36,590
Imagine I had a lot of enrooted digits,
a lot of images of handwritten digits,

122
00:07:36,620 --> 00:07:38,450
and that those were actually not labeled.

123
00:07:38,451 --> 00:07:41,020
I didn't know the no humans
went over the images,

124
00:07:41,040 --> 00:07:44,870
actually specify the label
for each image. And, uh,

125
00:07:44,900 --> 00:07:48,440
and now imagine that what I really want
to do actually is classify a handwritten

126
00:07:48,500 --> 00:07:49,430
English characters.

127
00:07:49,431 --> 00:07:54,431
So alphabetical characters and not
numerous numerical characters and uh,

128
00:07:55,700 --> 00:07:59,930
but imagine that actually have
very little training data for that.

129
00:08:00,350 --> 00:08:05,350
And now can we still try to improve
things by leveraging my unlabeled data,

130
00:08:05,810 --> 00:08:10,100
even though it's, it's a different input
distribution. So they're not, uh, uh,

131
00:08:10,460 --> 00:08:15,280
these are numeric characters and these
are, uh, alphabetical characters.

132
00:08:15,281 --> 00:08:19,230
So you're going to look quite different.
And also they might come from,

133
00:08:19,231 --> 00:08:23,150
from different people that
actually drew them. So, uh,

134
00:08:23,151 --> 00:08:28,130
so here's the experiment, uh, taken
from this paper by a reign of battle,

135
00:08:28,131 --> 00:08:31,970
the packer and Ang. Uh, so,
uh, they're the authors,

136
00:08:32,080 --> 00:08:35,900
the researchers that introduced
the notion of self taught learning.

137
00:08:36,740 --> 00:08:40,790
So in this experiment,
they did exactly what I described.

138
00:08:41,090 --> 00:08:44,150
They are,
took a lot of unlabeled digit,

139
00:08:44,600 --> 00:08:47,610
a character's Hendrickson characters,
uh,

140
00:08:47,660 --> 00:08:50,780
and they trained a idea
or did nothing with it,

141
00:08:50,810 --> 00:08:55,470
so they didn't use it to extract a
feature representation or they trained a

142
00:08:55,471 --> 00:08:58,770
linear PCA representation
from the digits data,

143
00:08:59,010 --> 00:09:01,770
or they train these parts
coding representation.

144
00:09:01,771 --> 00:09:05,420
So they train the dictionary matrix
from which they could extract these past

145
00:09:05,421 --> 00:09:06,254
representation.

146
00:09:07,200 --> 00:09:12,200
And now they use these features to
classify a Hendra did English and written

147
00:09:13,291 --> 00:09:15,900
characters. So, uh,
alphabetical characters.

148
00:09:16,590 --> 00:09:19,950
And so what we have here is the,
uh,

149
00:09:19,980 --> 00:09:24,270
performance for different
training sets, sizes, uh,

150
00:09:24,330 --> 00:09:26,490
in the, uh, label Dataset.

151
00:09:26,790 --> 00:09:31,320
So we see that these data psychically
fairly small in this case. So, uh,

152
00:09:31,321 --> 00:09:35,640
we can hope that we can leverage the
information in the large unlabeled data.

153
00:09:36,270 --> 00:09:40,450
And, uh, so we have the performance
if we use no feature representation.

154
00:09:40,451 --> 00:09:44,580
So we just use the, uh, the x t, uh,

155
00:09:44,970 --> 00:09:47,760
raw representation or
the PCA representation,

156
00:09:47,790 --> 00:09:52,500
or does parts coding representation
as learned on the, uh, difference,

157
00:09:52,740 --> 00:09:57,450
uh, data on the digits data. And what
we see is that using some sparse coding,

158
00:09:57,690 --> 00:09:57,901
uh,

159
00:09:57,901 --> 00:10:02,901
we often see an increase in the
courtesy for a 500 and a thousand.

160
00:10:03,480 --> 00:10:06,420
In this particular experiment,
we saw better performance,

161
00:10:06,421 --> 00:10:10,590
better accuracy than if
I'd use the raw original,

162
00:10:10,640 --> 00:10:12,270
a representation,
or even that,

163
00:10:12,300 --> 00:10:17,300
just a PCA representation and a for a
hundred in this particular experiment that

164
00:10:18,930 --> 00:10:23,500
the performance was almost the
same as, uh, if I've used the, uh,

165
00:10:23,880 --> 00:10:27,210
uh, the raw representation. So
we didn't lose much really here.

166
00:10:27,810 --> 00:10:32,070
So this is another illustration that
sparks coding features can be useful for

167
00:10:32,071 --> 00:10:34,650
performing classification as a,

168
00:10:34,651 --> 00:10:38,880
if we use that as in a sense of
preprocessing step for a, uh,

169
00:10:38,881 --> 00:10:40,860
our favorite a classifier.

