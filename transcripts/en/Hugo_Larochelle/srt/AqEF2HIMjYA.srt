1
00:00:00,230 --> 00:00:00,600
Yeah.

2
00:00:00,600 --> 00:00:04,290
In this video will introduce the different
type of neural network for natural

3
00:00:04,291 --> 00:00:07,770
language processing, uh, known
as the recursive neural network.

4
00:00:10,750 --> 00:00:12,550
So here we'll start,
uh,

5
00:00:12,580 --> 00:00:16,570
interesting ourselves with the
problem of learning representations,

6
00:00:16,571 --> 00:00:20,830
but not just for single words,
but also for multiple words,

7
00:00:21,300 --> 00:00:26,140
same phrases or sentences. Specifically.
We like to be able to learn, uh,

8
00:00:26,141 --> 00:00:30,510
representation, uh, of phrases
of essentially arbitrary length.

9
00:00:31,060 --> 00:00:31,691
So for instance,

10
00:00:31,691 --> 00:00:36,691
could we model the relationship
between individual words and multi word

11
00:00:36,701 --> 00:00:39,460
expressions, which are
equivalent. For instance,

12
00:00:39,620 --> 00:00:44,620
you could have a model that I learned a
representation for the word consider and

13
00:00:44,651 --> 00:00:49,651
also can output a vector representation
of the phrase take into account,

14
00:00:50,740 --> 00:00:55,510
which is similar to that a consider
because effectively to consider or to take

15
00:00:55,511 --> 00:00:57,760
into account is essentially
the same thing. They're,

16
00:00:57,761 --> 00:01:02,110
they're related essentially synonyms,
uh, with respect to each other.

17
00:01:03,040 --> 00:01:06,870
So could we learn representation
with a model learning route,

18
00:01:06,880 --> 00:01:09,190
presentation for arbitrary length,
uh,

19
00:01:09,370 --> 00:01:14,370
phrases that allows us to compare full
sentences and that actually is able to

20
00:01:14,801 --> 00:01:19,790
maintain as much information about it.
Semantic meanings. So fun. Since uh,

21
00:01:20,050 --> 00:01:23,230
a model that would upload the vector
representation for the sentence,

22
00:01:23,231 --> 00:01:25,660
word representations were
learned from our corpus,

23
00:01:25,870 --> 00:01:30,180
that would be very close to an
equivalent sentence, which is a,

24
00:01:30,190 --> 00:01:34,060
which as we train word
representations on a text dataset.

25
00:01:34,810 --> 00:01:38,620
So that's the question we're going to be
interested in, in, in trying to address,

26
00:01:38,621 --> 00:01:42,060
see how well we could learn he
fixed size representation of,

27
00:01:42,150 --> 00:01:47,150
of these felony long sentences and still
maintain as much semantic information

28
00:01:47,231 --> 00:01:52,030
as possible.
So to tackle this problem,

29
00:01:52,031 --> 00:01:56,980
Richard Socher and his
colleagues introduced a
different type of neural network

30
00:01:57,340 --> 00:01:59,880
today called eight
recursive neural networks.

31
00:02:00,640 --> 00:02:03,890
So first describe the general idea
and the next videos we'll see it,

32
00:02:03,940 --> 00:02:06,280
the look at the different
components and how they work.

33
00:02:06,950 --> 00:02:09,430
So what they propose to do,
if we had a sentence,

34
00:02:09,431 --> 00:02:14,050
say a small crowd quietly
enters the historic church, uh,

35
00:02:14,080 --> 00:02:17,050
then to get a representation
of the full sentence,

36
00:02:17,590 --> 00:02:22,180
they would first take the sentence and
extract from it a, a syntactic tree,

37
00:02:22,181 --> 00:02:23,860
a binary syntactic tree,

38
00:02:24,130 --> 00:02:29,130
which essentially identifies the internals
and tactic structure of the sentence.

39
00:02:29,860 --> 00:02:34,330
So I'm not gonna say much about
what syntactic tree our tweets are.

40
00:02:34,390 --> 00:02:35,223
Um,

41
00:02:35,350 --> 00:02:39,610
but effectively essentially we have
data sets that identify four different

42
00:02:39,611 --> 00:02:42,340
training sentences.
What is the syntactic tree?

43
00:02:42,520 --> 00:02:44,890
And we were both to train models for,
for performing that.

44
00:02:44,891 --> 00:02:48,040
And actually we'll be able
to use a recursive neural
network for performing that

45
00:02:48,041 --> 00:02:48,874
task.

46
00:02:48,940 --> 00:02:53,530
But essentially what the syntactic
tree gives you is that it identifies

47
00:02:53,560 --> 00:02:56,260
essentially the meaningful,
uh,

48
00:02:56,261 --> 00:03:00,670
phrases within the sentence
and how relate with each other.

49
00:03:00,671 --> 00:03:05,050
So the syntactic tree here is saying
that there's a sentence which is composed

50
00:03:05,110 --> 00:03:09,490
of in noun phrase and then the verb phrase
where the noun phrase covers a small

51
00:03:09,491 --> 00:03:11,770
crowd.
So normally here there'd be a bigger tree,

52
00:03:11,771 --> 00:03:16,210
but since we don't have enough space,
you should imagine that there, there,

53
00:03:16,211 --> 00:03:19,820
there would be a small tree here and,

54
00:03:19,850 --> 00:03:22,810
and so that forms a noun phrase.
And then we have a verb phrase,

55
00:03:22,811 --> 00:03:26,140
which essentially covers
all of these words here.

56
00:03:26,141 --> 00:03:31,120
And this verb phrase is actually a smaller
verb phrase covering quietly enters

57
00:03:31,420 --> 00:03:33,910
and a followed by a noun phrase.

58
00:03:34,110 --> 00:03:37,110
Just probably the object
of quietly enters and a,

59
00:03:37,120 --> 00:03:41,890
this non phrase covers the historic
church and this non phrase,

60
00:03:41,891 --> 00:03:45,070
it's actually a determinant
followed by a smaller noun phrase.

61
00:03:45,220 --> 00:03:49,900
And this smaller numb phrase is an
objective followed by her now specifically

62
00:03:49,901 --> 00:03:51,400
historic following church.

63
00:03:51,430 --> 00:03:55,240
So that's the kind of information that
is contained in these barn recent tactic

64
00:03:55,241 --> 00:03:56,074
treats.

65
00:03:56,920 --> 00:04:01,360
And now to get a representation in vector
representation of the full sentence,

66
00:04:01,460 --> 00:04:06,280
a d idea and recursive neural net
is to recursively merge pairs of

67
00:04:06,700 --> 00:04:11,140
representations of smaller segments to
get representation is not cover bigger

68
00:04:11,141 --> 00:04:11,974
segments.

69
00:04:12,040 --> 00:04:16,750
Specifically we'd start with the word
representations themselves and then going

70
00:04:16,810 --> 00:04:17,680
up in the tree,

71
00:04:17,980 --> 00:04:22,980
we will take the two parents of a
note and merged our representations,

72
00:04:23,620 --> 00:04:25,030
uh,
using a neural net,

73
00:04:25,060 --> 00:04:29,320
which will take as argument the two
representations and now put a new merged

74
00:04:29,410 --> 00:04:31,660
representation so far.
For instance,

75
00:04:31,661 --> 00:04:35,440
the first thing you could do is take
the word representation of the word

76
00:04:35,500 --> 00:04:39,370
historic and then the vector
representation of the word church.

77
00:04:39,670 --> 00:04:42,940
Feed that to our neural network,
which would then output a new vector,

78
00:04:43,060 --> 00:04:44,360
which would be the,
uh,

79
00:04:44,530 --> 00:04:48,520
which we use as the representation
for the noun phrase historic church.

80
00:04:49,270 --> 00:04:51,670
And then to get the
representation at that level here,

81
00:04:51,880 --> 00:04:56,200
vector representation that we'd merged
the representation of its to children.

82
00:04:56,201 --> 00:05:00,880
That is the word, the, and then the
noun phrase for which we just extracted,

83
00:05:01,090 --> 00:05:05,890
computed. It's a merge representation.
So now we'd feed to the same neural net,

84
00:05:06,060 --> 00:05:08,500
this vector and this factor,

85
00:05:08,501 --> 00:05:11,890
and then it would output the vector
representation at that level.

86
00:05:12,280 --> 00:05:16,930
And we continue like this for each node
in the syntactic tree until we reached

87
00:05:16,931 --> 00:05:21,130
the route. And that vector would be, uh,

88
00:05:21,160 --> 00:05:24,820
considered in our model as the vector
representation of the full sentence.

89
00:05:25,900 --> 00:05:30,900
So what we need to specify at this point
is what shape will take this model that

90
00:05:31,571 --> 00:05:34,710
merges pairs of representations. And, uh,

91
00:05:34,720 --> 00:05:37,690
also all we going to model
actually the tree structure.

92
00:05:37,691 --> 00:05:41,030
And specifically how are we
going to extract this? Uh,

93
00:05:41,050 --> 00:05:44,530
this street's a structure
from the sentences.

94
00:05:44,770 --> 00:05:48,100
And these are the things we're going
to discuss in the following videos.

