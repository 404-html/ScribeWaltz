1
00:00:01,420 --> 00:00:01,991
In this video,

2
00:00:01,991 --> 00:00:06,640
we'll look at some example of
results we can get using unsupervised

3
00:00:06,641 --> 00:00:07,474
pre-training.

4
00:00:09,670 --> 00:00:12,460
So we saw the procedure
funds provide speed training.

5
00:00:12,940 --> 00:00:17,650
What we first do is that we initialize
each layer from the first hidden layer to

6
00:00:17,651 --> 00:00:21,930
the last hidden layer by
training a, uh, what we call it,

7
00:00:21,931 --> 00:00:25,700
greedy modules or either an
rpm or an auto encoder, uh,

8
00:00:25,710 --> 00:00:30,710
on a dataset that was generated by
transforming the input data into the

9
00:00:31,061 --> 00:00:34,030
representation of their
previous, uh, they didn't layer.

10
00:00:35,110 --> 00:00:40,110
And then we use the weights and the biases
found for that hidden there that was

11
00:00:40,211 --> 00:00:44,800
trained using an RBM when a tone
corridor as the initial value for the

12
00:00:45,340 --> 00:00:49,150
parameters of that hidden layer in our
deep neural network they want to use for

13
00:00:49,160 --> 00:00:49,993
classification.

14
00:00:50,770 --> 00:00:54,550
And then we initialize randomly the
last part of the neural network,

15
00:00:54,551 --> 00:00:58,570
the output layer, and then we, uh,
training using regular back pro,

16
00:00:58,780 --> 00:01:00,460
which is referred to as fine to me.

17
00:01:03,770 --> 00:01:08,770
So what we'll see is a few results on a
training deep neural networks on deep,

18
00:01:08,960 --> 00:01:11,930
uh, on different, uh, image
classification problems.

19
00:01:12,020 --> 00:01:15,470
So these were generated by
myself and my colleagues, uh,

20
00:01:15,471 --> 00:01:18,140
to get an idea of the performance of,
of uh,

21
00:01:18,290 --> 00:01:22,460
deep neural networks in different settings
with different p training algorithms

22
00:01:22,461 --> 00:01:26,780
and different configurations
are architectures of the
neural networks and so on.

23
00:01:27,500 --> 00:01:30,170
So we had a bunch of variations on DM,

24
00:01:30,171 --> 00:01:32,710
this data set where we added rotations.

25
00:01:32,730 --> 00:01:36,260
We see here characters with
different amounts of rotation.

26
00:01:36,690 --> 00:01:41,690
The incorporation of a random background
or background extracted from a,

27
00:01:41,961 --> 00:01:43,890
an actual image,
uh,

28
00:01:43,940 --> 00:01:48,940
taken from the web and a combination
of a rotation and some background.

29
00:01:49,830 --> 00:01:53,270
I also had another classification problem,
uh,

30
00:01:53,360 --> 00:01:56,810
visual classification problem where we
have a rectangle in some image and we

31
00:01:56,811 --> 00:02:01,330
have to say whether it's taller or
wider when, since this would be a,

32
00:02:01,590 --> 00:02:04,640
a wide rectangle and this
would be a tall rectangle.

33
00:02:04,640 --> 00:02:08,090
So these would be two different classes
and this would be binary classification

34
00:02:08,180 --> 00:02:09,013
problem.

35
00:02:09,170 --> 00:02:13,220
And then the harder version of that
problem where the rectangle correspond not

36
00:02:13,240 --> 00:02:17,870
to a set of black and white pixels,
but instead of different,

37
00:02:17,960 --> 00:02:18,291
uh,

38
00:02:18,291 --> 00:02:22,040
so there's a background and then there's
a patch corresponding to the rectangle

39
00:02:22,041 --> 00:02:25,040
in question, which was extracted
from a different image.

40
00:02:25,850 --> 00:02:28,430
And then we have another binary
classification problem and we have to say

41
00:02:28,431 --> 00:02:32,480
whether the shape in the image is
convex to, for instance, in this case,

42
00:02:32,481 --> 00:02:36,530
this is not convex because I can draw
a line between say that white pixel and

43
00:02:36,531 --> 00:02:41,531
that white pixel here and this line goes
over a black pixels so it doesn't stay

44
00:02:42,981 --> 00:02:47,690
in the white shape in the image.
And whereas this would be convex shape.

45
00:02:47,691 --> 00:02:51,440
So these would be the two classes for
this classification problem as well.

46
00:02:51,441 --> 00:02:54,650
We trained deep neural networks on these
different data sets and looked at the

47
00:02:54,651 --> 00:02:57,440
result and see what kind of
conclusions we could make out of them.

48
00:03:00,690 --> 00:03:04,960
Okay. In this first experiments, uh, we'll
look at the impact of initialization.

49
00:03:05,020 --> 00:03:09,750
So we considered the deep neural
network without pre-training. Uh,

50
00:03:09,830 --> 00:03:14,140
another one with pre-training using
irregular auto encoder or portraying using

51
00:03:14,141 --> 00:03:17,290
the RBM.
And in each case we considered a depth.

52
00:03:17,530 --> 00:03:21,040
So number of hidden layers
of one, two, three and four.

53
00:03:21,041 --> 00:03:24,190
So I guess this would be one would be,
wouldn't be very deep, but of course two,

54
00:03:24,191 --> 00:03:27,010
three and four would be
considered the neural network.

55
00:03:27,820 --> 00:03:30,130
And then we performed
experiment on amnesty,

56
00:03:30,160 --> 00:03:34,060
the original feminist but a
which we referred to as small a.

57
00:03:34,061 --> 00:03:37,790
So we just essentially reduced
the size of the training sets, uh,

58
00:03:37,840 --> 00:03:41,320
but it's essentially original m this
digits as extracted from the original

59
00:03:41,321 --> 00:03:43,760
Dataset.
And we compared with a,

60
00:03:43,810 --> 00:03:48,070
and also did experiment with the
rotation variation that we generated.

61
00:03:49,810 --> 00:03:53,950
So there are are three
different conclusions we can
draw from these experiments.

62
00:03:54,370 --> 00:03:58,840
The first thing is that a deep
neural network actually helps.

63
00:03:58,841 --> 00:04:02,700
So in all cases the performance without a,
Oh sorry.

64
00:04:02,710 --> 00:04:07,130
It actually pre-training helps. That's
the first conclusion. So if, uh,

65
00:04:07,150 --> 00:04:12,070
we don't use any portraying a four m,
the smaller we get about 4% performance.

66
00:04:12,550 --> 00:04:14,690
If we use auto encoder for training,

67
00:04:14,700 --> 00:04:18,400
we're more at 3.3% a error.

68
00:04:18,640 --> 00:04:21,760
And then with RBM you
get even better at 2.7.

69
00:04:22,390 --> 00:04:27,100
And then a similar conclusion for say
a the rotation where we get better and

70
00:04:27,101 --> 00:04:28,300
better performance,

71
00:04:28,480 --> 00:04:31,370
better performance in general and for
use be training either doing quarter in

72
00:04:31,371 --> 00:04:32,204
RBM.

73
00:04:33,280 --> 00:04:38,170
The other conclusion we can draw is
that pre training also helps training

74
00:04:38,180 --> 00:04:42,190
success, uh, with success.
A deeper networks,

75
00:04:42,520 --> 00:04:46,330
indeed the deepest neural
network. Well, I guess the, um,

76
00:04:46,480 --> 00:04:51,480
the shallow is neural network with a
performance that's as good as the best

77
00:04:52,241 --> 00:04:55,270
performance we can get
in this case would be 4%.

78
00:04:55,270 --> 00:04:59,230
So this is statistically
undistinguishable from these two results.

79
00:04:59,231 --> 00:05:04,150
If with two or three hidden
layers in this case, however, uh,

80
00:05:04,210 --> 00:05:06,820
training to hidden,
there's is better than just one.

81
00:05:07,270 --> 00:05:12,070
And similarly here for m this small
using either autoencoder quarter and RBM

82
00:05:12,120 --> 00:05:12,910
portraying.

83
00:05:12,910 --> 00:05:17,590
So we see that we've been able to train
with success in their own network that

84
00:05:17,591 --> 00:05:21,850
had one more hidden layer using their
unsupervised speed training procedure.

85
00:05:22,060 --> 00:05:26,950
And we actually get the same conclusion
on this rotation where the best we could

86
00:05:26,951 --> 00:05:30,670
do is to hidden layers here.
And then three with retraining,

87
00:05:30,671 --> 00:05:32,350
either with auto encoder on RBM.

88
00:05:33,640 --> 00:05:38,140
And then the third
conclusion is, um, that,

89
00:05:38,620 --> 00:05:42,790
uh,
by going from a somewhat simple problem,

90
00:05:42,970 --> 00:05:46,540
the original Lemonis
problem to a harder problem,

91
00:05:46,541 --> 00:05:49,270
which is to classify rotated digits,

92
00:05:49,930 --> 00:05:54,280
the best performance we
obtain was actually a,

93
00:05:54,281 --> 00:05:58,190
with a more with a deeper neural
network for them rotation problem.

94
00:05:58,190 --> 00:06:00,590
Then for the original end,
this problem. So in the,

95
00:06:00,591 --> 00:06:05,591
we went from one hidden layer to to
hidden there as our best result from two

96
00:06:06,291 --> 00:06:11,240
hidden layer two 300 in there as our best
result here and two to three also with

97
00:06:11,330 --> 00:06:14,550
the RBM be training. So this, uh,

98
00:06:14,580 --> 00:06:18,770
sort of confirms our intuition that for
more complicated problems we should have

99
00:06:18,771 --> 00:06:23,390
more success and get a better result
using a deeper neural network.

100
00:06:23,420 --> 00:06:25,040
When we went from m this small,

101
00:06:25,070 --> 00:06:29,180
do you have this rotation using one
more hidden layer actually helped.

102
00:06:29,960 --> 00:06:33,980
Okay. So this sort of justifies even
more the use of deep neural networks

103
00:06:37,190 --> 00:06:40,850
and these experiment from
another paper by these guys here.

104
00:06:41,450 --> 00:06:45,860
Uh, they did experiment with one, two,

105
00:06:46,070 --> 00:06:51,050
or three hidden layers with, uh,
either RBM pre-training or a,

106
00:06:51,080 --> 00:06:54,560
in this case they use de
noising autoencoder pre-training
or no pre training at

107
00:06:54,561 --> 00:06:56,630
all. So the NOPA training
would be the black line.

108
00:06:57,380 --> 00:07:01,040
And what they did is that the varied
the number of hidden units in the neural

109
00:07:01,041 --> 00:07:05,570
network and saw with just one
or two or three hidden layers,

110
00:07:06,020 --> 00:07:10,110
uh, what was the progression
of the performance on a,

111
00:07:10,290 --> 00:07:12,650
the test set performance
in terms of generalization.

112
00:07:13,610 --> 00:07:17,690
And what we see is that if the
neural network has little capacity,

113
00:07:17,691 --> 00:07:22,190
so a few hidden units, um,
then pre-training doesn't help.

114
00:07:22,310 --> 00:07:26,060
We get better test error without
retraining then with training.

115
00:07:26,570 --> 00:07:28,340
But as we increase the capacity,

116
00:07:28,370 --> 00:07:33,370
eventually these two set of lines cross
and now without retraining over fits.

117
00:07:33,500 --> 00:07:37,280
So it has a, uh, worse test
error then with be training.

118
00:07:37,370 --> 00:07:40,880
And we get the same shape either with,
uh,

119
00:07:41,060 --> 00:07:42,710
two hidden layers or three hidden.

120
00:07:42,711 --> 00:07:47,711
There's so that confirms that really we're
getting a preacher tiny acts as their

121
00:07:48,121 --> 00:07:53,121
regular riser because if the neural
networks deep or not has few parameters,

122
00:07:53,131 --> 00:07:55,760
so little capacity, then, um,

123
00:07:55,940 --> 00:07:59,810
much like any regularization
will we expect to see is, uh,

124
00:08:00,170 --> 00:08:03,500
under fitting a, so less, uh,

125
00:08:03,650 --> 00:08:07,610
so worst result in terms of test Eric
was that as we increase the capacity,

126
00:08:07,850 --> 00:08:12,290
then the better regularized model
starts doing better. And in this case,

127
00:08:12,291 --> 00:08:16,850
the better regularized model correspond
to the neural networks with a free

128
00:08:16,851 --> 00:08:17,570
training

129
00:08:17,570 --> 00:08:18,800
here,
here.

130
00:08:20,590 --> 00:08:21,161
Okay.
So again,

131
00:08:21,161 --> 00:08:26,130
this confirms our intuition that we can
think of retraining on surprise speed

132
00:08:26,131 --> 00:08:30,700
training as a rigger riser. It will,
uh, over fit less with large capacity,

133
00:08:30,701 --> 00:08:32,980
but it will lead to under
fitting with small capacity.

134
00:08:37,070 --> 00:08:42,070
This other experiments is a one
where we try to see what was the best

135
00:08:42,560 --> 00:08:47,450
architecture, uh, to use in a deep neural
network with three hidden layers. So,

136
00:08:47,451 --> 00:08:49,160
uh,
if we have three hidden layers,

137
00:08:49,161 --> 00:08:51,860
we have to choose the number of
hidden units and each hidden there,

138
00:08:51,950 --> 00:08:55,070
that can be a bit cumbersome
and annoying to do. Uh,

139
00:08:55,080 --> 00:08:57,690
so here we considered
three types of choices.

140
00:08:57,840 --> 00:09:01,260
One where we would as we go deeper,

141
00:09:01,440 --> 00:09:04,200
decreased the number of hitting
it so they keep decrease.

142
00:09:04,201 --> 00:09:07,920
The width of the hidden there. So
decreasing with would be a hidden there,

143
00:09:07,960 --> 00:09:09,150
kind of like this,

144
00:09:10,970 --> 00:09:12,160
like this movie.

145
00:09:13,340 --> 00:09:18,340
And the constant would be the same
number of heating units each in there.

146
00:09:18,560 --> 00:09:23,560
And then increasing would be
more and more hidden units as we,

147
00:09:24,320 --> 00:09:28,370
uh, get closer to the output
layer. Okay. So these,

148
00:09:28,400 --> 00:09:31,700
this is what these three set
of three lines mean here.

149
00:09:32,990 --> 00:09:36,500
And then we considered the
performance for different number,

150
00:09:36,650 --> 00:09:40,130
total number of hidden units
and the neural networks.

151
00:09:40,730 --> 00:09:45,550
And uh, and here we have the
performance either on em,

152
00:09:45,551 --> 00:09:47,720
this small or an end,
this rotation.

153
00:09:48,110 --> 00:09:51,050
And here we have a
performance for each column,

154
00:09:51,080 --> 00:09:53,970
either with RBM free training
or Toyland quarter paternity.

155
00:09:55,160 --> 00:09:59,810
And the general trend seems to be that
using the same number of hidden units, uh,

156
00:09:59,811 --> 00:10:04,040
so constant with, uh, tends
to do, uh, either, uh,

157
00:10:04,041 --> 00:10:08,330
at least as well or better
than other options. Um,

158
00:10:08,360 --> 00:10:13,250
so normally we see that the often the
confidence intervals between the different

159
00:10:13,251 --> 00:10:18,030
performances overlap with,
uh, the constant with and,

160
00:10:18,140 --> 00:10:22,070
uh, or it's actually doing a better
say for instance in this case.

161
00:10:22,550 --> 00:10:26,810
So this is kind of reassuring. It
means that perhaps we can just, uh, uh,

162
00:10:26,811 --> 00:10:28,760
at least in our first experiments,

163
00:10:28,761 --> 00:10:32,030
concentrate on neural networks with the
same number of heating units and just

164
00:10:32,031 --> 00:10:35,900
try different number of hidden
units for each, uh, hidden there.

165
00:10:39,420 --> 00:10:43,230
And he rubbed the performance on all
of the data sets I mentioned before.

166
00:10:43,231 --> 00:10:46,040
So that just in this rotation
and, uh, and this, uh,

167
00:10:46,080 --> 00:10:49,510
small for either a Svm,

168
00:10:49,530 --> 00:10:52,050
so I'm not going to describe what it is,
but it's a,

169
00:10:52,390 --> 00:10:56,070
a very popular type of
classifier and machine learning.

170
00:10:56,250 --> 00:10:59,580
And here we're using the colonel
Verint within RBF kernel.

171
00:10:59,970 --> 00:11:02,100
And here we have the
results with pre-training,

172
00:11:02,190 --> 00:11:05,700
with stacked auto encoders rbms
or the nosing two quarters.

173
00:11:06,030 --> 00:11:09,630
And we always use three hidden layers.
So,

174
00:11:09,631 --> 00:11:13,460
first thing we see is that deep
neural networks, either the A,

175
00:11:13,480 --> 00:11:18,480
what I call DBN here with is really
just a stack of rbms for p training or,

176
00:11:18,780 --> 00:11:19,220
uh,

177
00:11:19,220 --> 00:11:24,220
stacking the nosing twin quarters
tend to do better than using an SBM,

178
00:11:24,391 --> 00:11:25,470
which is,
uh,

179
00:11:25,490 --> 00:11:29,600
which correspond to a very good
but shallow class classifier.

180
00:11:29,610 --> 00:11:34,050
And so not a deep classifier.
Um, we see also that, uh,

181
00:11:34,051 --> 00:11:38,070
auto encoders tend to perform less well
than either rbms or the noisy motor in

182
00:11:38,080 --> 00:11:41,940
quarters and rbms they
dinos and auto encoders, uh,

183
00:11:42,480 --> 00:11:44,080
tend to do about as well.
So Tom,

184
00:11:44,081 --> 00:11:48,570
sometimes the free training that RBM
works better than de noising and sometimes

185
00:11:48,571 --> 00:11:52,920
it's the other way around.
Um, and so in practicing,

186
00:11:53,230 --> 00:11:56,980
we do observe that they tend to be
somewhat equivalent than in terms of the

187
00:11:56,981 --> 00:12:00,460
performance, but very
on, on various dataset.

188
00:12:00,520 --> 00:12:03,740
It might be that one will work
better than the other. So, um,

189
00:12:04,480 --> 00:12:08,380
so they're both really good
reasonable options for a pre training,

190
00:12:08,500 --> 00:12:12,700
whereas regular auto
encoders, uh, might not, uh,

191
00:12:12,760 --> 00:12:16,690
is not worthwhile compared say to
just de noising or two in quarters.

192
00:12:16,750 --> 00:12:18,820
So this confirms what we discussed before,

193
00:12:18,821 --> 00:12:22,180
that the nausea and auto encoders are
really a way of improving on the regular

194
00:12:22,450 --> 00:12:27,220
auto in quarter. All right?
So that ends our examples. So,

195
00:12:27,280 --> 00:12:29,980
uh, we see that unsupervised
training is helpful.

196
00:12:29,981 --> 00:12:32,650
It helps in terms of
regularizing the model,

197
00:12:32,860 --> 00:12:37,860
and it also allows us for more complicated
task to get better performance with a

198
00:12:38,320 --> 00:12:39,520
deeper neural networks.

