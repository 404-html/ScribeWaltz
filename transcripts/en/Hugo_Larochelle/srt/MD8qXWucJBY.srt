1
00:00:00,790 --> 00:00:01,391
And this video,

2
00:00:01,391 --> 00:00:05,350
we'll see an efficient algorithm for
training or restrictive Boltzman machine

3
00:00:05,380 --> 00:00:06,940
known as contrasted divergence.

4
00:00:07,040 --> 00:00:07,873
It's

5
00:00:09,440 --> 00:00:14,440
so we've seen a poster machine how it
defines a distribution over x and it's

6
00:00:14,991 --> 00:00:18,050
hidden layer h. Uh, we've seen that, uh,

7
00:00:18,051 --> 00:00:21,440
we could also write down p of x,
uh,

8
00:00:21,441 --> 00:00:24,860
using the free energies
and we've looked at uh,

9
00:00:24,861 --> 00:00:29,660
how exactly it tries to model an increased
probability for certain values of the

10
00:00:29,661 --> 00:00:32,680
input vector that's actually see how,
uh,

11
00:00:32,720 --> 00:00:37,130
we can actually train the restrictive
bolsa machine UNSOM training data, uh,

12
00:00:37,131 --> 00:00:39,950
in order to obtain, they were suitable
to machine that model as well,

13
00:00:39,951 --> 00:00:42,530
that data and assigns
high probability to it.

14
00:00:44,210 --> 00:00:45,043
Okay.

15
00:00:45,260 --> 00:00:47,420
So like we've seen before,

16
00:00:47,421 --> 00:00:51,890
we'll treat this problem of training
as an empirical minimization problem,

17
00:00:52,220 --> 00:00:56,060
uh, will, uh, do without the
regularization here and just, uh,

18
00:00:56,120 --> 00:01:01,120
tried to minimize the average loss
where the loss is going to be the most

19
00:01:01,311 --> 00:01:02,930
natural thing here to do,

20
00:01:03,380 --> 00:01:08,380
which is to use the negative log
probability as our last for this model.

21
00:01:09,410 --> 00:01:12,830
And so what we'd like to do
is optimize this average,

22
00:01:13,160 --> 00:01:16,470
a negative log probability
of the training data, uh,

23
00:01:16,490 --> 00:01:20,060
using an optimization procedure and
particular would like to use to castigate

24
00:01:20,150 --> 00:01:23,660
the scent because works well and
scales well to large data sets.

25
00:01:23,990 --> 00:01:28,130
And so it means that to achieve this
I need the partial, they'll live,

26
00:01:28,131 --> 00:01:30,920
have any parameter theater of my loss.

27
00:01:31,540 --> 00:01:36,410
Now if I make the derivation and write
down the expression for it actually has

28
00:01:36,411 --> 00:01:37,101
a,
uh,

29
00:01:37,101 --> 00:01:41,210
an expression which looks kind of similar
to other things we've seen before in

30
00:01:41,211 --> 00:01:45,320
terms of partial the lives of parameters
for neural networks. We have again,

31
00:01:45,350 --> 00:01:47,660
a difference of two terms.

32
00:01:47,661 --> 00:01:52,160
One which depends on the observation
and another one which depends on more

33
00:01:52,161 --> 00:01:53,900
explicitly,
only on the model.

34
00:01:54,460 --> 00:01:58,430
And so this partial derivative
is going to be the, uh,

35
00:01:58,490 --> 00:02:02,570
expect expectation over,
uh, what we never see,

36
00:02:02,571 --> 00:02:07,550
which is the hidden layer value of
the partial narrative of the energy.

37
00:02:07,551 --> 00:02:11,240
With respect to my parameter,
a given my observation.

38
00:02:11,480 --> 00:02:14,780
So an expected,
a partial derivative,

39
00:02:15,380 --> 00:02:19,990
we have to do an expectation because h we
never know where its value is. And, uh,

40
00:02:20,060 --> 00:02:24,680
so it's the expectation of this partial
there live condition on the observation.

41
00:02:25,790 --> 00:02:30,230
And then we subtract that,
the expectation over age and uh,

42
00:02:30,280 --> 00:02:35,240
and now also ex, uh, of the
again, uh, the partial, the,

43
00:02:35,241 --> 00:02:38,060
the, the energy function with
respect to our parameter,

44
00:02:38,270 --> 00:02:41,750
but not as now we do an expectation
over x as well. And, uh,

45
00:02:41,780 --> 00:02:46,430
this is an expectation according to
our model. So under our models, uh,

46
00:02:46,490 --> 00:02:51,020
distribution, uh, so we often
refer to that part computing,

47
00:02:51,021 --> 00:02:52,440
this part of the,
uh,

48
00:02:52,540 --> 00:02:57,380
gradient as the positive phase and this
the negative phase for obvious reasons.

49
00:02:58,520 --> 00:03:01,660
And, uh, but now the problem is that, uh,

50
00:03:01,690 --> 00:03:06,340
this part here is actually hard to
compute. It's generally intractable. Uh,

51
00:03:06,341 --> 00:03:09,670
and the reason is that we
have to make an exponential,

52
00:03:09,671 --> 00:03:13,510
some over both h and x. And so, um,

53
00:03:13,750 --> 00:03:18,220
perhaps you'll be convinced that if I
give you a value of the visible layers,

54
00:03:18,221 --> 00:03:20,020
so if I give you an observation x,

55
00:03:20,320 --> 00:03:25,320
then summing over h is actually
something that's tractable to do, uh,

56
00:03:25,340 --> 00:03:29,130
for this partial derivative
in particular for the RBM. Uh,

57
00:03:29,131 --> 00:03:32,440
and then we can again leverage the
fact that we get an expression,

58
00:03:32,740 --> 00:03:35,890
a of nested nested sums,

59
00:03:36,220 --> 00:03:39,670
a over something that fact arises
with respect to each hidden unit.

60
00:03:39,671 --> 00:03:43,600
And then we can actually write this down
into an expression that's cylinder and

61
00:03:43,601 --> 00:03:44,590
the number of hidden units.

62
00:03:44,591 --> 00:03:49,560
So we'll see that more in more
details later in this video. Uh,

63
00:03:50,140 --> 00:03:53,410
but now if you have to do
is some over both x and h,

64
00:03:53,530 --> 00:03:58,120
now this becomes intractable. And so we'll
have to approximate that term somehow,

65
00:03:58,600 --> 00:04:03,040
uh, in order to perform a stochastic
gradient descent efficient.

66
00:04:05,410 --> 00:04:06,130
Okay.

67
00:04:06,130 --> 00:04:08,200
So, uh, to address this problem,

68
00:04:08,230 --> 00:04:13,230
Jeff Hinton in 2002 propose the
contrast divergence learning algorithm.

69
00:04:14,100 --> 00:04:17,770
Uh, so there's some theory for
what this is actually doing.

70
00:04:18,030 --> 00:04:19,480
I'm not going to go over that.

71
00:04:19,481 --> 00:04:23,980
I'm just going to describe what the
algorithm is and the more intuitive terms.

72
00:04:24,050 --> 00:04:28,900
And, uh, I tried to give more intuition
for why it should actually work.

73
00:04:30,130 --> 00:04:32,430
So really the idea is to try to,
uh,

74
00:04:32,530 --> 00:04:36,520
do without this double expectation
and a instead estimated.

75
00:04:36,850 --> 00:04:40,840
So are there are really three main
components to a contrasted divergence.

76
00:04:41,260 --> 00:04:46,260
The first idea is that the expectation
over x and h in the negative phase will

77
00:04:46,691 --> 00:04:51,691
actually replace it by a point
estimate at a single observation excel.

78
00:04:52,330 --> 00:04:53,800
So the expectation over X,

79
00:04:53,801 --> 00:04:58,801
we'll replace it by a a point estimate
at axtel because if we have that point

80
00:04:59,621 --> 00:05:01,570
estimate, if we give me a
value of the visible layer,

81
00:05:01,571 --> 00:05:06,571
then I can do the expectation over age
and then get an estimate of the uh,

82
00:05:07,570 --> 00:05:10,750
double expectation.
Uh,

83
00:05:10,751 --> 00:05:14,770
so that's just really saying we'll do a
Monte Carlo estimate of the expectation

84
00:05:14,771 --> 00:05:18,340
with a single data point. Then, uh, we'll,

85
00:05:18,490 --> 00:05:21,670
we need to obtain that extent somehow.
Uh,

86
00:05:21,671 --> 00:05:25,030
and ideally would like to a
sample from the true distribution,

87
00:05:25,031 --> 00:05:27,370
not the true distribution,
but our model distribution.

88
00:05:27,400 --> 00:05:31,840
And then we'll do this by Gibbs
sampling. Uh, so remember that Gibbs,

89
00:05:31,841 --> 00:05:34,870
something corresponds to a sampling,

90
00:05:34,900 --> 00:05:39,670
each variable in my model,
given the others,

91
00:05:40,150 --> 00:05:43,730
and specifically for a restricted Bolsa
machine performing that sampling is

92
00:05:43,731 --> 00:05:47,080
actually quite as efficient
because condition on one layer,

93
00:05:47,140 --> 00:05:50,140
all the other elements and the
other layer, uh, are independent.

94
00:05:50,410 --> 00:05:54,760
Then I can sample all the
values in one layer in parallel,

95
00:05:54,790 --> 00:05:59,480
given the value, uh, of the opposite
layer and then alternate between,

96
00:05:59,800 --> 00:06:01,400
uh,
each layer like this.

97
00:06:01,850 --> 00:06:04,550
And so this is actually very
efficient to do in practice.

98
00:06:06,020 --> 00:06:07,010
And then the third idea,

99
00:06:07,011 --> 00:06:12,011
which is perhaps the most
important contribution behind
contrasted divergence is

100
00:06:12,080 --> 00:06:14,210
to uh,
perform gib sampling.

101
00:06:14,210 --> 00:06:18,410
But by starting our assembling at,
uh,

102
00:06:18,500 --> 00:06:21,700
a state where the visible layer,
uh,

103
00:06:21,800 --> 00:06:26,780
is set to the training example for which
I'm trying to compute the gradient and

104
00:06:26,781 --> 00:06:27,614
do an update.

105
00:06:27,950 --> 00:06:31,760
And so instead of starting like we
usually do and give sampling at a

106
00:06:31,790 --> 00:06:35,750
configuration of my, uh, on my
layers, my random variables,

107
00:06:35,780 --> 00:06:38,780
that is a simple, uh, perhaps uniform the,

108
00:06:38,781 --> 00:06:43,781
and just randomly according to some of
initial distribution out actually use the

109
00:06:44,661 --> 00:06:49,640
value of the training observation for the
value of the visible they are when I'm

110
00:06:50,480 --> 00:06:53,660
performing gib sampling.
And the other thing is that I'm,

111
00:06:53,720 --> 00:06:56,150
I'm not actually going to
do give sampling for a long,

112
00:06:56,151 --> 00:06:59,270
I'm actually going to do it for one,
two,

113
00:06:59,300 --> 00:07:02,870
or just a few iterations and a,
as we'll see,

114
00:07:02,871 --> 00:07:04,490
this actually works well in practice.

115
00:07:05,030 --> 00:07:09,980
So to illustrate this process more
visually. Uh, so there's, oh, Aaron,

116
00:07:09,981 --> 00:07:12,590
my finger, there should be a
little the same circle here.

117
00:07:13,220 --> 00:07:17,180
So what we'll do is that at training
time for giving training example,

118
00:07:17,210 --> 00:07:22,210
I'll take the value of the input vector
x and I'll set it as my value from my

119
00:07:23,331 --> 00:07:24,164
visible layer.

120
00:07:24,860 --> 00:07:29,860
Then I'll assemble all the hidden
units condition on the observing this

121
00:07:30,291 --> 00:07:34,010
particular value of the visible there.
This training example.

122
00:07:34,380 --> 00:07:39,380
So I'll sampled from p of h given that
x is equal to x t in this case here,

123
00:07:42,520 --> 00:07:45,680
uh, just a note on performing
that sampling. So, um,

124
00:07:46,130 --> 00:07:47,660
each neuron lung condition independent,

125
00:07:47,730 --> 00:07:50,630
so they each have Bernoulli
for which I can compute.

126
00:07:50,900 --> 00:07:55,220
What's the probability for
that Bernoulli random variable,

127
00:07:55,221 --> 00:07:57,680
that hidden unit being holed to one,
given x.

128
00:07:58,490 --> 00:08:00,680
Now to obtain a sample from a Bernoulli,

129
00:08:00,890 --> 00:08:02,870
would that probability
of being equal to one,

130
00:08:03,110 --> 00:08:08,110
what I can do is just sample from a
uniform distribution between zero and one.

131
00:08:10,220 --> 00:08:12,590
And then if that value,

132
00:08:12,680 --> 00:08:16,850
that uniform value that I
sampled is actually, um,

133
00:08:17,540 --> 00:08:20,180
uh, sorry, it should be
the other way around.

134
00:08:20,180 --> 00:08:25,180
So if this value is greater
than so greater than,

135
00:08:27,830 --> 00:08:32,210
uh, the value of the probability is
greater than the value of sample from my

136
00:08:32,211 --> 00:08:36,590
uniform, then I'm going to set the, um,

137
00:08:36,620 --> 00:08:41,130
that I'm going to set the hidden layer,
unit digit hidden layer,

138
00:08:41,131 --> 00:08:45,230
the unit two I want. And
otherwise I'm going to set it
to zero. So in other words,

139
00:08:45,500 --> 00:08:50,030
the value of the hidden yet is going to
be the identity function of whether the

140
00:08:50,031 --> 00:08:55,031
probability of the hidden unit is greater
than a random sample from a uniform

141
00:08:55,321 --> 00:09:00,200
between zero and one. So we can
see that this, uh, will be, uh,

142
00:09:00,390 --> 00:09:05,150
equal to one with a
probability of this value, uh,

143
00:09:05,400 --> 00:09:10,400
because the mass of a
uniformly distributed random
variable between zero and

144
00:09:11,520 --> 00:09:15,900
this value is going to be
exactly because it's uniform,

145
00:09:16,110 --> 00:09:20,850
it's going to be p of d
people to one given x. So, uh,

146
00:09:20,851 --> 00:09:25,851
so indeed the property that this is one
is going to BP of Hg equal one given

147
00:09:27,280 --> 00:09:28,113
x.

148
00:09:29,710 --> 00:09:34,600
Okay. So now I've taken my
training sample. I've simple
each of the hidden units,

149
00:09:34,980 --> 00:09:37,160
uh, conditionally, uh,

150
00:09:37,310 --> 00:09:40,300
a condition on the physical
layer of taking that value.

151
00:09:40,720 --> 00:09:44,570
And then I'm going to reconstruct a,
uh,

152
00:09:44,580 --> 00:09:49,580
the visible layer by sampling from a p
of x given the current value of my hidden

153
00:09:50,891 --> 00:09:54,220
there.
So I could call this a x one.

154
00:09:55,690 --> 00:09:58,530
And then I'm going to
do that four k steps.

155
00:09:58,550 --> 00:10:01,360
I'm going to take x one and then
sample the new value of the hidden.

156
00:10:01,361 --> 00:10:04,960
They're given my previously
sample value of the visible layer.

157
00:10:04,990 --> 00:10:09,010
I'm going to alternate light this
performing Gib sampling for case steps.

158
00:10:09,700 --> 00:10:14,700
And now this last value after I've
done my case steps is going to be the,

159
00:10:15,760 --> 00:10:19,630
uh, a negative sample x still. So Xcel,

160
00:10:19,690 --> 00:10:21,520
we often refer to it as a negative sample,

161
00:10:21,521 --> 00:10:25,510
which is used to estimate the
negative phase part of the gradient.

162
00:10:26,200 --> 00:10:30,910
And so I'm going to use that
as my negative sample to
perform my point estimate

163
00:10:31,090 --> 00:10:32,870
of the expectation,
uh,

164
00:10:32,890 --> 00:10:37,420
over X. Okay. So visually,

165
00:10:37,421 --> 00:10:38,440
what does this look like?

166
00:10:39,100 --> 00:10:42,730
So I get a training example and uh,

167
00:10:42,790 --> 00:10:46,660
in the positive phase I'd had to
estimate this conditional expectation to

168
00:10:46,661 --> 00:10:51,370
simplify. Imagine I actually don't
perform my expectation. I just sample and,

169
00:10:51,560 --> 00:10:51,790
uh,

170
00:10:51,790 --> 00:10:56,790
h given this XD and I'll
call that h tilty and uh,

171
00:10:57,970 --> 00:11:00,200
I've also performed case
steps of Gib samplings,

172
00:11:00,300 --> 00:11:05,300
[inaudible] and similarly how to perform
that expectation estimated at exhale

173
00:11:06,670 --> 00:11:08,230
and h still wear,
uh,

174
00:11:08,670 --> 00:11:13,670
age still would be simple based on the
conditional distribution of the hidden

175
00:11:13,811 --> 00:11:17,170
layer,
given that x is equal to x still.

176
00:11:18,740 --> 00:11:19,070
Okay.

177
00:11:19,070 --> 00:11:19,680
Um,

178
00:11:19,680 --> 00:11:24,210
so then I get these two pairs and if I
look at the green in the sense procedure,

179
00:11:24,211 --> 00:11:29,211
what it's telling me is that I should
decrease the energy at the training

180
00:11:29,701 --> 00:11:34,701
observation and I should increase it
at the simple value x still in its

181
00:11:35,701 --> 00:11:39,540
associated hidden layer. Uh, because uh,

182
00:11:39,570 --> 00:11:42,090
low energy means high probability,

183
00:11:43,110 --> 00:11:48,060
then this means that I'm going
to increase really the, uh,

184
00:11:48,240 --> 00:11:51,660
probability of observing
XT with a tendered layer.

185
00:11:51,840 --> 00:11:54,580
And I'm going to at the same time,

186
00:11:54,581 --> 00:11:59,290
the probability that x still is
going to be observed under my models

187
00:11:59,291 --> 00:12:00,124
distribution.

188
00:12:01,060 --> 00:12:05,350
And so if my training example
corresponded to images of digits,

189
00:12:05,740 --> 00:12:09,940
then I'd be increasing the probability
of observing this particular digit here

190
00:12:10,000 --> 00:12:14,120
under my model.
And then say initially my uh,

191
00:12:14,170 --> 00:12:16,570
restricted bolsa machine
is randomly initialized.

192
00:12:16,571 --> 00:12:21,340
So it essentially corresponds to a uniform
distribution over a binary vectors.

193
00:12:21,670 --> 00:12:25,180
Then initially what I'm going to sample
is really going to look like noise.

194
00:12:25,180 --> 00:12:27,430
Essentially it's going to
look like something like this.

195
00:12:27,700 --> 00:12:31,450
And so what I'll be doing is then making
the probability of sampling something

196
00:12:31,451 --> 00:12:35,980
like this from my model and much smaller
and then I'll continue iterating like

197
00:12:35,981 --> 00:12:40,270
this. So next time around it will be less
likely that it still would look like a

198
00:12:40,271 --> 00:12:45,040
random image and the sample value for
exterior going to be looking more and more

199
00:12:45,041 --> 00:12:49,600
like actual training examples because
I keep pushing down the probability of

200
00:12:49,601 --> 00:12:51,580
anything that doesn't look like a digit.

201
00:12:52,180 --> 00:12:54,970
And so we can see that as
we keep sampling like this,

202
00:12:55,090 --> 00:13:00,090
then eventually the gradient should
become smaller because the value of XD is

203
00:13:00,131 --> 00:13:03,970
going to be more and more similar
to the sample value of exhale.

204
00:13:05,020 --> 00:13:06,610
And so until they lead this algorithm,

205
00:13:06,611 --> 00:13:10,780
what it's doing is that it's
increasing the, uh, energy, uh, sorry,

206
00:13:10,781 --> 00:13:14,740
decreasing the energy of things that look
like what's in the training set while

207
00:13:14,741 --> 00:13:17,360
increasing the energy of things that,
uh,

208
00:13:17,470 --> 00:13:19,960
are as elucidated or sample by the model.

209
00:13:20,290 --> 00:13:24,460
And we keep doing this until the
model spits out or generates, uh,

210
00:13:24,500 --> 00:13:28,270
observations that are very similar to
what's in the model. In other words,

211
00:13:28,271 --> 00:13:32,140
it's become a good model
of, uh, uh, our dataset.

