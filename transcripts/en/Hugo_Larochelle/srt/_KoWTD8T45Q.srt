1
00:00:01,210 --> 00:00:04,320
In this video, we'll put everything
together we seen before in,

2
00:00:04,330 --> 00:00:08,660
in expressing the different gradients
in a neural network and, uh, obtain, uh,

3
00:00:08,670 --> 00:00:12,340
one efficient procedure for computing
the gradients with respect to our

4
00:00:12,341 --> 00:00:15,580
parameter. And that procedure
is called backpropagation.

5
00:00:18,000 --> 00:00:23,000
So now we've seen in a previous video
and expression for getting the grain and

6
00:00:24,331 --> 00:00:28,470
with respect to all the parameters in
the neural network, that expression,

7
00:00:28,471 --> 00:00:31,650
depending on the gradient
of the hidden layers, uh,

8
00:00:31,651 --> 00:00:35,790
which d also depending on the gradient,
uh, with respect to the output.

9
00:00:36,570 --> 00:00:40,620
And so now we'll see how can put together
all of these different expressions to

10
00:00:40,621 --> 00:00:44,700
obtain a procedure that will efficiently
compute the parameter gradients.

11
00:00:47,270 --> 00:00:48,103
Yeah.

12
00:00:48,110 --> 00:00:52,550
All right. So, um, we've
seen in the first video how,

13
00:00:52,580 --> 00:00:53,240
uh,

14
00:00:53,240 --> 00:00:57,830
the gradient of the loss function with
respect to the pre activation at the

15
00:00:57,831 --> 00:01:02,410
output layer correspondent to
this expression. And then, uh,

16
00:01:02,450 --> 00:01:07,250
we also saw that the gradient with respect
to the activation in the hidden layer,

17
00:01:07,520 --> 00:01:08,570
a curse upon the two,

18
00:01:08,571 --> 00:01:13,571
that expression and a then we saw that
the gradient of the loss function with

19
00:01:14,421 --> 00:01:19,421
respect to the pre activation of a hidden
layer responded to that expression.

20
00:01:20,210 --> 00:01:24,710
If finally we saw that the gradient of
the last function with respect to either

21
00:01:24,920 --> 00:01:29,920
the a matrix of connections or devices
corresponded to these two expressions

22
00:01:29,991 --> 00:01:32,480
here and now we know this,

23
00:01:32,481 --> 00:01:37,130
that in each case except
for this one, uh, s uh,

24
00:01:37,160 --> 00:01:38,870
in the expression on the left,

25
00:01:39,080 --> 00:01:42,620
it would often depend
on the gradients of uh,

26
00:01:42,650 --> 00:01:44,210
other parts of the neural networks.

27
00:01:44,211 --> 00:01:49,211
So the gradient of the weights and the
biases depends on the gradient with

28
00:01:49,941 --> 00:01:52,470
respect to the pre activations,
uh,

29
00:01:52,471 --> 00:01:57,471
the gradients of the hidden
unit activations dependent
also on the green and

30
00:01:57,591 --> 00:02:01,790
the, the pre activations. While
the grain of pre activations, uh,

31
00:02:01,791 --> 00:02:06,791
actually dependent on the gradient of
the activation of a hidden layer and at

32
00:02:07,881 --> 00:02:08,571
the output layer.

33
00:02:08,571 --> 00:02:13,571
Also we had the formula for getting the
gradient of the pre activations at the

34
00:02:14,091 --> 00:02:17,090
output layer.
So here the procedure we see,

35
00:02:17,091 --> 00:02:19,160
which is the backpropagation algorithm,

36
00:02:19,340 --> 00:02:22,340
the idea is to apply
these formulas in order.

37
00:02:22,341 --> 00:02:25,160
So that on the right side here,

38
00:02:25,161 --> 00:02:30,161
we always have a precomputed everything
to actually compute that expression.

39
00:02:32,060 --> 00:02:35,240
So let's go into the steps of
the backpropagation algorithm.

40
00:02:35,570 --> 00:02:40,190
So first we assume that we've already
done a forward propagation. That is,

41
00:02:40,191 --> 00:02:44,780
we've computed, given the input,
the value of all the hidden units,

42
00:02:45,020 --> 00:02:49,040
uh, all the hidden layers and the output
layer. So we've computer already f of x,

43
00:02:49,250 --> 00:02:53,540
and that's because before we
computed each of the, uh, activation,

44
00:02:53,870 --> 00:02:57,740
uh,
of each hidden layer h of x.

45
00:02:59,230 --> 00:03:04,160
And now we'll, we'll do is that we
initialize the pre activation gradient,

46
00:03:04,550 --> 00:03:07,990
uh, at the output layer
that's layer l plus one,

47
00:03:08,380 --> 00:03:09,670
and we'll uh,

48
00:03:09,760 --> 00:03:14,230
we'll initialize it or we'll set it
to the value of what that gradient is.

49
00:03:14,230 --> 00:03:17,040
That is minus the 100 factor,
uh,

50
00:03:17,350 --> 00:03:20,440
for labeled y minus the output layer.

51
00:03:21,400 --> 00:03:25,270
And then we'll iterate from layer,
uh,

52
00:03:25,360 --> 00:03:28,120
l plus one to layer one,

53
00:03:28,630 --> 00:03:32,470
and we'll perform these updates,
these computations here.

54
00:03:32,860 --> 00:03:37,860
So first we'll actually compute
the gradient for the current layer.

55
00:03:37,991 --> 00:03:42,130
So I'll call Kay the current hidden layer
at which we are during our iteration

56
00:03:42,131 --> 00:03:46,960
here. So we compute the gradients
of the hidden layer parameters,

57
00:03:46,980 --> 00:03:47,380
the lid,

58
00:03:47,380 --> 00:03:52,380
the parameters for a hidden
layer cake or hidden or you know,

59
00:03:52,840 --> 00:03:54,850
cake because it can be,
I'll plus one that could be,

60
00:03:55,090 --> 00:03:58,420
this could be the parameters
for the output layer as
well depending on the value

61
00:03:58,421 --> 00:03:59,254
of k.

62
00:03:59,500 --> 00:04:04,500
So we can compute this because
we've already precomputed the uh,

63
00:04:05,050 --> 00:04:08,350
reactivation gradient.
So we have the value for this.

64
00:04:08,530 --> 00:04:12,430
So now we just take the outer product
of that vector with that vector and that

65
00:04:12,431 --> 00:04:15,680
gives us the gradient for the uh,

66
00:04:15,790 --> 00:04:20,790
weights and then the gradient for the
biases as we seen as just a gradient with

67
00:04:20,981 --> 00:04:25,180
respect to their p activations.
And then will compute our,

68
00:04:25,210 --> 00:04:27,760
propagate the gradient
to the hidden layer.

69
00:04:27,761 --> 00:04:32,680
Below that is will compute the grain of
the loss with respect to the activation

70
00:04:32,740 --> 00:04:35,700
for the layer right below
the layer came on a swan.

71
00:04:36,190 --> 00:04:40,960
And we've seen that this just corresponds
to taking the pre activation gradient

72
00:04:41,020 --> 00:04:46,020
and multiplying that by the transpose
of the connections between the current

73
00:04:46,240 --> 00:04:51,240
hidden layer k and the inner layer below
came on his phone and then will compute

74
00:04:51,910 --> 00:04:56,860
the gradients. But before the activation.
So the pre activation gradient,

75
00:04:58,000 --> 00:05:00,580
which we see here,
and we've seen that occur,

76
00:05:00,581 --> 00:05:03,940
spawns to taking the
gradient of the activations.

77
00:05:04,240 --> 00:05:08,890
And I'm doing an element
wise multiplication with
the activation function,

78
00:05:09,010 --> 00:05:13,570
the narratives at the value
of the activation function.

79
00:05:14,410 --> 00:05:19,300
And so once we, uh, reach
that part of this, uh,

80
00:05:19,360 --> 00:05:21,320
iteration in the loop,
uh,

81
00:05:21,370 --> 00:05:25,850
then we now have the pre activation
gradients for came on this one.

82
00:05:25,960 --> 00:05:30,960
So now we're ready to go from a
given value of k two a one a two.

83
00:05:31,391 --> 00:05:35,980
The value of k, uh, that's,
uh, well, one less. So we,

84
00:05:36,370 --> 00:05:40,390
for instance, if we started at a
value of k, which is Alpha Swan,

85
00:05:40,391 --> 00:05:44,980
we'd be ready to go at a Caples
l and then kcals l minus one,

86
00:05:44,981 --> 00:05:48,580
and so on until we reached
the last hidden layer.

87
00:05:49,690 --> 00:05:54,640
So a, this is the backpropagation
algorithm and we see that actually, uh,

88
00:05:54,870 --> 00:05:58,250
uh, does the sort of the
opposite, the forward propagation,

89
00:05:58,280 --> 00:06:01,640
instead of going from the input
layer to the output layer,

90
00:06:01,700 --> 00:06:05,010
it goes from the output layer and then,
uh,

91
00:06:05,030 --> 00:06:09,260
it goes through the hidden there in the
reverse order from the top most in layer

92
00:06:09,261 --> 00:06:10,820
to the bottom line.

93
00:06:13,930 --> 00:06:17,530
There's a visual illustration of the
computations we have to do when we do fall

94
00:06:17,560 --> 00:06:18,860
forward and,
uh,

95
00:06:18,960 --> 00:06:22,990
backpropagation and this is also going
to give us some hints on how we could

96
00:06:22,991 --> 00:06:25,210
implement for them backward propagation.

97
00:06:26,200 --> 00:06:31,200
So we can write for propagation as an
acid clip flow graph where we have boxes.

98
00:06:32,861 --> 00:06:34,600
So we see the boxes here with the,

99
00:06:34,720 --> 00:06:38,980
the different variables that we
have to compute when computing, uh,

100
00:06:39,010 --> 00:06:41,860
all the hidden layers and upload
layer and then the last function.

101
00:06:42,520 --> 00:06:47,170
And so initially we have to set
our input layer to some value x.

102
00:06:47,560 --> 00:06:52,210
Uh, well, so assume that we know
what are the current parameters of,

103
00:06:52,270 --> 00:06:56,590
so these are the boxes that contain the
priorities of the current value for the

104
00:06:56,591 --> 00:06:57,424
parameters.

105
00:06:57,940 --> 00:07:01,240
And then what we do is that we compute
the reactivation for the first in the

106
00:07:01,241 --> 00:07:01,601
layer,

107
00:07:01,601 --> 00:07:06,250
we compute that from the input and the
biases and the weights of that hidden

108
00:07:06,251 --> 00:07:07,084
layer.

109
00:07:07,300 --> 00:07:10,480
Then from the pre activation
and we compute the activation
of the hidden layer.

110
00:07:10,900 --> 00:07:14,410
And then from that and the parameters
for the second hidden layer,

111
00:07:14,500 --> 00:07:19,200
we can compute the pre activation for
the second hidden layer. And for uh,

112
00:07:19,230 --> 00:07:20,530
two hidden layer neural network.

113
00:07:20,560 --> 00:07:25,560
We could then compute the output and
then from the output we could compute a

114
00:07:26,051 --> 00:07:30,850
loss associated with this output.
Given that the true label is why,

115
00:07:31,390 --> 00:07:32,223
okay,

116
00:07:32,380 --> 00:07:36,790
so that actually gives us a nice
way of potentially implementing for

117
00:07:36,791 --> 00:07:37,481
propagation.

118
00:07:37,481 --> 00:07:42,481
So each buck could be an object and would
have a forward propagation or f prop

119
00:07:42,881 --> 00:07:44,590
method that would take uh,

120
00:07:44,591 --> 00:07:49,591
the parents of the box to the parents of
that object and a compute the value of

121
00:07:50,351 --> 00:07:52,150
the box.
So for this box here,

122
00:07:52,151 --> 00:07:55,720
this would be a box that would
compute a linear combination of this,

123
00:07:55,721 --> 00:07:57,610
this and that.
So the reactivation,

124
00:07:57,850 --> 00:08:02,200
this box would be an type of object
that computes a particular activation

125
00:08:02,201 --> 00:08:04,960
function from the pre
activation and so on.

126
00:08:05,860 --> 00:08:06,260
Okay.

127
00:08:06,260 --> 00:08:11,140
So then performing for propagation would
just be crisp onto calling the F prob

128
00:08:11,150 --> 00:08:12,830
method for each box,

129
00:08:12,831 --> 00:08:17,420
for each object in the correct order
from the parents, which have no children.

130
00:08:17,690 --> 00:08:20,880
And then computing it
for, uh, the other, uh,

131
00:08:20,960 --> 00:08:25,760
boxes for which we have the value for
all of their parents and proceeding

132
00:08:25,761 --> 00:08:27,740
forward until we reached
the last function.

133
00:08:31,440 --> 00:08:36,060
And then if for this,
these, all of these objects,

134
00:08:36,061 --> 00:08:40,920
we also had a beep prop method, then
we could compute, uh, backpropagation.

135
00:08:40,921 --> 00:08:43,140
In fact,
we'd get the sort of for free,

136
00:08:43,141 --> 00:08:47,230
we get a procedure for
computing all the gradients, uh,

137
00:08:47,240 --> 00:08:50,280
of all of these boxes with
respect to the last function.

138
00:08:50,430 --> 00:08:54,210
And this is called actually,
uh, automatic differentiation.

139
00:08:54,870 --> 00:08:58,110
Um, so what it's a, in this setting,

140
00:08:58,111 --> 00:09:02,610
what the be prop method would do is that
it would compute the gradient of the

141
00:09:02,611 --> 00:09:07,590
loss with respect to its children. So, uh,

142
00:09:07,890 --> 00:09:08,550
and uh,

143
00:09:08,550 --> 00:09:13,550
so we have a beep prop method that
would depend on the boxes parents.

144
00:09:15,930 --> 00:09:20,340
So, uh, the f prop
depends on its children.

145
00:09:20,730 --> 00:09:23,310
It performs a computation
with respect to its children,

146
00:09:23,460 --> 00:09:25,980
whereas be prop with
performance computation,

147
00:09:25,981 --> 00:09:30,780
but with respect to the boxes parents
and indeed if we go and look back at the

148
00:09:30,790 --> 00:09:35,640
formula for computing the great influence
suspect to the reactivation or the, uh,

149
00:09:35,990 --> 00:09:39,510
uh, the, sorry, the activation
or the parameters, the, uh,

150
00:09:39,511 --> 00:09:42,600
the formula always depends, uh, uh,

151
00:09:42,630 --> 00:09:47,580
on the parents of these boxes in
this illustration here. So in fact,

152
00:09:47,910 --> 00:09:51,690
doing backpropagation would
correspond to Coleen be prop,

153
00:09:51,750 --> 00:09:56,100
but in the reverse order of
the graph. So, uh, in fact,

154
00:09:56,101 --> 00:09:58,860
we'd start with this box and then
it would refers to the Arrow.

155
00:09:59,280 --> 00:10:03,960
And then at this point we'd be ready
to call be prop for this box here,

156
00:10:03,961 --> 00:10:06,300
which would compute the,
uh,

157
00:10:06,330 --> 00:10:10,740
this will compute the greatness of
the last with respect to f of x.

158
00:10:11,220 --> 00:10:15,880
And then from that we could reverse
this Arrow and now we'd have a,

159
00:10:15,930 --> 00:10:19,350
we'd be ready to compute
the gradient of the loss.

160
00:10:19,351 --> 00:10:21,030
But now with respect to this,

161
00:10:21,090 --> 00:10:26,090
using the value and the great end that
I've been computed at this box here and

162
00:10:28,801 --> 00:10:30,060
we can continue.
So like this,

163
00:10:30,061 --> 00:10:33,400
we could back propagate the
gradients up to w so the grain,

164
00:10:33,401 --> 00:10:37,050
so w would be computed based on the
value in the gradients computed here.

165
00:10:37,200 --> 00:10:39,000
Same thing for B and we couldn't,

166
00:10:39,280 --> 00:10:42,480
we could continue like
this and the inverse order.

167
00:10:43,420 --> 00:10:43,710
Okay.

168
00:10:43,710 --> 00:10:46,440
The reverse order to get the
greatest respect to everything,

169
00:10:46,480 --> 00:10:49,130
you actually don't need the
grain with respect to x. Uh,

170
00:10:49,380 --> 00:10:53,580
because we're only interested in
updating the values of the parameters.

171
00:10:53,880 --> 00:10:57,330
So for that reason, we don't actually need
to continue in that part of the graph.

172
00:10:57,360 --> 00:11:01,020
You don't need to compute the grading
with respect to x goes for performing

173
00:11:01,021 --> 00:11:04,560
backpropagation and training the
neural network. We don't need it. Okay.

174
00:11:04,561 --> 00:11:08,430
So this is just a, you know, a few
hints on how we could actually,

175
00:11:08,730 --> 00:11:10,110
by using your flow graph,

176
00:11:10,111 --> 00:11:14,390
the compensation of the
forward propagation and uh, uh,

177
00:11:14,400 --> 00:11:18,120
using in this photograph objects to
have a forward propagation and the

178
00:11:18,121 --> 00:11:20,800
backpropagation method.
And that's proper nippy prop.

179
00:11:20,801 --> 00:11:24,750
We could actually get a fairly
flexible implementation, uh,

180
00:11:24,751 --> 00:11:27,000
Ford propagation and backpropagation.

181
00:11:27,450 --> 00:11:30,330
And we actually get from
the Ford Propagation Graph,

182
00:11:30,331 --> 00:11:33,990
we essentially get a procedure for back
propagating the gradients almost for

183
00:11:33,991 --> 00:11:34,824
free.

184
00:11:37,510 --> 00:11:42,510
Now whenever we implement
a backpropagation and the
computation of you really

185
00:11:42,941 --> 00:11:47,250
have any gradient, it's always a
good idea to verify whether, uh,

186
00:11:47,290 --> 00:11:50,260
we haven't introduced any
bugs in our implementation.

187
00:11:51,160 --> 00:11:55,680
And a one approach for that,
which is often news, is, uh,

188
00:11:55,790 --> 00:12:00,460
uh, is to compare with a finance
difference approximation of the gradient.

189
00:12:01,090 --> 00:12:04,340
So the idea is to notice that,
uh,

190
00:12:04,390 --> 00:12:09,130
by definition the partial they live of
a function with respect to x is taking

191
00:12:09,131 --> 00:12:10,090
this expression here.

192
00:12:10,091 --> 00:12:15,091
So F of x plus some epsilon minus F of
x minus some epsilon divided by the size

193
00:12:16,931 --> 00:12:20,370
of this gap between experts
apps and on the x minus epsilon.

194
00:12:20,520 --> 00:12:24,820
So to my times epsilon and
taking that expression and uh,

195
00:12:24,940 --> 00:12:28,880
taking epsilon, uh, um, um,

196
00:12:29,320 --> 00:12:34,000
pushing it towards zero. And, uh, of
course we can't do that on the machine,

197
00:12:34,001 --> 00:12:38,590
but what we could do is to actually
take epsilon and, and fix it to a very,

198
00:12:38,591 --> 00:12:42,550
very small value to sort of get an
approximation of the real thing,

199
00:12:42,551 --> 00:12:46,630
which would be to, uh, push,
uh, uh, epsilon towards zero.

200
00:12:47,980 --> 00:12:51,670
So in our case,
where we could do is in this formula here,

201
00:12:51,671 --> 00:12:55,990
f of x would be our loss x
would be one of our parameters.

202
00:12:55,991 --> 00:12:56,980
So maybe,
you know,

203
00:12:57,460 --> 00:13:01,540
it could be w I a Jay
for some hidden layer k.

204
00:13:02,830 --> 00:13:07,830
And so computing f of x plus epsilon
would correspond to computing the loss

205
00:13:08,170 --> 00:13:10,720
where we've taken the current
value of our parameter.

206
00:13:10,721 --> 00:13:13,420
But we've added some epsilon value,
some very small value,

207
00:13:13,421 --> 00:13:18,421
like say 10 to the minus six and F of
x minus epsilon would correspond to a,

208
00:13:21,701 --> 00:13:22,810
again,
computing the loss,

209
00:13:22,811 --> 00:13:26,770
but where we would have subtracted
epsilon from the current value of our

210
00:13:26,771 --> 00:13:29,380
parameter.
And so by doing this,

211
00:13:29,381 --> 00:13:33,280
we could compute that expression and that
would give us an approximation of what

212
00:13:33,281 --> 00:13:37,330
the partial derivative of the loss with
respect to our parameter a would be.

213
00:13:38,020 --> 00:13:40,960
And, uh, then we to, uh,

214
00:13:41,020 --> 00:13:45,460
so we could compare that value of the
estimate of the partial data for our

215
00:13:45,461 --> 00:13:50,461
parameter with the actual value that
backpropagation as a computer and a,

216
00:13:51,851 --> 00:13:52,661
so this knee,

217
00:13:52,661 --> 00:13:56,890
this means that if we want to test the
implementation of the computation of the

218
00:13:56,891 --> 00:13:59,050
partial derivative with
respect to every parameter,

219
00:13:59,051 --> 00:14:03,650
so all connections between all the
hidden layers and also all biases bik,

220
00:14:04,120 --> 00:14:08,410
it means that we will need to do a
forward propagation with plus epsilon and

221
00:14:08,411 --> 00:14:10,990
minus epsilon for each parameter.

222
00:14:11,470 --> 00:14:14,310
So when we do this computation,

223
00:14:14,320 --> 00:14:17,410
this finite difference
approximation for debugging,

224
00:14:17,411 --> 00:14:21,040
we actually do it on a very small
neural network with very few parameters,

225
00:14:21,050 --> 00:14:22,840
few connections and few biases.

226
00:14:24,130 --> 00:14:28,150
And so once we get all the partial
dude is approximated with this fine I

227
00:14:28,151 --> 00:14:32,530
difference. And when could we
can compare it with the uh, uh,

228
00:14:33,070 --> 00:14:35,140
the, uh, computer gradients.

229
00:14:35,350 --> 00:14:37,780
The computer partial data is
with respect to all parameters.

230
00:14:37,781 --> 00:14:41,560
We could look at the absolute difference
between the average absolute difference.

231
00:14:41,561 --> 00:14:44,650
We could look at the angle between the,
uh,

232
00:14:44,651 --> 00:14:49,180
approximated gradience and the actual
grade we computer with a finite

233
00:14:49,181 --> 00:14:53,300
difference. And if the, if the, if
this difference is a very, very small,

234
00:14:53,301 --> 00:14:55,160
then we, uh, can be failed.

235
00:14:55,161 --> 00:14:59,060
He assured that our implementation
and backpropagation is, is correct.

236
00:14:59,390 --> 00:15:03,590
So that's a very good trick to know about
if we want to debug and implementation

237
00:15:03,591 --> 00:15:04,610
of backpropagation.

