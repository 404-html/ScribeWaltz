1
00:00:00,850 --> 00:00:01,391
In this video,

2
00:00:01,391 --> 00:00:05,320
we'll discuss the concept
of under complete and over
complete hidden layers in

3
00:00:05,321 --> 00:00:10,110
auto encoders, which will help us
identify another weakness of a deal.

4
00:00:10,120 --> 00:00:12,070
Two encoders as we've defined it so far.

5
00:00:14,730 --> 00:00:19,730
So now in this video we'll talk about
what's the impact of the size of the

6
00:00:21,181 --> 00:00:24,780
hidden layer.
So the output of the encoder,

7
00:00:24,840 --> 00:00:26,460
the size of that factor,

8
00:00:26,760 --> 00:00:31,460
what's the impact of choosing different
values for that size when we're training

9
00:00:31,520 --> 00:00:36,390
20 quarter in terms of the types of
features or that type of information that

10
00:00:36,391 --> 00:00:39,600
we'll learn about, uh,
our input distribution,

11
00:00:43,320 --> 00:00:45,060
well distinguish two cases.

12
00:00:45,150 --> 00:00:48,330
The first case is known as
the under complete case,

13
00:00:48,420 --> 00:00:51,900
so hidden layers under complete if
it's smaller than the input layer,

14
00:00:51,901 --> 00:00:56,370
if it contains less elements,
uh, then the input vector.

15
00:00:57,480 --> 00:01:01,500
So in that sense we see that the
hidden layer has to compress the input.

16
00:01:01,560 --> 00:01:04,860
It has more,
it has less bits to a store.

17
00:01:05,100 --> 00:01:10,100
The what we would like to be the
same information as the input layers.

18
00:01:10,751 --> 00:01:14,280
So there'd be fewer bits in the
representation of that vector.

19
00:01:14,310 --> 00:01:16,220
Then the representation of that vector,

20
00:01:16,230 --> 00:01:19,260
just by the fact that this is a
larger vector and then that factor.

21
00:01:20,160 --> 00:01:20,971
And so because of that,

22
00:01:20,971 --> 00:01:25,560
when we're training the autoencoder really
it needs to learn to compress and it

23
00:01:25,561 --> 00:01:29,820
needs to learn to compress or it
will be trained to compress well, uh,

24
00:01:29,880 --> 00:01:33,240
only inputs that are generated
by our training distribution.

25
00:01:33,241 --> 00:01:35,940
So all the inputs that are
present in our training,

26
00:01:36,010 --> 00:01:38,730
a set aren't going to be welcome pressed.

27
00:01:39,090 --> 00:01:42,750
And because a d a hidden layer has,

28
00:01:42,780 --> 00:01:47,400
because it's lossy compression, we have
to remove certain bits, uh, quote unquote,

29
00:01:47,670 --> 00:01:50,400
then it means that there are certain
factors that he won't be able to

30
00:01:50,401 --> 00:01:54,960
reconstruct perfectly.
And so really in the under complete case,

31
00:01:55,260 --> 00:01:58,800
we have the nice thing
that the auto encoder,

32
00:01:58,801 --> 00:02:03,030
we'll learn to learn good features for
the training distribution in the sense

33
00:02:03,031 --> 00:02:08,031
that these are going to be these hidden
units here are going to be a good for

34
00:02:08,760 --> 00:02:12,510
being able to reconstruct inputs that are,
uh,

35
00:02:12,511 --> 00:02:15,480
that look similar to what's
found in the training set.

36
00:02:16,050 --> 00:02:20,010
But it's going to be bad for other
types of inputs, say a random input.

37
00:02:20,920 --> 00:02:25,860
So if we put the random vector here and
our training set does not contain random

38
00:02:25,861 --> 00:02:30,840
vectors, then um, because
this is a new example and, uh,

39
00:02:30,870 --> 00:02:35,000
because our training layer
are hidden layer has, uh, uh,

40
00:02:35,160 --> 00:02:38,940
less capacity than the
original input space,

41
00:02:39,180 --> 00:02:43,740
then it will possibly not be able
to reconstruct perfectly. Okay.

42
00:02:43,741 --> 00:02:45,810
Because it will have to
lose some information.

43
00:02:46,080 --> 00:02:49,740
And presumably that's information that
in this case it's important to have that

44
00:02:49,741 --> 00:02:52,830
information to encode that vector,
but not in this case.

45
00:02:56,680 --> 00:03:01,340
Now what about the other
case is when the encoder,

46
00:03:01,550 --> 00:03:05,290
the hidden layer is a
bigger than the input layer.

47
00:03:05,950 --> 00:03:10,950
So then we say that we're in the over
complete case and now in this case,

48
00:03:11,411 --> 00:03:15,970
then there's actually no compression
that's required by the autumn quarter.

49
00:03:16,300 --> 00:03:18,160
Uh, and in fact, if you think about it,

50
00:03:18,640 --> 00:03:23,640
each had an unit could just copy an
individual element of the input vector.

51
00:03:24,840 --> 00:03:29,450
Uh, so, you know, this could go here,
this here, this here, this here, uh,

52
00:03:29,460 --> 00:03:31,570
and so on like this.
Uh,

53
00:03:31,571 --> 00:03:35,920
and then there even be some hidden units
that we'd be left and would not need to

54
00:03:35,921 --> 00:03:40,921
be a use because by just copying each
of these elements here into the hidden

55
00:03:41,651 --> 00:03:42,400
layer,

56
00:03:42,400 --> 00:03:46,240
then we could just put back
the information into the
reconstruction and get a

57
00:03:46,241 --> 00:03:49,360
perfect reconstruction.
So you think a little bit about,

58
00:03:49,690 --> 00:03:52,870
you can try to think a little bit
about how you could parent tries no 20

59
00:03:52,871 --> 00:03:53,200
quarters.

60
00:03:53,200 --> 00:03:58,200
So it's that it does this in
the over a complete case and uh,

61
00:03:59,200 --> 00:04:04,030
and then if we do this, then we think
about the value of that representation.

62
00:04:04,031 --> 00:04:06,370
Well, really it's, it's, it has no value.

63
00:04:06,371 --> 00:04:08,920
It doesn't extract anything
that's meaningful. Uh,

64
00:04:08,950 --> 00:04:12,430
it's just copied the
information element by element.

65
00:04:12,431 --> 00:04:17,380
So I'm really feeling that as the input
say to some other classifier is going to

66
00:04:17,381 --> 00:04:21,090
be equivalent to feeling
just the original input. Uh,

67
00:04:21,160 --> 00:04:25,990
and yet if say we're interested in
extracting features for linear classifier,

68
00:04:26,080 --> 00:04:30,940
we actually want to have a lot of
features Alania class fire as a very small

69
00:04:30,941 --> 00:04:34,210
capacity.
And the more features we give it,

70
00:04:34,240 --> 00:04:38,020
the more opportunity we give it to be
able to distinguish between different

71
00:04:38,021 --> 00:04:42,730
classes. So that's actually in a sense
of bug in the irregular or twin quarter.

72
00:04:43,150 --> 00:04:48,150
And also this partly explains why we
weren't not learning some particularly

73
00:04:50,591 --> 00:04:54,880
meaningful features when
we saw the example or we're
training an autoencoder or

74
00:04:54,881 --> 00:04:59,830
numbness a, it could just do something
very similar to just copying the,

75
00:05:00,010 --> 00:05:01,100
uh,
um,

76
00:05:01,360 --> 00:05:05,170
copying the pixels into individual,

77
00:05:05,171 --> 00:05:10,070
separate a hidden units, and then
reconstruct them back into the, uh,

78
00:05:10,130 --> 00:05:13,990
the reconstruction output. And so
that's another problem with, you know,

79
00:05:13,991 --> 00:05:17,530
tone quarter where in the
over complete case, uh,

80
00:05:18,040 --> 00:05:21,520
it's actually not encouraged
to learn anything meaningful.
So in the next videos,

81
00:05:21,521 --> 00:05:26,521
we'll see two ways of addressing that
problem and such a way that we actually be

82
00:05:26,651 --> 00:05:30,100
able to extract some
meaningful features for, uh,

83
00:05:30,160 --> 00:05:34,120
be getting good results in terms, for
instance, of our classification task.

