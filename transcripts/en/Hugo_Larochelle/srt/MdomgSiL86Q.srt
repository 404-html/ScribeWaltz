1
00:00:00,640 --> 00:00:01,181
In this video,

2
00:00:01,181 --> 00:00:05,650
we'll discuss a very
interesting relationship between
sparse coating models and

3
00:00:05,651 --> 00:00:06,790
a v one neurons.

4
00:00:08,860 --> 00:00:09,693
Yeah.

5
00:00:09,950 --> 00:00:13,950
So v one is a region in the brain
that is part of the visual cortex.

6
00:00:14,370 --> 00:00:18,390
The visual Cortex is responsible for,
uh, visual processing in the brain.

7
00:00:19,170 --> 00:00:22,440
And, uh, what's special
about the one is that it's,

8
00:00:22,500 --> 00:00:27,390
it is fairly well known and
well understood compared
to other regions in the

9
00:00:27,391 --> 00:00:28,140
brain.

10
00:00:28,140 --> 00:00:32,730
And in particular their neurons that are
there are a well known for being able

11
00:00:32,731 --> 00:00:37,731
to detect a very simple
specific edges in images.

12
00:00:37,951 --> 00:00:42,951
So that is very a simple contrasts
in a elimination in images.

13
00:00:44,340 --> 00:00:48,330
And each neuron is really a
responsible for detecting, uh,

14
00:00:48,360 --> 00:00:53,190
detecting these edges at different
positions and different angles in,

15
00:00:53,220 --> 00:00:55,170
uh,
and someone's field of view.

16
00:00:56,190 --> 00:01:00,990
And so a very famous experiment was
to see what, how this compared with,

17
00:01:01,050 --> 00:01:03,270
uh, a, as far as coding models does.

18
00:01:03,271 --> 00:01:07,380
So does as Parsco Inc model
learn features that are, uh,

19
00:01:07,560 --> 00:01:12,560
similar to the types of features that v
one neurons might be extracting in the

20
00:01:13,111 --> 00:01:13,944
visual field.

21
00:01:14,850 --> 00:01:19,850
So they experiment that was done is
to take a image of a natural scenes,

22
00:01:20,281 --> 00:01:23,800
like a big image like this
with some trees and uh, uh,

23
00:01:23,820 --> 00:01:27,000
a mountain behind and so on. And then, uh,

24
00:01:27,030 --> 00:01:30,310
because this is a very large image
and it'll be compassionately, uh,

25
00:01:30,320 --> 00:01:33,360
expensive to actually train as part
getting model on this whole image,

26
00:01:33,520 --> 00:01:38,280
we instead take certain regions in
it and extract patches from it. So,

27
00:01:38,281 --> 00:01:41,100
uh, so for instance,
from a, you know, these,

28
00:01:41,101 --> 00:01:45,120
we'd get patches that look kind of like
that part of a tree and then we have

29
00:01:45,121 --> 00:01:47,460
some background behind it. Uh, so this,

30
00:01:47,461 --> 00:01:52,310
these patches would look at this part
of the tree here and I should say I'm

31
00:01:52,320 --> 00:01:56,340
giving the reference for the image,
but this is a, uh, did, did, uh, uh,

32
00:01:56,341 --> 00:01:59,730
an experiment based on that.
But the actual experiments,

33
00:01:59,970 --> 00:02:03,480
the first experiment on this is
a, is not from, from this paper.

34
00:02:03,890 --> 00:02:07,080
This is actually an extension of sparse
coding, which is actually worth reading.

35
00:02:08,820 --> 00:02:09,300
Okay.

36
00:02:09,300 --> 00:02:13,700
The actual reference for this experiment
is this paper here by all cells and

37
00:02:13,740 --> 00:02:18,450
fields is a very famous paper. And so
they took these natural image patches.

38
00:02:18,451 --> 00:02:22,170
So we say patches because it's very
small portions of a much bigger image.

39
00:02:23,250 --> 00:02:26,640
And they, uh, train a
sparse coating model,

40
00:02:26,760 --> 00:02:31,530
a variant of this possibility mandala
was described in the previous videos, uh,

41
00:02:31,531 --> 00:02:35,880
on these patches. And then they looked
at the features that were extracted.

42
00:02:35,910 --> 00:02:39,270
So here we have a visualization
of these features.

43
00:02:39,480 --> 00:02:41,520
So for each neuron,

44
00:02:41,550 --> 00:02:44,730
each element in my
sparse codenamed Vector,

45
00:02:45,060 --> 00:02:49,080
a I day essentially looked at at,
so at the Associated Adam,

46
00:02:49,081 --> 00:02:53,670
also the dictionary column and
visualized it. And when we look at it,

47
00:02:54,030 --> 00:02:57,300
I'll,
each neuron actually seems like a look,

48
00:02:57,301 --> 00:03:02,240
like it's actually extracting a
Ajay formation. So for instance, uh,

49
00:03:02,310 --> 00:03:06,520
this, uh, let's say this
neuron here seems sensitive.

50
00:03:06,521 --> 00:03:11,020
If we have a sharp contrast change where
we go from white and then black and

51
00:03:11,021 --> 00:03:15,820
then white. So that's, that's an edge. Uh,
we have another one here, which looks at,

52
00:03:16,090 --> 00:03:20,070
again, an edge, but that is with this
angle as opposed to this angle, it's,

53
00:03:20,140 --> 00:03:24,850
that corresponds to a slightly
different position in the patch. And,

54
00:03:24,851 --> 00:03:25,181
uh,

55
00:03:25,181 --> 00:03:29,830
we see there's a big variety
in the position of the
edge that's being detected,

56
00:03:30,130 --> 00:03:30,510
a,

57
00:03:30,510 --> 00:03:35,290
the orientation as well as
well as the scale or um,

58
00:03:35,950 --> 00:03:40,200
the uh, and signal processing
term, their frequency of the, uh,

59
00:03:40,270 --> 00:03:42,280
of the edge and a,

60
00:03:42,281 --> 00:03:45,920
so actually each Adam will say
is tuned to particular, uh,

61
00:03:46,000 --> 00:03:49,180
position orientation
and spatial frequency.

62
00:03:49,630 --> 00:03:53,630
And we see something very similar,
similar in v one neurons where, uh,

63
00:03:53,680 --> 00:03:58,680
we actually have neuron cell are tune
again to the detection of particular edges

64
00:03:59,261 --> 00:04:01,480
but with different position orientation,

65
00:04:01,481 --> 00:04:03,370
then spatial frequency
in our field of view.

66
00:04:03,970 --> 00:04:08,440
So this was a very surprising
discovery by Alsace and, and feel,

67
00:04:08,590 --> 00:04:12,190
which, uh, really motivated
the exploration of,

68
00:04:12,250 --> 00:04:14,710
of sparsity in the neural networks.

69
00:04:17,170 --> 00:04:21,050
So it really suggests that the brain
might be doing something similar.

70
00:04:21,051 --> 00:04:23,680
It might be trying to learn a sparse,

71
00:04:23,770 --> 00:04:28,000
a representation of our visual stimuli,
uh,

72
00:04:28,001 --> 00:04:32,560
such that it maintains as
much information, but that
it is sparse in sparsity,

73
00:04:32,561 --> 00:04:36,550
might just be related to the
fact that, uh, neurons, uh,

74
00:04:36,850 --> 00:04:41,640
waste energy when they are firing.
So they don't want to have, uh, uh,

75
00:04:41,720 --> 00:04:45,130
they want the fire all the
time. So that would be the, uh,

76
00:04:45,131 --> 00:04:47,140
so this sort of energy consumption,

77
00:04:47,460 --> 00:04:51,430
a constraint would be related to the
sparsity constraint and as far as coding.

78
00:04:52,180 --> 00:04:57,180
And so this has motivated a
lot of researchers to look
into many more variants

79
00:04:57,521 --> 00:05:00,640
of, of models with, uh, uh,

80
00:05:00,641 --> 00:05:05,350
which look at different types of sparsity
and actually a lot of other variants

81
00:05:05,410 --> 00:05:08,680
of the sparks scoring models,
which are not technically sparse coding,

82
00:05:08,681 --> 00:05:13,330
but in core incorporate the notion of
sparsity as been shown to also extract

83
00:05:13,331 --> 00:05:18,080
these types of, uh, uh, sparse
features. So edge detectors, uh,

84
00:05:18,100 --> 00:05:22,570
at different positions and uh,
and uh, frequency and so on.

85
00:05:23,470 --> 00:05:28,240
And so there's a huge body of literature
on different types of models on

86
00:05:28,241 --> 00:05:33,241
supervised more leg track sparsity
that extract features and are based on

87
00:05:33,730 --> 00:05:35,870
sparsity a notion of sparsity.

88
00:05:35,920 --> 00:05:40,920
And I encourage you all to go look at
this literature and also look at a website

89
00:05:41,231 --> 00:05:43,960
of, for this course, for
references for such models.

