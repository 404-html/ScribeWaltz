Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video, we'll describe the operation of a discrete convolution, which will be used for computing before propagation in the conditional neural network.

Speaker 1:          00:11          Okay.

Speaker 2:          00:12          So I mentioned before that, uh, when we computing the reactivation of a hidden unit, what would be doing is that, uh, each in the end would be segmented into different feature maps. And then for each feature map I mentioned that the last video that will actually perform a discrete come pollution operation. Uh, this, this week convolution operation would actually give me the pre activation that comes from each of the IAF, uh, channel. So performing this with correspond to a, for each hidden units, multiplying it's weights with the it's receptive field, and then summing the accumulated p activation and then with some, also over our possible input channels. So that would give us the P activations of all the hidden units in the same feature map, and then we could apply some nonlinearity to get an activation. And so now in this video, we'll look at these specific definition of this convolution operation and how it relates to a computing reactivations in neuro and the feed forward neural network.

Speaker 1:          01:17          Okay.

Speaker 2:          01:18          All right. So let x be some input image. So it's going to be a matrix and a k is going to be a colonel and it's also going to be a matrix. So now this year I'm not using my usual dictation of using capital letters and bold to uh, uh, refered to matrices. That's just because I'm trying to stick with the notation of Jared at all in their paper. I just want to sort of look back at the original paper and be able to, uh, relate the notation I'm using with, there's a, but normally I would use capital letters, so just don't forget that this is a matrix and that's a matrix. So if you performing the district convolution of that image x with some colonel came, uh, we will perform the following computation. So the convolution of x with k that's going to yield a matrix as well and image and at position I j so to the AIA throw and the GF column in that Matrix will have the following value.

Speaker 2:          02:18          So what we'll do is that will sum over all offset possibles possible where the opposite p and Q range between zero and r minus one, where r is the number of rows and the number of columns of gate. So we're assuming the kernel is a square matrix. So for all possible offsets all combinations of these numbers here we'll look at the Pixel at position I and J, but offset it by p and Q. And we multiply that by the element in the kernel matrix at upset p and Q. But where now the offset is taken with respect to the bottom right corner of the matrix. So if I have a matrix for my criminal matrix, which is a three by three and say hi, have a p equal one and Q equals zero, then it means that the elements, uh, K R minus P and r minus Q, where are always three p's and q's, zero with Curtis, fun to, uh, essentially counting, uh, and uh, relatively to the bottom, uh, right element of that Matrix.

Speaker 2:          03:32          So I would be at row one. That's uh, sorry, it's the second row. So that's uh, r which is three minus one, which is one. So that's two. And then I would be at our three minus Q zero. So at the third column, so this would be that element. So upset one zero instead of being, uh, this element here where counting if we're counting relatively to the top left corner like we would normally do when we accessing elements in the colonel were counting a relatively to the bottom left corner of the matrix. In this particular example, we would eat this. So in this, some were doing over all possible offsets where we go from zero to a two, four articles, three and two would be zero one or two. And we take all combinations. And so in each case, when multiplied the element that I've said P and Q at position I and j and d image and multiply it by the corresponding element in the kernel matrix.

Speaker 2:          04:37          So that's for the mathematical definition. Now let's look at visually what this computation corresponds to and it's going to be much more intuitive once we do that. So we can think of this operation as first taking the Colonel Matrix. Kay. So here's an example of a kernel matrix, two by two Colonel Matrix. And then we will do is that we'll flip the rows and the columns. Uh, what would this we'll do is we'll make everything more intuitive where we won't have to uh, uh, compute the offset with respect to the bottom right corner, but instead will completely offset the prospective. That top left corner of that flipped matrix. So this matrix, so I'm going to use still the four, like the operation of flipping the rows and columns. It would be, it would correspond to that Matrix here. So we see we've switch the rows and we've switched the columns with respect to that matrix here. Okay. So once we have this Kate tilled, what we do is that for each position where we can fit that kernel, we do an element wise multiplication of all elements with the, uh, part of the image that is at the current, uh, at any given position. So in this example, we're starting at the top left corner. So I Jay, this would be dead for the elements. I equal one j equals one. So we do see this, this element wise multiplication, and then we suck. So we have one times zero plus 0.5 times 80

Speaker 2:          06:10          plus two point 25 times time, times 20 and plus zero times 40. She's here. So that gives us 45 and then we continue this but for all values of ing. So for our positions where we can position the top left, uh, element of the kernel matrix in the image. So then we perform the same operation, element wise, multiplication, and then some summation of about multiplication here, here, and here. So all of these terms and then summation of all these four values. In this case, this gives 110 and we continue like this for all positions. And that gives us the convolution of k with this three by three image here that I made up for this example.

Speaker 1:          07:07          Okay.

Speaker 2:          07:07          And so notice that here I'm using a, uh, some sort of great color coding, uh, uh, illustration, uh, to sort of illustrate the intensity of the, uh, resulting image. So we can sort of think of it that is visualize eating it, that's uh, uh, as an actual image.

Speaker 2:          07:29          So we know this, that this operation, this convolution operations where we're taking a kernel and we're placing it in all positions in the image. And then we're doing these elements wise, uh, uh, multiplications with a sum that's very similar to the types of operations we have to do. When will computing reactivation and a feet forward conclusion, all neural network and a in turns out that indeed we can convert a, what I described as the way we compute the activations using this, uh, with a convolutional neural network kind of connectivity with the layers, uh, using a convolutional, uh, operation. So, uh, let's, uh, so actually the pre activations that are going to be computed from channel XII in order to compute the feature map. Why Jay? Where, uh, why'd you would be after the nonlinear already here? Uh, I could compute that part of the p activation by just taking a, deriving a convolution kernel, which is just going to be my connection Matrix Wij Mike and boosting all neural network and then flipping it's columns and rows.

Speaker 2:          08:39          Okay. And that will become my conversion or kernel so that when I'm applying the discrete convolution, then, uh, as we've seen in the description I gave before, I'm going to be re flipping the rows and columns. So I'll, I'll, I'll get back to the actual matrix w so from that colonel then to get my p activations that are computed from the IAFF a channel, I just perform a convolution between the IIF channels. So this slides in this image, if I have multiple channels with the kernel k I checked and then to compute all of why, then I would sum the result of these convolutions across the input channels. And then I'd apply some, none of them the already in it. I would give me the activations of that particular feature map and a to get all the feature maps that I would do this for all values of Jay's, for all feature maps. I have in my neuro network.

Speaker 1:          09:36          Okay.

Speaker 2:          09:36          All right. So, um, it might seem complicated that we would, uh, you know, after this operation where have to flip columns, like I said, uh, cons and rose, like I said, we're, uh, one thing I mentioned this connection because that's where the name convolutional neural network comes from. And also because you might have access to a library that computes convolutions very efficiently, in which case you want to do this transformation. There's another type of signal processing operation known as the correlation, which actually corresponds to doing the same essentially what I've described before, but without flipping rows in columns. So it would be exactly that, the quick correlation between x and my matrix of connections. And um, uh, so there's a relationship here again, between the discrete correlation and uh, the discrete convolution, which I won't get more into besides just saying that it's essentially the same thing without the row and column flipping before we do the element wise multiplications with [inaudible] at every position where we placed a colonel. But so from this, it means that now we have, uh, we have some efficient code that do, uh, uh, convolutions. Then we can compute all of my pre activations and compute all my feature maps, uh, fairly efficiently.

Speaker 1:          10:52          Okay.

Speaker 2:          10:53          So, uh, to get an idea of the kind of, uh, the, the result of applying, uh, these discrete convolutions or computing reactivations between, uh, some inputs, uh, channel for some GF, uh, uh, feature maps. So imagine that I have a matrix Wij which corresponds to this matrix here where have zeros on the diagonal and 0.5 on the, uh, other elements. So in this case, it simplifies everything because flipping rows in columns is exactly equivalent. It gives the same matrix. But if I were to do this discrete convolution on this image, then I would get this contribution to the p activations of that particular feature map. And we see that we have a fairly high value, uh, at places that curse fund to regions of the original image where I had a pattern that is similar. So like this with a kind of a diagonal edge.

Speaker 2:          11:48          So we have two 55 that corresponds to this super position here up the uh, up the filter or the kernel and two 50 dive again, uh, where I had this value here. And so, uh, uh, I see that once I've computed this, I already have some uh, illustration of where do this particular kind of feature with an edge, uh, with that angle is present in the image. And then the idea is that with a nonlinearity that we have in commercial neural network that we might be able to emphasize, uh, uh, whenever this a value of correspondence between the filter and a particular region of the image. So emphasize when it reaches a certain threshold. So in particular, if I were to apply this element wise, none in the red on the feature map, uh, pre activation, assuming we have only one input channel. Uh, so Francis by multiplying 0.02, the result of the conclusion and subtracting four in passing this through a sigmoid dad get fairly close to one values and the part of the image that has this particular kind of pattern and everywhere else, something closer to zero. And that's the kind of a feature detection that a convolutional a hidden layer is going to do. Uh, it's going to be able to detect certain type of features. And, and the, uh, in a feature map, again, we will have the same feature of being detected everywhere in the image.

Speaker 1:          13:20          Yeah.

Speaker 2:          13:21          Final notion related to convolution is the idea of a zero panning. Uh, it's going to be useful in particular when we talk about computing the gradients, when we're going to be deriving the procedure for a computer ingredients with respect to some last ones, the classification, a lost that we've been using in regular neural networks. So, uh, so essentially, uh, a procedure for back propagating gradients across the conclusion on their own network. Zero panning is when we allow to go over the borders by essentially panning Zeros everywhere around the image enough so that we can fit our filter, uh, everywhere as long as there's at least one element from the filter that, uh, or the kernel that overlaps with the original image. So in this case, uh, by adding, if I add zeros here, it's a one row of zero and I have a filter which is the same as before, which is just two by two.

Speaker 2:          14:19          And this way I'm able to fit the filter, uh, in the position where it overlaps with just one element here. The other types of padding, we can do a kind of reflective padding where, uh, we would act as if there were like a mirror here. And then, uh, uh, so if the value here would be two 55, and the value if we're to a pad here would be zero because it's a zero here. So I sorta symmetric, uh, kinds of padding a zero padding is probably the one that's most used in convolutional neural networks. And, and that's the one I'm going to be referring to whenever I'm adding this. Underline a notation here with a convolution operation. That's the one that we're going to involve in particular when we talk about great guns. So if we do the convolution with the same granolas before, but with zero panning that we get this, uh, uh, convolutional output in stem. And we know this, that because we were able to fit the colonel in more positions, now we get a larger, uh, larger, uh, output, the larger matrix after the contribution. I guess. So that's it for this convolutions.