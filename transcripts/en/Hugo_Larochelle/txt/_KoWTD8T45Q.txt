Speaker 1:          00:01          In this video, we'll put everything together we seen before in, in expressing the different gradients in a neural network and, uh, obtain, uh, one efficient procedure for computing the gradients with respect to our parameter. And that procedure is called backpropagation. So now we've seen in a previous video and expression for getting the grain and with respect to all the parameters in the neural network, that expression, depending on the gradient of the hidden layers, uh, which d also depending on the gradient, uh, with respect to the output. And so now we'll see how can put together all of these different expressions to obtain a procedure that will efficiently compute the parameter gradients.

Speaker 2:          00:47          Yeah.

Speaker 1:          00:48          All right. So, um, we've seen in the first video how, uh, the gradient of the loss function with respect to the pre activation at the output layer correspondent to this expression. And then, uh, we also saw that the gradient with respect to the activation in the hidden layer, a curse upon the two, that expression and a then we saw that the gradient of the loss function with respect to the pre activation of a hidden layer responded to that expression. If finally we saw that the gradient of the last function with respect to either the a matrix of connections or devices corresponded to these two expressions here and now we know this, that in each case except for this one, uh, s uh, in the expression on the left, it would often depend on the gradients of uh, other parts of the neural networks. So the gradient of the weights and the biases depends on the gradient with respect to the pre activations, uh, the gradients of the hidden unit activations dependent also on the green and the, the pre activations.

Speaker 1:          01:59          While the grain of pre activations, uh, actually dependent on the gradient of the activation of a hidden layer and at the output layer. Also we had the formula for getting the gradient of the pre activations at the output layer. So here the procedure we see, which is the backpropagation algorithm, the idea is to apply these formulas in order. So that on the right side here, we always have a precomputed everything to actually compute that expression. So let's go into the steps of the backpropagation algorithm. So first we assume that we've already done a forward propagation. That is, we've computed, given the input, the value of all the hidden units, uh, all the hidden layers and the output layer. So we've computer already f of x, and that's because before we computed each of the, uh, activation, uh, of each hidden layer h of x. And now we'll, we'll do is that we initialize the pre activation gradient, uh, at the output layer that's layer l plus one, and we'll uh, we'll initialize it or we'll set it to the value of what that gradient is.

Speaker 1:          03:14          That is minus the 100 factor, uh, for labeled y minus the output layer. And then we'll iterate from layer, uh, l plus one to layer one, and we'll perform these updates, these computations here. So first we'll actually compute the gradient for the current layer. So I'll call Kay the current hidden layer at which we are during our iteration here. So we compute the gradients of the hidden layer parameters, the lid, the parameters for a hidden layer cake or hidden or you know, cake because it can be, I'll plus one that could be, this could be the parameters for the output layer as well depending on the value of k. So we can compute this because we've already precomputed the uh, reactivation gradient. So we have the value for this. So now we just take the outer product of that vector with that vector and that gives us the gradient for the uh, weights and then the gradient for the biases as we seen as just a gradient with respect to their p activations.

Speaker 1:          04:23          And then will compute our, propagate the gradient to the hidden layer. Below that is will compute the grain of the loss with respect to the activation for the layer right below the layer came on a swan. And we've seen that this just corresponds to taking the pre activation gradient and multiplying that by the transpose of the connections between the current hidden layer k and the inner layer below came on his phone and then will compute the gradients. But before the activation. So the pre activation gradient, which we see here, and we've seen that occur, spawns to taking the gradient of the activations. And I'm doing an element wise multiplication with the activation function, the narratives at the value of the activation function. And so once we, uh, reach that part of this, uh, iteration in the loop, uh, then we now have the pre activation gradients for came on this one.

Speaker 1:          05:25          So now we're ready to go from a given value of k two a one a two. The value of k, uh, that's, uh, well, one less. So we, for instance, if we started at a value of k, which is Alpha Swan, we'd be ready to go at a Caples l and then kcals l minus one, and so on until we reached the last hidden layer. So a, this is the backpropagation algorithm and we see that actually, uh, uh, does the sort of the opposite, the forward propagation, instead of going from the input layer to the output layer, it goes from the output layer and then, uh, it goes through the hidden there in the reverse order from the top most in layer to the bottom line.

Speaker 1:          06:13          There's a visual illustration of the computations we have to do when we do fall forward and, uh, backpropagation and this is also going to give us some hints on how we could implement for them backward propagation. So we can write for propagation as an acid clip flow graph where we have boxes. So we see the boxes here with the, the different variables that we have to compute when computing, uh, all the hidden layers and upload layer and then the last function. And so initially we have to set our input layer to some value x. Uh, well, so assume that we know what are the current parameters of, so these are the boxes that contain the priorities of the current value for the parameters. And then what we do is that we compute the reactivation for the first in the layer, we compute that from the input and the biases and the weights of that hidden layer.

Speaker 1:          07:07          Then from the pre activation and we compute the activation of the hidden layer. And then from that and the parameters for the second hidden layer, we can compute the pre activation for the second hidden layer. And for uh, two hidden layer neural network. We could then compute the output and then from the output we could compute a loss associated with this output. Given that the true label is why, okay, so that actually gives us a nice way of potentially implementing for propagation. So each buck could be an object and would have a forward propagation or f prop method that would take uh, the parents of the box to the parents of that object and a compute the value of the box. So for this box here, this would be a box that would compute a linear combination of this, this and that. So the reactivation, this box would be an type of object that computes a particular activation function from the pre activation and so on.

Speaker 2:          08:05          Okay.

Speaker 1:          08:06          So then performing for propagation would just be crisp onto calling the F prob method for each box, for each object in the correct order from the parents, which have no children. And then computing it for, uh, the other, uh, boxes for which we have the value for all of their parents and proceeding forward until we reached the last function.

Speaker 1:          08:31          And then if for this, these, all of these objects, we also had a beep prop method, then we could compute, uh, backpropagation. In fact, we'd get the sort of for free, we get a procedure for computing all the gradients, uh, of all of these boxes with respect to the last function. And this is called actually, uh, automatic differentiation. Um, so what it's a, in this setting, what the be prop method would do is that it would compute the gradient of the loss with respect to its children. So, uh, and uh, so we have a beep prop method that would depend on the boxes parents. So, uh, the f prop depends on its children. It performs a computation with respect to its children, whereas be prop with performance computation, but with respect to the boxes parents and indeed if we go and look back at the formula for computing the great influence suspect to the reactivation or the, uh, uh, the, sorry, the activation or the parameters, the, uh, the formula always depends, uh, uh, on the parents of these boxes in this illustration here.

Speaker 1:          09:46          So in fact, doing backpropagation would correspond to Coleen be prop, but in the reverse order of the graph. So, uh, in fact, we'd start with this box and then it would refers to the Arrow. And then at this point we'd be ready to call be prop for this box here, which would compute the, uh, this will compute the greatness of the last with respect to f of x. And then from that we could reverse this Arrow and now we'd have a, we'd be ready to compute the gradient of the loss. But now with respect to this, using the value and the great end that I've been computed at this box here and we can continue. So like this, we could back propagate the gradients up to w so the grain, so w would be computed based on the value in the gradients computed here. Same thing for B and we couldn't, we could continue like this and the inverse order.

Speaker 2:          10:43          Okay.

Speaker 1:          10:43          The reverse order to get the greatest respect to everything, you actually don't need the grain with respect to x. Uh, because we're only interested in updating the values of the parameters. So for that reason, we don't actually need to continue in that part of the graph. You don't need to compute the grading with respect to x goes for performing backpropagation and training the neural network. We don't need it. Okay. So this is just a, you know, a few hints on how we could actually, by using your flow graph, the compensation of the forward propagation and uh, uh, using in this photograph objects to have a forward propagation and the backpropagation method. And that's proper nippy prop. We could actually get a fairly flexible implementation, uh, Ford propagation and backpropagation. And we actually get from the Ford Propagation Graph, we essentially get a procedure for back propagating the gradients almost for free.

Speaker 1:          11:37          Now whenever we implement a backpropagation and the computation of you really have any gradient, it's always a good idea to verify whether, uh, we haven't introduced any bugs in our implementation. And a one approach for that, which is often news, is, uh, uh, is to compare with a finance difference approximation of the gradient. So the idea is to notice that, uh, by definition the partial they live of a function with respect to x is taking this expression here. So F of x plus some epsilon minus F of x minus some epsilon divided by the size of this gap between experts apps and on the x minus epsilon. So to my times epsilon and taking that expression and uh, taking epsilon, uh, um, um, pushing it towards zero. And, uh, of course we can't do that on the machine, but what we could do is to actually take epsilon and, and fix it to a very, very small value to sort of get an approximation of the real thing, which would be to, uh, push, uh, uh, epsilon towards zero.

Speaker 1:          12:47          So in our case, where we could do is in this formula here, f of x would be our loss x would be one of our parameters. So maybe, you know, it could be w I a Jay for some hidden layer k. And so computing f of x plus epsilon would correspond to computing the loss where we've taken the current value of our parameter. But we've added some epsilon value, some very small value, like say 10 to the minus six and F of x minus epsilon would correspond to a, again, computing the loss, but where we would have subtracted epsilon from the current value of our parameter. And so by doing this, we could compute that expression and that would give us an approximation of what the partial derivative of the loss with respect to our parameter a would be. And, uh, then we to, uh, so we could compare that value of the estimate of the partial data for our parameter with the actual value that backpropagation as a computer and a, so this knee, this means that if we want to test the implementation of the computation of the partial derivative with respect to every parameter, so all connections between all the hidden layers and also all biases bik, it means that we will need to do a forward propagation with plus epsilon and minus epsilon for each parameter.

Speaker 1:          14:11          So when we do this computation, this finite difference approximation for debugging, we actually do it on a very small neural network with very few parameters, few connections and few biases. And so once we get all the partial dude is approximated with this fine I difference. And when could we can compare it with the uh, uh, the, uh, computer gradients. The computer partial data is with respect to all parameters. We could look at the absolute difference between the average absolute difference. We could look at the angle between the, uh, approximated gradience and the actual grade we computer with a finite difference. And if the, if the, if this difference is a very, very small, then we, uh, can be failed. He assured that our implementation and backpropagation is, is correct. So that's a very good trick to know about if we want to debug and implementation of backpropagation.