Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video we'll see different potential choices for the activation function of the neuron. So we've seen previously the computation that is made in the neuron, and we started to involve this Gi function, which we call the activation function. So here we'll just see different popular choices for activation functions in the neural networks

Speaker 2:          00:24          at first choice is to simply use a linear activation function. Uh, so in this case, the activation function g takes it's pre activation and simply outputs it. So if we were to plot the this activation function, you'd get a straight line like this. Uh, so it doesn't perform any squashing of the input that is, uh, it takes the input and reproduces it. So, uh, it won't be upper bounded or lower bounded. And partly for that reason, it's not a very interesting, it doesn't introduce any nonlinearities in the computation of the neuron, which might be a useful for performing more complicated computations

Speaker 2:          01:07          and more interesting choices. The sigmoid activation function. Um, so it takes this form a, which, so we'll sometime note it as a, with the acronym, a, s, I, g, m and a for short. And it's simply takes deep reactivation and it computes this formula here. So it's one over one plus the exponential of minus the pre activation. And so if we draw this function, what we see is that it will squash the neurons a pre activation between zero and one. We see that all output values are between zero and one here. And so the bigger the p activation, the more it will saturate towards one and the smallest activation towards minus infinite, the more it will saturate towards zero instead. So this neuron is always positive, uh, because it's always strictly greater than zero. It's a bounded, that is the, uh, uh, uh, output of the neuron. The activation of the neuron cannot be smaller end zero nor greater than one. And uh, this activation function is a strictly increasing. That is the bigger the activation, the higher the activation of the neuron will be.

Speaker 2:          02:26          And other populations choices. The hyperbolic tangent a or tench. So it's a t, a n, h for short, and it's a slightly more complicated. It also involves some exponential. So one form is this form here, why it's the exponential reactivation minus the exponential of minus EPA activation divided by the sum of the exponential of the reactivation, plus the exponential of minus the pre activation. And a, it can also be written in this form, which is more convenient because it computes just one exponential. So a which is we used here, so it might be more computationally efficient. And uh, this is just obtained by multiplying by the exponential of the pre activation, the numerator and the denominator of this formula here. Uh, so if you plot this function, we see that now we get an output which is strained between minus one and one. So this neuron will say that it squashes the neurons, uh, pre activation. So this should be pre activation here between minus one and one, so it can be positive or negative. Uh, it's also bounded similarly like the uh, sigmoid activation function and it has also strictly increasing.

Speaker 2:          03:49          And a final popular choice for the activation function is what is called the rectified linear activation function. So we know that Recklin for short and it's simply the maximum between zero and the pre activation. And so if we plotted, we get a straight line here, like a linear function. If the input is positive is greater than zero, and otherwise it's just zero. So it's a straight line, a horizontal line, uh, and if the input is negative, it always outputs zero. So here it's only valid by below, uh, below by zero. So it's always non negative. Uh, it's uh, not upper bounded because the greater the p activation here, the greater the output will be. So this can grow, uh, uh, this always grows again, uh, converge to the output, can converse to, uh, or diverged towards infinite. If the input goes towards the, uh, towards, uh, the infinity and, um, and in practice we see that it tends to give neurons that are sparse, that have sparse activity activities.

Speaker 2:          05:00          What we mean by that is that it tends to get neurons that are often exactly zero, which was not the case for the sigmoid or the tach. The similar, the attention need to have a pre activations that are, uh, exactly a particular value. So in the sigmoid case, to get zero, you need, it needs to be minus infinite. So it doesn't happen really. And for the times to be equal to zero in the pre activation needs to be exactly zero. In this case, there's this whole rage of [inaudible], which are the output exactly zero. So for, for this reason, we often get, uh, uh, a zeros as the activation of the neuron across many different inputs. And so we'll say that for that reason, this neuron will have a sparse activations or sparse activities. And so those are the four different choices, public choices for the activation function of a neuron.