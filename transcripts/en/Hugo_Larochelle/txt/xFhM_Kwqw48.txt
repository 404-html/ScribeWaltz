Speaker 1:          00:00          Yeah.

Speaker 2:          00:00          And this video will derive the gradients of the loss function with respect to the, uh, different hidden layers in the neural network. So now we are again pushing forward with getting an expression for the private or gradients, uh, so that we can put those in into, uh, the description of this took gastic great in the center with them, applied for neural networks.

Speaker 1:          00:25          Okay.

Speaker 2:          00:27          All right. So, uh, we looked at the gradients, which is sector, the outputs of the neural network. And then we looked at the pre activation and a, at the output layer. And so we could continue like this, deriving it for any other, a hint in there. But if were to do this every time by hand, like we've done so far, this might get a little bit complicated. So can we sort of take a step back and, uh, try to see how, instead of looking at individual neurons and then computing, what's the, uh, what's the actual, um, what's the actual, a great intro, partial derivative, and then putting those back into, uh, gradients. Can we get a more general formulation for the other near a hidden layers? Uh, the other layers in the neural network.

Speaker 2:          01:18          So what we can do is actually use the chain rule to get a more genero formulation for the greatest respect to any hidden layer. And then from that we'll be able to derive a procedure for computing the gradients for all, uh, hidden layers. So we know that from the chain rule that if we have a function p of a, which depends on a, and if dysfunction is such that he can, uh, we written as a function of intermediate results, which we'll call queue. I have a, and so where I indexes the intermediate results then from the chain rule, we know that the partial derivative of Pov with respect to a is just the sum over all partial results of the, uh, partial derivative of Pov with respect to the IMF parcel results times deep partial. They were the of of Q, I have a which respect to eight.

Speaker 2:          02:13          So Feinstein's uh, in the neural network, if we're honored the derivative with respect to the activation of this neuron and well then we, we'll need to consider, uh, and the, and, and then we could use say, the pre activation gradient here and here as the partial a result. Because if we have this, then we can compute the last function. So what chain would would tell us is that to get the partial there, they've, for the activation here, we would need to combine the A, we need to take a sum over all paths between, between what we're, uh, making, uh, parts of their, the over and what, uh, we, uh, want to, uh, partial, uh, with respect to what we want our partial derivative. So in particular we would have to add the product of the partial dividends of our last function with respect to the pre.

Speaker 2:          03:05          So sort of this part of the path times the partial, did it live up the pre activation at the output layer, which respect to this. So that's this part of their path plus the other, uh, components in the other path between the activation here and the output. So that would be the partial derivative of the loss with respect to the pre activation times, the uh, preact the parts of their lives of the pre activation with respect to the activation here. Okay. So we get invoked this chain rule and use it in computing are gradients by using for EI a, the activation of a unit in some layer, the parcel or intermediate results would be the pre activation in the layer above for all the, uh, uh, for each I have, uh, layer, uh, unit in the layer above. And then p of a would be our function would be the loss function. Okay. So that's what we'll use to derive the Greens with respect to the hidden there.

Speaker 1:          04:08          Okay.

Speaker 2:          04:09          All right. So if we are at d kith hidden layer and won the partial derivative of the loss, which was with respect to the GF, uh, uh, GF activation, uh, up the GF, uh, hidden unit for the Kia [inaudible] layer. Well, the a chain rule tells me that if I want to use as the intermediate result, the pre activation a at the k plus one, if, uh, hidden in there. So that's the hidden there above. But I need to sum over all of the APP reactivation that the layer above, I need to multiply the partial derivative of the last week, respected the, that reactivation and then multiplied that by the partial derivative of that reactivation, the IFP or activation of the layer above with respect to the activation at the Keith layer with respect to the GF, uh, hidden unit, which is the thing we want to get our parcel derivative, uh, with respect to, uh, what we want our partial derivative. So, uh, so for instance, if we are here where something over this pack and this back or if we were at this hidden layer, then we'd need to take into account, uh, the gradient that comes from this partial, uh, this pre activation. And then this practice reactivation and this practice is p activation and so on.

Speaker 1:          05:37          Yeah,

Speaker 2:          05:38          it's remove some ink. Okay. So let's assume that we've already computed that before. And so for instance, if we started at the output layer, or we could assume that we've already computed the reactivation gradient, uh, at the output layer. Now, the part of this expression that we don't yet is this part here, what's the partial derivative of the pre activation at the layer above with respect to the activation that the layer below for Dj JF unit? Well, uh, let's remind ourselves the formula for the reactivation activation that a given layer. So a, so this is k here and a half came in this one here. So this could instead be k plus one, and then I would have just gay here and I keep last one here, last one here. So, um, for the unit at the layer above, uh, it's computation is just a linear transformation.

Speaker 2:          06:31          So a bias plus a linear combination of all of the activation of the units in the layer below. So all the, uh, j unit in the layer of below. So I can find one, the partial derivative of that expression with respect to a specific hidden units. Uh, so for that specific hidden unit, I all the other terms except the one for the actual value of jam interested in, uh, the partial, their lives would be zero. And so for the case where this Jay is the same JFF as this one here, um, then I have a constant times the actual uh, activation of the unit. And the partial derivative of that would just be the scaler it would be just be wk I Jane. So this term here is simply equal to the connection between the JF new neuron in the hidden layer, Kay and the IFE neuron in the hidden layer above.

Speaker 2:          07:29          Okay. Again, this removes some ink. So now this expression corresponds to, we can think of it as taking the vector of gradients which respect to the reactivation that the layer cake loss one multiplied by the column of that Matrix. And so it's multiplied by the column, the, the, the product of the vector here with the vector corresponding to the JF column. And that's because we're summing over the row index. So then we'd be going from the first role, second row and so on. So we're taking the product of these terms along the column, the JF column up that Matrix. And so we can write it in this way. We can say, I'll take DJF column of Matrix W in, sorry, there should be a k plus one here. So I get the get the uh, uh, and sorry, this should be in there should be changed here. So that means I'm taking all the elements for the row index and Jay here means that I'm taking Dj if uh, elements, uh, DJ column. So I think the [inaudible] column, that Matrix and so that would be a column vector. So I take the transpose to make it into a row vector and then I can multiply that by my gradient retrospect to their pre activation. And so this would be exactly equivalent to that.

Speaker 1:          08:56          Yeah.

Speaker 2:          08:57          Now if I want the gradient, so if I want the a vector partial derivatives for all activations in the hidden layer k, then all I can do is I take my whole matrix, I transpose it. And then I multiplied that by the factor of gradients for the reactivation that the hidden layer k plus one. And that's because the definition of multiplying in vector by a matrix is just multiplying that factor with all the rows of that Matrix. And so, uh, if you, if we go back at this formula here, uh, so here and here we had the thought in here we had a j. And so by taking the transpose a, it means that, uh, we're multiplying by this row vector and, uh, this rule vector would curse fun to the JF row vector of the transpose of that Matrix. Okay? So that's one way of taking our partial their lives and then converting that into a vector of partial derivative. So the gradient, we simply take the partial derivatives of a salary, the gradient of the activation at the layer above, and then multiply multiplying by the transpose of the connections between these two adjacent layers.

Speaker 2:          10:19          And now if one of the gradient, which respected the pre activation at a given layer. So if you're on the partial limited of what they are hidden, they are k of the GEF reactivation. Introspect all lost function. But again, I can use the chain rule, you know, now, uh, in this case I could, uh, look, uh, computed as as the product of the partial derivative of the last retrospective, the GF activation in layer k times the partial there live of the hidden there, k JF activation with respect to the JFF pre activation of hidden layer cake. So in this case, I don't have a sum and that's because, uh, I compute as I see here, the uh, JF activation in the layer is, does not depend on the reactivation of the other, uh, units in that layer is just a nonlinear function. The activation function applied on the JF pre activation of the reactivation vector. So in this case, I only need this only depends on the JF reactivation.

Speaker 2:          11:30          And now, uh, so for this term, this is simply the, uh, partial there. They've of my activation function evaluated at the, uh, value of their pre activation. Okay. So this would be simply the pre, uh, the, the gradient or the, the, the, the narrative of my chosen activation function. Now, if I want to compute the gradients of the last function with respect to my pre activations or the vector of partial derivatives, I have to do is to take all the partial lives and put them in the vector that we remember, that, uh, uh, it only corresponded to taking the partial derivative of the last function with respect to the activation of a neuron and multiplying it by the gradient or the derivative of the activation evaluated at the pre activation of that same neuron. So we can just achieve that by taking the factor. Uh, so the gradient, uh, up the last with respect to the activations and multiplying that element.

Speaker 2:          12:38          Why do we need another doing an element wise multiplication with a vector that contains all of these, uh, gradients of the activation function for all neurons, neurons, uh, one, two up to Jay, then up to the number of neurons in that layer. Now those are familiar with a vector calculus. Uh, they might have taken this step just taking the, uh, and that's, uh, you can think of this as the, uh, vector version of the chain rule. You take the gradient with respect to all your partial results and then you multiply that by the, uh, gradient of the partial results with respect or the intermediate results with respect to the, uh, in our case, the activation. And now I just want to, uh, uh, I just want to mention that this expression here is college Jacobian. It contains the grain, the partial derivative of all elements of that vector with respect to all elements of that vector.

Speaker 2:          13:42          So the partial derivatives of all activation, uh, in the hidden layer with respect to all pre activation and a, and it turns out in this case that this Jacobian is actually a diagonal matrix. So all off diagonal elements are zero. And that's because the activation for a neuron on the depends on it's pre activation and doesn't depend on DP activations of the other neurons. So the partial data, food, their lives would be zero. And also talk about that be to justify why I didn't use that notation to mean that vector, if I remove some ink actually actually wrote sort of use, uh, a bit strange notation by saying I'm going to construct a vector where all elements is the narrative of the activation function. And that's because this here is not equal to that. But doing the matrix, the vector product of this with that Matrix actually in this case corresponds to taking that Matrix and doing an element wise multiplication with, uh, sorry that vector in doing it otherwise multiplication with that vector, the vector of, uh, narratives, uh, for, uh, all of the activation functions evaluate that. All of the, uh, uh, p activation, uh, uh, values. All right, so this is how we compute the, uh, last gradient, which respect to hidden there is both at the activation and pre activation level.