Speaker 1:          00:00          And this video will introduce the idea of multitask learning.

Speaker 2:          00:04          Okay.

Speaker 1:          00:05          So in the previous video we discussed the conventional architecture for neural network that would give us a model for the urinary potential in a linear chain, conditional random them fields for solving any individual, a word tagging problem, and a then at the end I mentioned that, uh, uh, so we could use such a neural network individually and the single specific neural network for each task and solve them individually. However, uh, this might seem some optimal in the sense that these tasks, my are, they appear a little bit related to, for instance, pause tagging, which, uh, helps us identify nouns and verbs in a sentence. It's probably related to the problem of identifying noun phrases and verb phrases. Also identifying verb and noun phrases might be useful for identifying what are the segments which correspond to a single role, which respectable verbs. So there are probably relationships between a chunk game between identifying noun and verb phrases and identifying the semantic roles of a words with respect to different forbs in the sentence. Well, semantic role labeling.

Speaker 2:          01:16          Okay.

Speaker 1:          01:16          And so one way of tackling this problem would be to die, uh, to essentially develop a single conditional random fields, uh, along, uh, um, tomorrow the joint distribution of all the labels for all the different tasks. Uh, however, if we do this by say, introducing connections are or pairwise potentials between the labels of different tasks, then we'll miss eventually face a problem were performing inference at test time and during training, uh, will be, uh, will not be possible to do exactly what we have to do. Approximate inference, which is some, uh, it's just a little bit inconvenient. And so here we'll look at a different approach that columbarium Winston decided to use in order to still allow some sharing between the different tasks while not having to design and conditional random fields. It would be to, uh, complex, uh, to allow for efficient and none approximately inference.

Speaker 1:          02:17          The approach is actually very, very simple. Um, so we'll use individual neural networks for each of the different tasks except for the lookup tables. That is, except for the matrices that contain the word and feature representations fall the words in our vocabulary. So the idea is that each task will go and update the parameters of the lookup tables. And the only thing, the only parameters of the unit were potentials are going to be specific for a given task are a, is the rest of the neural networks. So the convolution, the linear transformation, uh, into hard times and so on, everything else is actually going to be a, so all of these parameters are going to be specific to different tasks. The only thing is that they're all shared the same word representations. And so we'll be able essentially to have a better representation for a word by combining information about what it's pasta idea is, what is a, it's a role in participants participating in two noun phrases or the semantic roles with respect to verbs or whether it's in a named entity or not and so on.

Speaker 1:          03:33          And so this will help, uh, make the word representations across the different task richer. But we'll still have specific parameters for solving each of these tasks individually. So this is a form of multitask learning in general. We refer to multitask learning as a type of, uh, learning where we tried to solve multiple tasks, uh, simultaneously. And it's a very simple one where we just share parameters across different models, which otherwise would be, uh, would be treated separately. And so it's a very simple, straightforward way of at least allowing for some sharing of information between data sets of different, uh, problems.

Speaker 1:          04:19          And, um, and other thing we could do is actually invent certain tasks to try to make even more richer the word representations that we learned. Um, so one task that they thought about is a, a task inspired by language modeling. Uh, so what you would do is that, uh, so the task that they thought about was that it would take a window of tech, say five words, and then the task would be to determine if I give you a particular window of words, whether the word that is place in the middle is actually the original word that was taken, that, that was observed in the original text. Uh, so one way of, of, uh, performing a, of generating a data set for that is to take all the word windows. They're fab words. And uh, so we pick, uh, these different windows of words from some unlabeled corpus like Wikipedia, and then we generate a bunch of negative or imposter, uh, words, context by just replacing the middle word with a different randomly chosen word.

Speaker 1:          05:30          So the actual original windows would correspond to the positive example of that problem. And then the negative example would be these other, uh, word windows where we've taken the middle word and replaced it by some randomly chosen word. Now you might think that, um, you might actually pick random, the, a word that would still make sense as a, if we actually observed it in that context. So for instance, if we were replacing the, and instead of the, we picked randomly, so it'd be cat sat on a mat. A that would still make sense. So it is perhaps a little bit strange to treat that as a negative example. Something that we don't want the neural network to think is likely. But I'm still, this is only trying to influence the, the, the, the word representations. And because there are many more words aren't going to be wrong than there are words that are going to be good words.

Speaker 1:          06:26          This is still a good way, which works well in practice for generating a data for, for this task particular the um, uh, this, uh, extra task which is going to be solved using the same word representations and the neural net that tries to do word tagging. Uh, the way they're going to tackle it is using a neural network, which takes just this context of five words. So it's, in this case, this neural network is not going to be a convolutional neural net that's going to be a regular neural net, but that uses where representations and tries also to influence them through gradient descent. And the last function that they decided to use a, so we could design some sort of binary classification problem using a soft Max output with two possibilities fund since that that could have worked. But instead they use a, a margin inspire, uh, sorry, am I lost function?

Speaker 1:          07:20          That's a inspired by the idea of maximum margin between, uh, between different classes. And specifically what they're trying to do is that if we know f data x as the output of the neural net, which outputs a score as to how good the particular configuration of the word window and his input, uh, is how meaningful it is based on according to their neural net. So if we have f data vacs for the original window, and then for a window of texts where we've replaced the, uh, window, uh, the middle word in the window by some random word w, so f data of x Doubleu, if we know that has the score for this imposter window, then they would try to minimize this expression here. That is the maximum between zero and one minus, uh, this core plus that score. So in other, we're brightening, this is, we'll put parenthesis here and then replaced them that buy a minus because it will be canceled out by this minus.

Speaker 1:          08:23          That's why it's a, it's a plus in the original expression. So effectively what we're doing is that we're looking at the difference between the score as assigned by the neural net, which has just a single output and it provides us with the score for the input, uh, the, the context window we're providing it. So we look at that score and we subtracted by the score for an imposter window. And now if that difference is larger than one, then we have my one minus a number that's larger than one, which is negative. So the maximum between zero and that is going to be zero. And so in this case, the loss is exactly zero. We're not incurring any loss from that prediction. However, if the s Corp for the true, uh, the original window and subtracted by the score of a D impasto window is smaller than one.

Speaker 1:          09:14          So if the margin between the score of the true window and the score of the impossible window is not bigger than one, um, in this case, then we're going to have one minus the number, which is smaller than one. So we actually be incurring a loss, which is none, zero. So what the neural network is trying to do is to push the score of the original window to be at least one more than the score of any other impasto window. Okay. So that's it. Uh, you know, intuitively essentially trying to separate imposter windows from, from true windows. And, uh, like I said, it's, it's so it's, it's, it's trying to enforce the margin between the true windows, the original windows and the imposter windows.

Speaker 2:          09:57          Okay.

Speaker 1:          09:58          So this is similar also to language modeling in spirit in the sense that instead of predicting the next word, we're predicting what is the word in the middle. And we're trying to make sure that we assign a higher score to the true, uh, to, to true a windows that contained the right word instead of the, uh, instead of an imposter word. And so for that reason, in the original paper, sometimes they talk about this task as a language modeling tasks, even though it's not a language model. And based on the definition of a language model as being a probabilistic model, uh, that assigns a probability to any sequence of text. But, uh, just so you know, if you look at the paper, that's why they call it language modeling task. It's because it's similar in spirit with language modeling in general.

Speaker 1:          10:44          So let's look at some results taken from, uh, the, uh, general version of that paper by color bag and western. Um, so we have here the results for the past tagging, chunking, name, entity recognition in semantic role, labeling tasks for some benchmark systems. So I take him from the literature, so they're usually not neural networks. And then the, uh, neural net that chooses the convolutional approach and a different convolutional neural net for each different tasks, uh, that is, uh, used as the union rep, potential for conditional and them feel. And, uh, also in this particular neural net, uh, no features are used. The only feature that's used as the original word, but we're not extracting suffixes and prefixes. So we're trying to see whether the neural net can learn a representation from scratch without any information as to what are useful features, the extract from the original word, uh, to get a good performance, which is something that these benchmarks systems do. And we sit a performance is not too bad, but it's usually below the performance of the benchmark systems.

Speaker 1:          11:50          However, if now we, uh, either, uh, use the language model as an extra task that is used to influence the word representations that we're learning. And also if we add up on, uh, on top of that, the possibility of, uh, having all tasks influenced the word representation. So if you do multitask learning, then we see that we get a very big improvement with respect to the original result we got with the neural net. And we're actually quite close to the baseline system. So here were almost exactly at the baseline system. Uh, here we're a bit more below but not, that's a much worse again here, 88 x instead of 89. Uh, and so we got a big jump from 81 that we had initially and here is 74, uh, which is low and in 77, but much better than the 70, almost 71 that we have before. So what's interesting here is that just based on the learning, the training data and without any information about what are the good features to extracts or whether we should use suffixes and prefixes, uh, it actually was able to do a really good job and almost match the benchmark systems which have a lot of hen tuning in terms of what the, what are the features that were extracted. So which required a lot of work from the, uh, the human experts behind the model that, that designed it.

Speaker 2:          13:16          Yeah.

Speaker 1:          13:17          And then they tried to see, okay, well what if for these different tasks, we actually introduce these other features that are known to help for, uh, trying to, uh, solve these other problems or suffixes are known to help or pass tagging in. Indeed, then we matched the performance of the best paths, stagger gazzard tier features are helpful and indeed then we matched the performance for a named entity recognition, pos, stags. Uh, if we add as the input, the past tags at the words,

Speaker 2:          13:46          okay,

Speaker 1:          13:46          uh, this is useful for chunking and indeed we matched the performance of the best system, which usually do this kind of post tagging extraction and a force, a cementing cruel labeling. Uh, we get a little bit of a boost compared to the original performance, but we don't match yet the benchmark system more for the three of the, these three other tasks. We actually did a sometimes a little bit better. So we see that as a, this is a very competitive approach for a word tagging for a bunch of different work, tagging problems, uh, and uh, based on, uh, and this is all based on the neural network that learns word representations.

Speaker 1:          14:29          And a fun thing to do then is also to look, uh, try to visualize the quality of the word representation. And one way of doing this is to take different words, say friends, which has a word id, four 54, that's not really meaningless. It could have been anything else really. And then look at what are the nearest neighbors in word representation. If we consider all other words. So we go over all the words in our vocabulary and look at their word representation compared the, uh, distance. So euclidean distance with the word representation of safe friends and then report the nearest neighbors. For instance, we see here, that's if our friends, we get a lot of other different countries in, in Europe. Uh, so for Jesus we got a lot of rigid village, unrelated words as Xbox. We got some, uh, technology and, and often gaming console words, uh, and read, we get greenish, bluish, pinkish, uh, mega bits, other computer science related terms. So we see that it seems to have in some, in some fairly real way, understood the, what these words are. And a, you can also try to get a two d visualization of these word representations and you can look on that, a URL here to get the two d mapping of the words and you'd see it. It's quite impressive. Uh, the, uh, understanding of the, it was words that was learned by this model. And, uh, and so that's, it's for how to solve word tagging problems with the neural networks.