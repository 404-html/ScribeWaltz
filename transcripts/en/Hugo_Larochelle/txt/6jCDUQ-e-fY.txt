Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video we'll discuss a convolutional network approach for addressing the problem of a word tagging.

Speaker 1:          00:09          Okay.

Speaker 2:          00:09          So as we mentioned in the previous video, we will discuss a, uh, an approach proposed by [inaudible] and in Western for are trying to do a word tagging in natural language processing.

Speaker 1:          00:24          Okay.

Speaker 2:          00:26          So, um, like I mentioned before, one way of tag tackling a sequential prediction problem. So we're tagging really is the sequential prediction problem, whoever sequence of words and each word is associated with a class, which is a tag. And so we could use and conditional random fields approach with the linear chain much like we've seen before. And uh, and, and then we would use funds, it's a neural network, uh, to model the urinary potentials of that conditional random fields. And each neural network could, uh, take a, are a window as input, which would correspond to a context of a certain number of words centered at some given position to model the potential value at that position. And then we could use where representations for that neuro network and so on. So that would be sort of the most straightforward thing of addressing the problem of work tagging.

Speaker 2:          01:17          Given what we've seen so far. One problem with that approach is that for and tasks, much like semantic role labeling, um, it might be that the words that we need to make a particular prediction of what the word, uh, idea, sorry, the word tax should be for this given word. It might be that, um, there's information that's useful for determining for performing that prediction pretty much anywhere in the sentence. That is we cannot really make the assumption that using the neighboring words in the sentence wearable able to make a good prediction of what is the label of that word at the center. And semantic role labeling is one example of that. The relevant context might be very far away from the word at some given position that we're trying to label. So for that reason, uh, Colabella and western suggested a convolutional network approach where, uh, the output of the neural network, which would give, will be used as the urinary potential in the CRF.

Speaker 2:          02:19          But now because it's convolutional neural network, the way it's going to be a constructed, the output, we actually potentially depend on any word in the input sentence thanks to the convolution plus some Max pooling, uh, that will be applied in the architecture. All right, so that's um, I'm going to give you a very brief description and, and generic description of what the network does and then we'll look at in more details at the different stages of the neural net. So the way they've structured things, instead of having the input size here and going up there in their paper, they go, uh, from the, uh, the top to the bottom.

Speaker 1:          02:58          Okay.

Speaker 2:          02:59          Uh, which is different from what I've used to just be aware of that. So at d input we have the sentence, so the full sentence with all the words and then each words are going to be mapped into a word representation space. Uh, quite similarly to what we've seen so far. That is the words are going to be associated with the lookup table that gives you the representation for that word. And in this way, we are able to represent all of the words in our sequence as a vector.

Speaker 1:          03:26          Okay?

Speaker 2:          03:26          So this is done by going through some lookup table, which gives us these vectors. And then by concatenating the vectors which we treat here as column vectors, if he couldn't cat donated, uh, uh, um, horizontally. So this would be the representation that the first word, the other word, the second word, the third word, and so on. Then we will get a matrix and on that matrix will perform a convolution. Convolution is, uh, the same kind of operation that we've seen for a computer vision. Uh, and it's going to be a convolution but in one dimension across positions of the sentence here. So in a sense, you can think of it as a two d convolution, but where the, um, the height of the filter matches exactly the, a number of elements in the word representation, in which case we can just do a convolution, uh, in this direction.

Speaker 1:          04:27          Okay.

Speaker 2:          04:28          Because there's a conclusion to also apply some padding. I won't go too much into the details of that, but you can look at it in the original paper. And once you've done this convolution, then to get a fixed size representation for the whole sentence, we'll apply a max pooling operation, doing a Max over, uh, all of the, uh, difference, uh, all of the different roles of that matrix that is the result of the convolution with perform. And then for the rest of the network will use regular kind of operations. So Matrix, multiplication, multiplication. So then your mom, uh, linear, uh, formation all by nonlinearity, all by another linear, uh, uh, an inner transformation which gives us the output, which is going to be used as the and every potential. So we're going to get a vector of size, number of possible tags, which we'll introduce in a conditional random fields. So now let's look at the specific details of each of these different operations. So, uh, to represent the words, uh, we will use a word representations that w that is, we'll have some, uh, way. Uh, we will train a, a word representation Matrix, a look up table, which we'll learn a real valid vector representations of the words.

Speaker 1:          05:46          Okay.

Speaker 2:          05:47          One thing that's interesting in this work is that instead of having a word representation, just for the word itself, the word id, we'll also have a work representations for other types of, of features. So we'll treat the word itself as a feature and we'll map that to a word representation of some given size. But then we'll also extract from the original word, other types of features like substring features. So we could look at the prefix or suffix, constructive vocabulary out of these prefixes or suffixes. And then, uh, we would have now a word representation, the vector representation for all possible prefixes and suffixes. So we would also learn this matrix that maps prefixes and suffixes to where a word vector representation other features that they use our gas at tier features. So whether or not, uh, a word belongs to a list of, of known locations, because these are things that are useful in particular for a named entity recognition. So again, you can look at the original paper for more information. And um, so by using all these features and treat them them like essentially like words, they were like each have their own lookup tables. And, uh, ultimately what we'll do is that for a given word and it's suffix and prefix and so on, we look struck all the vector for all of these features and concatenate it them to get a more, a richer, uh, con, uh, vectorial representation of that word.

Speaker 2:          07:18          Um, and other thing that we must do is that we need to encode somehow a what word in the input sentence at which position we're trying to make a prediction. So, uh, because we're doing a conclusion for the whole sentence and then we do a Max, it means that, uh, based on, on that computation, there's nowhere it information about based on this, uh, Max pooled representation, which word in the input sentence do we actually want to make a prediction for? So I have to provide that information somehow. So that influences the way that, uh, the, the actual representation after Max pooling and a one way they've done this is that they would add another word feature, uh, which would correspond to the relative position of the word. So if I is the position of some word in the sentence and pause w is the position of the current word, that is the word that for which we're computing the urinary potential.

Speaker 2:          08:19          Um, uh, sorry. Um, uh, the, yes, that's right. The word for which we're trying to compute the uh, uh, the urinary potential for getting a preference over what is labeled could be. Then we would just add I minus pause w as another feature along with the prefix and the suffix and we'll learn a vector representation with its own lookup table for that number. So for instance, if, uh, we were taking this sentence and we wanted the unit potential for the word sat, uh, then we would also upenn amongst the different features with W's. Here are the features, but it would be another feature which would be what is the relative position of all of the words with respect to sat. So this one would be zero. So it'd be one, two, three in here. What happened? Minus one, minus two. And now each of these integers would be associated with a, uh, a word representation or a vector we're representation.

Speaker 2:          09:16          That would also be alarmed. And so taking all of this feeding data from the competitional net, that would give us an output vector, which is the, uh, you never, potentials and knife wanted the union potentials for the word on. Then instead of using these values would actually use all the same other features. But for the uh, position feature, it would be zero here. Now one, two here minus one minus two minus three here. So we see that the input is changed or the output will also change. So we get a different vector for the year and every potential prediction of the neural network.

Speaker 1:          09:55          Okay.

Speaker 2:          09:55          And for semantic role labeling, uh, not only don't we need to know which word where, uh, currency computing, the a which position we want the, you never potentials, but we also need to know, uh, we're trying to predict the roles for which verb in the sentence. And so what we would add another feature, uh, which would be do you relative position of that verb. So Pause v would be the position that verb and I did work for, which we are computing the, uh, unit potentials a. For instance, uh, in this case we only have sat and if you want it to get the urinary potential for what role them, I'd be playing, uh, well for one thing we'd have the, this feature here. So we'd have zero one minus one minus two minus three minus four. And then we would also need to add the relative position of a given work with respect to what verb, uh, from what we're, we're identifying the roles. So here the, uh, there's only one verb, so it's zero. So we'd have another feature which is zero, uh, one, two, three and then minus one minus two. So now for semantic grow labeling, we need to add this set of features and this set of features to get, uh, to, to as the input of the conversion lower net to get the you and every potential vector for the a word there in that sentence.

Speaker 2:          11:26          Okay. So now we have all of these different type of features and uh, we have get a mapping those features, these ideas into a vector representation. Uh, so for each lookup table we get this factor and then we can just concatenate them for, uh, in order to obtain a single vector for each a word in the sentence. Now if we'd just stack them horizontally, we get a matrix. And then next to get some, uh, we apply linear transformation, specifically a conversional, uh, transformation, uh, on this matrix of word representation vectors.

Speaker 1:          12:05          Okay.

Speaker 2:          12:05          Specifically at every position will compute some linear activations, uh, based on a window of word representations and their features. So for instance, we would apply a, we would take that vector, that vector, and that vector. So this whole vector here, this whole vector here, this whole vector here, we, uh, would concatenate these factors and we will apply some tens formation by multiplying by some matrix say that we call m one. And this will give us effector, which would be this factor. Now, uh, to get this vector here, we'd perform the same transformation, the same type of operation. But now we take that vector, that vector, that vector, concatenate it and multiply it again by m one. And that would give us this vector. So we see that we applied the same dinner transformation but on all a overlapping windows of a word representations in the, uh, the, uh, sentence.

Speaker 1:          13:07          Okay.

Speaker 2:          13:07          So this, uh, this thing, uh, that I just described can as easily be understood as taking that matrix and performing a convolution with a vector where the summary with a filter where the filter has, uh, it's uh, height, uh, where the, the height of the, the filter matches the size of the joint vector fall features for each word. And then the width of that filter would be the size of their context window. Okay. So if that's not obvious, maybe we want to sit down and think about that a little bit and see how this is actually equivalent. But effectively what we're doing is that we're taking all context sizes and we computing this linear transformation.

Speaker 1:          13:51          Okay.

Speaker 2:          13:52          And then finally to get a fixed size representation, uh, we'll just do it. And Max pooling operation where we doing the Max over all positions of the uh, of the sentence. So each vector we have here, we can think of each of them as being deep reactivation in some hidden layer for each position. And then we do max pooling across position for each individually for each hidden units two. And by performing the Max we get the nonlinear representation. So an activation, uh, for the whole sentence and activation vector that represents the whole sentence.

Speaker 2:          14:34          And then finally, now that we have this fixed size representation, so it's fixed size because no matter what was the size of the original sentence, now the size of this vector we'd gotten, the Pv step is the number of hidden units that we wanted to have at that point in the neural network. So we can take that fixed size, uh, uh, representation. Then then connected with some regular nonlinear hidden there by performing the inner transformation. So multiplying by some other matrix and two, and then applying some squashing function on top of that vector and their paper. They propose a hard tense. It's a, instead of having a 10 age that is smoother like this, it's a tangent that is a piecewise linear function where it's zero here, one here, exactly, sorry, minus one here, one here. And here. It's exactly an inner function. So that's just another alternative.

Speaker 2:          15:24          You can look at the paper to get more details about that. And then you take that final, that top most hidden layer, and then you apply a linear transformation using same matrix m three, and then you use the result of that transformation, which has a size of a B that SQL to the number of tags that we're, that the different tax that the words can take. And uh, we use that as the urinary potential in the linear chain, conditional random fields. And so for the linear chain conditional in the field, we know how that works. We've seen that previously. So we could use that convolutional neural network architecture for, uh, an a different neural net with this convolutional architecture for each of the different, uh, we're tagging task we'd like to solve and then we could solve them, uh, separately. And though would already give us a fairly good performance and, uh, uh, are the one problem with this is that there are actually no connections between the conditional and then feels, even though they might each benefit from each other if they're somewhat related. And, uh, and what we'll see in the next video is a way of actually combining the information that are available between the different tasks, between the different board tagging tasks.