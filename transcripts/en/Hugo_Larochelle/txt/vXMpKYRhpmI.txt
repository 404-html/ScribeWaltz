Speaker 1:          00:00          And this video, we'll start looking at the specific problem of learning and training neural networks with many hidden layers. And a, this is known in the research community as a problem of deep learning.

Speaker 1:          00:16          We have to find before a multi layer neural network, which could have in arbitrary number of hidden layers. So in this case, we had one here and one here. Now, if you've played a little bit with a neural network and you've trained them with regular backpropagation, you might have noticed that, uh, actually rarely you would benefit from having two hidden layers that often for many problems, a single hidden layer will actually work best. And that this has been noticed in the literature. And people have mainly focused on training or when they were using neural network in the, uh, in the somewhat recent past, uh, they've focused on learning just a single hidden there. And yet intuitively as we'll see for certain problems, it actually makes more sense that, uh, we would be training neural networks with many hidden layers. And yet we're sort of, uh, notice that we've, uh, we're hitting this wall where we're not actually able to train them properly, uh, for certain cases for certain problems.

Speaker 1:          01:18          And so, uh, what we'll see is that there's been a lot of research on, I'm trying to understand why that is, what it is that we have problems turning these deep neural networks and what are the solutions that have been found, uh, which, uh, work well and actually allow us for certain problems to actually train, uh, well these deep neural networks and obtained much better results than if we had focused on neural networks with just a single hidden layer. Okay. So in this video, actually tried to motivate and give you the intuition for why we think it's a good idea too, to try to put some effort into training these deeper neural networks.

Speaker 1:          01:57          So when I'm talking about deep learning, I really talking about, uh, the problem of just learning models, which, uh, incorporate this notion of multiple layers of representation. So a multilayer feedforward neural network is one example, but it could be also a multilayer graphical models like deep belief networks, which we'll see in the, uh, in the video later on. And a deep Boltzmann machine, which you can go look up in the paper that's referenced on the, uh, website of discourse. Um, so really we're interested when we talk about deep learning of learning models that have multiple layers of representation, and it might not just be a feed forward neural networks. We're particularly interested in models where the layers correspond to so-called distributed representation. So by representation, I mainly mean the a vector of units which eat, try each, tried to extract some information about perhaps a presence of a particular feature or not.

Speaker 1:          03:02          And when we're talking about distributed representations specifically, we're talking about, uh, layer, uh, where the units are not essentially are not mutually exclusive. That is each unit can, uh, will compute a separate feature about the input. And the fact that they're not mutually exclusive means that two units can actually be active, so have a high value at the same time. So, uh, for a representation of, uh, features and images, then you might have features that represent what's going on in the background and you might have features that represent what's going on in the foreground. And if the background and the foreground are not related, uh, are, and uh, they're not, there aren't things are mutually exclusive with respect to what's happening in the background and the foreground. Then having these features that are separate makes a lot of sense because then we don't have to have features that are, um, that need to represent specific combinations of what's going on in the background and the background.

Speaker 1:          04:06          We can just separate the modeling part. Uh, the way the model, uh, sort of learns and models this information through different units and different, uh, elements in my distributed representation. So that's the, the quality of distributed representations. The fact that there are units that are mutually exclusive and I thought are not mutually exclusive and that can be active at the same time and can represent separate things, uh, that are quote unquote independent. So this can be contrasted with say, a clustering approach to, uh, learning features are a representation of a, of inputs, uh, of data where in clustering, uh, each unit is essentially a different cluster and a unit can only belong. So a, an input can only belong to a single cluster. So in that sense, the units and my representation in the Costa rain representation are mutually exclusive. And that's less interesting because in this case, a particular, the, the, the representation is less rich because you cannot malo many things at the same time.

Speaker 1:          05:13          Uh, your representation will tell you whether this particular input falls into a particular box and, and, and it needs to be this specific box. Uh, so for instance, if you were to model images to have things happening in the background and the foreground, then the cluster will tell you, you know, this, say this image is represented by this particular configuration of the foreground and a particular configuration of the background. And you cannot sort of separate these two factors of variation into different parts of your, uh, of your model. Okay. So that's what we'll be interested in. What we're talking about deep learning, learning these models with many layers of representation where each representation would be, each layer would correspond to a, uh, representation.

Speaker 1:          06:02          So, okay, why a deep learning? Why do we think that it's important that we have these multiple layers of, uh, of representation? Uh, a, an inspiration and a motivation that's often a reference is that of the visual cortex. So the part of the brain that processes visual stimuli. And uh, what we know from neuroscience is that when light hits the Retina, uh, then, uh, the way the brain will process that information is that it will go through several regions of the brain like this to the region known as Lgn and then v One v Two v four and so on. And also what we know by examining the behavior of neurons in these different regions is that, uh, some of these regions actually extract specific visual features. So for instance, in the region, the one we know that we get neurons that are sensitive to a simple visual forms like edges or corners.

Speaker 1:          07:01          When we reach v four, then we get neurons that are more representative of particular groups, uh, of a, of features until we reach the region known as ait, where we get a much higher level description, uh, such as, uh, neurons, highly sensitive to particular faces or objects. And because pro information is being processed into the sequence of regions, uh, which are akin to our layers in our neural networks, uh, then we know that whenever we, uh, neurons, essentially activity is dependence on the combination of the activity of what's going on say in the region before and then, and then that region itself is a combination. The neurons essentially represents a combination of patterns of neurons in the previous region and so on. So I have this idea that the neuron, the way that the brain, the way it extracts particular, uh, information about visual input is by combining features which are itself themselves, combinations of lower level features, which are themselves combinations of lower level features.

Speaker 1:          08:08          And that's, we can actually model this with a, to model this, we need a multilayer a representation. And so our cartoon representation of this is that, you know, when we might want to have a first layer, which represents very simple things like edges or distributed representation of that where we have a hidden, you can extract this particular edge and another one that can tell you whether this particular edge is present and this one or whether there's something like this and so on. So at the first layer of representation, we'd get these types of features and then another layer we'd get a combination of some of these features. So for instance, a nose would be a combination of this feature with that feature. So we'd have a second layer hidden unit that would tell us, and this would be the nose hidden unit in this cartoon where it would tell us that this feature and this feature is active. So I think there was a nose. Um, and so for a mouth it'd be that feature with that feature. And so one of the, until we get a third layer of a feature, uh, or third day or unit, which would tell us whether there's both eyes, mouth and nose. So does there is a face in the, in the input. Okay. So that's this intuition, uh, for how we would be a good way of processing visual information tells us that perhaps a multilayer representation, the visual data would be better.

Speaker 1:          09:31          Um, there's also, uh, some theoretical justification that have been put forth in the literature. Uh, we can actually show that, uh, the model with a deep architecture. So with a, what I mean by architecture is just a structure of the computations that is decomposed into multiple layers. Um, Ken represent certain functions much more efficiently and more compactly with fewer units. Uh, then if we don't allow this, uh, if we, if we had used instead the a single layer architecture say, so. One example that's well known that that's been proposed as an illustration for why deep architectures are important is the case of bullying functions. So consider the case of bullying circuits, which are very similar to feed forward neural network, but they're essentially neural network where the hidden units, all the new units really in the network are logic gates that, uh, compute things like an or and not functions.

Speaker 1:          10:31          Uh, with respect to their arguments. And so, uh, in this case, the network would be a combination of all of these logic gates. So we can show that any bullying function can be represented into what we could respond to a single hidden layer bullying circuit where we just have, you have your inputs, you have a layer of these or not functions type of functions, uh, in some hidden there. And then eventually another combination of that hidden layer to get an output that would represent that could, uh, if the hidden layer is large enough represent any bullying function. Uh, however, there are cases where that would actually require an exponential number of these bullying hidden units. And Moreover, we can actually show that there are bullying functions such that if you restrict yourself to a single hidden layer, then you would need an exponential number of these hidden units in that single hidden there.

Speaker 1:          11:29          Whereas if you actually allowed yourself to adapt a number of layers and have as many layers as you want, then you could obtain the representation of the same functions or model exactly the same bullying function, but with a polynomial number of hidden units. So I want to go through the technical details of how you can show this. Uh, I found a discussion of this particular result. You can look at this paper here by myself and colleagues, but this is more of a theoretical as opposed to a more intuitive and practical justification or motivation for using, uh, and, and, and considering seriously the problem of training and learning deep models.

Speaker 2:          12:12          Okay.

Speaker 1:          12:13          And I just want to end with illustrations that this has paid off in the recent past. So for instance, Microsoft Research in, uh, 2011, uh, uh, showed that they could gain very, very good speech recognition systems by using deep neural networks. So, uh, what's interesting here is that we're not talking about images. Now we are talking instead about speech. We can make a similar argument that, uh, you know, it would make sense to have lower level features are combined into mid level features, combined them into higher level features. Uh, but, um, even in practice, we actually see that using these deep neural networks in the speech system actually has, is, is very, very helpful. And they were able to get a historically very significant gains and beats the current state of the art at that time, which was mostly based on a hidden Markov models. And so now in 2013, the best speech recognition systems are really based on, on deep neural networks. So that's one of the illustration that really it pays off to consider and do well at training these deep neural networks.

Speaker 2:          13:21          Okay.

Speaker 1:          13:22          Another example is, uh, for vision and giving an example of a story by a particular experiment by Google researchers, uh, which we're able to train on, uh, many cores, these very big and multilayer neural networks to extract useful visual features out of images. What they did is that they took a, essentially thumbnails from youtube videos and a train in some unsupervised way, a deep neural network. And then they were able to extract some useful features, which at the time was, uh, giving very good results for say, performing a task like, uh, uh, the recognition of objects in images. And you'd even have some features which you could sort of somehow analyze the activity and actually know this, that some of these features, funds would be sensitive if we'd be active, if in the input that was the face of a cat. And so people joked a little bit about the fact that there are neural networks.

Speaker 1:          14:17          We're using 16,000 cores to be trained to use, uh, many, many cores. And of course, Google has access to that type of infrastructure. And, uh, that, you know, it took 16,000 cores to get a feature and then fired a cat. But of course, there were many more features that did other things. Some were less well understood than others. But, uh, it did yield essentially at that time, a state of the art system for extracting visual features for, uh, object recognition and the particular data set they considered. So again, here we have another example where deep learning has paid off. So, uh, what we'll see in the, uh, uh, next videos is, uh, uh, tried to get some intuition for why it was actually hard before to train these deep neural networks and then talk about some of the advances that have been made and have allowed us to get these, uh, very good system for either speech or vision.