Speaker 1:          00:00          And this veto will introduce the notion of a context window. The previous video we've introduced the, uh, model of a linear chain conditional then and feel which consistent in adding these lateral connections parametrized rise by matrix fee between the computations made at different positions in the, uh, in the sequence that we're modeling. And so this allows us to express some preferences for the assignment of a adjacent labels in a sequence. So the goal of doing this is so that the probability assigned at a given position is also influenced by the probability mass that we're assigning, uh, at, uh, at Jason Positions. And, uh, and so this allows us to model essentially dependencies within the sequence. In other way of doing this could have been to have the inputs say, uh, on the left of the center position here, k also influenced directly the computation of the pre activations that position came. And similarly we could have had, uh, the input at a, the position on the right, a k plus one also be used directly in computing. The, uh, DRP activations at the output layer for position came. So we'll see two different ways of doing this in the conditional random fields.

Speaker 1:          01:31          All right. So, um, generally we'll talk about a context window, uh, referring to, uh, the set of inputs that will influence our prediction. So our, uh, the, our preference about the value of a given label at a given position. So, uh, can text window is going to include all of the inputs, uh, at a given position that influenced the, uh, preference, uh, that we are assigning to, uh, some, uh, uh, two different labels. So more specifically, currently in the linear chain Crif we have the P of y given x is the exponential of as set of terms that depend, uh, that involve each label. Why K for different positions,K and that involved the current input at the same position, x, k plus our pairwise term. Now, if we consider the, uh, an example, simple example, we have a context window of radius one. What this will mean is that

Speaker 1:          02:37          we would also add two other terms, a set of terms or to a set of terms, a first set of terms which will essentially influence how, uh, the input on the left of a position gay, uh, uh, how it influences how like we, we think why k is so, uh, we are essentially going to AV in neural network for a position, uh, that will take the input that's always on the left of the current position. K, so that's x came on as one and that will compute a pre activation outputs, which will assign for each value of y k a particular preference, whether, uh, uh, this neural net thinks that, uh, how like he does this neural net thing, that different values of y k a r to be observed and similarly kind of an order in their own network, but for the position on the right. So it's gonna tell us how we think why case likely given the input on the right of the position, Kay in the sequence. And so it's going to take x, k plus one. And then, uh, and then this neural network is going to compute a pre activation vector from which we'll be able to consult how this neural net things based on what's on the right, uh, that the value of yk, whether the white, you, uh, the value of white gay is likely or not.

Speaker 2:          04:01          Okay.

Speaker 1:          04:02          And notice that I've changed likely the notation here. I've added here zero here minus one and here plus one just to highlight the fact that essentially these three reactivation vectors are computed by three different neural networks. So there's one neural network centered exactly on the uh, value k one that looks at the relative position on the left. So relative position minus one and another one relative position plus one. Okay. So these are really three different neural networks that take a different, uh, input at a difference relative position with respect to a current position came. Okay. So this is one way of having the, uh, probability for a label at a given position to be dependent. Now directly under the input, uh, that is not at the position gay, but is in this, in this case of the position came at a swan on a position, k plus one. So again, if we look at the, uh, flow graph or computations, then it essentially means we're adding a, a set of connections that go from, uh, the input on the left and then go eventually on the reactivation vector, uh, at the position on a relatively on its right. So at position K and, uh, similarly we have another set of connections which are different. So it's a different neural net that goes from x, k plus one and an influences the reactivation that determines the, uh, that influences the probability of y k.

Speaker 2:          05:47          Okay.

Speaker 1:          05:48          In other, perhaps simpler alternative would be to use a single neural network, but feed it the whole context window. That is the concatenation of all the input vectors that belonged to the context window at a given position. And so this will allow their own neural network to learn about the whole context. Junk lead the relationship of the whole context with respect to, uh, the labels at the different positions. And so in this case, the notation is much simpler. So we have a single neural network, we don't have a comma zero here, uh, but it's input is a computed based on the both ex gay as we had before, but also x came on the Swan and x k plus one. So one way of doing this is just concatenate these three vectors and feed that to, uh, uh, set that as the input layer of, uh, uh, of the neural network and then, uh, perform a Ford propagation until we reached the last hidden layer. So of course in this case, we'd have more parameters for the first hidden layers because we'd have to connect also x k minus one and escape plus one to the hidden layer. But, uh, the other hidden layer is computation with would not really change. It's just the first hidden in there that would now depend on the whole context window.

Speaker 2:          07:02          Okay.

Speaker 1:          07:03          And as before we have our linear chain parameters here, which influence, uh, how much we liked seeing a at Jason values of the labeling in our sequence for which we computing a probability and a not because this is some runs from one, two k a and because x k minus one equals one is x zero and that does not exist in our sequence. And the same thing for at a k equals capital k then x Cape at capitol x Capitol capable plus one does not exist as well. Well arbitrarily you can just choose to set those vectors to zero. Um, or we could choose some special vector that would essentially inform the neural network that it's on either edge of the sequence if we think that's a practical information, that useful information for influencing the probability of the symbol on the borders of the sequence.

Speaker 1:          08:01          So now are a linear change conditional random field would look like this. Uh, so now the input, uh, at position k would be the concatenation of x, k minus one and x k plus one. And so now I would have the, all these pixels and these pixels and these pixels as input, and then it would propagate until we reach the reactivation. I'll put layer, uh, so the upper layer and compute the reactivation there, uh, to, uh, get our term in the, uh, uh, the conditional random fields. And, uh, so similarly, for instance, this character would be on the right here and it would be on the left of the context here. So the same character will actually be present at different positions in the, uh, in the, uh, in the, uh, uh, input of the, uh, here three neural network.

Speaker 3:          08:54          That's

Speaker 1:          08:58          now we'll introduce a different notation, which is simpler and we'll simplify the math for the rest of it.

Speaker 3:          09:05          Uh, uh, the v of the videos.

Speaker 1:          09:08          Um, so first we'll introduce the notion of the urinary luck factor. Uh, so we'll see more a, why we call these luck factor. Uh, but their unit re, because they are the terms that only depend on the single why at a single position came. So, for instance, if we look at the case where we had the three different neural networks, one that looked, uh, at the center and other one on it's left, another one on it's right. Well all of these terms here, the each depend on the dare expressional each depends only on yk. So essentially these are the terms that express a preference for the value of y at the position. Uh, k only end up at that. That is based on the, it's a preference only on why gain not on a pair of values of four wives. And similarly for the case where we have just a single neural net with a uh, context window has input. While in this case it's only the term p activation term that looks at the whole context window. And this term only depends on the single y value at the specific position came. Okay. So for those, for both of those, I'll just use the notation a Uy gay you is for you neri because it depends on only one label and why k to specify a which for which variable are we computing?

Speaker 3:          10:31          Uh, the, uh, the urinary factor.

Speaker 1:          10:35          And you'll notice here that the key here also specifies the position. So whyK is a variable, but also, uh, uh, we'll use just to simplify again the notation. Uh, we use the value of k here to inform us that uh, this term is computed based on the position came. And uh, so these will be the urinary lot factors and we also have pairwise luck factors and there are pairwise because they involve a pair of labels, adjacent labels. And so we'll have a p four, sorry, four pairwise. And then in this case it's only the term that looks, uh, at the entry. Why K Y K plus one in the matrix. It's the only pairwise luck factor. And so a quickly, the reason why we call them lot factor is because, because here, uh, so if you write down p of y given x with this new invitation, we get the exponential of the, some of the urinary log factors plus the sum of the pairwise luck factors where we iterate over a possible positions.

Speaker 1:          11:44          And a why, the reason why they are locked factors is that we could write this as the, uh, because the exponential of Assam is the product of exponentials could write this back as a product of some exponentials of June. They're re log factors and pairwise luck factors. And so the exponential of the lug would a, would just be a factor. And so for this reason, because they appear in the, uh, in the, uh, they are because they are exponentiate it in the, uh, uh, this expression here, we say that they, instead of being factors that are being multiplied together, there are lot of factors which are being an exponentiate it and then multiplied together. Okay. So we'll continue using this notation and just know that, well, for the pairwise factor, we always use this, uh, for the next videos. Uh, and for the unit factors, well, we could either use this case here or we could use a single neural net with the big context window.