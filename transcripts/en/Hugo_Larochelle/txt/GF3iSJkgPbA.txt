Speaker 1:          00:00          In this video will introduce a motivation for using conditional random fields. So what we've seen so far with neural networks is that, uh, they were a model that could take a single input and then through the computation of different hidden layers, we'd get at the output of the neural network, a vector of pre activations. And then by using the softmax a nonlinearity, we could get a full distribution over what the potential label label of the, uh, actual inputs in this case this image could be. So then based on this distribution, which we can think of as what is the, uh, beliefs of the neural network about what is the most likely, what is the likeliness of a assigning x to any different labelled? By using this distribution, we could make a prediction by a classifying the input to the class that is the most likely according to the neural network.

Speaker 1:          01:02          And then if we gave it a new or different inputs, uh, here, then it would again compute all the hidden layers at the output layer, get a p activation vector, and after the softmax get a distribution over what the label could be. Now, uh, imagine that, uh, we are actually facing with a problem where the examples are organized in a sequence. So in particular, if we're thinking of classifying images that corresponds to handwritten characters, these characters probably belonged to at work. And then actually knowing what, uh, the previous character was, might be helpful in identifying what is the, uh, next character. Uh, so for instance, we don't necessarily expect, uh, to have a one, a two eyes in a, in a row. So I have a word with one eye than another, I or then three eyes. So a, we know the sequence is very unlikely.

Speaker 1:          01:59          So if we know that before the previous step that was an eye, then maybe we don't want to discourage making a prediction for there to be an eye for the second time step in the sequence. And we can think of other, uh, uh, such sequences of, of strings, which would be quite unlikely. And just in general, the fact that, uh, you know, certain, uh, sort of, uh, um, characters tend to be more likely after, before other characters. So we'd like to exploit this, uh, in our prediction. And, uh, get some gains in terms of accuracy when we use these neural networks. So that's the idea behind conditional random fields. In a, if we have a sequence of observations, in this case, a sequence of images that corresponds to all the characters in the word. Then what we would like to do is to actually model the joint distribution over the whole sequence.

Speaker 1:          02:53          So I have a prediction, given the sequence of images as input, get a distribution over what the sequence of labels. So the sequence of classes for each character in the input. Uh, so I have a distribution of the whole sequence and then be able to reason, uh, based on, on, uh, uh, that distribution. So for instance, find what is the most likely sequence of labels as opposed to making an element wise classification and they're trying to get a, a, the full sequence and a, so this is what conditional random fields are going to allow us to, uh, to do.

Speaker 1:          03:31          So let's first introduce some notation now in this context of classifying sequences. Then our examples would be pairs of sequences of the same length where we have capital, Aleks t, which is going to be our sequence of images. And then why d, which is going to be our sequence of labels. So notice that why now is a in bold because it's a vector and XD is going to be, uh, our sequence of vector x one tee up to x, uh, Katie, uh, team. So here the Katie is going to be the length of the TF sequence in my training set. So a, so now the inputs XD is going to be a list of vectors and the targets, why tea is going to be a list of individual scalar labels. And another way of thinking like stairs could also be as a matrix where either the rows are the vectors or the columns are the different factors in the sequence depending on, uh, what we favor is in our notation or in an actual implementation. So with this notation, what we'll end up with the conditional random field is the, uh, uh, full distribution p of y bold for all. Why one, up to y, k where k is the length of the sequence given capital x, where x is the, uh, capitol Xis the sequence of x one, two x k input vectors. And the sequence that I'm, that I'm modeling, so what we'll see is how we can, uh, incorporate the notion of a conditional random fields to obtain a, a forum for this full, a conditional distribution.