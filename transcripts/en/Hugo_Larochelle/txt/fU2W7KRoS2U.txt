Speaker 1:          00:00          In this video, we'll see how to compute the gradients of the loss function with respect to the parameters of the lug, urinary factors in a conditional random fields. So, uh, we are deriving the stochastic gradient descent training algorithm for conditional random fields and in their chain conditional random fields. We've seen the last function before and now we're going to start looking at how to compute the gradients with respect to the different parameters of the log linear conditional random fields. So before we've defined the loss as the, uh, negative log of the probability of the true label sequence given the inputs sequence. So now let's look at the gradient.

Speaker 2:          00:45          Okay.

Speaker 1:          00:47          Alright, so we'll focus now on the gradient with respect to the urinary luck factors. That is a u r y k prime, where why keep prime can be any class labels. So it's not necessarily the observe class labels for any class label, sort of partial. There it live of the negative log of their property, of a given sequence. Why given some input sequence x with respect to the urinary factor for some arbitrary, uh, label, why prime acquisition k is going to be actually quite similar to the, a gradient of the pre, uh, activation at the output layer of a neural network. It's going to be minus.

Speaker 2:          01:30          Okay.

Speaker 1:          01:30          The, uh, uh, the, uh, identity function of weather. Why k? So the Keith label and why here is equal to y Prime K, which is d label for which we're considering, uh, the, uh, the parameters of the urinary factors minus the probability according to our model of actually observing for the Keith label the actual value of y prime gay, uh, Ford, they give an input. So again, we have this a formula which is essentially minus what we would like to see, which is a, that's a w, oh, sorry. What we actually observe, which is a label of white gay minus what, uh, we currently predict. So we have essentially a, uh, a term here, which is only one when we're considering the union factor for the correct answer. Mine is the probability distribution, uh, uh, the, uh, current value for the current value of the parameters of the conditional random fields for the key of label.

Speaker 1:          02:34          Cause that's actually quite similar to the neural network reactivation, uh, uh, value add the output layer for classification that we seen before. So if we consider the specific case of a conditional random feel where we have for a given context of we just won three different neural networks, one that computes sum reactivation based on the uh, current position and other one based on the input on the left. And then the one based on the input on the right, we consider the gradient, which respect to uh, all of these pre activation, a function at the output layer of these three neural networks that would give us the following, uh, gradients with the minus the one hot a vector with a one at the real value of the label at position k minus the vector of marginal probabilities for what the label might be according to the, uh, current conditional random fields for position.

Speaker 1:          03:36          Okay. So that's for the neural network that looks at the input at the current position came. And uh, if we're for the one that's looking on the left, well the grading is going to be zero if k is equal to one. So if it's not greater than one, and that's because, uh, and four k equals one. There is no neural net trying to predict from x zero since zero doesn't exist. And similarly, we have also an indicator function, uh, for the one that looks to the right, uh, so that the grain that is zero if k is equal to capital k. So now let's look at the derivation for this formula, uh, for uh, how, so let's try to see why this is true.

Speaker 2:          04:21          Okay?

Speaker 1:          04:22          All right. So we're looking at the partial derivative of the negative log probability of some vector of labels. Why given some input x with respect to the, uh, uh, lug a factor, the urinary luck factor, uh, and its value for a, some given label. Why prime cane so gay, it means we're looking at the position came the sequence and right. Prime key would be some value between one and capital's seat.

Speaker 2:          04:51          Okay,

Speaker 1:          04:51          so, uh, here, the only thing I've done is just write what it is minus the log of p of y given x for a log linear, sorry, for linear chain CRF. So remember we have dialogue of the exponential of these terms. So we just have these terms. Once we applied the log minus the log of zed, because before we had dividend divided by the partition function. So the log of that would be minus the log of the partition function.

Speaker 2:          05:23          Okay.

Speaker 1:          05:23          Now, uh, the gradients of this, sorry, the partial derivative of this whole expression, which respect to that term is going to be one. If the KF label is actually y prime k, otherwise that particular expression is not going to be, it's not going to appear here. Okay. So the gradient for any other position then gay of these terms here is zero. The grid, the derivative with respect to that, uh, of that, sorry. With respect to this, it's also zero. So it's only the case for the same k which is important. And if whyK is equal to pry prime k, then we'll have the exactly the same term. So the death of thousands going to be one now. Otherwise they're going to be zero. And then minus that they're, they've upped the log partition function.

Speaker 2:          06:10          Okay.

Speaker 1:          06:11          So we're just like the narrative to this minus that narrative to have that.

Speaker 2:          06:16          Okay.

Speaker 1:          06:18          So let's do that specific term that they would have the lug partition function. So the derivative of the log is just one over that one over zed types. It of, of, of whatever we were taking the logarithm logarithm of,

Speaker 2:          06:35          okay.

Speaker 1:          06:36          Now here I'm just riding the expression for the partition function. So because I've used a symbol, why prime gay? I'll use instead. Why prime prime for, uh, referring to the sequence of overs over which I'm taking this sum here, the nest at some.

Speaker 2:          06:53          Okay.

Speaker 1:          06:53          Now I can push the, uh, so did the narrative of a sum is the, some of the derivatives, so I can push that they're the insight here. So right here. So that's what I've done right here.

Speaker 1:          07:09          And now, uh, they're the, about the uh, exponentiate UD. Uh, some of, uh, factors of luck, the factors, well, it here, uh, Aau, right? Prime K does not appear then it's zero and it's, it won't, it won't appear if y Prime K is not equal in the sequence. Y prime is not equal to right prime prime cake. Uh, oh, sorry. In the sequence. Why [inaudible] if I had the position gay, uh, it's value is not the value of y prime gay then, uh, the, uh, then, uh, the derivative of that is going to be zero. It's going in because it's going to be constant with respect to that variable. However, if it does appear, then the live of the exponential is the exponential itself. So it's going to be that term and times to their develop the argument. Well, it's just going to be one much like it was one right here. So essentially what we'll get is the indicator function of weather. Why prime case equal to y prime prime k times the exponential of the sum of all luck factors.

Speaker 2:          08:17          Yeah.

Speaker 1:          08:17          Now let's notice that here, that's the numerator of the probability to conditional problem probability in a conditional random fields. And that's the partition function. So I can just,

Speaker 2:          08:32          yeah,

Speaker 1:          08:32          remove it here and just divide by here.

Speaker 2:          08:38          Okay.

Speaker 1:          08:38          And then what do I get? Well I get that this whole term here is just the probability of observing as the sequence of labels, a y prime prime law and followed by why program to up to why pride pride the capital k and then this here is here

Speaker 1:          08:59          and now if I think there's some over opposite all sequences of uh, and only consider the terms were in all these sequences, I won't consider the sequences such that y prime prime k is equal to y prime k, then I'm just summing the probability of our sequences, uh, which satisfied this condition. So all sequences such that the key element that the sequence is right, prime came. And so there's some of all of these, uh, John probably is just the marginal probability that y prime gay, uh, is the key if label in the sequence. So I do obtain that. This term here is p of y prime gay given x. And so I do have that the gradient, the parts of their, they would just back to the logging area factor is the indicator function minus the probability under the model.

Speaker 1:          09:56          So that's great. I know, uh, we now have the gradients of the and the partial data they've, with respect to the log you neri factor, but we don't have it with respect to the parameters in the login error factor. Fortunately we've seen that, uh, the backpropagation that allows us to very efficiently applied the chain rule and back propagate the gradients to all the parameters. So essentially from the parameters of the login, there factors that gives me essentially the uh, uh, uh, the partial though the which respect to the pre activation values at the output layer of the neural nets, uh, inside the CRF. And then I can just take those gradients, those partial derivatives and back, propagate them towards all the parameters of the model. Now notice that because it's the same neural network at each position in the conditional random fields, I need to accumulate all the gradients for every position and the sequence. So all of these greatest need to be accumulated into, uh, the, uh, my computation of the parameter gradients.

Speaker 2:          11:01          Okay.

Speaker 1:          11:01          So let's look at the specific case of linear, linear chain, conditional random fields. So the case where the luck factors are linear. So, uh, this will be the neural network from the center, the computation that looks at the aligned input. So it'd be in linear transformation plus bias. Then we have the, uh, uh, computation that looks at, and sorry, that's a mistake. That should be, um, oh, sorry. Yeah, so that's one way of writing it. I could have written it came out is one here and k minus one here. I could have rhythm Kate bus one here. Keep the last one here. But let's just assume now that, uh, we have this, uh, lug linear model where the, uh, uh, urinary luck factors are just linear function of the context window.

Speaker 2:          11:51          Yeah.

Speaker 1:          11:52          So in this, the, uh, Great Edward, respect to the bias. And also, I just want to say that notice I didn't put a bias here or here and that's because I would just be redundant since, uh, I mean just a single bias, uh, is sufficient. So the greatest of respect of the bias would be the gradient, uh, with respect to the, uh, uh, pre activation function, uh, for the center of the window, but summed over all positions. So that's the green of the activation function at our position case or for all inputs xk all position can't actually, and we've seen before that the gradient was essentially this the one hot vector minus the vector of marginal probabilities for the position gain. So I'm just summing over all positions game and now if only the greatest of respect to the weights, uh, that connects the input that's at the current position. K or that's to the left or to the right? Well, I get something that's essentially very similar. It's again, the a 100 vector minus the vector marginal probabilities. But for, uh, this part of the Pr, this parameter Matrix, I'm also surprised by the transpose of the vector aligned with the current position. K. This one I multiplied by the vector to the left, the transpose of the vector to the left, and he transposed to the of the vector to the right.