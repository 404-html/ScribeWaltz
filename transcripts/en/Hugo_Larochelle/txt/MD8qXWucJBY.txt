Speaker 1:          00:00          And this video, we'll see an efficient algorithm for training or restrictive Boltzman machine known as contrasted divergence.

Speaker 2:          00:07          It's

Speaker 1:          00:09          so we've seen a poster machine how it defines a distribution over x and it's hidden layer h. Uh, we've seen that, uh, we could also write down p of x, uh, using the free energies and we've looked at uh, how exactly it tries to model an increased probability for certain values of the input vector that's actually see how, uh, we can actually train the restrictive bolsa machine UNSOM training data, uh, in order to obtain, they were suitable to machine that model as well, that data and assigns high probability to it.

Speaker 2:          00:44          Okay.

Speaker 1:          00:45          So like we've seen before, we'll treat this problem of training as an empirical minimization problem, uh, will, uh, do without the regularization here and just, uh, tried to minimize the average loss where the loss is going to be the most natural thing here to do, which is to use the negative log probability as our last for this model. And so what we'd like to do is optimize this average, a negative log probability of the training data, uh, using an optimization procedure and particular would like to use to castigate the scent because works well and scales well to large data sets. And so it means that to achieve this I need the partial, they'll live, have any parameter theater of my loss. Now if I make the derivation and write down the expression for it actually has a, uh, an expression which looks kind of similar to other things we've seen before in terms of partial the lives of parameters for neural networks.

Speaker 1:          01:44          We have again, a difference of two terms. One which depends on the observation and another one which depends on more explicitly, only on the model. And so this partial derivative is going to be the, uh, expect expectation over, uh, what we never see, which is the hidden layer value of the partial narrative of the energy. With respect to my parameter, a given my observation. So an expected, a partial derivative, we have to do an expectation because h we never know where its value is. And, uh, so it's the expectation of this partial there live condition on the observation. And then we subtract that, the expectation over age and uh, and now also ex, uh, of the again, uh, the partial, the, the, the energy function with respect to our parameter, but not as now we do an expectation over x as well. And, uh, this is an expectation according to our model.

Speaker 1:          02:44          So under our models, uh, distribution, uh, so we often refer to that part computing, this part of the, uh, gradient as the positive phase and this the negative phase for obvious reasons. And, uh, but now the problem is that, uh, this part here is actually hard to compute. It's generally intractable. Uh, and the reason is that we have to make an exponential, some over both h and x. And so, um, perhaps you'll be convinced that if I give you a value of the visible layers, so if I give you an observation x, then summing over h is actually something that's tractable to do, uh, for this partial derivative in particular for the RBM. Uh, and then we can again leverage the fact that we get an expression, a of nested nested sums, a over something that fact arises with respect to each hidden unit. And then we can actually write this down into an expression that's cylinder and the number of hidden units. So we'll see that more in more details later in this video. Uh, but now if you have to do is some over both x and h, now this becomes intractable. And so we'll have to approximate that term somehow, uh, in order to perform a stochastic gradient descent efficient.

Speaker 2:          04:05          Okay.

Speaker 1:          04:06          So, uh, to address this problem, Jeff Hinton in 2002 propose the contrast divergence learning algorithm. Uh, so there's some theory for what this is actually doing. I'm not going to go over that. I'm just going to describe what the algorithm is and the more intuitive terms. And, uh, I tried to give more intuition for why it should actually work. So really the idea is to try to, uh, do without this double expectation and a instead estimated. So are there are really three main components to a contrasted divergence. The first idea is that the expectation over x and h in the negative phase will actually replace it by a point estimate at a single observation excel. So the expectation over X, we'll replace it by a a point estimate at axtel because if we have that point estimate, if we give me a value of the visible layer, then I can do the expectation over age and then get an estimate of the uh, double expectation.

Speaker 1:          05:10          Uh, so that's just really saying we'll do a Monte Carlo estimate of the expectation with a single data point. Then, uh, we'll, we need to obtain that extent somehow. Uh, and ideally would like to a sample from the true distribution, not the true distribution, but our model distribution. And then we'll do this by Gibbs sampling. Uh, so remember that Gibbs, something corresponds to a sampling, each variable in my model, given the others, and specifically for a restricted Bolsa machine performing that sampling is actually quite as efficient because condition on one layer, all the other elements and the other layer, uh, are independent. Then I can sample all the values in one layer in parallel, given the value, uh, of the opposite layer and then alternate between, uh, each layer like this. And so this is actually very efficient to do in practice. And then the third idea, which is perhaps the most important contribution behind contrasted divergence is to uh, perform gib sampling.

Speaker 1:          06:14          But by starting our assembling at, uh, a state where the visible layer, uh, is set to the training example for which I'm trying to compute the gradient and do an update. And so instead of starting like we usually do and give sampling at a configuration of my, uh, on my layers, my random variables, that is a simple, uh, perhaps uniform the, and just randomly according to some of initial distribution out actually use the value of the training observation for the value of the visible they are when I'm performing gib sampling. And the other thing is that I'm, I'm not actually going to do give sampling for a long, I'm actually going to do it for one, two, or just a few iterations and a, as we'll see, this actually works well in practice. So to illustrate this process more visually. Uh, so there's, oh, Aaron, my finger, there should be a little the same circle here.

Speaker 1:          07:13          So what we'll do is that at training time for giving training example, I'll take the value of the input vector x and I'll set it as my value from my visible layer. Then I'll assemble all the hidden units condition on the observing this particular value of the visible there. This training example. So I'll sampled from p of h given that x is equal to x t in this case here, uh, just a note on performing that sampling. So, um, each neuron lung condition independent, so they each have Bernoulli for which I can compute. What's the probability for that Bernoulli random variable, that hidden unit being holed to one, given x. Now to obtain a sample from a Bernoulli, would that probability of being equal to one, what I can do is just sample from a uniform distribution between zero and one. And then if that value, that uniform value that I sampled is actually, um, uh, sorry, it should be the other way around.

Speaker 1:          08:20          So if this value is greater than so greater than, uh, the value of the probability is greater than the value of sample from my uniform, then I'm going to set the, um, that I'm going to set the hidden layer, unit digit hidden layer, the unit two I want. And otherwise I'm going to set it to zero. So in other words, the value of the hidden yet is going to be the identity function of whether the probability of the hidden unit is greater than a random sample from a uniform between zero and one. So we can see that this, uh, will be, uh, equal to one with a probability of this value, uh, because the mass of a uniformly distributed random variable between zero and this value is going to be exactly because it's uniform, it's going to be p of d people to one given x. So, uh, so indeed the property that this is one is going to BP of Hg equal one given

Speaker 2:          09:27          x.

Speaker 1:          09:29          Okay. So now I've taken my training sample. I've simple each of the hidden units, uh, conditionally, uh, a condition on the physical layer of taking that value. And then I'm going to reconstruct a, uh, the visible layer by sampling from a p of x given the current value of my hidden there. So I could call this a x one. And then I'm going to do that four k steps. I'm going to take x one and then sample the new value of the hidden. They're given my previously sample value of the visible layer. I'm going to alternate light this performing Gib sampling for case steps. And now this last value after I've done my case steps is going to be the, uh, a negative sample x still. So Xcel, we often refer to it as a negative sample, which is used to estimate the negative phase part of the gradient. And so I'm going to use that as my negative sample to perform my point estimate of the expectation, uh, over X. Okay. So visually, what does this look like? So I get a training example and uh, in the positive phase I'd had to estimate this conditional expectation to simplify. Imagine I actually don't perform my expectation. I just sample and, uh, h given this XD and I'll call that h tilty and uh, I've also performed case steps of Gib samplings, [inaudible] and similarly how to perform that expectation estimated at exhale and h still wear, uh, age still would be simple based on the conditional distribution of the hidden layer, given that x is equal to x still.

Speaker 2:          11:18          Okay.

Speaker 1:          11:19          Um, so then I get these two pairs and if I look at the green in the sense procedure, what it's telling me is that I should decrease the energy at the training observation and I should increase it at the simple value x still in its associated hidden layer. Uh, because uh, low energy means high probability, then this means that I'm going to increase really the, uh, probability of observing XT with a tendered layer. And I'm going to at the same time, the probability that x still is going to be observed under my models distribution. And so if my training example corresponded to images of digits, then I'd be increasing the probability of observing this particular digit here under my model. And then say initially my uh, restricted bolsa machine is randomly initialized. So it essentially corresponds to a uniform distribution over a binary vectors. Then initially what I'm going to sample is really going to look like noise.

Speaker 1:          12:25          Essentially it's going to look like something like this. And so what I'll be doing is then making the probability of sampling something like this from my model and much smaller and then I'll continue iterating like this. So next time around it will be less likely that it still would look like a random image and the sample value for exterior going to be looking more and more like actual training examples because I keep pushing down the probability of anything that doesn't look like a digit. And so we can see that as we keep sampling like this, then eventually the gradient should become smaller because the value of XD is going to be more and more similar to the sample value of exhale. And so until they lead this algorithm, what it's doing is that it's increasing the, uh, energy, uh, sorry, decreasing the energy of things that look like what's in the training set while increasing the energy of things that, uh, are as elucidated or sample by the model. And we keep doing this until the model spits out or generates, uh, observations that are very similar to what's in the model. In other words, it's become a good model of, uh, uh, our dataset.