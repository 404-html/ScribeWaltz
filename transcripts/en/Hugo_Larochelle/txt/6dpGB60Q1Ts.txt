Speaker 1:          00:00          Yeah.

Speaker 2:          00:00          In this video we'll finally see how we can train a conditional random fields. And so we'll first discuss the loss function that we'll use to train and learn our conditional rendering fields.

Speaker 2:          00:14          So in the previous videos we've defined what we meant by a linear chain conditional random fields. So that's the specific conditional random fields we'll consider. So p of y given x square, why is a sequence of label and exit sequence of inputs is going to be the exponential of a, some of you and there re uh, potential Oh, a lot factors and a pairwise luck factors divided by normalization constant that we have to compute with some dynamic programming music, the forward backward a algorithm. And uh, so, uh, thus far in the previous videos I always assumed that are conditional. Random fields was already trained in and I've talked about how we can perform inference compete, the partition function, performed classification and so on. So now we'll finally look at how we can train a conditional random fields on some available data.

Speaker 1:          01:06          Okay.

Speaker 2:          01:07          So again, we'll use empirical risk minimization as our guiding principle for the writing and learning algorithm. That is we are going to choose a loss function and to train on conditional and them feel on a training day, a training set that is uh, a set of pairs of input sequences and target sequences or label sequences. We'll minimize the average up to some of the loss function where we compare to what extent our model fits well with the, uh, target, uh, sequence information we have. So we'll optimize this average loss alone sold plus the a way the regular riser to a control to what extent we want our model to fit well the data and to be able to control for overfitting. So again, we're casting learning as optimization like we've done in the regular neural network. And a though ideally would like to optimize the castigation error. So in this case, perhaps you'd like to optimize us some of per label classification there. Uh, this error again is non smooth, so we have to use what we usually call a surrogate loss. So an alternative laws that sort of looks like what we want to optimize, but that is a better behaved. Uh, and in specifically in this case is going to be differentiable so we can perform great in descent training

Speaker 2:          02:27          and again we'll use a stochastic gradient descent that is uh, that we were training is going to proceed is that we'll first initialize the parameters of our model. If we're sending a number of iterations, will cycle through our pairs of input sequences and target sequences will compute the uh, update direction, which is going to be minus the gradient of the loss for my current pair of uh, input and target sequences minus also the gradient doctor regular riser. And then I'll take a step. Uh, so alpha times the update direction to change my, uh, vector from is my set of parameters. And so again, I have to specify a loss function as specified procedure for computing. The gradient with respect to the loss function also have to specify the irregular riser and its gradient as well as the initialization method. So these two parts are not really going to change or we're not really going to discuss them. We'll focus on choosing a loss function and also, uh, specifying what we compute the perimeter great hymns. And is this particular video we'll look at. The last function would optimize.

Speaker 2:          03:38          So again, because a conditional random feel estimates the conditional probability of the target given the input, what is a natural thing to do is to maximize the probability assigned to the true target. The sequence given some input sequence, uh, as extracted from our training set and a, as we've seen in the case of irregular neural network, it's of maximizing probabilities. We can minimize the negative log of the probability, which is more a better behaved and yield simpler math. And so, uh, to form it has a minimization problem. We minimize the negative log likelihood. So our loss function will be minus the log of the probability of the true target. So normally this would be the true target given the inputs sequence. So this is really the same that we've done in irregular neural network where we have an input vector and we're trying to classify it into a set of a CE classes.

Speaker 2:          04:37          The only difference is that now this is a sequence and this is also a sequence. One sort of subtle difference also is that, uh, while in a non sequential concert vacation, if you have 10 classes, we might actually explicitly compute the full conditional distribution p of y given x by iterating over all classes. And so obtaining the full vector of probability. Now with a sequence, we can do this because the number of sequences is exponential. So really when we're computing, this will, uh, use, uh, forward, backward to get our outdoor, our bed a table and to compute the partition function and we'll use them explicitly the partition function into the expression for p of y given x. And then we'll take minus the log of that. So we won't actually compute a, we don't have the full distribution of all sequences that just because that's too big of an object. It's something that's, uh, there's an exponential well sequences. So that would be an exponential number of probabilities. So that's the last function we'll use for training or conditional random fields.