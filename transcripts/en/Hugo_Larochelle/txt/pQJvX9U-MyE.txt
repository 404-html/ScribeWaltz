Speaker 1:          00:00          In this video, we actually see how we can perform sequence classification in a linear chain conditional random fields. So, so far what we've seen is how we could compute the probability, the churn, probability of a whole a sequence of labels given an input sequence. Uh, so we've seen how we could compute in particular this nasty partition function using dynamic programming.

Speaker 2:          00:27          Okay.

Speaker 1:          00:27          And we are also seeing how we could compute marginal probabilities. For instance, the marginal probabilities of a label like a given position. K given the full sequence and again, performing this computation required the same dynamic program, uh, that uh, uh, we can use for computing the partition function, what would compute these log alpha and lug better tables and use them in the computation of the marginal probability.

Speaker 2:          00:58          Yeah.

Speaker 1:          00:58          Now here what we'll see is how we can actually perform classification. So if I give you a sequence of inputs now I ask you to, uh, provide with, provide me a sequence of labels that you think is a good prediction for the sequence of, uh, the, the actual true sequence of labels, uh, that, uh, that a human or an expert would have predicted. So, um, Darra two options here. So we'll see two different procedures for performing that prediction. The first, uh, is perhaps those simplest, uh, what we do is that for each position, Kay, we compute the marginal p of y k given the sequence. So we compute the probability for any potential label, why k of it being the true key if label in the sequence. And then as our prediction, we just say that, uh, in our predicted sequence, the label for the prediction, uh, the key, if the position is going to beat the label that has the highest marginal probability.

Speaker 1:          02:00          Now I'm, so this is very similar to how for a neural network, uh, that would take just a single input. So none in the sequence prediction sending, we would just feed the input to the neural net. It would produce a, a output distribution, uh, for what the true label would be for the given input. And we just take the label with the highest probability and now we can actually show that this procedure for constructing a prediction sequence. So a sequence of labels that we would provide us an answer, uh, is, uh, corresponds to the prediction that minimizes the sum of classification errors over the whole sequence if we assume that our linear conditional random fields. So our model is actually the true model is the true distribution that produced the a sequenced labels in our dataset. So we actually, uh, this gives some justification for using that procedure. And, uh, uh, because this is an interesting concept, I actually will actually do the demonstration for why maximizing for each position, the marginal distribution actually corresponds to finding this, uh, minimizing sequence of the, some of classification there. If we assume our model is the treat distribution.

Speaker 1:          03:22          So what do we want to do is, uh, find the expression for what is the predicted sequence? Why start that minimizes in expectations. So I should say this is the minimization in expectation. According to our model of the, some of the classification there are for each position. So where you have these, some of the identity function of whether the true label according to our model, why Cain soars to label in the sense that it's the label that uh, based on, uh, uh, the distribution of our model is different from the predicted label at position Kate. So why star Kay? So we want to minimize in expectation that some of castigation there per position. So here I'm just writing explicitly what this expectation here is. It's the nested some over the first a label up to the last label of the, some of Perella a preposition classification error weighted by the probability according to my model that I would observe this sequence of labels a y. So these white case here, yeah, I'm running the fact that the minimization over all sequences, why star is equivalent, so perform that immunization is the same as performing minimization, uh, uh, sequentially over the first label until the last label. So it's this minimization over all sequences is the same as these nested minimization over first seek first, uh, uh, position. Second position until the last position.

Speaker 1:          05:01          Now here in what I'm doing is that I'm going to take that, uh, first, I'm going to introduce this here, uh, which allows me to just remove this parent. This is, and I'm going to change the order of the songs. So I'm going to actually some first over the position k and then some over the value of the first label up to the last label. So that's why I have key here. Then first label up to the last label. Okay. So, uh, that, that's all I've done, introduced this here and change the order of the sums up the nest itself. Now, uh, I noticed that this term here only depends on the value of a y k for the position k that's indexed here. So I can actually, this is a constant with respect to y one Y K minus one. Why k plus one up two. So there should be three dots here up to the last label. So I can actually push this here, uh, up upwards in the nested sums, writes in from the, the sum over the value of the Keith label. And the, uh, sequence position gay ass, which I'm measuring the error.

Speaker 2:          06:19          Okay,

Speaker 1:          06:20          next. I know this, that this expression here, that's just the marginal p of y k given this sequence. So I'm taking the full joint distribution, well conditioned on the input sequence, the joint distribution over the whole sequence. Why? But I'm summing over all the values that every label can tech take. Accept the label. Why King? So that's the marginal p of y given X. Next,

Speaker 1:          06:47          I, uh, realized that this sum here over all position of what is essentially the expected error for that position. Uh, well that's essentially the sum of the sum over why one of whether I'm making in there or by predicting why star one times weighted by the probability of actually seeing why one as the first label plus a. So I do this, the sum over all, uh, case. So then, uh, sum up to the sum over y capital gay, uh, of whether I'm making an error by predicting why prime at, why star capital gay weighted by the probability that the true Labor would actually be, why capital gain. Now, uh, so I have a son and a minimizing over this sum for each label. Well, because each term in the, some actually it depends on this single label. Well that's the same as minimizing the first terms are with respect to why a one then actually, sorry, why star one only. And uh, and I don't need to minimize over the other terms because they're not involved in that expression plus the minimization with respect to why start to up to the, uh, so plus up to the minimization over the last label of this term here.

Speaker 2:          08:07          Okay.

Speaker 1:          08:08          Okay. So that's because the minimization over all labels for a term that depends on, on the one label is the same as minimum as a minimum, minimizing over a single position or a single label.

Speaker 1:          08:24          And so, uh, now the, uh, uh, we noticed that you mean remove some ink. So we know this, that this expression here, which is the sum of a one, if why one is not, why star one weighted that by the probability of observing why one uh, so that's the sum of the property of why one for every case, except when, why one is equal to y star one. So in other words, so that's the probability of the first label not being why star one, which is equal to one minus the probability of the label of being why star one. And so similarly for this term we also get one minus the probability of y Star capital k

Speaker 1:          09:10          and then find a need performing each minimization, minimizing one minus this storm is the same as doing one minus the maximum of disturbed. So indeed this is a constant, so I can't minimize it really the the choice of why star one won't change the value of one. And so I can introduce this here and the minimization of the negative of something is the same as minus the maximization of that's something. So that's what I have here. So this is equivalent to this. And in particular the Arg, Matt, the argument here or the Arg Max is the Arg Max here. And in other words, it's the a sequence where the first label is the one that maximizes the marginal probability for the first element in the sequence. And then same thing for the second level, third level up to the last label, which in my prediction should be the label, why star gay, which maximizes the marginal probability of the, uh, of the, uh, label at the last position at the position capital came. Okay. So this ends our demonstration that if I assume the true, uh, labels were generated by my model, my linear, conditional random fields, then if I wanted to minimize the expected classification, there are per position, uh, the right thing to do would actually to be to produce a sequence of all labels that maximize the probability of being observed according to my model for each position and concern constructing my prediction by concatenating all of these uh, sequence, uh, all of these labels.

Speaker 2:          10:57          Okay.

Speaker 1:          10:58          So that's the first option. And other option would be to do the equivalent of what we were doing in the non sequential, uh, prediction part that is I'll just produce as my sequence output. The, uh, so actually this should be a star here. I'll actually just produce as the sequence output, the single sequence with the largest, probably the largest joint probability condition on my input sequence. And so I'll go rapidly over this, but it turns out that much like we've seen a way of actually summing, uh, uh, over the, uh, something like this stone, the exponential, uh, an annex exponentiate it, uh, some of, uh, lug factors. We can also perform the maximization with the dynamic program. Uh, so we'll do, uh, we get essentially as a baritone guitar be decoding where in the Ford pass so much like in the Alpha, uh, dynamic programming where we, for the log Alpha, we had luck, some ex operations, well in the this bitter be decoding algorithm, we replaced those by the maximization with respect to why prime canes.

Speaker 1:          12:11          That and then we fill the table with those values. And, uh, in fact, if you think about it, luck, some x is actually can be thought of as a smooth version of the maximisation operation. And that's because, because of the exponential exponential here, the term diet is going to dominate. This sum is going to be the term that is largest because the exponential increases very rapidly with large values of its argument. And then taking the log, we go back to the original scale. And so really the term that's going to dominate this whole term here is going to be the maximum. So, uh, so that's just, you know, a nice fact to know about the relationship between the lugs, Amex and the maximization.

Speaker 3:          12:55          Yeah.

Speaker 1:          12:55          And uh, but so yeah, so we take the Ford Pass, say in the, uh, diamond program that confuses the Lug Alpha table and would you replace the Lug, some x operations by a maximization and then in the backward, then we have to produce it backward pass. And in the backward fast, well, we start, we'll essentially compose are decoded. Uh, so I predicted sequence by finding the last label that maximizes the value in our table. And then falling backward, the trace of the Max operations. So falling backward, the links that are going to correspond to what were the Arg Max when I was performing this maximisation here.

Speaker 1:          13:37          So more specifically, let's look at the pseudo code for this, uh, algorithm. We, uh, first initialized the value of my Alpha stars. I'm going to use star for the dynamic program, the Viterbi decoding algorithm that is going to fill this alpha start table as opposed to the Alpha table. And notice that here we have in maximization instead of a Loxa Mex operation where we do a maximization over what would be the argument of the Loxa Mex operation in the computation of the log Alpha table. So initialize the first column of my Alpha Star table by performing this maximization. And I also remember for each value of y star, uh, sorry, why prime too? What was the actual value of y prime one that was maximum is maximizing the, some of the luck factors. So see here, this is the same thing as here. So this is just the Arg Max of this maximisation.

Speaker 3:          14:36          Okay.

Speaker 1:          14:37          And then much like in the log Alpha table, I'll go from the first, from k equals two to the last till the last column. And then for each value of the label, I'll perform the similar similar computation where I've the Luxor Max by the maximization. And here I have you in urinary refactor or ice factor and the Alpha Star. So the value at my previous column in my Alpha Star Table. And similarly I'll remember what is the Arg Max. So I'll put that into this other table, which I'm uh, where I put the Arrow to distinguish it from the Alpha star table. And then finally, if I want to know what's the probability of the most likely sequence, why star? So what's the maximum of why start off of this should be a why star also? Well, it's just, it just corresponds to taking the maximum over this expression here.

Speaker 3:          15:37          Okay.

Speaker 1:          15:37          And uh, so that's, uh, actually not exactly taking, I said the taking the Max over the last column, but that's not exactly true. It's the Max over the last column plus the urinary luck factor for the last position.

Speaker 3:          15:50          Yeah.

Speaker 1:          15:51          And then to perform decoding that is to actually construct my predicted sequence. Why Star? I'll take the, again, the Arg Max of Dicks. This maximisation here. So the Arg Max of the urinary factor at the last position plus the y at the Alpha Star, uh, for the last column of my Alpha Star Table. So that's going to be the predicted label for the last position. And then I'll go backwards from the previous, the last position till the first. And what I'll do is that why star gay, it's just going to be what was the, uh, Arg Max. So you see the Arrow here was what was the Arg Max, uh, when I performed the maximisation for Alpha Star Gay, assuming that the value for y star keep last one for y prime plus one is actually why star keep us one, which is the value. Why star in my sequence that I computed in a previous iteration. So we see that by initializing, why star capital key here?

Speaker 1:          16:51          Well, I always have the value for that. So I'm going from right to left using my proven Swi star value to construct my, uh, uh, sorry. I guess the, the, the value of why star at the position, uh, to the right took a constructor, my prediction for the position, the current position. Okay. And then going from right to left. Okay. So this is how I get a, this is an algorithm, but that is also a, that has, you know, a nice, a complexity of a that's polynomial in k and a capital gain and capital c four a computing, uh, constructing your prediction. And so classify a whole sequence of inputs and producing a sequence of labels for my sequence of inputs. So the two types of predictions, this Viterbi decoding approach and the other one that maximizers the purpose, mission marginal are two valid, uh, predictions to perform.

Speaker 1:          17:46          If we really care about the sum of the, some of the, uh, per element, uh, do some of per element classification errors. We might want to prefer the first procedure, but if instead, our prediction a, an error, if the way we evaluate the performance of our classification, a sequenced classification system is different. And so for different systems, I can speech, for instance, we'd be more interested in a phone error rates, which is more complicated than, uh, performing. Uh, this Vittoria be decoding procedure is a, should be preferred because then we're not maximizing the sum of per, uh, position, uh, per position, uh, errors.