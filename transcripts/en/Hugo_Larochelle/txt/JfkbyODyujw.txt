Speaker 1:          00:00          In this video, we'll look at the concept of regularization and uh, see what it's good for and how we introduce it into the training of a neural network.

Speaker 2:          00:09          Yeah.

Speaker 1:          00:10          So now in, uh, describing the full procedure for us, the Cassie, great in the sense where are at the step where we have to specify a irregular riser and a, and also how we actually compute the gradients of the regular riser with respect to our parameters. So we see the regular risers here. So it's involved in how we update the parameters of the neural network. The role of the regular riser is to penalize certain types of functions and specifically we will want to, it's best to penalize a neural network style are very complicated in the sense that they correspond to a function that is very nonlinear and uh, might be able to overfit on the training set that is essentially learn more or less by heart the information that's contained in the training set and not be able to generalize well to new examples. So we'll first look at a few, uh, common regular riser that are used for neural network training. And then we'll try to build some intuition for why it's actually important to use regularization in neural networks.

Speaker 2:          01:13          Yeah.

Speaker 1:          01:13          Now, uh, first regularize it, it's very common is l two norm regularization. Uh, typically we only regularize the weights, so the connection weights between the hidden layers and in Ltl regularization what we do is that we penalize for each a hidden layer, uh, and for each connection I j between the, uh, JF a neuron in the previous, uh, layer and the IIF neuron in the lyric, Kay, we will penalize the square of the weight value and other way too, right? This is to notice that this sum here actually corresponds to the, uh, for being this norm. X is the square of the food Venus norm there for being as normally is just the square root of the sum of the square of the elements in the Matrix. And so if we think the square of that, that corresponds to this whole sun here, uh, so this is essentially shrinking the weights towards zero with a squared penalty, uh, the gradient of the regular riser, uh, with respect to the weights for the KF, uh, layer.

Speaker 1:          02:20          Uh, it's actually very simple. It's just two times the weight matrix. So a, as an exercise we can try to do it, but you'll see it's very simple to obtain very easy result to the right. Like I said, usually we only apply it on the weights and not so much on the biases we don't expect to over fit the training set, uh, by changing the biases a lot and more by changing the weights, which really dictate how the function can become more or less nonlinear. And so more or less flexible for a fitting and a for those that are familiar with, uh, graphical models and, uh, the idea of probabilistic modeling, we can actually think of Alto regularization as, uh, a, uh, sending where we assume that the weights of the neural network, uh, came from a [inaudible] prior. So that's because of the square here. Uh, if, um, if you were to do maximum a posterial refitting of your problems stick model on the training set, then you'd have a term which would correspond to the sum of the square of the parameters. So that comes out of assuming that there's a gosh in prior and the weights, I want to discuss this so much, but uh, uh, this is a, a notion that people are familiar with probabilistic modeling and graphical models will be familiar with.

Speaker 1:          03:38          Then there's El one regularization in which case we replaced the sum over the square of the weight values with an absolute value. So we penalize the absolute value of the weights and a, if you look at the gradient, um, actually the grading, there's only well-defined for weights that are non zero. And uh, if, uh, so that's because the absolute value looks like this. And so where this would be zero. And so if we're here, we see that the gradient would be negative. So if the weight is negative than the gradient is negative. And otherwise if it's positive, then the gradient is positive and it's actually equal to one. This is a linear function with a slope of minus one, and this would be a linear function with a slope of one and at zero it's not defined. Uh, so, uh, uh, we own the, we essentially assume that the gradient is Dan zero, uh, to be more technical.

Speaker 1:          04:30          This corresponds to you taking a subgrade. And so, uh, if you ever see that name, that's just the idea that we just take the gradient of a slope that's below the function locally. And so if we had a function that's flat, it would be a, it would be below the function that the 0.0. So, um, instead of using a gradient, which is not the fine, we use the subgrade into zero. Um, so if you write this in math, the gradient of the regular riser is going to, with respect to some weights, it's just going to be two going to be the sign of the weights. So a sign of the weight, that's just a, we're going to define it as one. If the weight is greater than zero and minus one, if the way it is instead smaller than zero in otherwise the two conditions with not be satisfied.

Speaker 1:          05:17          So it would be the sine function would return zero for the case where the way to zero. Um, one nice thing with l one which is different from l two, you don't get this from l two regularization is that with a strong regularization, the weights I actually going to be pushed towards being exactly zero. And so at the solution, once the stochastic gradient descent as converge, You have a certain number of whites which are going to be exactly at zero, which can be understood as a procedure for pruning away some of the connections between the weights. So with Ellen regularization, it's kind of equivalent to learning which neuron should be connected, uh, to each other. And then we see also that if we remove connections between neurons, then implicitly that means that the neural network is less complex. So really by penalizing the [inaudible] a norm of the weights, uh, we should be getting a neural network, which is a less flexible and less able to overfit on the training set. And in this case, there's also a probabilistic interpretation where, uh, this is equivalent to assuming that we are putting a, uh, some, uh, La Plaza priors or plus in distribution as the prior distribution for what could have generated the weights of the neural net, which, uh, under our assumption as generated our training data. So again, I will discuss this more, but, uh, uh, these are, uh, topics that are concepts that are frequent in province stick modeling graphical models.

Speaker 1:          06:44          Okay. So why do we want regularization? Uh, and what we really want is not do well in the training set is actually do well on new examples. For instance, examples that would come that would be in the test set and a, so that's the, uh, we're interested then in the generalization performance of our model and, uh, it can be shown more formally that he, the generalization error of elearning or algorithm decomposes into a, the sum of two terms, a bias term and the [inaudible] term. Uh, so the variants of a learning algorithm a is essentially a measure of how much will would the model very if I changed the training set. So for instance, if I had an infinite amount of data and I was able to construct many separate training sets and then I ran my algorithm on these different training sets, uh, to what extent would the model very between training sets, to what extent would it be different than an correspond to a different, uh, f function that we're, that we're trying to learn?

Speaker 1:          07:50          In our case, a neural network. The bias, uh, will instead be, to what extent is the average model I'm, I would be getting by changing the training set. And so sampling many different training set. To what extent would the sort of average model, uh, be close to the true solution of my learning problem? That is the true function, which I could call f star of x, the true function, which really has generated these trainings, uh, samples and, and is behind the any new training, any new test example I would encounter and have to do predictions on. Um, and so to get the more intuitive understanding what this means, here's a cartoon illustration. So, uh, so the gray areas correspond to different neural nets. So each point in a gray area is a neural net that I would be able to obtain for a particular training set amongst all training that I could sample, uh, for a given problem.

Speaker 1:          08:49          So these are the possible F's, the possible neural nets I could be obtaining by training, uh, on a given training set and so forth. Different training sets, I'd be getting all of these different neural nets. And F star is going to be the point in my space of function, which corresponds to the true solution, the, the, the true function that actually generates my, uh, my labels that I'm serving. So in this case here, we have low variance because we see that the region of potential neural networks I could get by varying the training set is actually quite small, but I actually have high bias because on average or let's say maybe that's this point, the average sort of typical neural net that could be getting is actually quite far from the true solution. For that reason, I have high bias, uh, on the opposite of that spectrum, I have a case where I've had high Verun.

Speaker 1:          09:41          So the region of potential neural networks is actually quite large, but low bias because on average, maybe the point on average would be sort of here then that's actually quite close to, uh, what the true solution is here. So let's think about why this is bad. And this is also bad. So this is bad because, uh, in a given situation I get one training set so I can get a point that's anywhere here as solution. The neural net that can be anywhere here. So say it might be here, but because the region is large and I won't be very far from what the average case would be, so that's good. But unfortunately here the average case because I have high bias is actually quite far from what would be the best solution, which would be to report the true function at the opposite case because of a high by a high variance, uh, for a given training set, I could be very unlucky and get this neural net or I could get this neural net.

Speaker 1:          10:40          Actually the probability of getting a neural net, which is fairly close to the true solution is actually quite small. This region is very small. It's very small compared to the whole region of potential neural nets. I could be getting. So, even though on average I might be lucky and get something that's close to here, even though the average neural net, quote unquote, could be close to the true solution, because I have so much fermions for a given training set, it's fair likely that I won't be in this region. I would be actually quite far from the truth solution. So the best thing to do would be to sort of explore a trade off between low variance and high bias and high Verun slow buys. And so we have a trade off here, which seems fairly reasonable. So I have a bias because the average case is not a close to f star.

Speaker 1:          11:26          Uh, and I have some parents. So this region here is bigger than that region here. However, I see that, you know, for different training set, I'm always fairly close to what the true solution might be, the f star. And so, uh, back to the idea of regularization, uh, the amount of regularization, which is, uh, uh, controlled by the lambda wait, uh, which is multiplied by their regular riser. So it determines how important I think it is to minimizing the regularization penalty by changing diff for different values of, of the lender hyper parameter. Uh, I can essentially control whether I'm in Laverne's high bias region or high viral low bias region. If dilemma lambda, the penalization on the regularization is very high, then I'm going to get very little variation because, uh, I can only explore neural nets that have very small weights and are fewer neural nets.

Speaker 1:          12:25          Uh, we can't count them, but essentially the regional neuro that's with small weights as much smaller than the region of neural nets with small and big weights. And so with a big, uh, lambda, I'm in the, uh, uh, low variance a region, but with the small lambda them in the high variance region, low bias region. And so by trying different values of lambda, I'm able to explore this tradeoff between bias and parents and hopefully get a lender that provides the best trade off and better generalization. So this is the idea behind generalization and, uh, the use of a regularization in neural nets. And so now we know how to introduce two different regularization l two and l one into neural network training.