Speaker 1:          00:00          Yeah.

Speaker 2:          00:00          In this video we'll see the approach known as unsupervised retraining for improving the training of deep neural networks.

Speaker 1:          00:10          Okay.

Speaker 2:          00:11          So we talked, uh, before that Dart, two reasons why we can, uh, observe that a training deep neural networks is hard. One, which is, uh, related to under fitting, which might require to use better optimization methods. And, uh, if I need a second, uh, possibility, which is more associated with ora fitting, where would use better regularization. And so here, uh, what we'll see, it was one approach based on the corporation of unsupervised learning as a one approach for addressing this. A second hypothesis for why deep learning is hard.

Speaker 2:          00:49          So, um, the solution is actually fairly simple given what we've seen so far. Uh, the idea is just to change how we initialize the parameters of the hidden layers. And specifically we're going to use unsupervised learning to initialize the parameters of the hidden units in the, uh, hidden. There's d idea here is that, uh, we want the neural network to not just extract features or representation, which is used for classification, but also extracted in units that, uh, also extract a extract more information about what's, uh, the specificities of the input distribution. And in particular, we want to encourage neural networks that can represent this late instructor that's specific to the input distribution on which we're training. So for instance, if we're training on images of characters, uh, Dan, uh, we'd like it in units to contain information for why this is a character image and that this is different from data would have been generated from any other distribution such as a random distribution where each pixel is uniforms.

Speaker 2:          01:55          So which would generate this something like this, like a random image. So we want to add this, um, regularization that says amongst all neural networks that do well in classification, I'd like to favor a neural networks that also understand why my input data is, is special. And it's different from say, random data. And that's essentially what unsupervised learning is trying to do. We've justified restricted Boltzmann machines or throwing quarters as a training to sort of make that distinction in some way. And, uh, and so now we'll try to, uh, encouraged the mar deep neural network that Duke classification and also try to do this to some extent.

Speaker 2:          02:40          And so now we asking a bit more about, uh, our neural network and we're not just asking you to be good at supervised learning classification. Also asking you to learn what makes the input Datas and special. So this should reduce the variance of our estimating a procedure. Uh, and, and because of that, we, we should reduce the variance and dust, uh, uh, uh, reduced over fitting and by doing this well, so hope that we're not increasing the bias too much and it needed practice often that's what we observe in the sense that we do observe, improve generalization performance, uh, by incorporating this unsupervised learning idea.

Speaker 1:          03:20          Okay.

Speaker 2:          03:21          The specific procedure we'll use for doing this, uh, which is very common. Uh, you could use, incorporate unsupervised learning in some other way, but the procedure that's, uh, perhaps most popular and quite simple is known as a greedy, Leora wise pre-training. Um, and so the way this is going to work is that will effectively add hidden a hidden layers one at a time. And then whenever we're adding a new hidden there, we're going to train it's parameters locally using an unsupervised learning algorithm for a single layer. And so for instance, we could start with this neural network here where we just added our first hidden unit, a hidden layer, and then we'll pretrained or train these connections here based on some unsupervised learning algorithms like an Rpm, uh, contrasted divergence, argue with them or the noising autoencoder regular to quarter and so on. Then we do this from some iterations.

Speaker 2:          04:18          Then we stop and then we fix these parameters. So this is illustrated by having these grayed out. And now we add a, say a second hidden layer, and then we train this hidden layer using again and unsupervised learning algorithm like RBM learning algorithm, um, to, uh, but we train only that hidden layer on top of the data represented through the representation of the first hidden layer, which is extracted by using these parameters that we've pretrained before. So in other words, we're training say an RBM just on the same training said, but that's been preprocessed by this first hidden layer. And we do this for some number of iterations and then we continue like this for, uh, as many hidden. There's as we want to, we could pretrained then a third hidden layer, which is trained on the representation extracted by these, uh, two subsequent hidden there. So we trained on data represented, uh, by this hidden layer and so on. Paul, uh, as many a hidden, there's we want, so now not, we train layers one at a time using someone supervise objective. Whenever we're training a hidden, there were fixing the parameters of the previous, and then there's that have been retrained. And so we can view these other hidden, there's these previous hidden, there's as just feature extractors, uh, that provide an input representation for the unsupervised learning algorithm that we run for the current hidden layer.

Speaker 2:          05:49          Oh yeah. This procedure is known as unsupervised pre-training. If you read that into the literature, that's what river we're making a reference to. So the first lien hidden there will learn hidden unit features that are a characteristic or more common in the inputs in our training said, then say in random inputs or any other types of inputs. And then the second layer would learn combinations of firsthand layer unit features that are more common in the trainings, uh, sets, uh, in the training set examples, then say in random, a hidden first hidden layer, a unit features. And then third layer would learn combinations of combinations and so on. And now the hope is that this initialization procedure, this pre training procedure will initialize the neural network in their region. The parameter space where, uh, the local Optima we will reach when we're doing subsequent backpropagation, uh, will over fit less, would correspond to a congregation that, uh, is, uh, uh, it will, uh, els overfitting. Then if we add an initialized in the random, a neural network parameter configuration.

Speaker 2:          07:06          So like I just said, after this, uh, after, uh, onset by speech training, we just do regular backpropagation. This is often referred to as fine tuning. So, uh, once we've trained say our first and in there, and then the second and the third one, we add an output layer and we train their whole narrow network using supervised learning bike backpropagation. So regular backpropagation, uh, in the feet forward and roll network. Um, so again, we call this procedure often refer to it as fine tuning, uh, the idea. So the reason for that being that the parameters are most of them are pretty much all train really. Uh, they're not going to move that much. They've got to move somewhat, but not as much as safe. We started from a random initialization, so really the parameters are tuned, uh, four to supervise, supervise task at hand. And so in other words, the representation is going to be adjusted to be more descriptive for the particular classification problem.

Speaker 2:          08:06          I'm interested in, one nice feature of this pretrade unsurprised p training followed by fine tuning is that if we have a lot of unlabeled data, we could do pre-training, a more data than we do find tuning or backpropagation where we need labeled data. And so, and other sort of, uh, uh, advantage we get from this procedure is that if we have not a lot of supervised data labeled data would a lot of unlabeled data, then we can get a, uh, improvements by doing pre training on the big amount of unlabeled data and then fine tuning on the, on the small training set with supervised data.

Speaker 2:          08:49          So here's just a pseudo code to pass, make everything a bit more precise about what I just said. So the pre training parts iterates from the first hidden layer to the last we build and unsupervised training set. That is, we take all of our training inputs and we ignored the labels if, if it was available. And then we compute the representation that the previous a hidden layer, so four l equals one h zero is just going to beat the actual input, the right input. And then a four l equals a source. That's four l equals one four l equals to a, then we'd be a computing h, one of XD here. So we construct this unsupervised training set. We feed it to what we sometimes call the greedy modules. So that's just uh, unsupervised learning algorithm that can be trained, that trains a single hidden layer based on a representation that we compute here.

Speaker 2:          09:45          So it could be an RBM when it was one quarter learning off of them. And then we use the hidden layer of weights and biases of that really module, that RBM or that auto encoder. We train on that Dataset to initialize the parameters for the, uh, lef. A hidden layer in our deep neural network will want to find too, once we retrain it all hidden layers. Then once all hidden, there's our pretrained, we add connections to an output layer. So that's w capital l plus one and B capitol. That was plus one. We can internalize them randomly as usual. And then fine tuning is just a supervised learning using backpropagation with ses to Cassie gradient descent. All right, so that's unsupervised pre training for learning deep neural networks.

Speaker 1:          10:39          MMM.

Speaker 2:          10:40          Now you might wonder, uh, what kind of a unsupervised learning I'll go to them. You could be using, uh, it's during quede. This unsupervised pre training procedure was invented and they're proposing these papers by Geoff Hinton and his colleagues. Um, and that's where the suggested using rbms there was a whole rationale for why rbms were at the right algorithm to use for doing free training. But then later on, uh, different people, including myself, uh, proposed to in stent stack or throwing quarters instead of stacking rbms. So, uh, and, uh, the paper I was involved in, we proposal to in quarters and in this other paper, but yeah, people that Nyu, uh, uh, liquid in a macro hairdo, hands, Eto and a young liquor sparser twin quarters, we're instead proposed. But essentially that the idea of us was very similar. The algorithms were different, but we were in both cases suggesting that we use another algorithms, then a algorithm then restricted Boltzmann machine for initializing yeah. Deep neural network.

Speaker 1:          11:42          Yeah.

Speaker 2:          11:42          Um, then later on d nosing or doing callers were proposed. They were actually proposing that context, the context of training deep neural networks, uh, and not just for extracting in general. And there's been a, also a lot of other papers suggesting other types of unsupervised learning for p training. Uh, so there's an example here using the idea of a semi-supervised embeddings or Colonel Pca or independent subspace analysis. Again, in all cases showing that there were some gains from doing this, from stacking these modules of unsupervised learning. And so there's been a lot of research and pretty much every time we'd see that unsupervised learning was, was being helpful. So this, uh, essentially confirms that it's really the general idea of unsupervised learning for initializing the parameters of a deep neural network that works well. And the choice of the learning algorithm will make some difference. But, uh, uh, but, but really the main characteristic that they all share a is that they're unsupervised learning algorithms.