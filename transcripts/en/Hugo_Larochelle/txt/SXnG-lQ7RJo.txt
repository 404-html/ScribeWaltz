Speaker 1:          00:01          In this video, we'll look at some example of results we can get using unsupervised pre-training.

Speaker 1:          00:09          So we saw the procedure funds provide speed training. What we first do is that we initialize each layer from the first hidden layer to the last hidden layer by training a, uh, what we call it, greedy modules or either an rpm or an auto encoder, uh, on a dataset that was generated by transforming the input data into the representation of their previous, uh, they didn't layer. And then we use the weights and the biases found for that hidden there that was trained using an RBM when a tone corridor as the initial value for the parameters of that hidden layer in our deep neural network they want to use for classification. And then we initialize randomly the last part of the neural network, the output layer, and then we, uh, training using regular back pro, which is referred to as fine to me.

Speaker 1:          01:03          So what we'll see is a few results on a training deep neural networks on deep, uh, on different, uh, image classification problems. So these were generated by myself and my colleagues, uh, to get an idea of the performance of, of uh, deep neural networks in different settings with different p training algorithms and different configurations are architectures of the neural networks and so on. So we had a bunch of variations on DM, this data set where we added rotations. We see here characters with different amounts of rotation. The incorporation of a random background or background extracted from a, an actual image, uh, taken from the web and a combination of a rotation and some background. I also had another classification problem, uh, visual classification problem where we have a rectangle in some image and we have to say whether it's taller or wider when, since this would be a, a wide rectangle and this would be a tall rectangle.

Speaker 1:          02:04          So these would be two different classes and this would be binary classification problem. And then the harder version of that problem where the rectangle correspond not to a set of black and white pixels, but instead of different, uh, so there's a background and then there's a patch corresponding to the rectangle in question, which was extracted from a different image. And then we have another binary classification problem and we have to say whether the shape in the image is convex to, for instance, in this case, this is not convex because I can draw a line between say that white pixel and that white pixel here and this line goes over a black pixels so it doesn't stay in the white shape in the image. And whereas this would be convex shape. So these would be the two classes for this classification problem as well. We trained deep neural networks on these different data sets and looked at the result and see what kind of conclusions we could make out of them.

Speaker 1:          03:00          Okay. In this first experiments, uh, we'll look at the impact of initialization. So we considered the deep neural network without pre-training. Uh, another one with pre-training using irregular auto encoder or portraying using the RBM. And in each case we considered a depth. So number of hidden layers of one, two, three and four. So I guess this would be one would be, wouldn't be very deep, but of course two, three and four would be considered the neural network. And then we performed experiment on amnesty, the original feminist but a which we referred to as small a. So we just essentially reduced the size of the training sets, uh, but it's essentially original m this digits as extracted from the original Dataset. And we compared with a, and also did experiment with the rotation variation that we generated. So there are are three different conclusions we can draw from these experiments.

Speaker 1:          03:54          The first thing is that a deep neural network actually helps. So in all cases the performance without a, Oh sorry. It actually pre-training helps. That's the first conclusion. So if, uh, we don't use any portraying a four m, the smaller we get about 4% performance. If we use auto encoder for training, we're more at 3.3% a error. And then with RBM you get even better at 2.7. And then a similar conclusion for say a the rotation where we get better and better performance, better performance in general and for use be training either doing quarter in RBM. The other conclusion we can draw is that pre training also helps training success, uh, with success. A deeper networks, indeed the deepest neural network. Well, I guess the, um, the shallow is neural network with a performance that's as good as the best performance we can get in this case would be 4%.

Speaker 1:          04:55          So this is statistically undistinguishable from these two results. If with two or three hidden layers in this case, however, uh, training to hidden, there's is better than just one. And similarly here for m this small using either autoencoder quarter and RBM portraying. So we see that we've been able to train with success in their own network that had one more hidden layer using their unsupervised speed training procedure. And we actually get the same conclusion on this rotation where the best we could do is to hidden layers here. And then three with retraining, either with auto encoder on RBM. And then the third conclusion is, um, that, uh, by going from a somewhat simple problem, the original Lemonis problem to a harder problem, which is to classify rotated digits, the best performance we obtain was actually a, with a more with a deeper neural network for them rotation problem.

Speaker 1:          05:58          Then for the original end, this problem. So in the, we went from one hidden layer to to hidden there as our best result from two hidden layer two 300 in there as our best result here and two to three also with the RBM be training. So this, uh, sort of confirms our intuition that for more complicated problems we should have more success and get a better result using a deeper neural network. When we went from m this small, do you have this rotation using one more hidden layer actually helped. Okay. So this sort of justifies even more the use of deep neural networks

Speaker 1:          06:37          and these experiment from another paper by these guys here. Uh, they did experiment with one, two, or three hidden layers with, uh, either RBM pre-training or a, in this case they use de noising autoencoder pre-training or no pre training at all. So the NOPA training would be the black line. And what they did is that the varied the number of hidden units in the neural network and saw with just one or two or three hidden layers, uh, what was the progression of the performance on a, the test set performance in terms of generalization. And what we see is that if the neural network has little capacity, so a few hidden units, um, then pre-training doesn't help. We get better test error without retraining then with training. But as we increase the capacity, eventually these two set of lines cross and now without retraining over fits.

Speaker 1:          07:33          So it has a, uh, worse test error then with be training. And we get the same shape either with, uh, two hidden layers or three hidden. There's so that confirms that really we're getting a preacher tiny acts as their regular riser because if the neural networks deep or not has few parameters, so little capacity, then, um, much like any regularization will we expect to see is, uh, under fitting a, so less, uh, so worst result in terms of test Eric was that as we increase the capacity, then the better regularized model starts doing better. And in this case, the better regularized model correspond to the neural networks with a free training

Speaker 2:          08:17          here, here.

Speaker 1:          08:20          Okay. So again, this confirms our intuition that we can think of retraining on surprise speed training as a rigger riser. It will, uh, over fit less with large capacity, but it will lead to under fitting with small capacity.

Speaker 1:          08:37          This other experiments is a one where we try to see what was the best architecture, uh, to use in a deep neural network with three hidden layers. So, uh, if we have three hidden layers, we have to choose the number of hidden units and each hidden there, that can be a bit cumbersome and annoying to do. Uh, so here we considered three types of choices. One where we would as we go deeper, decreased the number of hitting it so they keep decrease. The width of the hidden there. So decreasing with would be a hidden there, kind of like this,

Speaker 2:          09:10          like this movie.

Speaker 1:          09:13          And the constant would be the same number of heating units each in there. And then increasing would be more and more hidden units as we, uh, get closer to the output layer. Okay. So these, this is what these three set of three lines mean here. And then we considered the performance for different number, total number of hidden units and the neural networks. And uh, and here we have the performance either on em, this small or an end, this rotation. And here we have a performance for each column, either with RBM free training or Toyland quarter paternity. And the general trend seems to be that using the same number of hidden units, uh, so constant with, uh, tends to do, uh, either, uh, at least as well or better than other options. Um, so normally we see that the often the confidence intervals between the different performances overlap with, uh, the constant with and, uh, or it's actually doing a better say for instance in this case. So this is kind of reassuring. It means that perhaps we can just, uh, uh, at least in our first experiments, concentrate on neural networks with the same number of heating units and just try different number of hidden units for each, uh, hidden there.

Speaker 1:          10:39          And he rubbed the performance on all of the data sets I mentioned before. So that just in this rotation and, uh, and this, uh, small for either a Svm, so I'm not going to describe what it is, but it's a, a very popular type of classifier and machine learning. And here we're using the colonel Verint within RBF kernel. And here we have the results with pre-training, with stacked auto encoders rbms or the nosing two quarters. And we always use three hidden layers. So, first thing we see is that deep neural networks, either the A, what I call DBN here with is really just a stack of rbms for p training or, uh, stacking the nosing twin quarters tend to do better than using an SBM, which is, uh, which correspond to a very good but shallow class classifier. And so not a deep classifier. Um, we see also that, uh, auto encoders tend to perform less well than either rbms or the noisy motor in quarters and rbms they dinos and auto encoders, uh, tend to do about as well.

Speaker 1:          11:43          So Tom, sometimes the free training that RBM works better than de noising and sometimes it's the other way around. Um, and so in practicing, we do observe that they tend to be somewhat equivalent than in terms of the performance, but very on, on various dataset. It might be that one will work better than the other. So, um, so they're both really good reasonable options for a pre training, whereas regular auto encoders, uh, might not, uh, is not worthwhile compared say to just de noising or two in quarters. So this confirms what we discussed before, that the nausea and auto encoders are really a way of improving on the regular auto in quarter. All right? So that ends our examples. So, uh, we see that unsupervised training is helpful. It helps in terms of regularizing the model, and it also allows us for more complicated task to get better performance with a deeper neural networks.