Speaker 1:          00:00          This video will introduce and define the sparse coding model.

Speaker 1:          00:05          So sparse coding is a model in the context of unsupervised learning. So a, that's in the context where we have training data, which is not labeled. So we only have a set of, uh, x factors in our training set. And a, it's going to be a model which would also help us extract some interesting features out of, uh, some training set, uh, and allow us to perhaps leverage a large set of unlabeled data, see a lot of unlabeled images that we extracted from the web. And uh, as, uh, so we've seen before, uh, other neural networks for, uh, unsupervised learning or we seeing a wreck suspected Bolsa machine. And you know, it's one quarter in the previous videos. And so now we'll look at another, uh, as far as calling, sorry, another unsupervised, a neural network, which is known as this par scoring model.

Speaker 2:          01:00          Okay.

Speaker 1:          01:01          All right. So the idea behind sparse coding is that for our, uh, for any input x, we want to find a latent representation. So a hidden layer, h t ah, till is to note that it's the hidden layer for that training example. And A, we want to lead the representation that is first a sparse, which means that in the Layton representation we want there to be many Zeros, the only a few, none, zero elements. And uh, we also want the representation to, uh, contain meaningful information about x t. And that we were going to formulate that is that we want a liter and representation to be able to reconstruct the original input, uh, as well as possible. So to translate these two, uh, uh, concepts that we want to implement in sparse coding, uh, we'll translate that into an objective function, uh, which, uh, which we see here. So first, whoops.

Speaker 2:          02:04          Yeah.

Speaker 1:          02:06          So first we want to be able to reconstruct the original data as well as possible. And so for that reason, uh, will, uh, enforce XD, uh, we won't XD to be close to a reconstruction a, which is going to be my Leeton representation times a matrix of weights and inspires coding. We usually refer to that matrix of weights as a dictionary matrix. And we'll see a little bit more later why we use the term dictionary. And so this we will call our reconstruction and we want the reconstruction to be as close as possible to uh, the original input.

Speaker 2:          02:48          Okay.

Speaker 1:          02:49          But we also want that latent representation to be sparse. So for, because of this will penalize the l one norm of a delete and representation that we've seen before. We mentioned the l one norm, actually, uh, uh, we'll force certain elements to be a exactly zero. So we'll achieve our, uh, our goal of getting a lead and representation, which contains many Zeros. And now this lamb, the term here is going to control to what extent, uh, we wish to, uh, get a good reconstruction error compared to achieving high sparsity. And so these two objectives sort of fight each other. Um, the sparsity term, we'd be very happy if all the ht vector was only Zeros. However, the reconstruction would be very bad. It would be always zero. On the other hand, we could get a perfect reconstruction of XD if we had no constraints of what the value of ht could be.

Speaker 1:          03:46          The later representation. However, the l one, a penalty here would not be happy. It would be high, uh, because presumably the values in age who need to be, uh, you know, none, zero and perhaps fairly large to get a good reconstruction. So this is to control the trade off between a good reconstruction and sparsity in the representation. Okay. And so that's the objective. We want to optimize, uh, and one to optimize it for each training example XD. So we'll have a sum over all the training examples and a notice here that the loss is, uh, requires a minimization over the later and representation. So for each XD we, uh, wanted to, uh, we want to optimize our problems such that if I tried to find the best representation that reconstruct, well the input and sparse and is also sparse, then the sum of the reconstruction term and the sparsity terms going to be as small as possible.

Speaker 1:          04:48          And so at the, uh, outer loop of this some, what we do is for all the training set, we want to find the dictionary Matrix d, uh, which is such that it's going to lead to a very small reconstruction. And the sparsity a combined with as far city penalty, a very small loss for all the training examples. So that's all we're going to formulate our a learning problem. So, uh, we're going to formulate as this optimization problem here. Um, notice that we have to constrain the columns of d somehow. That's because, uh, say we are penalizing age if we're not constraining dean anyway, uh, then we could increase the size of the elements in d and the decrees accordingly. The size of the elements in age and that would make the sparsity determine, are more happy with it would become lower and lower and lower.

Speaker 1:          05:42          And so it could sort of cheat by transferring the size of age into d and then a satisfied those parts of the penalty. So to avoid this and get a badly, a defined objective, uh, what we'll do is that will the columns of d to have a norm of one. So all the vectors that form the columns at the got up norm what? Um, and uh, I've sometimes in the literature you find that people instead a for the constant, they'll just constraint the norm to be no greater than one. It's just an alternative formulation. But the idea behind it is a, is the same.

Speaker 1:          06:21          So, um, if, uh, if we remember a d autumn quarter that was discussed in previous videos, well we can think of Ddi as the equivalent of the auto encoder output weight matrix. So indeed it takes a, they don't representation and uh, and uh, our auto encoder weight matrix multiplies that and that becomes our reconstruction. So it's kind of like we have in the twin quarter with a linear output activation function. What changes though is that, uh, the way we are going to obtain the latent representation for an input, it's gotta be much more complicated, uh, for, uh, as part squirting model. So in no color, it was just a linear transformation followed by nonlinearity. Uh, but in spar schooling, uh, the age for which will be measuring and counting this loss is going to be the age that minimizers does some of the loss and the sparsity term. So another way of seeing it is that a d h term here, uh, which depends on XD is the argument of the, some of the reconstruction term and the sparsity term. So now the encoding function, given some input is no longer the inner activation followed by knowing the narrative, it's the result of the more complicated optimization problem.

Speaker 1:          07:46          All right, so, um, let's talk about the dictionary. Why do we call this a dictionary and what does it represent? Um, so we said that the x had the reconstruction is just my, a latent representation extracted from D, which is the result of optimizing, finding the best code in terms of reconstruction and sparsity. And then multiplying that by d. That's my reconstruction now because, uh, this is spars. Uh, it means that many of its elements are going to be zero. And, uh, so it means that we can write it this a term here as the sum over all k four, which the element in the late and representation, the Keith Element is not zero of the column. The key if column of the Dictionary Matrix Times the value of decay if elements in the late and representation. So, uh, if we think of this as some sort of texts, then this is saying that, uh, were, are decomposing our texts into a sum of a bunch of words, which would be the column of d.

Speaker 1:          08:54          And, uh, the sort of quote unquote frequency of that word is going to be the, uh, value of, uh, the latent representation and so forth. Those are zero. It's kind of like saying this document doesn't contain that word. Uh, instead of calling the dictionary elements words, we often call them atoms instead. But, uh, you can see that dictionary is essentially, it's kind of like a language with which you're going to try to describe your inputs that your observations and you're going to assume where, assuming that each observation is a composition of a fairly limited number of these elements in my language, in the dictionary. So for instance, if we run the spar scoring model of uh, images of characters, well one thing we could get a, so by the way, this is another Australian from a paper by Mark Rela was runs Thatoh, um, and colleagues.

Speaker 1:          09:49          So if we had this image, then one thing we could obtain is that this image can be decomposed as the, uh, as a linear combination of the different pen strokes. We have a pen stroke here, one like this one, a little bit similar, another one here like that. Then more like the tail of the seven. Another one like this one, maybe like that. And uh, so we see that we've decomposed this fairly complicated image in terms of pixels into representation, which just identifies which pen strokes are present, uh, into, in this image of a character. And a, and so if we think that our complex observations, which might be very high dimensional, can all be decomposed into uh, uh, sort of language. So a sparse representation where everything is just a linear combination of a few vectors and as far as coding is going to be a good option for representing that type of data.

Speaker 1:          10:45          And then now we can use the h vector, which is mostly going to contain zero except for the non zero elements, uh, as a representation say for performing classification. Um, so in certain application, uh, will actually know what so good a dictionary to use. Uh, but in general we will actually want to adapt our dictionary or learn it from data, learn a good dictionary for describing our data. And so that's what we'll see in the coming videos. And, uh, just to be explicit. So now these little image, they would correspond to the images we have here of each character. They would correspond to the columns of d and then the values here, they correspond to the Leighton representation values. Now, this example, they turn out to be mostly one, but they can, we'll generally value a bit, a very big, uh, between, uh, uh, they can be either positive or negative. In this example, it happens to be positive, but it can be anything between Madison 20%. Venti. It's just that many of them because of sparsity term, are going to be zero. All right? So that's just, uh, uh, intuitive and, and, uh, well formal and then a intuitive, uh, description of sparse code in model.