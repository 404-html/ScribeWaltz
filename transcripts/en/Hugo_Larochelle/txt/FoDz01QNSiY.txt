Speaker 1:          00:00          Yeah.

Speaker 2:          00:00          In this video will introduce the neural network language model.

Speaker 2:          00:06          The previous video we discussed, the end Graham models and particular, we talked about the fact that a while in an application to get a, a good model we want and to be large, if n is large, then we started suffering from the problem data sparsity which means that it's quite likely at test time that will be faced with a context corresponding to words that we've never observed that training time. And while there are some smoothing approaches for improving in grand models, uh, uh, these can help them model before much better. But, uh, data sparsity is still an issue. And so what we'll see in this video is a neural network language model approach to this problem, which is I actually going to do, uh, perform better than a regular and grand model.

Speaker 1:          00:55          Yeah.

Speaker 2:          00:57          So the idea is actually exactly what I've explained before in the context of learn a word representations. We're going to train a neural network that will take a representation of the context and at its output is going to produce the conditional distribution of uh, the next word to the word appearing to the right of the context and add the input of their neural net. We'll just put the concatenation of the vectorial word representation of each of the words in the context. So in more detail in the neural network, the first thing that we do is that we take the ideas of each words in the context and then we look at, uh, our lookup table to extract the vectorial were representations of each of these word ids. So in other words, we take the ID here and we look at the corresponding row in the Matrix C and we put that as the first vector in the context.

Speaker 2:          01:58          And we do this for all words in the context concatenating than each of the word representation to get our x vector, which is going to be the input of our neural network. And then the rest of the computations is the, are the regular computations we do in a feed forward neural network. So we take the factor x consisting in all the concatenated word representations, multiply by the Matrix, uh, say w here, plus the bias than some nonlinearity. So in this work, tench was used. And then, uh, to get an output, we just multiply that hidden layer by the connections between the hidden layer, the output layer, and then apply a softmax nonlinearity to get at the outputs, the distribution for the next word. So the eff output. So the earth element and that output layer here is going to be the probability that the next word, wt is the word with Id. I. So, um, this model was proposed by Yoshua Bengio and his colleagues in 2003. It's also in this work that the idea of learning word representations was first presented. And, uh, and again, in this model then when we train, it will actually train not just the parameters of the neural network, but also the parameters inside the matrix. CP word representation of the different words.

Speaker 1:          03:26          Okay.

Speaker 2:          03:27          Now the reason why this model is better at finding the data sparsity problem for large n four large contract sizes is that, uh, dinero electric is actually potentially able to generalize to context that have not been seen in the training set. Uh, so as an example to illustrate why, so imagine that we wanted to, uh, evaluate the probability of seeing the word eating after seeing the words the cat is now imagine that the four gram the cat is eating, which we would need in their regular and Graham model in order to assign a positive nonzero probability, uh, for the property you've seen eating after the cap is. So imagine that this foreground is actually not in the training corpus, but imagining that the dog is eating is actually in the corpus. Now, if the word representations cat and dog are similar and if the neural network has learned to predict a good probability of, uh, seeing eating after the dog is then if the road representation of Canon dog is similar, then it should also give a good probability, a similar probability, uh, in the context where instead of dog, we have cap.

Speaker 2:          04:46          And that's because since the part of the input that corresponds to cat dissimilar to the part of the input corresponds to dog. And since for the other words in the context then is they're exactly the same words, then they shouldn't, the neural network should have a similar output and does generalize way well to that case. Now then you might ask, well, if the neural network in its training data as in seen the cat is eating, but as seen the dog is eating, why you should it actually learned that cat and does should have similar representations. Well maybe in the training corpus a while it hasn't seen the cat is eating, it actually has seen the cat was sleeping and then maybe it also has seen the dog was sleeping. So from these two observations, and it as learned that for both the cat was sleeping and the dog was sleeping, it should, uh, have some, uh, meaningful probability of seeing sleeping, uh, after the cap was and after the dog was.

Speaker 2:          05:43          So that's a signal that perhaps the representation of cat and dog should be similar. So because of this, because cat and dog, maybe in the training set in other contexts, uh, Kevin and Doug would appear in similar context, then it should learn similar representations for cat and dog. And so for that reason, we can expect that we'd be able to generalize well for this example, even if the specific context is not, this exact context has not been observed. So in that sense is not as much rely. We're relying on, uh, explicitly seeing exactly the given context for which we have to value the probability for that reason. Uh, it should be able to generalize to these new context.

Speaker 2:          06:29          Now a few words about actually performing, uh, backpropagation and in this model now, uh, we already know how to compute the gradients of in particular the linner pre activation of the hidden layer, which is just this. So l will be the last, we're optimizing, we're optimizing the negative log likelihood. So minus the log of the probability of the next word. And so with regular backpropagation, we are both to compute that gradient. Um, so, um, now what's next is that we have to take that Graydon and back, propagate it to, uh, the, uh, word representations. Now in my notation, I'm going to know w I capital w I as the sub matrix that connects the a word with relative position. I with respect to the word, we're trying to predict that when your team, so w capital w I is the matrix connecting wt minus I with the hidden layer.

Speaker 2:          07:32          So we can show that the gradient of the, of any word representation, CW. So for any word w is going to be, so this expression, uh, where we sum over all position relative positions in the context from the Ipos one. So that's this word here to I calls and minus one. So that's this word here. So we some over all relative positions. And then if the word at that relative position is the word w. So this w here, then we add the gradient at the hidden there multiplied by the transpose of the part of the Matrix, the part of the uh, uh, connection Matrix, uh, that connects this, uh, the representation of the word at relative position. I with the hidden layer. So that's w hi. So that expression should not be so surprising. We know that if we, uh, we've seen in the regular neural network that if we compute the matrix multiplication, uh, so for instance, we take that hit in there, multiply by a matrix, then the gradient with respect to, uh, the hidden there is going to involve multiplying the gradient at the layer above, uh, by the transpose of the connection.

Speaker 2:          08:51          So for the same reason, that's why we see these terms appearing. Now the reason why, and I remove some mink. The reason why we have this sum here and we have to some over all position is that the word w might actually appear in more than one position in the, in the context. And also it might actually not appear in some given context for some, uh, given training example, a corresponding to one context and the following word. So for each time it appears we get this great income contribution and then we accumulate those to get the gradient of the word representation. So that's for computing the great end of the word representations and of course the gradient for all the other parameters in the neural networks. So the matrix w here than matrix here and so on, uh, all of these, uh, uh, gradients that, uh, we need to compute, uh, they were computed as usual in a neural network. And I'll just add some notes. Um, so these dotted lines here is just to note that, uh, and that's something that they considered in that paper. We might have also direct or direct connections between the input layer, the output layer. That's a potential, a modification to the neural net there that one might consider. And if you want more details about, about that, I suggest you go look at the, at the corresponding paper.

Speaker 2:          10:14          So let's do an example of, uh, the gradients, uh, computing the gradients for a given context. And it's, uh, next word. So imagine we're training the neural network and now we're, uh, we're trying to infer the probability of observing cat after seeing the context, the dog and that. So a, for this training examples. So this whole sequence of techs corresponds to one training example, far neural network. We are trying to optimize the loss of a minus the log of the probability of seeing cat given all the people's words. And so this is, we see here that this is a five gram model because we're a concerning the four previous words, um, not imagine that the, which is w three here as an ID of 21 Doug, which is w four idea of three and w five idea of 14, and the which is also present that the pus the absolute position six.

Speaker 2:          11:16          Uh, so that's w six also again, has the ID 21. Now if we're interested in what is the gradients for the word with ID three. So that's dog. Uh, then it's going to be the gradient with respect to the reactivation of the neural network multiplied by the transpose of part of the input connections that connects the a word at relative position three. We take the transpose of that. Then that's what we multiply with the grading of the pea activation. So indeed dog is that relative position three. So that's three to one, that would be four. So that's why I'm taking that gradient of the activation of the hidden. They're multiplying by a w three transpose for 14, which is end. It's that relative position too. So we take the same reactivation gradient and multiply by the transpose of w two and now four 21 it actually appears that two positions that is at relative position, a one so right here and four right here.

Speaker 2:          12:26          So we have to add these two uh, back propagated gradients. The reactivation graded multiply by w transpose and then preoccupation Graydon again, multiply by W for Trans folks. And then for all the other words, w all the other words in our vocabulary, then they're gradient for this particular training example would be zero. And so it means that for the vast majority of word representations, um, there gradient for that training example is going to be zero, so we won't actually have to update them. So in our implementation, we should only update the representations c three c 14 and see 21. So a, instead of updating the whole Matrix C, we should just update these role, uh, rows in the Matrix c, uh, so the third row, 14th row and 21st row based on these gradients that we have here. So that's important to do this to get an efficient implementation.

Speaker 2:          13:28          So when we're evaluating a language models, a common evaluation metric that's uses the perplexity, so it's, uh, an information theoretic measure. I won't describe it. I'll just say that it's essentially the exponential of the average negative log likelihood of some given dataset. So we, you know, what's the average negative log likelihood? So it just take the hands, financial turns out it's deep perplexity and um, um, so again, the lower the, the number of the, uh, the better, much like the average negative log likelihood. And so in these experiments reported in that paper on the Brown Corpus, uh, which is a well known data set for uh, for language modeling. If we take a engram model, which is smooth using a state of the art, smoothing a method known as niece, her name, we got the perplexity of 321. And with the neural network language model, we get a much lower perplexity of 276. So a gap of 50, like this almost 50 is actually quite big in the realm of language modeling, uh, performances. And then, um, in language modeling also usually combining models, for instance, creating an assemble of the neural network and the engram model by just taking the average of the probability of each model also gives an even better performance. So we see that by just with the neural network we already do, we do much better. And then if we already had an inquiry model, combining it with the neural network actually adds even more

Speaker 2:          15:07          now a more interesting but a bit more difficult to do in practice and less straight forward way of evaluating language model is really do validated in the particular application that uses a language model such as a machine translation system or speech recognition system and, uh, later work. So the initial work on neural net language modeling was in 2003, but, uh, uh, actually much later, it's been shown that, uh, you can get a really, really good performance of, uh, with a neural network language models that incorporate into some standard, a model that does either, uh, speech recognition as in this paper or statistical machine translation as in this paper. So I encourage you to look at these references if you want to see examples of the gains you can get from using a known neural network language model and in some given NLP system.

Speaker 1:          16:07          Okay.