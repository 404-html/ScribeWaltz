Speaker 1:          00:00          And this video will introduce the notion of a mark off network we've seen before that a conditional random fields can be written in a slightly different for who would we seem perform, uh, as a, essentially a product of some factors. And in the case of a linear chain conditional random fields where we have urinary potentials which correspond to neural networks, then these factors would or corresponds to a with crisp onto these terms here, what would we have urinary factors. So the factors that involve this single y and pairwise factors where this factor involves two adjacent labels, two wives.

Speaker 1:          00:45          So these would be the urinary factors. These will be the pairwise factors. Uh, a very useful visualization of a given conditional random fields model is known as the mark of network. So it's a network where the nodes are the random variables in our model and a edges take, uh, uh, our construct and I added into the network, uh, for, uh, uh, based on a very specific reason. So the way we construct a mark of network for a conditional random fields, which we've written as a product of factors, is that first we would write all of the different, uh, so we have a node for each of the random variable in our models. In our case, each of the labels. If we had the second sequence of uh, uh, length five, then we have y one y two y three by five, as well as a node in this case for each of the input vector x one x two x three x four and x five.

Speaker 1:          01:45          So now, uh, then we would add edges for, uh, between each random variable that shares a factor in the product of factor form for our conditional random fields, for the linear chain, a conditional random fields. Then this means that, uh, we would have, and also for a context size of a radius, one context window, read this one, then we'd have these diagonal edges between each label, why? And it's input at its left. So then we'd have also an edge between each label and the input at the same position. So those are the, uh, vertical edges here. And similarly we'd have edges, a diagonal edges like this between each y and the label too. It's right. So those correspond to the, this factor here, the factor between why K and x, k plus one. So those were all of the [inaudible] factors. And then for the pairwise factors, they would correspond to the original, uh, edges here between each why gay and its neighbor white k plus one.

Speaker 1:          03:01          So, uh, what's useful, uh, with the mark of Network visualization is that, uh, we can very easily identify sooner unconditional independencies between, uh, the random variables of our model. So, uh, for instance, uh, he, the mark of network respects, uh, what's known as d local mark off property, meaning that in Denmark of network, uh, a random variable is going to be independent of all other random variables that, uh, are not, it's labors given its neighbors. So for instance, if I'm considering this random variable, it's neighbors in this graph would be this one, this one, this one, this one. And this one, well, the mark of a local market property tells me that why three is going to be independent of y one x one y five [inaudible] given it's a mark of a blanket. So it's neighbors y two x two x three x four. And why four?

Speaker 1:          04:08          Okay, so that can be useful if uh, we, uh, want to determine in our model whether a certain random variables are dependent on each other or independent given other variables that we might observe. Sophia, for some reason I was observing sore in the conditional random fields. All these Upserve x one, although the inputs, so that's x one x two x three x four and x five. So I've used a double circle dinand fied those notes that are observed when imagined that also happened to observe in the sequence y two and Y for. So maybe some, uh, for some reason I actually have this information, well the local market property would tell me that d distribution over y three given these five nodes actually does not depend on why one. So I can just ignore, ignore it and compute the conditional, which is only going to depend on these four or these five nodes. And the particular, the probably would of course depend on the particular value of y three, four, which I want to compute the probability.

Speaker 2:          05:12          Yeah.

Speaker 1:          05:13          In fact, we can say something even a bit more general, which is, uh, if I have two nodes, well then there are going to be conditionally independent if all the paths between the two nodes, uh, contain at least one of the condition and notes. So one of the node that, that I might be observing and I'm conditioning over. So, uh, for instance, if I'm wondering, so imagine in the conditional then and uh, again, uh, at, uh, when we use it while he's observe all of the input. Now I imagine that, uh, I'm also observing why three and I want to know whether a, why. One depends on why five in this case, there's one path between why one and Wifi. So it's this path. Actually I could, there are other paths I could go like this. And so there's more than one path. I could go like this and like that.

Speaker 1:          06:09          And so on. Well, it turns out in this case, we can easily see that each path between y one and y Phi actually contains one of the note I'm conditioning that is either contains one of the inputs. So each path that goes through an ex node, a would be blocked by, uh, by, uh, the Ex, uh, note that subserve that I'm conditioning on. And if I'm assuming I'm also observing this, well then any node that stays up like this and goes through white three a is going to contain conditioning notes. So it's going to be locked in a sense. And so for that reason, we could say that y one and y fi the is, uh, is independent conditionally independent given x one, two x five. And why three? So that's a useful property of the markup network. Uh, it allows us to identify a lot of a conditional and dependencies very easily.

Speaker 1:          07:09          Another thing that's important to understand about conditional random fields is that they're known as undirected graphical models, uh, to be distinguished from a directive graphical models like a hidden mark of networks and Bayesian networks in general. Um, and so what this means is that when I write it in the form of a product of factors, well all of the factors, so I'm giving an example here where we are conditional random fields, but with uh, uh, with no context window or window of size one that only contains the input that the current position. So in this case, I only have, can think of this as only containing two types of factors. Uh, well in this case, in the, in the conditional random fields, these factors only need to be none negative, so they need to be greater or equal to zero. That's the only requirement that I have in an undirected graphical model is that when I write it as a product of, of, uh, the, the, uh, factors, well, the factors themselves, they only need to be such that there are always great or an equal then zero.

Speaker 1:          08:19          So that's to be contrasted with a hidden Markov model. Uh, so if we visualize it as a Bayesian network, that is a network where the notes are the random variables and the edges are now directed, well, each edge in this case curse ponds to a conditional probability in the model that's a property of a Bayesian network. And in this case for a hidden Markov Model, the, uh, this arrow here would correspond to a Dick Conditional with illustrate that, uh, um, with corresponding conditional of y two given y one. And, uh, also, uh, x to the Arrow from y two two x two would correspond to the conditional probability of x two given y two. So at the arrows, what are the are saying essentially is that, uh, in the generative story behind the model y two was generated based on why one only and an x two was generated, uh, based on, uh, why two only.

Speaker 1:          09:22          So, uh, because now if you ride in factor form, then all the factors, uh, will correspond to these probability distributions. And it means that we do satisfy the fact that these, because they're probably the distributions, they're going to be great or an equal than zero. However, in a director graphical model, the factors also need to sum to one retrospect too. One of the arguments here. So for instance, this factor with sum to one with respect to x, k, uh, because it's a conditional property, it's, so this distribution over x game and similar lead, this factor between pairs of labels would something one with respect to why k plus one because it's a conditional distribution over why k plus one. Okay. So that's a, an important distinction between a director graphical model like in hmm. And a conditional random field, which is an undirected graphical model.

Speaker 1:          10:18          So now the edges do not correspond to conditional distributions. They just come correspond to factors or uh, they're associated to fat. Where with factors are compatible compatibility function. We just express a preference for the values, uh, that can take each of the random variable in the factor of compatibility function. And a in particular if we were to try to identify conditional independence is in the director graphical model. And so from the Bayesian Network, uh, it would actually be more complicated. So there's the, the separation role, which I want to explain here, but you can find in most, uh, uh, textbook in the graphical graphical models, which requires a bit more complicated rule than just looking at the past between the variables and identifying whether on the, each path between the random variables there is a, at these one node, which career respond to a conditional note. There's a slightly more complicated rule due to the fact that the, uh, edges between the nodes are, are directed and correspond to a normalized factors to conditional distributions. All right? So that covers mock of networks and as well as the distinction between undirected and director graphical models.