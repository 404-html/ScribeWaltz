Speaker 1:          00:00          In this video, we'll look at loss functions that a we can use for training or throwing quarters.

Speaker 1:          00:08          So we mentioned in the previous video that, uh, notone quarter is really just a feed forward neural network where at the output, instead of predicting, say a class label will be predicting or reconstructing the original input x. So we'll call it the input x hat. And uh, uh, and so now what we need to train an auto encoder is to define a loss function that will compare x with x hats somehow, uh, to measure how well, how good the reconstruction is. And then using that lost function, we can train the auto encoder to minimize it on average, on a training set by doing some grading dissent on that last function. So let's look at some loss functions for different types of data.

Speaker 2:          00:50          Okay.

Speaker 1:          00:51          So imagine we have binary inputs. That is the input vector x is a l a r vectors that contain elements that are either zero or one. The most popular loss function is this lost function here. It's often referred to as the cross entropy, uh, mentioned, uh, in a minute. Why we call it the cross entropy will just look at the uh, equation. So it's going to be minus the sum over all the elements in the input vector of ex gay. So that's the key element of the input vector times log of the reconstruction of the key input element, plus one minus x gay of log of one minus x minus x hat came. Um, so let's look at this a little bit more closely. For instance, imagine that, uh, for some given K ECC was equal to one, then this would be one minus one. So this whole term here would be zero.

Speaker 1:          01:53          So we can just ignore it. And so this would be one. So really in this case, what I'd be minimizing a is the loss here, which would be minus, we have a minus here. So it's the, I could remove the minus and just put it right in front here. So it'd be minimizing minus the log of the, uh, output. And so if I'm in demise, this is the same as maximizing log of the output. So I'm trying to push the output as close as possible to one. So remember that for binary inputs, we'll use a sigmoidal, a activation function at the output. So the, the, the biggest x had cake and be as one. So if Xb is one, we'll try to push the output towards one.

Speaker 1:          02:37          If Ska was zero, then this term would be zero, this would be zero. So this whole term would be one. So we just have this and using the same reasoning that we'd find that in that case we'll try to push x had k towards zero. That is, we'll try to push it closer to the actual value of x. K. So you can see that this particular construction function is actually well behaved. It's, it's a actually minimized if ex gay is equal to x, had gay for our case, it would actually be, uh, in this case, if x is, uh, uh, is either zero one, it would be zero. If we had this, if we had the perfect reconstruction, uh, we call it the cross entropy because, so if you don't know what cross entropy is, if I have a probability distribution p of x and another one Q of x, uh, then the cross entropy we did in the two distribution would be minus the sum over all possible values of that x, uh, a random variable of the probability of that.

Speaker 1:          03:46          A particular value of x times the log of the pro, the log probability, uh, Q of x for the other distribution. Now, if x was binary here, so if x was taking all these year or one values, then there's some would be a, some over the probability of x being zero and then the property of x being one. And so these is how you would get these two terms here. Uh, so this, uh, so in this case, p of x would be a s or p of x equal to one would be equivalent to our ex gay and then a Q of x being equal to one would be equivalent to x hat. Kay. Uh, so we see that we get the case where a p of x equal one that would be xk here and this would be x hat here. And then p of x equals zero.

Speaker 1:          04:45          Well because it's a binary variable, it's Bernoulli, then it will be more minus the probability of being equal to one. So that's one minus x gay log of one minus probably being pulled to one according to Q and Q is accept gay. That's the problem. The probability and that distribution of x, k of x being equal to one. So we have here, one minus x had cake. So essentially we get this form here. Uh, if we consider the cross entropy between Bernoulli random variables and then we have a, some of Bertolli Cross entropy's because we're summing over all the input elements. So all the dimensions in my input factor, okay, so that's why we call it the uh, cross entropy loss. Um, if we had inputs that were not exactly zero one but between zero and one, we could still use this last function here. It would still be well defined in the sense that the minimum will still be, uh, when we reach a ex, when we reach x gay equals two x hat case, so that we'll still minimize this.

Speaker 1:          05:54          Even if ex gay can be between zero and one, however you will not be zero. So if a ex gay, uh, say is equal to 0.4 and then it's x hat k is equal to your point forward, then the cross entropy for that input dimension is not zero. So, uh, you shouldn't be surprised if your inputs are not exactly zero one that you don't reach a, a perfect cross entropy, a perfect cross entropy loss. But another alternative people who have real value with inputs is to just use the square difference. So essentially, uh, a half times the sum over all the input dimensions of my reconstruction minus the actual input squared. So this is the sum of squared differences. Uh, we can actually think of it also as the squared Euclidean distance between the vector x hat and the Vac, the input vector x and a actually often when we're optimizing this, uh, and if the inputs are really real value in the sense that it can take value between minus infinity and plus infinity, then in this case often will leave. We'll use a linner activation function at the output. So x hat would actually be equal to a hat of, uh, ex. Uh, so if you go back into our previous rotation, uh, Ahab was the pre activation and the output layer. So we just, if we have real valid inputs, we would normally use a linear activation function. So in other words, no activation functions at the output layer. So that's another alternative if we have real value the inputs.

Speaker 1:          07:30          It turns out then that if we use either of these two loss functions, so the cross entropy with a sigmoid, uh, activation function or the square difference using a linear activation function, then in both cases, the gradient with respect to the pre activation at the output layer of the loss is just going to be the difference between the reconstruction and the input. So here I added the t because, uh, you know, you imagine you'd be computing this grain and four different inputs. So I'm indexing now the input here and I'm also putting the index for the reconstruction to say that it's, this is the reconstruction of that input. So we can do the math to verify this, but it's so turns out that it has a very simple form. And then to get the grain and with respect to the parameters and perform great gradient descent, we can just use regular backpropagation like we've seen it before because you know, tone color is really just a feed forward neural network and just back propagate the gradients towards all the parameters.

Speaker 1:          08:32          One thing that's important to notice, however, is that if we use tide waits, that is if you use for the reconstruction weights, w star, the transpose of the encoder weights the weights into the hidden layer. Um, then in this case we have to realize that the gradient with respect to the matrix w is really the sum of two Graydon's. It's going to be the sum of the group or the transpose, uh, sorry, sorry. It's going to do that. Some of the gradient of uh, w star transposed and it's going plus the gradient with respect to a w uh, uh, or respected the gradient in the encoder, uh, the in quarter part of the network. Okay. And that's because these parameters double you are present Indian quarter and the decoder. So they get gradients both from just a decoder part and the in quarter part. Okay. So that's something important to realize when you, if you implement a, uh, not long corner with tight weights.

Speaker 1:          09:36          And I'll just a quick word about how can we just adapt an auto in quarter four different types of inputs. If we have inputs that are, uh, not real value, they might be integer value in my correspondence to a multinomial observations or other types of observations, perhaps more structured. Uh, well here's a recipe for doing that. Uh, so what you could do is first you define the joint distribution over the input space, uh, choose, uh, so choose a distribution over the input space that will depend on some parameters of that distribution, which I'm calling new here. So it's going to be the vector of parameters of that distribution. And then you choose some Parametrix relationship between mew and the encoder, the a hidden layer of the computer by the quarter. So in our case, we usually have something like a linear transformation and possibly a nonlinear cavity.

Speaker 1:          10:28          And then the last function we'll use, we'll just be minus the log of the probability of x given mew. But where mew here depends on h of X. Okay. And so we'll just use that as the form for the last function. In fact, we can recover the sum of squared distance. Uh, if we, uh, use for P of x, given you a Gaussian distribution where the mean is new and the covariance matrix is just the identity matrix. So if we have p of x that corresponds to that, uh, and then if we choose that can you, is going to be just a linear transformation of the input, uh, sorry, of the, a latent representation h of x that we recover what I described before as the last function would be using for an auto encoder with real value the inputs. So I'll let you do that exercise, but essentially you see that minus log here is going to counsel cancel out with the exponential and the minus here. So we'd be left with half and this part, and this is just a constant so we don't have to consider it. So then we'd fall back on the square difference. Last function. All right, so that's all for the loss function. We can use it now we have different alternatives and even the recipe to generalize to, uh, many types of data.