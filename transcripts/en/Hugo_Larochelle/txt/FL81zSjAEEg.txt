Speaker 1:          00:00          In this video, we'll describe in a bit more detail how we can use [inaudible] to extract some features for a given problem. So one of the most popular, uh, usage of sparks coding is to extract features that is, say we want to perform a particular classification problem. Uh, and we would lie to use our data and find a representation, the new representation for our data, which is going to make the problem with classification easier. So a specifically imagine I have some label training set where I have, uh, some inputs and each input is associated to the class label. So imagine say that the inputs are a set of images and yt is the class of the image. Well, to extract some feature, what we could do is, uh, we could first train is parse coding dictionary on the data set. That only consists of the inputs.

Speaker 1:          00:57          So we've thrown away for now temporarily the labels yt. So if we run our sparse coding or a dictionary learning algorithm on that Dataset, this will give us a dictionary matrix deep. And from that dictionary Matrix, well, I could then take all of my inputs and compute what's the sparse later in representation, uh, based on this learn dictionary. And now what I could do is just take my original dataset and change it in a way where I'll remove the original input and I'll replace it by the sparse representation. And so now for any classifier, I can just use my favorite class fire, whether it's a neural network or a support vector machine or a decision tree or any costs of fire. I can't just train it on the new dataset where I've replaced the input by its parts coding representation. And from it, I now have to predict again the label, but now I have to predict it, uh, from the, uh, extracted features from the sparse representation.

Speaker 1:          02:02          And I've, uh, I had, uh, my system was confronted with a new input x. Well then the process of classifying it would correspond to first inferring it's parsed coding representation, h of x, and then I would feed it to require to my classifier that I've, that I've trained here. So while this part gives me a d matrix, this parts gives me a classifiers, say h a f of uh, of my input. But now the input that I have to give it for my cost fire f is actually a spar squirting representation h of x much like, and that's because it was trained on these types of features. Okay. Now, uh, for this to be meaningful to make sense, you essentially have to believe that uh, these parts coding representation is going to make the problem easier. That is my data is such that it's a, there is a sparse representation which will identify features that are more directly related to what's the class label associated with my data.

Speaker 1:          03:08          So one such type of data is a image data and in particular let's consider a image data that corresponds to handwritten digits. So I have a very small image and in the image I have a, a Henredon wreck characters. So this would be an image that would be labeled too. So I have one that would be one, uh, a pair of example, uh, with which my, uh, one, the train my learning of them. Well, uh, if I run sparse coding on a bunch of handwritten digits images, uh, so I would just scrap temporarily while I'm running my smart scoring algorithm, the label and just run on the images itself. Well, we have an illustration here, the types of features we would get. So now each lil square here, so that's a square, that's another square, that's another square and so on is a visualization of one atom in my dictionary.

Speaker 1:          04:04          You Matrix one column, so I can visualize it because each Adam, uh, is a vector of size. Number of inputs is the same size as the input size and a, and because the input is an image, then I can just reshape my vector also into the shape of an image. And then individualization, if the pixel is great, it means that the value of the vector for that Pixel is zero. If it's white, then it's positive and it's black, it's negative. So this essentially corresponds to, uh, uh, the, uh, sort of illustrative, a visualization of what each sparse code element, uh, and coats. So if I, in my responsibility in representation, the code associated say with that feature had the large positive value. So if this was, uh, uh, the first feature though, so x if h of x one was a, because this will be the first feature, if it had a large value, it would mean that if I was to reconstruct the original input, I would add one times this little limit here and say if this would be the other image, then I add also this part and so on.

Speaker 1:          05:12          Okay. So that's why this is a meaningful visualization of the representation. And now we see that each spar scope actually has learned, uh, what looks like essentially a bunch of pen strokes. So this will be a pen stroke like this. This would be a pen stroke like that. And this one will be like this. So essentially PowerSchool as parsing representation would be a big vector that identifies, which are this, the different pen strokes that are present in some given image. And, uh, in fact, if we were to ask someone to describe the problem of classifying digits, uh, enter the digits, then it would probably actually use a description that's based on pen strokes. So to describe it too, I might say, well, it has a sort of a bar on as a loop at the bottom and so on. So we can see that, uh, intuitively, this seems to make sense. This would be a nice way of representing digit that makes the class information much more obvious than just the raw value of the pixels. So this is why for image data and this particular case and with the digits sparse coding features work well in practice.

Speaker 1:          06:22          In fact, we could actually train our [inaudible] coding dictionary on the different type of data set. Uh, so if we had a large collection of unlabeled images that did not exactly correspond to the type of images we wanted to classify a test time, we could still use the unlabeled images to extract a better representation and see whether it transfers to, uh, uh, are, uh, my main important problem that, uh, I actually want to solve in terms of classification. So this is actually known as self taught learning. Uh, so it's a combination of unsupervised learning and transfer learning, uh, for those that know more about machine learning. So these are unsupervised and transfer learning how to well known types of, uh, of learning and a self taught learning can be seen as a combination of both. So a in south thought learning, the idea is that we want to train some features but on a different input distribution, uh, because we might have more data from that input distribution than the distribution, uh, for which we actually want to solve a particular, say, a classification problem.

Speaker 1:          07:30          So there's a concrete example. Imagine I had a lot of enrooted digits, a lot of images of handwritten digits, and that those were actually not labeled. I didn't know the no humans went over the images, actually specify the label for each image. And, uh, and now imagine that what I really want to do actually is classify a handwritten English characters. So alphabetical characters and not numerous numerical characters and uh, but imagine that actually have very little training data for that. And now can we still try to improve things by leveraging my unlabeled data, even though it's, it's a different input distribution. So they're not, uh, uh, these are numeric characters and these are, uh, alphabetical characters. So you're going to look quite different. And also they might come from, from different people that actually drew them. So, uh, so here's the experiment, uh, taken from this paper by a reign of battle, the packer and Ang.

Speaker 1:          08:29          Uh, so, uh, they're the authors, the researchers that introduced the notion of self taught learning. So in this experiment, they did exactly what I described. They are, took a lot of unlabeled digit, a character's Hendrickson characters, uh, and they trained a idea or did nothing with it, so they didn't use it to extract a feature representation or they trained a linear PCA representation from the digits data, or they train these parts coding representation. So they train the dictionary matrix from which they could extract these past representation. And now they use these features to classify a Hendra did English and written characters. So, uh, alphabetical characters. And so what we have here is the, uh, performance for different training sets, sizes, uh, in the, uh, label Dataset. So we see that these data psychically fairly small in this case. So, uh, we can hope that we can leverage the information in the large unlabeled data.

Speaker 1:          09:36          And, uh, so we have the performance if we use no feature representation. So we just use the, uh, the x t, uh, raw representation or the PCA representation, or does parts coding representation as learned on the, uh, difference, uh, data on the digits data. And what we see is that using some sparse coding, uh, we often see an increase in the courtesy for a 500 and a thousand. In this particular experiment, we saw better performance, better accuracy than if I'd use the raw original, a representation, or even that, just a PCA representation and a for a hundred in this particular experiment that the performance was almost the same as, uh, if I've used the, uh, uh, the raw representation. So we didn't lose much really here. So this is another illustration that sparks coding features can be useful for performing classification as a, if we use that as in a sense of preprocessing step for a, uh, our favorite a classifier.