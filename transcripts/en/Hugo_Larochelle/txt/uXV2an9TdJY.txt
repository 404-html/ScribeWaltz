Speaker 1:          00:00          In this video, we'll briefly overview some concepts that we often see in the literature on conditional random fields. That is the concept of factor, a sufficient statistics and uh, the specific case of a linear conditional random fields. Now, if you look at the literature and conditional random fields, uh, often you'll find that uh, the, uh, different CRF models that are being developed, uh, will be written in the form that's a bit different. So one, a more standard format you'll see in the literature is as follows, I will just write p of y given x as being one over a normalization constant. So again, we have our partition function here, but times the product of some factors index by F, which uh, so we called them factors up. Sometimes we call them also compatible with the functions that uh, look at, uh, part of the, uh, target why and the input sequence X. And I'll put a value that is not negative.

Speaker 1:          01:04          So between zero and infinity and the job of each factor or each compatibility function is to look at a particular part of why an ex and determine whether it likes it or not. Whether it prefers that these parts of why an x take the value, uh, that's why an XR currently taking or not. So we see that if this factor is, uh, for instance smaller than ones are between zero and one, then this will essentially decreased the value of the old expression. Whereas if it was greater than one, that would actually increase it. And so I'll just generally speaking, the bigger the compatibly function or factors output is then the more likely, uh, seeing why associated with the input x is going to be in this model. And if very frequent parameterization of these factors or compatibility functions in the literature is to use a log linear form that is the exponential of a Lennar some that combines the model's parameters.

Speaker 1:          02:10          Uh, no, no. That data as a thera here with a bunch of features or sometimes we call them sufficient statistics, they're called sufficient statistics. Because if you had a large data set where we have a first label, why one associated with an input sequence x one and then we'd, so we'd have one pair and then we have another pair of why to associated with its input sequence x two and so on. So if you were to compute a, in this model, what is the probability of seeing all of these sequences, y one and x and y two and so on, associated with the sequences, uh, x one x two and so on. Then assuming each pairs are a, each example is independent. That would be the product of the P of XD given a whitey given [inaudible]. So it'd be the product of these expressions. Now if we pair it dries are factors in the non log linear way.

Speaker 1:          03:14          So as the exponential of a, some of parameters multiplied by some features. Then so this would get a product of a bunch of exponentiate it sums so we could accumulate them all because the product of exponentials, the exponential of sons. So we essentially get an expression where we have parameters multiplied by the sum of these terms. Over the whole data set. And uh, so in other words, the probability of the whole data set could be essentially written down as an expression that only depends on the, some of these features or sufficient statistics, uh, over the whole dataset. And in fact, uh, technically, uh, we will say that the sufficient statistics is actually the, some of the features and not the feature themselves. So I'm going over this very quickly. You can, uh, look at, uh, the literature to, uh, learn more about what a sufficient statistic mean, but just know that normally when we talk about sufficiency statistics in a CRF conditional random fields, they're essentially the features that are being multiplied by the parameters of the model. And then in this case, the whole probabilities going to be the exponential of linear expression, linear with respect to the parameters. So they're going to curse fun if we use this parameterization, uh, they're going to essentially current respond to a log linear model in the features that we've chosen.

Speaker 1:          04:43          So if we are to write our, uh, use our notation and put it back in that form that we just seen, uh, then the conditional random fields factors, uh, so the Psi F, uh, would be as follows. So there'd be four types of factors. So this is for the specific case where we have, uh, a context window of radius one, and we have a neural network that maps, uh, either the left or the middle, the right part to, uh, the, uh, current position. So we have three neural networks. So in this case we'd have, uh, three urinary factors. So these five F's, there'd be some that would look at the current label and compare it that would compare it with the, uh, input on the left. There'd be some factors that could look at their current level and look at the, uh, input that's at the same position and others that look at their current label, but look at the input at the position on the right.

Speaker 1:          05:42          And then we'd also have pairwise factors that just look at adjacent labels and they would correspond to the exponential of, uh, they essentially to take this form, the exponential of the pre activation value, uh, computed based on the input on the left for the, uh, value of the Kia label in my sequence. Why? So this would be for, for this factor, he would be the exponentiate it be activation function of x gay, uh, for the label Ika, uh, and so on. So we now see that in this notation where we decompose p of y given x as a product of factors. Then the a terms as well as the, uh, pairwise Matrix v, uh, really do correspond to the exponential of a log factors essentially. So this is where the luck factor, uh, uh, expression comes from. Unfortunately, in this form, there's no simple expression, uh, for the sufficient statistics that is we can't ride these terms as just a, some of parameters time, some, uh, simple, uh, deterministic features extracted from x and y.

Speaker 2:          06:54          Okay.

Speaker 1:          06:55          And that's, so that's because essentially these expressions here with hidden units are essentially non linear.

Speaker 1:          07:04          If you look at the specific case of a linear conditional random fields in here, linear is, uh, refers to the parametrization and it doesn't refer to the linear chain structure of the interaction between, uh, the pair pairwise. Uh, so the adjacent labels. So linner means essentially that, uh, the, uh, neural network has no hidden units. So the p activation is a linear function of the input. Well in this case we can write the expression for uh, yeah, we can sort of further exploit the a and they have a more precise expression for a d a p of y given x in a, as a product of factors. So we'd have g five now, uh, types of factors. We have factors. You're near your factories that a look at whether the label takes a particular value. And if it does, then we just take the exponential of bias at the output layer.

Speaker 1:          08:02          Uh, we'd have factors again that look at the current, uh, label and that looks at some given input, the input at the position on the left. And it would just take whether a dn dicker to function of whether why Kate takes of labels c and multiply that by the IFM puts for the input vector position came minus one. And that would be multiply by the entry in the Matrix that connects the left input with the current position. It would look at the entry c and the uh, so the row C and d column. So we have a bunch of factors like this for each class, potential class C, each potential position in the input vectors. I N uh, each position, Kay. Uh, so this gay and this case, each value for that key in the sequence, but have also a bunch of other factors that, uh, would do the same but for the input gay at the same position or another for the [inaudible] position.

Speaker 1:          09:05          Cape last one. So, um, uh, so that's for the case where again, we have neural nets that uh, we have three different neural nets in the context window, but turns out we could essentially get a very similar expression if a we considered the case where we think of it that's just there being one neural network that connects a full context window directly to the label. And that's because there are no hidden units in the linear CRF. So we could essentially get the similar expression and a similarly the, uh, factors, uh, pairwise factors would be the exponential of VCC prime if y gay a is equal to c and if y k plus one is equal to c prime. So we have such factors in the expression, uh, for each value of c c prime and for each position came. So this would be, we could write this down as a, uh, in the expression that is the product of all of these types of factors for all values of y c and K for, uh, a pair of, uh, uh, uh, sequence label.

Speaker 1:          10:08          Why an X. And notice that in this expression now I written down the full model as a product of sufficient statistics or features times a scalar parameter. Okay. So, uh, now the, the notation seems more complicated in this form and this is just to make the mapping with the case where I have, uh, factors that are a lug linear. And if you remember in my expression the parameters that are f s there were, there were scalar parameters. So that's why I get some, uh, fairly, uh, you know, the annotation for the sufficient statistics is a bit more complicated. So in the case of a linear CRF or so, for instance, in this case, a linear linear chain Crf, uh, I can actually write down the, uh, into the standard form we saw in the beginning of this video where I can separate out my parameters and my sufficient statistics in this way. So we have a log linear model and, uh, um, and where all the parameters are scalers and they're multiply by sufficient statistics. So, um, you know, I invite you to look more at the literature. If you want more details on this, I'm going fairly quickly over this, but, uh, I just wanted to mention it. If you ever see the notion of sufficient statistics, essentially we were talking about a, usually a log linear model where we've defined a bunch of statistics that we think are useful for predicting the label.