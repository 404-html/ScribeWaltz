Speaker 1:          00:00          Yeah.

Speaker 2:          00:00          In this video, we're finally ready to put the different pieces together to get our final dictionary learning algorithm.

Speaker 1:          00:09          Okay.

Speaker 2:          00:10          So, uh, we remember that a Ds Pars coding problem. Uh, so the problem that corresponds to learning a good dictionary for being able to infer some sparse representations for our Dataset is a corresponds to a to minimization. One with respect to the dictionary and one retrospect to the spars representations for all of our training examples and what we've seen before, how given the training example and a dictionary Matrix d, How can I infer the spars coats with the Ista Algorithm and given some fixed sponsored representations, we've seen two algorithms, uh, for updating the dictionary matrix and so performing this minimization, assuming that this minimization has been performed. Okay. So now we're ready to put these two pieces together to get a joint, uh, a global algorithm for performing this whole minimization here. Uh, so retrospective, both d n a h d.

Speaker 2:          01:15          So the idea is very simple. We'll just alternate between performing the inference that is computing the age of x t vectors, parse codes, and then, uh, updating the dictionary. And A, so we'll go back and forth like this until convergence. Uh, so here's a Sudoko for the algorithm. As long as our Matrix d has not converged, are as far as coding Matrix, our dictionary Matrix, a first for each input XD in my training set, I'll stuck in memory and performed the inference of it's sparse coding representation and so forth that I can use Ista. Once that's done, then I am ready to update my dictionary using these fixed precomputed representations. And so if we use the block corner at the center algorithm, I again compute my matrix a matrix B, which corresponds to these, uh, sums over the Dataset of two different types of outer products, one between the input and there are other, uh, with the uh, between the input and these first and representation and the other one, the other product between, uh, the uh, uh, as far as going representation with itself.

Speaker 2:          02:28          And then I can run the block corner descent algorithm to until convergence, uh, to get my new value 40. And then I come back and I recompute new sparse representations. So now these are going to be different. Is Diaz changed. And that's because when we're performing an update of the dictionary Matrix d, we were assuming that these were precomputed. We're not changing when these changing, but that's, that's actually false. Uh, performing disinterests requires knowing a particular matrix team. And so what we'll do is that, uh, will update the representation for all of our training examples and then that's going to give us new matrix and B, and then we can rerun our block corner does scent with these new matrix, a matrix amb that's going to give us a new Matrix d and then we go back, we update the, as far as codes and we continue like this until the Matrix d has not changed significantly has converged.

Speaker 2:          03:28          Now the reason why this will this alternating, uh, so alternating between these two optimization problems. The reason why this was con this will converge is that well performing this optimization strictly will improve with respect to our previous, uh, value of the objective for our previous value of d and h. And that's because we keeping the fix, but we're are improving the optimization, which respect the objective optimization, uh, with respect to our sparse codes age. And then when we come here, if we optimize it well, we are strictly improving our objective with respect to, uh, d actually. Um, and so, uh, for, for that reason, uh, whenever we're a by alternating, we're always improving over the objective but just with respect to one variable. And so, uh, in, in that sense it's a, it's a kind of similar to this by itself is sort of like a block coordinate a algorithm because we alternate between optimizing one part of the problem, keeping the other part fixed and then going back and optimizing and other part of the problem.

Speaker 2:          04:36          Yeah. Keeping the other part fixed. So for that reason it's going to converge. And a, you might, if you're familiar with the eem algorithm, the expectation maximisation algorithm say port training mixtures of Galoshins or other graphical models with the, uh, with the latent, a latent, uh, random variables. Uh, this is kind of similar where, uh, inferring the sparse code would be akin to the east step. And the optimizing the dictionary would be akin to the m step. And so that's it for the sparse calling all of them. And so now we have all the pieces for you to implement it and run it on, say, some, uh, data set of images and so on. So get a good value for our dictionary, which will, uh, allow us to say, for instance, extract some good, sparse features for a given dataset.