Speaker 1:          00:00          In this video, we'll now discuss how to train a recursive neural network.

Speaker 2:          00:04          Okay.

Speaker 1:          00:06          So we've discussed the two main components behind recursive neural networks. Uh, the part that merges pairs of vector representations to obtain a new representation that covers more words in the sentence and also the part which, uh, assigns a score for the quality of mergers that from which we get get a procedure for determining what is the best tree structure according to the model. Uh, so the best the tree structure in terms of the score it science for the full tree. And in order to get an inferred the correct sync tactic tree for a given sentence. So now we'll see how we can actually train the recursive neural network to actually produce the, uh, true wanted parse tree for a given sentence, uh, that is provided in some training data.

Speaker 1:          00:55          So I'll refer to why as the true power street are given sentence and why hat as the parser that's predicted, uh, by the recursive neural network using the procedure we saw in the previous video. So, uh, what we would like is for the score, the full tree, why, which is the correct tree. Uh, so we called Das s of y to actually be hired than the score of the uh, tree that we predicted y hat where a, unless of course y hat is actually why, in which case our neural network is doing the, uh, doing the right prediction. But if it's doing an incorrect prediction, then what we want is for the score of s y to be higher than the score of, uh, sorry, the score of y to be higher than the score of y hat. So that next time around it's more likely that y will be the predicted a parse tree.

Speaker 1:          01:47          So, uh, and not do them for are trying to achieve this is that whenever y hat is not why we just increased the score of y a and then we decrease the score of y hat. So, uh, procedure for doing this would be to first take the sentence and infer the predicted parse tree and then we will increase the score s why have the correct guy and the crazy decreases a score of a y hat? No, that s why had by doing an update in the direction of the gradient of the score, s why the gradient, which respected of parameters, a model of this core s y minus the gradients of the parameter with respect to the parameters of the score for the predicted three y hat. So in other words, we have to take these two gradients and subtracting because this is giving us a direction that will increase the score a four y and this is going to give us a direction that is going to decrease the score, four of the parse tree y hat.

Speaker 1:          02:54          And now to get the gradients, uh, each of these two gradients and to combine it to get the single gradient, uh, uh, or, uh, saw a single direction, uh, in which to update our parameters. Then we'll, we'll have to do is do a regular backpropagation through the recursive network that follows the structure of a y and y hat. So I'm essentially for say why, uh, had, uh, was a parse tree corresponding to that structure. Then in this case, it means we'd have a neural net, which is taking the word representation here in here and computing a representation, which is like a hidden, they are at this level in computing a hidden layer at this level by combining this, a vector representation with the effect of representation here from both of them, we'd get as the output a score, the score of that merge and that merge.

Speaker 1:          03:51          And these would be some together to get the score of the full parse tree. Why? So that would be for propagation. And then backward propagation would be to assume that we have a great in retrospect to s why have one and then moving in the other direction back propagating the gradient, respect to all of our parameters, uh, to get the gradient, uh, which affects all the parameters for that four s y. And then if we wanted to get the gradient minus s y hat, then so imagine that y hat corresponded to different, uh, parse trees. So maybe this parse tree here,

Speaker 2:          04:35          uh,

Speaker 1:          04:35          then I'd beat would be competing this vector representation and then this one and from both of them we'd get a score which would be some together and this would give us s y hat. Then again, I would assume that the gradient, just like to s y hat is minus one cause we're interested in the gradient in, in the negative gradient. And then we'd back propagate through the neural network that was constructed, a recursively using this, this syntactic structure of y hat and then back propagate through to all the parameters. And so these two procedures would give us, uh, this part and this part of our update direction. And then we'd combine them by subtracting them. And that gives us the update direction to follow, to update all of the parameters. So that's how training would proceed in this case.

Speaker 2:          05:31          Okay.

Speaker 1:          05:32          Now, um, in the parse tree, uh, often the nodes are also labeled. They're labeled retrospect too, whether they correspond to a noun phrase or verb phrase and uh, uh, using these, these labels actually available in the training data. So one thing we could do is to just also add from each a representation at every level at a connections to a softmax, which would try to predict what that label is. So instead of having just as, uh, from each vector representation of a score that is output, it will also have other parameters of the neural network that would also try to predict what's the distribution over the label of that at that node and a, and then we could use the negative log likelihood of the true label as a gradient, which could be done back propagated through the whole network. And this weekend only do for the true parts tree.

Speaker 1:          06:31          Why? Because for the predictive process, three, we don't necessarily have notes that correspond to the original parts street. So we don't know what the label should actually be. So this other backpropagation that we can do to integrate also, uh, the notion of labels at the Inter internal note, uh, in the parse tree, we can only do it for the true part street. And so then we, we, uh, we checked what's the negative log likelihood at every internal note. And this would give us a gradient for that. Those output layers layers are which would be back propagated through the whole network. Similar knee has me back propagated back, propagate the gradient with respect to the score of the treats.

Speaker 2:          07:17          Okay.

Speaker 1:          07:18          Some other details about how uh, training proceeds, um, in their paper they actually use the word representation using the language modeling task of color bag and western. Uh, and then so they used that to initialize the representation and then they would find tune them. He sent in Chile because they use them as initializations. They would, they would still train them, uh, using the recursive auto encoder, sorry, recursive network, uh, uh, training procedure that I've described. So this addition of the auto encoder is good. This should be network.

Speaker 2:          07:54          Okay.

Speaker 1:          07:54          Uh, so that's another detail that's uh, that that was important for getting good results. The other thing, which I don't want to describe in more details because it goes a bit beyond what we've seen so far, is that the training criteria they actually use for training the recursive network to a gift. The correct a parse tree is actually a margin based criteria. Uh, essentially they don't want the correct parts tree to have a strictly greater, only a strictly greater or score it then any other parts, street y hat, sorry. Why Star? They actually want the score, the correct post street to be at at least a some margin greater than the correct. Uh, sorry. Then any other parts street and that margin for a parse tree. Why Star, uh, is going to be this delta? Why, why star here, which corresponds to essentially the number of errors in predicting why star, when the true tree is.

Speaker 1:          08:56          Why and this number of errors is actually, uh, the number of, of incorrect spends. So just give an example to make this more concrete. So imagine that the, our street was y and then, um, we considering what's the number of incorrect span with respect to say, predicting why star here. Uh, then there would be one incorrect span. And that's because at this internal node where spanning the words from, uh, from position three to four, whereas in the predicted, uh, or this, this other incorrect Parse, streetwise star, uh, there are no notes that span from three to four. There's instead a node that spanned from one to three, which is not present in the, um, in the original and the true par street. Uh, so we don't incur an error for emerging this year. And that's because we also merged that, uh, these two together. And also we don't incur an error at the global tree.

Speaker 1:          09:59          So I have this note here because this is spanning all words and similar to here at this level of the parse tree, we're also spending a all words, um, and it turns out that using we can just do a very small modification of the approximate, uh, inference algorithm that we saw before, uh, which is essentially a beam search kind of approach. And so we can actually modify, it's to instead try to find the tree that, uh, most violates this margin criteria. I'm going very fast over it as a, I don't have a really want to give more details about that, but I encourage you to know more about this training procedure. Uh, if you still have questions to look at the social al paper where more details is, is given, uh, about that. Um, and we're respected. This training procedure essentially corresponds to trying to do approximate structured as VM kind of training, uh, of the score given by the recursive neural network. But again, to get more details about everything that's described in this video about the training procedure, I encourage you to look at this paper here.

Speaker 1:          11:10          And in terms of the performance, if we look at the uh, uh, performance for a syntactic parsing, a which is usually measured using the f one, uh, measure, uh, the recursive neural network gets a performance of 90 point 25%. While, uh, the one of the state of the art parser, the Berkeley Pars or, uh, gets 91.6 33%. So it's a, it's a little bit worse, uh, but it's quite close. Uh, and, and so that's, that's pretty satisfying. Uh, if you look at the paper by rich Richard Socher, uh, you actually see that you can use the same algorithm for also parsing visual scenes and for visual scene parsing actually, uh, got at, uh, at that time a state of the art results. So while it might look like this is only applicable to parsing sentences, you can also parse visual scenes and, and, and in this case, it actually gets state of the art results.

Speaker 1:          12:08          And finally in the paper, they also provide a, an analysis to try to see these, uh, representation of phrases and then sentences, uh, whether they actually capture a similarity that's meaningful. So for instance, if we took that sentence, would you, is that why gained 52? Uh, so, uh, UNC, that's because this is for an unknown word or, or o v word, that's just the token they use for representing Ovi. And we look at, uh, uh, other sentences that have a or phrases that have very similar vector representations. We get, get phrases that indeed maintain some of the, you know, have very similar information as this contained in, in this particular phrase. Again, in the other example here where the dollar dropped was judged to be similar to the dollar retreated or dollar gained or bunk prices rallied. Where here we get a fairly different phrase than this, but we see it's talking about similar concepts. So again, I encourage you to look at, uh, this paper vibe, social and there are actually many more extensions that have been developed, developed in the following years. Uh, and so I encourage you to also look at that. And, uh, that's it. Or recursive neural networks.