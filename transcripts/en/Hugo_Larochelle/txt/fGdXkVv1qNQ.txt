Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video we'll see how we can compute the partition function in the linear chain conditional random fields.

Speaker 1:          00:07          Yeah.

Speaker 2:          00:08          The previous video we've introduced a, a notation that a separate, a two types of log factors, the urinary luck factors, which only depend on the label that a specific position and the pairwise factors, which depends on which depend on a pair of adjacent labels in a sequence. And if we take this general notation, then uh, we can write p of y given x. So the probability of a label, a sequence why given the sequence of inputs x as just the exponential of the, some of the you Neri luck factors plus the sum of the pairwise luck factors. So by taking the exponential, we know that this term is going to be positive and then we can divide by a partition function, which is uh, uh, or normalization constant with just just a, some of the numerator over all possible sequences. And then by doing this, then we are guaranteed that this is a valid distribution. It's sums to one and the terms are a non negative.

Speaker 2:          01:12          So imagine we actually want it to compute that expression. So for a given sequence of inputs, uh, we are considering a particular sequence of labels. They want to know according to our conditional random fields, uh, what is the actual probability of that label sequence being associated with the given input sequence? So a computing, the numerator is not too complicated. It's linear and the number in the size of the sequence, linear in, you know, capital k. Now we have this partition function, this normalization constant, which, uh, for which I give the expression here explicitly, it's the sum of the numerator. So that's the same as the numerator, but it's the sum over all possible value for the label that the first position, second position up to the label, add the last position. So I have a bunch of nested sums. And so if this is a, if there are capital's CE classes, then this is the sum over c times Capitol c times, times capital's see, so k times, uh, the product of c terms.

Speaker 2:          02:19          And so that would be c to the capital K s. So this is an expression that requires some over an exponential number of terms which, uh, for a long sequences, for large volumes of case that's going to be problematic. And so that's if we did a naive implementation of the computation of the partition function. However, as we'll see in this video, we can actually exploit the special linear chain structure in a linear chain conditional random fields to write a dynamic program that will allow us to compute the partition function in polynomial time instead of in a exponential time, which respect to uh, uh, which respect to k the long, the length of the, um, uh, the length of the sequence.

Speaker 2:          03:06          Oh, I also want to, uh, mentioned that. So notice I'm using why prying here? That's to distinguish between the variables that corresponds to a, the curse part to the labels over which I'm summing in the denominator in the partition function. So distinguish those from the labels. Why here, which are the labels for which I'm evaluating the probability according to my model that this particular value of the sequence would be observed, uh, would be associated to a given, uh, inputs sequence. Okay. So this is just too, this is just to mention that this is a separate variable in my expression.

Speaker 2:          03:48          All right, so what we'll do is that we'll just reorganize the terms in the partition function and then we'll notice that we can compute progressively different parts of it, uh, in sequence. And then to be able to get a, uh, an expert, uh, the value computed value of the partition function much more efficiently. So what I did here is that I just first reorganized the sequence of the labels. I'm going to some in the nested sons, I'm going to some over the last label and then the previous, the last label up to the second navel and then the first label. So I can do this because here, each of the psalms are independent, there are for variables and they don't depend on each other in the, uh, um, eight or of the summations. I can actually change the order of, of these nested psalms here.

Speaker 2:          04:40          So that's the first thing I did here. And then the second thing that we'll notice is that, well, when I'm performing the sum, when I'm iterating over y prime one, the first label, so I'm summing over the exponential of the summer, normally all of the uh, Lug a urinary factors and uh, uh, pairwise factors. However, however, within this, some of factors, the only two factors that only depend, uh, that depends on why prime one, there's the associated urinary factor and then the pairwise factor between y prime one and y prime to all of the other factors in the sun that's into the exponential do not depend on why prime one. So because the exponential with some is a product of exponentials. I can separate out into this term here and then the exponential of the sum of all the other luck factors. And then I can take this, uh, factor. Uh, so this, this, uh, constant exponential of the sum of all the other factors and push it out of the, some override prime one and a. So push it into the other outer nested sub. So that's what I've done here. I've taken all of the other terms, ain't going to be a, they're going to appear elsewhere in the expression. Now look at this some here, I would remove a bit of ink.

Speaker 2:          06:03          Now I can actually compute this expression if someone gives me a value for y prime to solo this die. Why prime too is being iterated in this sum here. So if you tell me, well, compute this expression for right prime two equals one say, well I can actually compute this, uh, efficiently. It's just a, an iteration over all the labels that y prime one can take. And now, uh, I could repeat this, uh, this computation performing this sum for each possible value of y prime too in the outer some here. And what I could do then is actually stored a result into a vector. So I'm going to call it this vector alpha one. And it's going to be index is going to depend on a alpha prime too. So you can think of this as a vector or you could think of it as a function as well.

Speaker 2:          06:52          But because there's a finite number of values for y prime to you, you might as well think of it as a vector or a just in array that contains all of the values of this sum for the different potential value of y prime too. So at this point I could just precompute this son for all potential value of y prime too, which I'll work and I'll need this summation here when I perform the sum over right prime too, because there's some of a ride. Prime too is the exponential of again, only the luck factors that depend on why prime to that is the unit refactor this pairwise factor between y prime two and 1.3 times. This sum here now because I've precomputed it, I can just multiply instead by uh, Alpha one y prime too. And so again, if you give me using the same reasoning, if you give me a particular value of y prime three I can now compute this whole expression using what I've precomputed before.

Speaker 2:          07:52          So I'm going to call that Alpha two y prime three. So that's the sum over white prime one and the y prime too. So that's why we have a two here, given a specific value of y prime three and to perform this computation. Now notice that I'm using the Alpha one y prime too. So this is, uh, multiplied here. So this is times alpha one y prime too. And then it can continue like this performing all of the sums that are nested until I reach a determine which I'm going to call it. Alpha capital came on us, one of y prime capitol cake. So again, I'm going to just go up like this, uh, doing a, uh, each time doing a summation over the current label, uh, of an expression which is going to depend on the precomputed sums over the previous label. So now, once I reached this level, uh, that almost almost ready to compute the partition function partition function is going to some over white prime capitol gates. Possible values from one to capitol c of the exponential of the Unia refactor. That involves why Prime Capital k times? Well, this whole expression, this whole expression here,

Speaker 1:          09:12          okay,

Speaker 2:          09:13          which at this point in my computation is stored into the vector Alpha capital k minus one index by the value of y prime cake. So again, I can perform this some where I just iterate over y prime capital, k of the exponential of the urinary factor involving why Prime Capita, okay. Times devalue in my store back there. Alpha capital k minus one. And performing that computation gives me the value for uh, uh, finally the value for the partition function on their normalization constant. Okay, so this is the rationale behind the algorithm, uh, for, uh, the dynamic program Algorithm for computing the partition function.

Speaker 1:          09:56          Okay?

Speaker 2:          09:56          So let's look at the pseudo code for this. So this is exactly what I've explained, but in pseudo code, uh, so first, uh, I actually, what I'm going to do to organize by computations is that each of the Alpha vectors, I'm going to store them as a column into a two dimensional array. So this here is going to be, this column's going to be Alpha one, Alpha two, Alpha three, and so on until Alpha a capital k minus one, that's the last outfit than I need. So I initialize the first column of my Ira by computing the Alpha one vector. And obviously we seen the Alpha one vector. Uh, so for I or what I need to do is iterate over the value of y prime too, and then compute the sum over y prime one, this sum here of the exponentiation urinary plus pairwise factor.

Speaker 2:          10:49          So that's going to initialize the value of these elements in my array. And then for, uh, going from the second column to the last column, the Meyer Ray for all values of y prime, a small k plus one that is for all elements in that column. Okay, so this, the value of this is essentially the index of the element in the column, uh, in the, uh, in their columns or indexes the row in a specific column where the index of the column is a small k, I'm going to compute that expression. So Alpha k given a y Prime Cape plus one a, which is just a sum. So that performs the summation of her, uh, y Prime Kay of the exponential of the urinary and pairwise factor times the Alpha terms from the previous column that corresponds to the summation over why one up to uh, why prime one up to y prime came on this one.

Speaker 2:          11:52          So do this. So here we're having illustration of what that corresponds to. So to here it would be the value of y prime k plus one in a given a duration for a given value of k. And then to compute that value while I'll need to use the alphas, all the Alphas at the previous columns. So all the Alpha vector came on this one. And so this is not just a summation of these terms, these terms are then weighted by this term here. The exponential of the urinary and pairwise factor that involve, uh, that, uh, in this case involves why prime cane. And so we continue like this. So now we have a procedure that goes from left to right and uh, that can compute each of the columns sequentially. And now ultimately, when I need to compute my normalization constant on my, uh, partition function, I just need to do the final sum over y prime capital gain.

Speaker 2:          12:53          Uh, the exponential of the urinary factor that only involves why Prime Kevin Old k times the elements in the last column here. Okay? So I'll take that term times the exponential, the urinary factor, uh, of y prime capital k being equal to one plus, uh, at the exponential of the urinary factor, uh, involve, uh, the UN every factor here for a y prime, the capital k equals, sorry, let me say that again. So when I'm computing the normalization constant here, I'm summing over all values of y prime capital, okay? Eh, of the exponential of the need, you and every factor Times d, uh, Alpha value, add the last column. Here's, I'm taking that value times the exponential, the unit urinary factor for why Prime k being called to one plus this value here at Times d exponential at the end, every factor at four. Why Prime Kevin?

Speaker 2:          13:51          Okay. Equals to a plus. This value here at times the exponential, the urinary factor for why Prime K equals three. So this is a specific example of course for uh, three labels. And if I had capital c labels that I do this over, all capitals see rose in that array. So notice that this whole procedure is in a, as a complexity of an o of uh, capital cane. This length of the sequence times c squared, capital c squares. So the number of potential labels for each, uh, element in the label sequence. So now we have something that's actually pulling pulley meal in the link. Does the sequence as well as being pulling on in the number of classes. Okay. So this is much more manageable. This is something we can do in a reasonable amount of time. And so we get this complexity because, um, we have two aid rates over a number of values.

Speaker 2:          14:47          That's in Oif gay. And then for each value of the label at Cape last one, there are s capitol c values such values, we have to perform a sum over the value of y prime k at so the label at position cane there are see such value. So we have c times c times case, so that's a k times c squared. All right, so that's a pseudo code for computing. Uh, the partition function and the at the same time computing this Alpha table, which as we'll see, we'll also be useful for performing other types of fingerprints.

Speaker 2:          15:24          As it turns out, we could have a actually done the computation from the last position. So summing over the last label and then finally summing over the first label instead. And that's because again, we can in decimation over all potential values for all labels it any place into a sequence a weekend. Instead, we can change the order of these nets that some like we want. And using the same argument that I could instead performed the sum over the last label, assuming a particular value for the previous to last label and that store that into a vector, which I'm going to call it better vector. So that's better capital case when I'm performing the sum over y prime capital k and then using that value, it could perform the son now, uh, over the previous, the last label. And I'm going to be reusing what I've computed before.

Speaker 2:          16:16          And then similarly, you have this better value instead of the Alpha value times the exponential of the unitary factor plus the pairwise factor. But now here over where I'm performing a some over the previous, the last label, I can get the new like this for, uh, all labels. And then until I reached the sum over the first label where I just have to take a sum of where the value for the first label of the exponential of its urinary factor value, a times the better value. So Beta two value for it. They given, uh, uh, value of y prime, one in the iteration for this sub because that's just exactly the same procedure. But instead of going from the first able to the last week, going from the last label to the first.

Speaker 2:          17:08          So I'm writing down here, uh, exactly the same algorithm, but now for computing what I'm going to call it, the Beta tables. Similarly in the Beta table, each column is going to correspond to the better vectors in the description of the algorithm, that previous slide. And so now what we'll do is that when stamp, we'll initialize the last column. So this column here is going to be better capital k y prime came on a swan or why prime key minus one is the, uh, uh, index of the row in that column. And so I compute it like this. That's the initial value for my table. And then I go instead from the, uh, uh, previous to last column until the first column. So this column here is going to be better too.

Speaker 1:          17:55          Okay.

Speaker 2:          17:55          And I perform a similar computation. But instead, when I'm computing the value here, instead of using a, as in the alpha table, the values and the previous column, I'm using the values in the, uh, what would be the next column if I'm ordering her columns like this, whenever I'm computing this value, I'm going to take each of the value in the column to the right, uh, multiplied that by the exponential living, a urinary factor plus the pairwise factor and perform a, performed a song that I'm going to put in the, uh, array element here. So I'm going to go from right to left like this until I filled out all of the table, the better table, and then I can finally computed partition function by summing over the first label. The exponential of these of its urinary factor times. The bit of value, uh, in the bit of values are taken from the first column here. Okay? So that's just an equivalent procedure for obtaining the partition function as that of x. And that's actually, if you're implementing, uh, the computation of the Alpha and Beta tables when we have debugging your code is to make sure that if you computing the partition function either from the Alpha table, the better table, you actually get exactly the same

Speaker 1:          19:13          value.

Speaker 2:          19:18          So computing both tables is a often referred to as the forward backward algorithm for conditional random fields. So where we've seen that Alpha is, uh, Chris wants to afford pass from left to right and better is a backward pass from right to left. Um, it has other names. So, uh, it's sometime referred to as the belief propagation algorithm for linear chain, sorry, f or The Sun Product Algorithm, uh, for the interchange CRFs. And notice that intuitively Alpha is the table that gives us what would be the value of the summation of the numerator, uh, for uh, all the terms that involve the labels from to the left of a given physician. And then bed, uh, gives us the similar some, but for the, uh, exponentiate a luck factors of all the labels that are to the right of the current position. And so for that reason there are actually going to be useful fall. So performing other types of inferences where I want to compute marginal probabilities over the sequence. Why want to some out other labels that could appear to the left or the right of a given position. So we'll see that in the following video.

Speaker 1:          20:32          Okay,

Speaker 2:          20:34          well now what I want to say is that if you want to get it as stable implementation of the forward backward or the belief propagation Algorithm for computing the uh, where we compute these alpha and Beta tables, it's actually better to work in log space. That is instead of computing and a table that contains the alpha or better values, it's contained instead the log alpha and the lug bed or values. So if we look at our recursions, it's just means that whenever we computing, uh, Alpha and Stan and we will compute luck Alpha and which would correspond to the log of the expression we had before the sum over the exponential locked factors. And then now, usually we would have times an Alpha here and we wouldn't have this term here, but because now in this procedure were reluctant work in log space, what we have is actually the Lug Alpha values.

Speaker 2:          21:27          Then I can just take the log alpha and put it inside the expectation, the Srd exponentiation here, uh, inside the exponential. Okay. So if I have this, then the exponential of that, uh, would indeed be alpha would be equivalent to multiplying by Alpha here. And similarly, we introduced the lug better into the exponential for the, uh, expression for the re re the update value in the forward or backward pass, in this case, the backward pass for the better table. So that's one part of getting the stable implementation. The other thing, however, is that whenever we computing a dialogue of a, some of the exponential of a set of terms, this, uh, this, uh, expression or this, um, operation lug, some ex operation could actually be a unstable if one of the exponentiate a term is actually quite large because then we get the exponential of our very large term and a, we might get a new miracle precision problems.

Speaker 2:          22:33          So actually a much more stable implementation of the Luxem x operation, uh, over a set of values is as follows. So imagine that, I'm just going to call it zed. I each of the terms that I'm doing over which I'm doing it luck, some ex operation. Uh, it turns out that this is equivalent to just taking the maximum value amongst all the setae values and then adding the log of the, some of the exponential of zed I minus the maximum, which is the same Max is here. So essentially I've, I've taken out that maximum term here and I pushed it then front here. And the advantage for this is that if I think the exponential of each setae minus the Max overalls at eyes, well then this term can only be either a zero of [inaudible] is the Max and maximizing that I or a, it's going to be negative.

Speaker 2:          23:28          And the exponential of a negative number is going to be smaller than one. So this storm will be relatively small. So we won't get an expression where computing numerically just explodes and against something like an infinite value, uh, for in terms of the position of the, uh, the machine and a, so I'll let you do as an exercise. Uh, why does expression is equivalent to this? Uh, but, uh, what we're essentially a leveraging is that fact that this here is actually a constant. And so, and then I'm leveraging also the fact that the sum of the exponential of is some is a product of exponentials. And if you leverage these two things, then you see that this will cancel out with that term and then you'd just get exactly disturbed here.

Speaker 1:          24:14          Okay?

Speaker 2:          24:15          All right. So this is actually very important. If you want to implement a belief propagation or forward backward, I'll get them in the linear chain. CRF, you really out to use this table, legs, uh, implementation that works in log space and implements a stable version of the lug, some ex operation here.

Speaker 1:          24:33          Okay.