Speaker 1:          00:00          This video, we'll see a few concepts related to optimization, which are going to help us understand how optimization behaves in training neural networks and, uh, give us ideas for how we could actually improve optimization of neural networks.

Speaker 2:          00:15          Okay.

Speaker 1:          00:15          So we've seen that neural network training is essentially based on optimizing a, an average loss or a regularized average lost if loss, if we have a regular riser. And, uh, what we've done is that, is that we've used a, uh, known, uh, a known optimization methods to cast a great in dissent and optimize that average loss, uh, to perform training to fit on your own network to our training data. So, uh, what about optimizing this optimization problem? Is it a, is it well behaved? What properties does it have? Unfortunately, for neural nets, uh, it's, uh, the optimization problem is, uh, not particularly easy. Uh, for one thing, there isn't a single global optimum to the optimization problem with trying to solve, uh, minimizing the average training loss or the average, uh, regularize training loss. Uh, we can see this because we can compete, we can permute all the hidden units along with their connections. And we actually don't change the value of f of x, the output value of the neural network. It hasn't changed at all. Uh, and so this means that there isn't a single unique solution that corresponds to the best training error we can achieve with a neural network. Any permutation of the hidden units will result in the same function, the same solution.

Speaker 2:          01:38          Okay.

Speaker 1:          01:38          Uh, so in that sense we can say that the parameters of a hidden layer, uh, are not identifiable. Okay. There we can't identify if their data set was truly generated from a neural network. We can't identify what's the actual weights of each unit. We can't identify actually the order in which these hidden units, uh, were, uh, organizing the original neural network. But that's not such a bad thing. As long as we get the right function f of x, uh, the right predictions, then we'd be happy with that. However, also the optimization problem is known as a non convex optimization problem, uh, and that even if they were identifiable, that would still be a lot of local minimum or local minima. So, uh, the optimization problem, instead of looking something like this where we have a sort of bullshit convex function where there's only a single global optimum, and if we followed the gradient steps, then eventually we would, uh, necessarily converge to The kobold optimum.

Speaker 1:          02:42          Uh, we actually have a function which has a lot of local, potentially a lot of local minima where for instance, we have a region here which locally looks likable. But if we got outside of it, we could find that there's another region which has a better solution. So this region here would correspond. And actually this value here that's optimizes the function locally here is known as a local minima and also other less obvious but still fairly annoying, uh, properties that we can find and optimization, optimizing, optimizing neural networks, a known as plateaus. So these would be regions where the function is essentially flat, is very close to just a flat line. Uh, if you think of the stochastic gradient descent algorithm, what it would be doing there is, or the just in general gradient descent a here, well it would follow the grain and the grain and because the function is almost flat is very close to zero.

Speaker 1:          03:39          So we make very, very small steps here as it's optimizing the function. And so, uh, because you don't get to see the whole function where you're optimizing this, you only see the progress of the training error because it doesn't change a lot. You might think, oh, I'm close to convergence and uh, maybe you'll stop before you actually reach this part here, which will drastically improve the error. And so these regions here can mean that. So the plateau here is as long it might be so long that for the amount of time you are willing to optimize your neural net, uh, you might actually never get out of the plateau. So that's another thing that we sometimes see in neural network and, uh, that are issues you need to think about for designing, uh, different potentially better training algorithms for neural networks. Um, so just keep in mind that we're optimizing neural nets.

Speaker 1:          04:30          We are not guaranteed to find a global Optima. We might just find a local optimum. And uh, and also it, even if the training or doesn't move much, it might just be that you're on the plateau and that eventually you'll, you'll be reaching a region where the training air will drastically improve. So if you see that your training error is moving very slowly and then eventually dropping a lot, this is something that's normal. It's necessarily a bug in your code that just might happen because the, the function is noncombat sin has long plateaus like these.

Speaker 1:          05:05          Now let's see these properties in action. We'll look at a demo that was provided by Andrei carpathy. Uh, and it's going to illustrate this a property of local optimum. That is the fact that if we take a neural network and initialize it at different values, we might converge to different local Optima. So if we look back at this, uh, cartoon case here, if we are an initialized gradient descent here, it would eventually converge to that optimum. Whereas here, it would actually have converged to slowly move to that optimum. So let's see this in action. Uh, so here, this is a classification problem where we have these dots here that are in one class. And then these dots here that are another class and that this is the, uh, uh, decision, uh, a decision boundary for, uh, the castigation that's performed by eight, this neural network. And here we have the training error. We see it's still going down a little bit, but it's, uh, it's mostly converged. Now let's see what happens if I reinitialize the neural net with different, uh, random weights. Now let's get to what it convergence. Does it change? Does it still have this shape or will they have a fairly different chain?

Speaker 2:          06:20          Okay,

Speaker 1:          06:21          sorry. It's retraining. And we see now that the shape has changed a little bit. So here we have, uh, this partier used to be not associated with that class, but with this class and that this part here also is a bit less flat. Um, interesting. That's initialize. Again, this looks a little bit more like before we've seem to have come, it seems to be converging to similar region. Uh, let's do that again, a little bit about, this is starting to be quite different. So we get a big region here associated with that class. So as we see with different initial, uh, different initialization, so different random seeds essentially that generate the random numbers and utilize the weights. We can get fairly different solutions. But of this is that it always at least perfectly classify as the training set. But a new examples which say might lie here, different neural nets, my life provide different answers depending on the initialization.

Speaker 2:          07:24          Okay.

Speaker 1:          07:25          Okay. Uh, I've also talked about grading the sand and I've, uh, you know, thus far just said that, you know, it works fine and it's a, uh, optimizes the training error. But what about convergence? So, uh, what are the conditions that we need to guarantee that it will eventually, uh, uh, converge to the true solution instead of, uh, at home point, uh, reach, uh, reach a value in maybe cycle and never actually zoom, zoom in on the actual solution? The two conditions we need is that a, so alpha t, which is the learning rate for the teeth, uh, update of the neural nets, uh, parameters. So the sum of all the learning waits for all iterations needs to divergent needs to go to infinity, but the sum of the square of the learning rates needs to converge. So that needs to be a converging some.

Speaker 1:          08:22          So notice that in the case where Alpha is constant, then this would, uh, this was actually a diverged. And so a constant learning rate does not guarantee that stochastic gradient descent will eventually converge. Okay. And I'll talk about why this isn't always a bad problem, but if you do want to satisfy these conditions, then uh, what we'll want to, uh, you know, the few decreasing, uh, strategies. One that's very common is this one. So we just take an initial learning rate. So that's our regular alphabet hyper parameter. And then we divided by one plus the number of updates we've done so far, times a decreasing constant or decrease constant, uh, which is another hyper parameter. Now that we need to select a, this is another strategy which has been proposed in the literature and it has certain properties. So we divide just by the number of either update so far, but uh, to the, uh, my decrease constant parameter.

Speaker 1:          09:25          And in this case we have to satisfy this condition. Uh, and, uh, looking at the literature, there are other proposals. This one is probably one of the most popular in general. It's actually a good idea to keep a fixed learning rate like we've seen in the originally I saw a fixed Alpha for uh, a few, uh, the first few updates with their first few epochs and then eventually start decreasing it. Uh, so we make a lot of, I guess initially, and then we start decreasing the learning rate so that it can zoom in and converge to a good value. Now, the reason that we might still use a constant alpha is that remember that we don't necessarily care about optimizing perfectly the training error because what we care about this generalization, where we care about this performing well on the test set. And so, uh, it might be that before we get to get, uh, to, uh, come close to the best training error configuration of the neural net, we actually weigh into a region that has a lot of our fittings.

Speaker 1:          10:28          So in the curve that we've seen where we have the validation set that's like this, the generalization performance in the training set, it might be that, um, you know, if we get to convergence, we actually going to get very, very bad a value on the validation set, a lot of over fitting. So because we don't care about optimizing the training set perfectly and we mainly care about getting a good generalization error, as long as the learning rate does not too big, then, uh, we very well might stop learning with early stopping way before the algorithm, uh, uh, starts cycling and not converging to particular lab value. So in practice, as long as Alpha is not too big, then, uh, we will, uh, early stopping. We'll have a stop training way before we get any problems in terms of convergence. Okay. So, um, unless you actually for some reason really, really care about getting a very good training error. Uh, if you think that actually you're not going to overfit, then you tend to almost be in the overfitting situation. A constant Alpha is often something that works well in practice.

Speaker 1:          11:37          Other, uh, variations on a gradient descent that we can see in the literature. Um, sometimes people use what's called minibatch learning. So a mini batch is just a set of maybe a hundred examples that we've picked randomly from the training set and idea is to, uh, instead of using the gradients over a single example to perform our update as in a stochastic gradient descent, we just use the average gradient of all the examples in my mini batch of enemy. Minibatch will often be about 128 examples, something like that. So the average gradient is the same thing as taking the grain of the average regularized loss or the average loss on the mini batch. And the idea is that this is going to be because we're taking the average of a bunch of, uh, uh, example, we're going to get a more accurate estimate of the true gradient of the, uh, uh, average, um, uh, of the average a loss.

Speaker 1:          12:39          I shouldn't say risk here. It's a more accurate estimate of the average, uh, loss gradient. And, uh, one advantage in terms of computations is that if we are working with mini batches, then we can write the uh, uh, forward prop and backdrop, uh, operations. So backpropagation, uh, using some matrix matrix operations, matrix multiplications and performing Matrix Matrix multiplications is going to be much more efficient with certain libraries like blast blast, which are just a very well known low level library for performing a linear Algebra operations. So Matrix, Matrix multiplications, I'm going to be much faster, uh, with the library like blast then, uh, performing 128 forward propagation and backpropagation. So performing all 128 grade him a computations simultaneously with Matrix Matrix operations will either procedure which is much more efficient. And that's because a library is like plus our cash aware and I try to optimize, uh, the memory access.

Speaker 1:          13:55          Um, so, uh, so yeah, I'll, I'll, I'll let you look at the literature for you can, you can do this. So you can try to think about how we can simultaneously compute gradients for many examples by performing instead of performing matrix vector operations and like in regular backdrop performing matrix, matrix multiplication. Um, and other approach that's often use is to instead of using the gradient of a single example, use an exponential average of gradients as we perform a gradient descent. So the uh, direction for updating our parameters for the teeth update could be in the instead of just, and so this is for the, uh, non non regularized loss. Instead of just taking the gradient of the loss for our current example will also combine it with better, which is going to be a hyper parameter times d exponential average that, uh, I had computers at the previous updates.

Speaker 1:          14:54          So if you look at this formula, you can see that at any time the update direction is a combination of not just the green and for the current training example, but also all the other previous training examples which are in this, uh, uh, previous term here. So this can be useful if we have plateaus by, uh, moving more quickly, getting a grade in which is larger. Uh, uh, if all the Graydon's always pointed the same direction. So intuitively it says if the optimization is gaining momentum when we, uh, when all the gradients are always pointing in the same direction. So it's as if we had a ball that's moving on a slope, which is always going straight. So then the ball keeps going faster and faster. It's gaining momentum. So it's the same idea here, which might allow us to move faster in the factors.

Speaker 1:          15:48          Funnily, I want to described a different method for, uh, in optimization for optimizing the function. Uh, it's called Newton's method. And a, it's an improved way of optimizing a particular function. And I want to mention it because we often see a variance of that basic Newton's method. In the papers on developing better neural network training algorithms. So I'll just give you a gist of the idea behind Newton's method and this will help you explore the literature on more advanced neural network training algorithms. So, uh, in Newton Newton's method, the idea is that if you want to minimize something, so imagine we have just a single example. You want to minimize, it's a training loss. So the loss when given x as input, uh, the neural network with a particular set of parameters, uh, predict something f of x and a that's comparable or why. So we want to optimize that loss.

Speaker 1:          16:49          Uh, well one thing we could do is say, well instead of optimizing this last, which is non comebacks and complicated, but just approximated around the point which corresponds to the current value of my parameter in my grading and descents, uh, migrant gradient descent procedure in my training procedure. And so what we could do is use a terrorist series of expansion up to, uh, the, uh, second order, a term where we say that, well, around a t, which is the current value of mine, neural nets parameters. Well, the value of the loss for any a set of parameters data is going to be close to that out. So it's going to be approximately this value, the value of the loss at the current parameter, plus the gradients at my current parameter times the difference between the theater, which is any potential value of the weights of my neural net, minus the current value of the weights.

Speaker 1:          17:51          And plus another term, the second order term, which involves a, uh, product, uh, left tried product with instead of being with the gradient, the, which is the, um, uh, vector of all first order derivatives, it's going to be the Eshun, which is the matrix of all second order, uh, dairy dips. And, uh, so we get, and then we get the left and right product here with, uh, the difference between the value of the weights minus the current value of the weights. And the idea behind Newton's method is to say, well, instead of optimizing the loss, I'll actually optimize this. Simpler function does simpler approximation, which comes from the dealer territories, series expansion. Uh, and, uh, what's nice is that the second order version, because it's a going to be quadratic, we get actually optimizing perfectly. So visually it means that they mentioned that it had dysfunction here.

Speaker 1:          18:49          So imagine this line was a true function and imagined that currently my weights. So this would be a very simple function with just a single parameter. Uh, well I would just locally approximate it with a quadratic function. Uh, sorry, like this. So I would just assume that this blue line is actually well approximated by this red line. And we see that in these very close to, uh, the current value of the parameter. It's very, the two lines are essentially on top of each other, but as we go further away, it's not as good. Um, but still locally it would be a good approximation. So it might be a good idea to just take that approximation and move directly to, it's a global minimum, which would be around here. And because it's a quadratic function, we can actually, uh, uh, find what's analytically, what's the true solution.

Speaker 1:          19:39          So we can take the derivatives with resect to our parameter of this a second order approximation and set it to zero. Uh, so this, you know, taking the derivative of a, with respect to theater of this whole expression, uh, uh, it gives me this and we can show that, uh, the solution then a curse sponsors. So the new point would actually be my current point, my current parameter value multiply as, sorry, minus the inverse of the session times degrading. So notice that this is actually quite similar to grading dissent. Great in the sense, instead of having a Hessian here, just has a single scalar alpha. Well, with Newton's method, we get to actually take into account the curvature of the function, which is taking into account by the, by the action. So in particular from a function which is very flat like this, um, this term here, the inverse session is going to be a quite big, which means that if I was here, then it would, um, this term being big, it means that it would, I, I would, uh, take a step that would be much, uh, bigger.

Speaker 1:          20:59          So I have a bigger term times the same gradient. Then if I had the function with high curvature, which would look something like this. So at this point here, because I have higher curvature, this term would be smaller. It would make a smaller smartest step. So the nice thing is that if you have a plateau like function here, the Hessian allows us to make bigger steps and so we might be able to move faster into a plateau where as if we end up very steep function, then it's actually going to adapt and take much smallest steps and updating the parameters. Now, um, there are still issues with this. The first one is that, uh, well this Hessian is a matrix of size, number of parameters by a number of parameters and then there will network with a few thousand units that easily reaches and the millions of parameters.

Speaker 1:          21:48          So this would be 1 million by 1 million, uh, matrix say, and this is way too big to even compute for every update. And it's also way too big to just, uh, inverse as well because inverting the matrix is also going to be very expensive. So this is only practical. They're very small neural network. And also it's only we can apply this update only if it's a, at least locally comebacks. If the function is locally convex, that is, if at a point I compute the question, uh, if it's locally convex at that point, I mean, uh, then the action is actually going to be positive. Definite, which means that this, this matrix here, this inverted matrix is going to be a positive, uh, definite. And, uh, so if I locally add a function, which is, uh, like this, then the Newton's method update. So in this case, I would have a Hessian which would be negative, definite.

Speaker 1:          22:42          And, uh, uh, if I was to, if I was to minimize the quadratic approximation of that function, say this was the quadratic approximation. And we clearly see that in this case, the optimum would, would be to take the value and increase it and an athlete. So a, so Newton's method is only practical if we have these two constraints. Uh, still there are a lot of albums. I take inspiration from Newton's method and, uh, try to, uh, either not have to rely on computing the full question and somehow make sure that the matrix that we have here is a still positive, definite. And so for that, I encourage you to look at the recommended readings on the course's website. Well, you see, uh, some, uh, examples of methods that try to correct for these issues and get a naturally practical logarithm for large neural networks.