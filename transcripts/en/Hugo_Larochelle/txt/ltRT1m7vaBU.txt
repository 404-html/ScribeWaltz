Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video we'll mention an alternative to maximum likelihood training of conditional random fields, Nolan as a sort of likelihood.

Speaker 2:          00:10          So we've seen in the previous video that, uh, we could get an expression for, uh, the, uh, negative log likelihood gradients, uh, with respect to the parameters, which allows us to, uh, then, you know, get a way, an expression to compute for performing gradient descent and perform minimizing negative log likelihood or perform maximum likelihood training of our conditional random fields. However, it did involve an expectation over, uh, the, uh, label, uh, or the target vector y. And while it might only involve a few variables, it's still requires performing an inference of what the marginal distribution over the subset of labels that are involved in the gradient, uh, might be. So we have to perform that in France. And unless we have a, uh, uh, uh, graph, which is a linear chain or a tree, uh, this, uh, this approximate inference, uh, so a estimating this marginal conditional distribution, uh, it's just an approximation and it might actually be very bad in practice. So some people have explored in instead, uh, an alternative, which is to just do without maximum likelihood training and just changed the last function to be used for training the conditional Mendham field.

Speaker 2:          01:29          So I just want to mention one such approach, which is known as pseudo likelihood. So the idea is that instead of trying to maximize the probability of the whole wide vector, given the input, uh, we might want to maximize instead each of the conditional of each why k given everything else, so all otherwise and the input so specifically will minimize the negative, some of the log of the probability of the true label at position k given all other labels at position one up two k minus one, then k plus one up to capitol came. So you can think of it as trying to predict a intern each y k not just from expert, from all other elements in the vector y. Uh, what's nice is that, uh, for this expression, we can usually tractably, uh, compute the exact gradience for optimizing that objective. Uh, so, and that's because each of these conditionals essentially just require that we normalize over all values of y, k and a usually white.

Speaker 2:          02:39          You will take a fairly small number of values. So it's going to be similar to performing irregular, soft Max computation and the regular neural network. Though now this probability will be influenced not just by the input but also by other label values. Uh, and the other label values are actually going to be the true label values, uh, in the target vector. And, uh, and the other thing that's nice is that each of these conditionals for, uh, the conditional random fields is typically only really going to depend on a few of the other target variables. And that's because of the local mark of property of a conditional random fields. If we look at its markup network, then we know that, uh, the, uh, probably have, why can't given everything else reduces to the property of white gay given only its neighbors. So it's actually going to be a much simpler expression than just this depending on the graph structure on the mark of network.

Speaker 2:          03:36          Um, so, uh, that being said, we still to make prediction, we would still normally a need to compute the marginal distribution, which might still be intractable. So it makes training more tractable. But at test time we make a prediction. If you want to use prediction based on the marginals, we still have to do some approximate inference. So that's doesn't entirely solve our problem. So all of this that the reason we have to do this and that this isn't a problem here is that here we are conditioning on the true labels. So we don't have to infer what the other labels might be. We actually know them at training time. So we exploiting that our training time. But at test time we can't, and I should say that in practice we often find that pseudo likelihood works less well. So it gives a models are not as good as models that are trained by maximum likelihood.

Speaker 2:          04:26          And there's some theory for, uh, backing up why we should observe this. So I would not necessarily a suggest someone uses, so the likelihood, except perhaps on a very, very complicated model. Uh, but it's good to know that there are alternatives to conditional random fields than, uh, training them by maximum likelihood, uh, and, uh, started alternatives will be sort of nicer in terms of the computations that are required for, for training them. And a training conditional random fields in general is a very active field of research. Uh, but with all we've seen in the Er, this veto in the previous ones, that should give you a, uh, a headstart for looking at that literature more.