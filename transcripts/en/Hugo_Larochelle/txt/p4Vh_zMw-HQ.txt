Speaker 1:          00:01          In this video will introduce a new type of neural network called the restricted Boltzmann machine and will present its definition. So this is our, our first example of a neural network for unsupervised learning. So what have we seen before is neural networks that would learn to predict from an input a particular target. So he could learn to predict a class label or I could learn to predict from an input sequence, a sequence of class labels. That's what we've seen before with the feed forward neural network and with the conditional random fields. Now we're going to talk about the neural network that performs unsupervised learning. That is, it's going to learn something about the data based on the training set that only contains input vectors, uh, they use for that. Uh, also application of unsupervised learning and, uh, an unsupervised learning or neural network like a restricted Boltzmann machine is going to be a to extract meaningful features, uh, about your data that will hopefully make it more explicit.

Speaker 1:          01:06          Uh, what is, for instance, the class to which a particular input belongs to or make it easier to predict other, uh, types of, uh, information about your data that you might be interested in. It's also going to allow us to leverage the availability of unlabeled data. So imagine you actually have a very small label training set, but you have a lot of unlabeled examples. So for instance, you might have a classification of image problem. Uh, perhaps you only have a few labeled images, but you could go on the Internet. They actually find a lot of unlabeled images that have not been classified into a as belonging to the particular labels you're interested in. So that's going to allow us to do that. And that's actually for, those are, uh, no more about machine learning. And that's known as semi supervised learning. The problem of learning from a Dataset which contains a little bit of labeled data and a lot of unlabeled data.

Speaker 1:          02:02          And we've also discussed that if we perform generative learning as opposed to discriminative learning, uh, then we in the, our objective function, we have a turn that looks like this minus the log of the probability of, uh, the, uh, input and, uh, we've, uh, justified using generative learning as a performing some sort of regular riser that's data dependent. And uh, so by introducing some unsupervised learning, uh, and sort of in models, we can, we will be able to think about it as a way of performing this data dependent regularization by doing unsupervised learning in particular this term, uh, we can actually compute it for uh, uh, an input vector even if we don't have a label for it. So again, you know, uh, dysregulation, we can think of it as helping us, uh, perform a particular castigation problem. If a or any other prediction problem, if we have some on labeled data. So I'm in the following videos, we'll see three different neural networks for unsupervised learning. And then the, uh, next few videos we'll concentrate on the restrictive bolsa machine and neural network.

Speaker 1:          03:15          So here's an illustration of an RBM. So an RBM is, uh, a, uh, an undirected graphical model that defines a distribution over some input vector x. And uh, it's going to model the distribution of that, uh, those vectors in my training data ex, uh, using a layer of a binary, uh, hidden units, which I'm going to call h. So here h is actually a random variable. Much like x in my model is also a random variable. Uh, so I'm going to assume that my visible, uh, layer, which corresponds to the random variable x is going to consist of a binary units. Uh, and we'll talk a bit later about how, whether we can and how we can actually generalize the restrictive Bolsa machine to other types of observations that might be real value door, uh, discrete value and not just binary value. So the respect of the most of the machine is going to define the distribution over x.

Speaker 1:          04:20          And that distribution is actually going to involve some latent variables which correspond to my binary hidden units that we were going to get. That distribution is that will for the fun and energy function, uh, which is as follows, it's linear in either age or x. So it's going to be the product of the vector of hidden units, times a matrix of connections w times x. Uh, then it's also going to involve, there's going to be minus a bias vector c times, uh, my vector x and then minus my bias vector B times h. So we can also write it into scalar form where we have explicit sons over the, uh, hidden units or the inputs and, uh, and, and so we see that essentially the energy is going to be the sum of each of these terms here and to obtain a distribution or probability to obtain probabilities from this energy function.

Speaker 1:          05:23          Uh, well we'll do what's the most sensible thing to do, a much like in a physicist goal system, the probability of observing a particular configuration of our variable of interest is going to be the exponential of minus the energy associated with the value of x and h. And then we're going to divide by normalization constant or partition function, which unfortunately for restrictive Bolsa machine is actually in tractable. So this is that. Here is the sum of the numerator overall values of x and h and since x n h r a binary, uh, there's an exponential number of values that they can take. And so computing that partition function here in practice going to be, uh, in the general case in tractable. So we have to correct for that problem and we'll see how we do that in the subsequent videos. Now know this, that, uh, first of bias vector, if a bias CK is negative, then it means that if x case equal to one, that's going to, uh, decrease this term.

Speaker 1:          06:31          So it's because we have a minus here, it's going to increase the energy and, uh, since high energies associated with low probabilities, then having a bias vector that's negative means that we're are going to, um, express a preference for ex gay, uh, not being equal to one. So instead of would prefer if it was zero, uh, and uh, the opposite would also be true. If a CK was positive, then we'll, uh, this would mean that we'd actually prefer xk being equal to one then being called to zero just based on that term, uh, the probability of x kb a one if CK is positive will actually be a greater, we have a similar thing with the biases be Jay here. If they're positive, then it means we have a preference for HJ being or Taiwan if they're negative at zero. And then the most important part of the model is really the, this connection Matrix here.

Speaker 1:          07:26          So what each connection is going to model is our preference for both h j an xk being equal to a one. So if it's negative, then it means that if h j or h and h k is equal to one, if this is negative, then it means that the probable the of observing this under a model is going to be decreased. Whereas if either one was zero, uh, then this would be preferred. This would because either one, if he, if he didn't want a zero, then we multiply. Then this is zero times whichever value is here. So that's going to be associated with a lower energy. And similarly, if this is positive that it means that when j n x gay is equal to one, then the energy is going to be decreased because of the minus here. So the probability is going to be greater.

Speaker 1:          08:15          Okay. So this is just a intuitive description of how these different parameters affect the probability of observing a particular configuration for the vector x and the vector h. All right, so that's for the definition or restricted Boltzmann machine. We have bias vectors. We have a matrix of connections. Uh, the part of the model that models the inputs that involves the variables corresponding to our input vectors. A, we call it the visible layer visible because this is the data who actually see where has the hidden layer, uh, which here is a random variable is a hidden, because we don't actually know for a given input vector x, what's the vector of hidden units? This is a latent variable nar

Speaker 2:          08:59          model.

Speaker 1:          09:02          We can represent that as a mark of network. Um, so what we had before was more a informal representation of what the model is. But if we use the mark of network illustration and a, if we assume that now our nodes are going to be the vectors x and h Dan, we just a have a, uh, an edge between x and h because they're sharing a factor. So indeed p of x h is the exponential of minus the energy. And remember that minus the n or the energy is a den negative. Some of this term, this term and this term. So the two minuses cancel out. So we get the exponential of that term plus that term. What's that term? And because the, some of the exponential of this psalm is the product of exponentials, then we get this factor here at times this factor times this factor. Okay.

Speaker 2:          09:52          Yeah.

Speaker 1:          09:53          So then what they should base on an energy function is really just an alternative way of representing a product of factors, uh, by just taking the exponential of minus the energy. Uh, all this, some terms here, they will translate into factors in the more factorial representation of the distribution.

Speaker 2:          10:13          Okay.

Speaker 1:          10:14          A more representative, a illustration of this would be to use, uh, a node for each scaler in our models. So for each scale or value in the vector x and each a component in our vector h and then in this case we see that, uh, we must draw edges between each pair of units in the visible layer with an end, uh, hidden there. Uh, and that's because if we write it as a product of factors, we do get this pairwise factor here, which involves both h j, an ex ex gay for all values of j and k. And then the, this factor is parameterized by the entry in the Matrix w. And similarly, we have these funerary factors here, which I express a preference individually for either x or h. And notice that here we don't have any interactions between either day hidden units when each other or the visible units with each other. And that's why we say it's a restricted Boltzmann machine. Uh, we restricted the connectivity by allowing only connections between the visible and the hidden layers.

Speaker 1:          11:24          And finally, if we instead look at the factor graph visualization of the restricted Boltzmann machine, then of course for a, we get a factor for each pair wise connection between elements in the visible and hidden units, or each of our pairwise factors are in the street up here. But it also have the urinary factors here, which involves just a single a scale or a variable in our model. Yeah. So that's just what we've gained from the fact they're graphic illustration is that we now explicitly see that we actually have, uh, that we have a urinary factors for both h and x. All right. So that's, uh, our definition of the restrictive bolsa machine. And in the subsequent video we'll see how to do France in that model and how to train it on some data.