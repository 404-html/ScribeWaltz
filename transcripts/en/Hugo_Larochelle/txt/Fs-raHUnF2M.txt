Speaker 1:          00:00          In this video, we'll discuss the problem of performing models selection with neural networks. Okay. So, uh, we've seen how to train a neural network with a specific number of hidden units, specific number of hidden layers. We saw that stochastic gradient descent could be used. Uh, in stochastic gradient descent we have to specify a number of iterations or epochs or the number of times we go over all the training examples, uh, and update the weights number of times. We'll repeat that procedure. Uh, we have to specify the learning rates. So the Alpha, the Alpha hyper parameter that we had to specify, I'm sorry, a lot of these, uh, met up parameters or we call them hyper parameters that are required. If you want to train a specific neural network, we need alpha, we need the number of iterations. We need a number of hidden units, uh, and the number of hidden layers as well.

Speaker 1:          01:00          Uh, and uh, uh, we need also the lambda regular riser for the virtualization. Um, so other, we set these values, uh, what's the most common procedure for, for performing this. So what we do is typically a and a, so this is a perhaps the most, the simplest procedure. Uh, and I'll only described this one. You can also see other procedures for performing model selection in the machine learning nurture. But the most common procedure is to take your whole data set d and split it into three subsets. It'd be a first subset, which will come call the training set. Uh, and uh, so that could be about 70% of your data and that's going to be used to train your model. In our case, to performance the CAHSEE grade in descent on a foreign neural network. Then we're going to have perhaps 15% of the data of all of our data, which we'll put in something, we call it a validation set.

Speaker 1:          01:59          And the validation set is going to be used to determine which hyper parameters, which values for number of iterations, Alpha and number of hidden units, the regularization wait, which values four doors, hyper parameters are good values, correspond to a good performance on that set of data. And then finally, once we've determined the hyper parameters and we've trained a neural network with these hyper parameters that we chose as being the best hyper parameters we can find, we'll have a test set, which in this case we'd be the rest of our data, the 15% remaining, which is only gonna need to be used to measure the performance of our models. So in classification, which is the main, the main problem we're looking at here, we'd measure our classification error on the test set of the neural net. We've trained with the best hyper parameters based on the validation set.

Speaker 1:          02:50          So one thing that's extremely important in machine learning that's really, really key is that what we care about is not whether our neural net is good on the training set. Where we care about is how well it generalizes to new examples. So how well does it perform an unseen examples? And that's the role of the test set. It's there to act as a collection of unseen examples that I've not been used either to fit the weights of our neural net or to choose the configuration, uh, of, uh, during that work, how many hidden units, how many layers, and to what extent we would actually train in on the training set. Okay. So this is what we care about, uh, in machine learning where are mainly interested. So the success of a machine learning algorithm is how well it performs on a test set, on new examples. So we'll look at, so we've seen that stochastic green in this sense is an algorithm for training and their own network with some given hyper parameters. Now let's see how we can actually choose, uh, the hyper parameters. So what's the method we should use for searching for good values of the hyper parameters?

Speaker 2:          03:58          Okay.

Speaker 1:          04:00          Um, so one of the most, uh, so this is the problem is selecting these hyper parameters known as model selection. Um, and, uh, here's a few different ways of performing. The most popular one is, is grid search, which I had described here. So I imagine though to simplify that we have to hyper parameters. Uh, we'll have the number of the number of hidden units in a single hidden layer neural network. So it wasn't just a one hidden layer and we'll have the Alpha, which is the learning rate. So in grid search, what we do is that for each hyper parameter we are interested in, are we going to list a set of values? Would like to try out for a number of vignettes could be say 10 a hundred and maybe a thousand and for the learning rate, maybe 0.05 and 0.005 and that will will do is that we'll try all possible configurations are combinations of these values will try 10 with 0.05 a hundred zero 0.0 5,000 0.05 and 10 with 0.005 and so what?

Speaker 1:          05:09          So because we have a three here, three values and we have, sorry, three values. And we have two values here. That means that all configuration I'll combinations correct, will give us six pairs of number of hidden units. And, uh, learning, learning, learning, learning rates. And so, so that's grid search. We try all of these six values and then we'll look at the resulting neural network that was trained for either 10 hidden units and 0.5, uh, learning right or 100,000 units and 0.05, sorry, 0.05 learning rate. And so on, which we look at all of these six neural nets, which one performs better on the validation set and the one that performs better? We remember what were the hyper parameters we used. And these will be our selected a hyper parameters, uh, that we'll consider that as the best options. Um, and other alternative, which works surprisingly well is to perform something like a random search.

Speaker 1:          06:11          So the idea is that instead of specifying your list, like here are a list of values for each hyper parameter will specify a distribution of values would like to try out. So for the number of hidden units, we might be saying that, well, we'd like a uniform distribution on the set of integers from one to a thousand. And for the learning rate, uh, we could have a uniform distribution, but, uh, between, uh, between one and zero, and maybe we'll have a lug uniform distribution. So a uniform distribution on the luck scale of, of Alpha [inaudible]. And then, so what we do here is that instead of trying all combination, so here we have distribution. So instead what we do is that we specify, okay, I want to try maybe, uh, 10 different configurations of the hyper parameters and to get each configuration, I'll sample independently from these distribution.

Speaker 1:          07:13          So maybe, uh, you know, the first trial that I want to do, we'll generate a sample for a number of vignettes of like 59, and then maybe the learning rate would be 0.01, two five a. So that will be one trial. And then maybe another trial might be try to a hundred, then a one hidden units and the learning rate of 0.13 a. So I'll get, you know, 10 configurations like this because I specify that I had enough time to do 10 experiments. And again, what we do is that we picked a neural net, uh, with it's a and the hyper parameters of the neural net that performs best on the validation set, uh, as the selected hyper parameter. Again, we always use the validation set to select the best configuration and, uh, it's you, we can always decide to actually go back and refine either our distribution if you're using random search or the, uh, uh, the values in the grid.

Speaker 1:          08:15          So in the list of values for each hyper parameter, uh, based on the performance that we got from a first grid search or first random search. So we can always go back and forth between specifying a grid, evaluating all points on that grid, and then re specifying a new grid, trying all the values on that grid, and tried to improve as best as possible, the performance and the validation set. Okay. But the important thing is that at the very end, once we've found our best pair of hyper parameters, we now evaluate the neural network with these hyper parameters on the test set. And that's the performance of a, that we can expect from that neural net on new examples because the test set was not used either to train the neural net or to choose these values of a hyper parameters.

Speaker 1:          09:07          Uh, finally if a for the number of epochs or sometimes I'll say number of the training iterations. Uh, so that's the number of times we loop over all training examples. Uh, so for the number of epochs, uh, instead of saying, okay, well I'll try 10 iterations and then run the simulation and see what's a performance and validation set. Then they'll say, okay, well maybe I'll try a hundred iterations instead. And so on, uh, so trained from scratch, a new neural network with a hundred iterations. What we can do is instead is to actually track the validation set error, um, as training progressive's. And what we'll typically do if you also track the training error, what we typically see is a curve like this. We'll see that the training error will always get better and better and better. That's because, uh, we're, uh, more or less directly optimizing the training error by doing stochastic gradient descent.

Speaker 1:          10:07          So we expect it to always decrease. However, the validation set era will start being better and better and then eventually we'll become higher. Actually, the, the real thing that's happening is that the gap between the training and the validation set error is expected to always increase. And so for that reason, at one point, the training or won't increase a one decrease enough for the validation set to also decrease, they'll start increasing in absolute terms. And so what we want to find is the number of the box here that corresponds to the best validation set error. And so what we can do is that we maybe train the neural network for one iteration and maybe that's the performance. And then for one more iteration, we start from that neural net and do one more iteration. So we get these values here and then do one more iteration and get these values.

Speaker 1:          11:00          And once we reach here, we'll see that now the validation set has increased. So that's an indication that we're starting to over fit. And so, uh, because you know, there might be some, uh, some variation across iterations. It could go back down a little bit. Uh, and so for that reason, instead of stopping right away, often we use something, I call a look ahead. So we'll instead allow for training a few more iterations and once say, I will use a look at a five once the a validation set or as not improved, uh, four, five iterations that I'll just stop and I'll go back to the neural net that had the best validation cetera. So I'll remember that. I'll maybe save the weights of my neural net for that number of iterations. And that's the, that, so that's a, the procedure known as early stopping.

Speaker 1:          11:51          And that's the procedure that we often use to specifically for determining the number of training our POCs or training iterations for the neural network. All right. And I should say that this curve here, uh, we expect to find this group not just for the number of epochs, but also for the number of hidden units. So if we train a neural net with say, 10 hidden units and then we'll train another neural net with 20 in a hidden units and other one at 30, the one with 40 and so on. We also expect that the gap between the training and validation set there isn't going to increase. And so if I have a neural net that's too small, that is, it could may be made bigger and we'd obtain a better validation set there. We say that we're under fitting and if it's, if it's the other way around, if now our neural nets that we're trying is actually too big and we should decrease it's size, it's size, we'll say it's overfitting.

Speaker 1:          12:43          So it's, it's essentially starting to learn by heart what's in the valid training set and that's not helping it, that's actually hurting it in terms of how it performs on examples that it's hasn't seen the examples in the validation set. And just in general here, this label could be replaced by any hyper parameter that increases capacity. So the more capacity I give to my neural network, uh, the more the gap between the training set and validation set there is going to increase. All right? So that covers the different notions for doing model selection. We can use grid search or randomize search, uh, to select, uh, the number of hidden units and the learning rates and so on. And for the number of iterations we can actually use early stopping. So given the number of immune, it's in a, say, a learning rate, I can use early stopping for these two hyper parameters to select a, what's the right number of iterations for that number of hidden units and that, that learning, right? Right. So that covers model selection.