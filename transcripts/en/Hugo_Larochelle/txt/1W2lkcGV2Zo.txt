Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video we'll see how to compute the pairwise luck factor parameter gradients.

Speaker 1:          00:07          Okay.

Speaker 2:          00:07          So we've seen before how to uh, what loss function we would use for training conditional random fields. And we also partly saw our, to compute the perimeter gradients of the last function. We've seen, uh, how to compute the perimeter gradients for the law involved in the log factors, the urinary luck factors. And I will see for the pairwise luck factors.

Speaker 2:          00:30          So we let their patients similar to what we've seen in the previous video. Uh, we can show that the partial they live up the negative log of y given x. Uh, with respect to the pairwise factor, uh, positionK for observing why the value of a label, why prime gay followed by a value of a label. Why Prime Cape Plus one is actually equal to minus, uh, whether the kith label in sequence y is equal to y prime gay and the k plus one yet label is equal to y prime k plus one minus the probability according to the model that the key of Labor would be by prime gay. And they keep plus one year Labor would be y prime plus one. So we see that it actually has a form which is quite similar to uh, the uh, previous, uh, green and for the uh, urinary luck factor, uh, partial derivatives.

Speaker 2:          01:28          Again, it's uh, what is observed in the data minus what the model thinks. And also the, uh, now the partial data with respect to the parameters of the pairwise luck factor is quite simple. Uh, because we're modeling directly the uh, pairwise lug you, uh, the pairwise luck factors with the Matrix v then, uh, so in other words, the, uh, remove the ink here, but the pairwise factor value for y prime, why y y prime keep loss one is simply the element that role, why Prime K and a column y prime plus one in the Matrix v? Well because of that, the partial their lives for that entry in the matrix fee is the similar term that we saw here. But now we have to summit over all position came because this matrix is used for all positions. K uh, for uh, essentially for all the pairwise log factor across the sequence.

Speaker 2:          02:31          And now we can write it also in vector form. So get the gradient, uh, by uh, so the gradient of the loss with respect to the full matrix fee and uh, we can show that it occurs funds to this, some of minus the one on vector with a one that position why k times the transpose that the one that vector with the one that position why k plus one minus the matrix a that contains all the pairwise marginal probabilities that he's all the probabilities of the kid label being a white cane and they keep us one year label being why keep plus one. So essentially this matrix would contain all of these probabilities here for a position,K and k plus one and a. Um, another way of writing this is to perform the sum here over these terms. So distribute the sum here. So I have to sum over the pairwise matrix and then I can just write this here as just a frequency of observing a given label followed by another given label. So I'm writing it, I'm noting this down as frequent a frack yk. Why K plus one? Uh, you know, I could've written just, uh, you know why and say why prime theK is not important. It's just a number of times that a given level of why was followed by another label. Why prime in the sequence. And I have the full matrix that contains all of these pay wise label frequencies. So that's just another way of writing down the expression for the gradient, which respective.

Speaker 1:          04:13          Okay.

Speaker 2:          04:14          So that's pretty much all for the parameter gradients. A quick word on regularization, uh, we can use the same regular riser as we use in the non sequential neural network. Um, and, uh, uh, normally we have a regularizing term for it, the connection Matrix, the matrix fee, uh, but we still don't recognize the biases and the neural network. Um, we could decide to scale the amount of regularization, uh, by the sequence size. Um, so if we, uh, thought that large sequences don't, uh, necessarily have more information about what the right parameters should be, then shorter sequences, then that would allow us, for instance, to penalize or regularize more or longer sequences and less on smaller sequences. So, you know, that's a veteran that you might see in the literature that, uh, people might try to adapt for some given problem.

Speaker 1:          05:11          Okay.

Speaker 2:          05:12          Uh, but, uh, otherwise the regularization gradients are, are essentially the same as we've seen before. And so now with the loss and the regularization gradients and the parameter gradients, uh, we have all the Greens and we need to apply stochastic gradient descent. And A, I haven't mentioned the initialization, but we use exactly the same as we've used, uh, in the, uh, non sequential neural networks. And also, perhaps I should say that for initialization, for the Matrix v, it's fine to initialize it to zero. So that pretty much gives us all the pieces we need to apply stochastic gradient descent training for conditional random fields. And, uh, that's it.

Speaker 1:          05:53          Okay.