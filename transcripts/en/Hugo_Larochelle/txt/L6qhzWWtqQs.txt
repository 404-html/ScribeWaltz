Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video we'll see how we can perform imprints and a spar scoring model and compute delayed and representation for giving input.

Speaker 1:          00:08          Okay,

Speaker 2:          00:09          so we've seen the objective in a sparse coding. We're trying to find a dictionary matrix such that we can encode each of our training input into a representation which contains information about what was the original input and is also sparse and a, so to compute this objective and also to perform, to optimize it, uh, we need in the inner lobe of this inner loop of the Psalm to actually be able to perform the minimization here. In other words, given an x t, uh, we need to be able to encode our input into the sparse representation. So to find these parts representation that optimizes the, uh, reconstruction error plus a sparsity penalty, uh, objective. So in this video, we'll see now go to them for doing that.

Speaker 1:          01:01          Okay,

Speaker 2:          01:02          so let's assume for now that, uh, we are given a dictionary matrix. Now we have to infer the latent representation, the sparse representation h of x team. So a what do we have to do is optimize this combination of the squared or reconstruction term and uh, sparsity term on the HD. We have to optimize that with respect to the later and representation. So we have to optimize the combination, the sum of a quadratic function with a, uh, an l one type function. Now I, one thing we could do is great dissent. We've uh, used gradient descent algorithms before, so why not for this particular problem? Um, and so we could compute the initialize, the later representation to some value computer gradient with respect to the current value of the late and representation and updated using the gradient and formation. So by going in the opposite direction of the gradient.

Speaker 2:          02:05          So we see that the gradient has this term here, which comes from the reconstruction term and a term here, which we can recognize from a, you know, with the sine function. And we typically see when we have an l one type function to optimize what are we looking at the grading of another one function so we could perform great in the updating the Leighton representation, uh, uh, until it converges. And, uh, uh, we found that an optimal value for this, uh, one thing that could be said about this particular problem is that because it's the sum of two convex functions, the, uh, than the actual objective is convex. So unlike, say, training a neural network, this optimization problem is, uh, has a unique global Optima. And so, uh, and that's, it's, it's a fairly easy problem to optimize.

Speaker 2:          03:02          Let's look at the gradient for single hidden unit. So, uh, we have the green and from the or the partial there, they've from the squared reconstruction. So it's the difference between the reconstruction and the actual input multiplied by the, uh, uh, the vector that corresponds to the, uh, the, uh, that hidden units, weights with the input. So that hidden units, uh, Adam or dictionary element to have this part. And then we have the sign here. Now we know that the l one norm encourages values. In this case, the hidden units to be a or forces summer will force some of them to be exactly zero. Now if we do regular great in dissent, um, uh, actually it's actually very unlikely that we'll, uh, do a grain and audit and we'll fall exactly on an h value, which is equal to zero. And just actually generally speaking, the l one norm is not differentiable at zero.

Speaker 2:          04:02          So in that sense, we can't really, uh, this is great in the sentence is not to properly define, uh, for this objective because we don't have the gradient at exactly zero, even though we actually expect that some of the hidden units will fall exactly. It should fall exactly on zero because, uh, he had one Norman courageous Parsi. So a solution for this, uh, uh, is uh, to do the following. So I won't explain the rational theory behind why this is a good thing to do. I'll just give you an intuition for why this could work. So, um, what we'll do is that we'll do something very close to grade in the sense, and a, I will alternate between performing an update based on the grain and reconstruction error and then performing it update based on the gradient of the, uh, s a l one. So, uh, based on this sign here, but what we'll do is that if the value of the of a hidden unit changes sign only because the l one norm gradient is pushing it towards Zeros and uh, because of the step size or learning rate we've been using, it's a, uh, it's actually gone past zero.

Speaker 2:          05:12          So it changed the sign then, uh, if it changes sign, we actually clamp the a latent representation unit or the hidden unit to exactly zero and a so intuitively the idea here is that if it's the gradient from these parts of the term that is switching the sign, uh, well actually the sparsity in turn, we know that it prefers everything to be zero. So presumably it's trying to set it exactly at zero. So we'll do that. We'll set it exactly two zero if it changes sign. So that yields the following algorithm. So first we are update each of the unit based on the gradient descent update rule for only the reconstruction. So we're taking the previous value and then we're subtracting some step size or learning rate times degraded or the reconstruction.

Speaker 2:          06:07          And then we check whether the sign of, um, uh, yeah, does sign of the new value for the hidden units is different from the sign of the same hidden unit. But after we would apply, uh, the, uh, gradient update based on the gradient for the l one norm. So in other words, this is another way of, of, uh, writing or this in words, this would correspond to checking whether h a key t is changing sign. If it's changing sign, then we set it to zero. And otherwise we just applied the regular updates. So we just subtract a alpha times lambda, which is the regularization or the sparsity wait times the sign of, uh, of H. Okay. So we essentially shrinking age towards zero. And so we applied these updates for all hidden units, and then we repeat the procedure until, uh, di values of the hidden units doesn't change too much. So by defining some sort of convergence criteria.

Speaker 2:          07:13          So this algorithm is known as [inaudible] iterative shrinkage and threshold and algorithm. Uh, so we have the web, it's version in the, and the vector form. We initialize all the whole vector of uh, uh, as far as coding representations, Francis, we could initialize it to everything to zero and then until the factor has converged, we updated based on the reconstruction era gradient. And then we applied this shrink function here. So the string function is, it does exactly what I said before for each of the unit. It looks at whether the, uh, update, uh, would make the HA each hidden unit change sign if it, and if it does, you can pick to zero. Otherwise you just shrink it towards zero. Okay, so the shrink function, uh, I define it more formally here between a and B. So a would be the age here and B would be this, um, it's actually a scaler, um, uh, in this case it's a scalar, it's the, it's a vector a that consists of on the disk value.

Speaker 2:          08:18          So what does shrink operation does is that it takes the absolute value of each element in the vector subtracts it a value here, which has got to be positive. So this is always going to be positive. So it shrinks it towards zero. But now if by shrinking it, it actually becomes negative, we're taking the absolute balance or in other words it's changing sign. Then we're going to set it to zero. So we're taking the Max between this n zero and then we multiply back by the sign because we have had we moved the sign up. The element here. Okay, so visually this is what the shrink function looks like. If the input of district function, the part in a here is smaller than be ISO, which in this case bi corresponded to 10 it'll be, I would be this Alpha Times lambda here. If it's smaller than that value in absolute value. So either, so it's eh within this interval then the shrink it, it's going to output exactly zero and otherwise it's going to grow linearly. So re so notice that one step 1.5 the output value is 0.5 so that's for bi equals two equal to one. So it's just, it shrank towards zero by subtracting one and if subtracting one would have changed a sign of the element that it just outputs zero. Exactly.

Speaker 1:          09:41          Okay.

Speaker 2:          09:41          And now this algorithm is known to converge. If a one over the step size, Eh, uh, is bigger than the largest can value of the, uh, of this matrix here. So it's just a theoretical condition for, uh, for convergence. So in other words, if a alpha, so if one over Alpha is big enough or in other words, if Alpha is small enough and then the dis will converge. And uh, and this is a fairly popular algorithm for performing infants. So once we're done, the algorithm returns what we call h of x t. It returns the sparse representation for, uh, an input. And we know this, that this, which is equivalent to encoding the input into a hidden layer is much more complex. Gated is they're fairly complicated iterative process, uh, just more complicated compared to in an auto encoder. We have, uh, uh, linear transform, uh, followed by non linearity.

Speaker 2:          10:41          So in fact, what happens here is that the hidden units are sort of competing with each other to explain it and they're competing because there's a parcel sparsity terms that wants everything to be zero. And so effectively what's going to happen is the hidden units that best explains something in the input and explains it better than another, a hidden units are going to be none, zero. And the other things that are sort of overshadowed by other hitting you and star, uh, that are more appropriate for the input are going to be shrink to zero. So there's this spark scoring model one property it has, is that hitting you? It's compete with each other for explaining the input.

Speaker 2:          11:18          Uh, there are other, um, uh, improvements we can make so fun since it's the updates all hidden units simultaneously, which might be a wasteful because, uh, maybe some hidden units will come verge faster than others. And, uh, so one idea is to update only the most promising hidden units at every iteration. So we do less computers in every iteration. Um, and uh, if I suggest, I recommend you look at this paper here, uh, which, uh, it actually mentions, uh, so this, uh, paper doesn't propose this algorithm, but it describes and the, uh, fairly well. Uh, so it describes the coroner descent algorithm for performing this inference problem. In freeing this part's codes. And one advantage it has is that it does not require a learning rate or step size for, uh, like Ista, which requires an Alpha in updating the, a latent representation. So, uh, if you're interested, I, uh, if you want those faster algorithm that also has the advantage of not requiring specifying a learning rate for doing inference, I suggest you look at this particular paper. All right? So that sums up, uh, uh, what we have to say about how to we in first schools for a given input.