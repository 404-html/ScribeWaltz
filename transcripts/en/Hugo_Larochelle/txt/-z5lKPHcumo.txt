Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video we'll see a more general algorithm for performing in France in general conditional random fields known as belief propagation.

Speaker 1:          00:10          Okay.

Speaker 2:          00:11          So we've seen in the previous video that we could, uh, visually illustrate a, uh, undirected graphical model like a conditional random fields that we can be written as a product of factors into a factor graph where we have a graph it notes that correspond to random variables and other notes that correspond to the different factors in the model.

Speaker 1:          00:35          Okay.

Speaker 2:          00:36          One advantage of the fact of graph is that it provides a good representation for a actually deriving and illustrating the types of computations that we must do to perform inference in a, uh, undirected graphical model of like a conditional random fields. Uh, in fact, the forward backward algorithm that we've seen before for a computing, the alpha bed, a tables which are useful for unnecessary to compute the marginals in a linear chain conditional random fields can be rented in the more general form where, uh, in, uh, it's a specific case, it boils down to the forward backward algorithm. But for a general, a conditional random fields, it provides now an algorithm for these more general cases or performing imprints.

Speaker 1:          01:23          Okay.

Speaker 2:          01:24          Specifically belief products or this algorithm is known as belief propagation. It's a message fashioning algorithm in the sense that now in the graph, each node, the notes are going to exchange messages between them and uh, the messages are going to be exchanged normally until convergence. That is until the messages that one though the sending to another node does not change. And, uh, the idea is that the messages that one node is going to send to another note is going to be a combination of, uh, things that come from the factors associated with that node and messages that were received from its neighbors.

Speaker 1:          02:05          Okay.

Speaker 2:          02:06          So there are actually going to be two types of messages. Much like there are two types of nodes in the factor of graft are going to be two types of messages. Um, so there are going to be messages from a variable nodes which are illustrated as a circle in the graph, uh, to its neighbor factor notes, which are the squares, uh, the messages they're going to be defining a recursive form. So the message from a variable node, which is a illustrated, uh, it's not an s to a, a node that is a factor. So we use f two of a straight factor notes. So the message from s two f when the node as takes value, I is just going to be the product of the messages that all the other factors f prime which are connected to s and are not f so the note f one that send messages recursively to itself.

Speaker 2:          03:04          So it's, it's going to receive the messages that other factors of prime I've sent to s four, again, the uh, uh, where we assumed that the node s is taking value. I, so we take all the messages that have been sent to s and we multiply them together and that becomes the message that s is going to send to f. And what we take the product of fat of messages, we only take the messages from factors f Primes, which are not f but our neighbors are connected to us. Okay. So in this illustration here, the message from s two F is going to be the product of the messages that come from this. And this note, this factor node here in this factor are not here. So this would be one of the F primes and this would be another APP. And now the messages from a factor node to a neighbor variable node, it's going to be slightly more complicated.

Speaker 2:          04:01          Um, so the message from f two s where, uh, s stakes is assumed to take value. I, so for instance, in, uh, if s is a label note than I would be a potential label for that, uh, for that label node is going to be now not just a product but as some. So we're going to have a sum over the value of all the other variables that, uh, are involved in the factor, the factor that's sending a message. Uh, but we're in this, some will keep the, uh, value of the node s two, which we're sending message to its value where assuming, which is I, which is this here. So we take the value of the factor for all vectors zed where, uh, all the values of all the other variables, uh, involved in this factor are being changed to their potential value. We trade of all of these values except for the note as which we keep fixed to eye.

Speaker 2:          05:02          And when you take that value, we would also multiply the messages that all of the other neighbor nodes. And so the neighbor nodes here are going to be variable notes as primes that are neighbors of f except the note to which we're sending a message. S So we're going to multiply all, all the messages that these, uh, notes s primes Av sent to f and uh, we're a US also take into account in this some, what is the value that that, uh, uh, note as frying is taking in the variables involved in the factor. So visually, uh, the message sent from this factor to that variable node is going to involve the messages that came from all other variable notes that are not s and a, it's going to be the product of these messages. But then, um, there are going to, uh, we're going to get the message for each potential value that each of these variable node can take. And you're going to be combined together and waited by the, uh, factor associated. They're sort of factor that is sending the message. And then we're going to some over all potential values that these, this variable known in this variable note could take.

Speaker 2:          06:25          So to make this more concrete, we're going to do the, uh, specify a specific case of a linear chain CRF, and it's going to be simplified so we won't have a context window. Uh, we'll just have why one be connected to x one and y two, I'll need two x two. So here we have our equations for the messages and now, uh, we'll uh, do a few iterations of the messages, uh, where we compute a, a few of them in, uh, actually will, we'll, uh, in this example only do from left to right for now. So, uh, first we, uh, initially, uh, we initialize all the messages to one. So the mew messages all going to be set to one initiative. That's usually the starting condition we use. Now let's consider the message that is stent from x one to this factor. So a message from a variable nodes to a factor, a node, it should be the product of all the messages that the, uh, variable no that's received from other factors except f. Now in this case, x is only involved in one factor. So there are no other messages when this case will the product of nothing. Uh, we'll set it to one by definition. Next, we'll consider this message here from this, this factor through that variable node.

Speaker 2:          07:47          So if we look at the formula here from the factor to a variable note, uh, what we have to do is that we have to some, so, okay, so this factor, uh, involves y one and x one. And then while what do we have to do is to some over all values involved in the factor that is x one and y one, except that we keep the value of the note to which we're sending the message. So that's s two, it's specified value in the message. So that's I hear and also we only some over the variables that are on observed. Uh, so in this case x one is given. So we keep it fixed also too. It's a given value. So in fact, in this case, we, uh, have only one term in the song and that's the term where a x one takes the specified value and why one takes a also, it's uh, the value. Hi.

Speaker 2:          08:48          So removing the ink, uh, the message being sent to why one from this factor is going to be the value of the factor four x one. And, uh, some given while you value of y one being cool to I, so that's this term times the message that was received by this factor from other value variable nodes. Stan, why one? So the only other message is this message here, which is one, so one times that cause this will be the expression of the message and all this that when I say that I see for a given value I, for the variable node, it means that I need to send what this message is for all values. I have the variable node in question. Uh, so in this case, when I'm sending a message to a y one, um, it can take as many values as the number of classes from one to see. So we'll need to compute this expression for eye being one, two, three up to capital's seat. And so when I'm sending, uh, this, uh, I'm, can, I can think of it as sending a vector actually where each element of the vector is a different value of the message for different values of, of I. And so I could just take the exponential of the lug you neri factor for all value possible values for [inaudible] and send that as the message of, uh, this factor to the variable node. Why one.

Speaker 1:          10:24          Okay,

Speaker 2:          10:25          then we have to send message from y one, two, this factor. So we have a message from a variable no to a factor node, assuming that our variable takes, uh, some given value. I, and that needs to be the product of all messages that were sent to the factor, um, to the factor. Sorry to the variable note f except, uh, s sorry. Except f. So in this case, uh, the, uh, other message I could have come in from other factors would be this message. So we'll have the product of just this. It's the only other factor message that this is receiving except from a message from this guy, this factor node. So the message sent by, uh, I, uh, why one to this factor would just be the exponential of the union. Every factor for the value of y one, which would be I, so we'd have the vector of exponential of [inaudible] exponentially. Like, hey you two and so on, this vector would be sent to that fact

Speaker 1:          11:28          factor.

Speaker 2:          11:31          Next we have the message from this factor no to that variable note. So that's this rule here. Now we have to some over all, uh, the values of the random variables involved in the factor f. So it's this factor. So the variables are y one and y two but have to keep zed s to its value. I, so in other words, we have to send the messages for each value of I where we do the sum and we take the variable to which we are sending a message and we keeping it to that specified value. I and otherwise for the other variables we some over their values. So in this case we're sending to y two. So where are you going to some over the value of y one. So we have to some over why one of this factor, the factor here, which is the pairwise factors. So it's the exponential of the pairwise factor here. Times all the messages that comes from other variable nodes as prime that are not s and that'll work send to f. So essentially this message here, so that message here, so have the exponential of a u y one times the exponential of a p y one y two. So that's equivalent to the exponential of Aui one and AP. Why One? Why too. And then we're summing over the other variables that are not y too. So that's in this case. Why One?

Speaker 1:          12:58          Okay.

Speaker 2:          12:59          And uh, so that's this message that's being sent from that factor to this variable Nov. Let's, so mink, oh, and notice that this is actually the initial value of the first column in the, uh, uh, computation of the Alpha table in the forward backward algorithm. So we see we're starting to recover pieces from the forward backward algorithm.

Speaker 1:          13:23          Okay,

Speaker 2:          13:24          next we could send the message from here to there. So with a similar reasoning as in this case here, we get one and would a similar reasoning. The message from this factor to this variable node would just be the exponential of the urinary factor, uh, for the, uh, position two in the sequence.

Speaker 1:          13:48          Okay.

Speaker 2:          13:49          Now the message sends from Y to, to this factor despair, wires factor. Uh, so we are in this rule here. S F would be the product of all the messages from other factors, F primes that are not f and that are connected to s. So it's the product of that message and that message, which is this expression here or equivalently alpha on, why two, we lose our notation from the Ford backward dollar with them, which we find here. It's the product of this times that,

Speaker 1:          14:23          okay.

Speaker 2:          14:24          And now if we compute the message to white three, then uh, again, where we get is the, uh, some over the other variable that shares the same factors. So that's why two of the pairwise factor here times all the messages that were sent to, uh, all the messages that were sent to f from variable known as prime, which is connected to f n which is not [inaudible]. So in this case it would be, uh, it would be that message here. So we see that we have some overweight to the other and a variable nodes of the exponential of a, a two why to just here times Alpha One y two, which is here and times the exponential of, uh, deep pairwise factor, which we can just put it into the same exponential here. And so this corresponds to the pairwise factor in this expression here corresponds to this factor.

Speaker 2:          15:32          So if you continue like this, in this sequence, in this order of the messages, we actually recover all of the, uh, of equations for the values of the Alpha. So the column in the Alpha tables. And so, uh, so this is just, you know, at the beginning of the demonstration that there are equivalent and you know, a full demonstration, we actually show you that, uh, uh, and so would the validate that a belief propagation, uh, applied from left to right and then right to left actually yields the computation of all the Alpha table and then the computation of the better table. So, yeah, so actually if you look at this, we actually do a recovered the expression for the second column of the Alpha Table. And so by proceeding like this, we recovered all of the, uh, of table columns.

Speaker 2:          16:32          So yeah. And for the, to sum up for the linear chain, a linear chain graph, belief propagation is actually the same as the forward backward algorithm. The Ford Pass computes the Alpha table, the backward pass the bed a table, uh, for the similar reason for an the miracles stability, we actually, uh, usually implement a version that passes log messages. So that's going to be more stable numerically. And one advantage of the message passing out with them is that, uh, the belief propagation algorithm is that we can do in France now on other types of structure. Now, interestingly, belief propagation yields Zack inference if are a structure of our factor graph is a tree and arbitrary retreat actually. So, uh, if we, uh, uh, so we can show that belief propagation actually eventually converges by computing the, uh, uh, if we compute the, uh, each of the messages, eventually the messages stop changing.

Speaker 2:          17:32          And, uh, and the actually correspond to what you would like the messages to be. So if you locked space, uh, the log some of the factors for all of the variables, uh, that's, uh, come from a given part of the tree. And so these long messages can be used then to provide a do approximate inference. And in particular compute marginals, the, uh, general algorithm with the same equations can all be so be applied to a graph which has loops. Uh, so in this case it would not correspond to a tree and, uh, and then it's often use it practice to do approximate inference. So, uh, uh, obtained some approximate estimates of what the marginals could be. Uh, you have to be careful. I'll ever, the algorithm could divert. Uh, so there are certain tricks that, uh, are used to make it converge. Uh, so I ref, you know, there's, there's a vast body of literature on a message passing algorithms and loopy belief propagation, which I won't cover.

Speaker 2:          18:38          Um, but so one trick that is tough time sometimes uses that instead of updating the messages by overriding them, you instead, when you recompute a message, a second password, you competing the same message a second time, you actually do some sort of weight of average between the current message and the previous message. Cause that's one trick that's sews for, uh, having a better behaved a algorithm. So I'm not going to the detail in part because there are actually a lot of general purpose libraries that are publicly available where we just pass the graph in a certain and data structure and then the algorithm will do this, uh, uh, message fascinating until belief propagation. We'll run it over your graph, uh, and uh, and these areas quite useful because then they can be used on arbitrary types of graphs.

Speaker 2:          19:34          So it means that now instead of constraining ourselves to a linear chain CRF, before doing a sequential classification, we could start considering adding a lateral connections between labels that are at two positions away from each other. So I have factors between whyK and why k plus two. Uh, so even if this would introduce some loops in the, uh, in the model, we could still deal with those. Uh, we could add also lateral connections, uh, for triplets of labels as well. Um, so really we can just explore a bunch of different types of graphs by, uh, uh, adding factors as we wish. And then from these factors, we get a factor of graph and from the fact that graph, this would, uh, describe the way of passing messages between a variable nodes and fat log fire and factors and from factors. So variable notes. So the idea really is when we do CRF modeling is that you want to add connections between the things that have dependencies are fairly strong and that you want to model directly. And in fact, you can have a connectivity that varies between examples. Uh, so if you have some information, uh, that differs between examples, which would tell you that for one example, for instance, the first label should really be connected to the last label, for instance. But for other, uh, maybe four other, uh, structures, you don't want this, you could actually have a separate, essentially graph, separate factor ref for each training example when you're actually, uh, uh, and you could actually train such a CRF as well.

Speaker 2:          21:16          So examples of, uh, graphs that we often see for a computer vision. We could have a grid structure. So imagine we were labeling each pixel in the image by, uh, uh, maybe a segmentation, uh, label, which tells us, uh, which may be separates a foreground from background. Well, then we flew on to, uh, the labeling to be fairly smooth. We could have factors between each pair of, uh, uh, pixels like this. Uh, Dell are adjacent in the grid of pixels so that the labeling is a somewhat smooth, uh, uh, spatially. But really in general, we could have any set of variables connected with, uh, a bunch of different factors. In this case, I'll need showing pair, uh, factors that are over pairs of variables. They, but they could be over a triplets or, or more. And then the Lou people leave propagation. I'm going to could be applied also on these, uh, types of graphs. Um, you have to be careful. It doesn't, like I said, loopy belief propagation or belief propagation on new peak graphs is not necessarily guaranteed to converge on this. You're a careful, that's not something I'm going to cover, but I encourage you to look at the literature. If you have you encountered problems where you actually want these more complicated structure, uh, to, uh, model your problem

Speaker 2:          22:39          and final lien. Uh, I just want to show the expression for what to do with these messages. So intuitively these messages correspond to a, what is the sum of all the variable nodes. Uh, that's, uh, that's, uh, our nuts. The current variable, no, they come from a surgeon, uh, direction from the graph. And so if you want it to compute the marginal probability of the label somewhere in the graph, well, uh, to get its approximate approximated marginal that approximately at marginal, we just take a similarly as we've done before, the exponential of the luck factor that involves on the dat a variable. And then plus the messages that come from all the factors that involve, uh, the a node. Why k, uh, except the factor that's, uh, the, the factor that owned the involves white gave. We have sun and then we just some the log messages from these other factors, uh, from other nodes outside of the Eh, from other, sorry, other factor nodes that are connected to this variable notes.

Speaker 2:          23:47          We'd some all of their log messages and then with just normalize the numerator by summing over all values that are possible for my variable of interest of the numerators. So the exponential, uh, exponentiate at some of luck factors and log messages. So to sum up, approximate inference would correspond to something over all the luck factors that involve only the variable of interest. Why Gay? So this is this term here. In this example I'm showing just one luck factor, but you can have, you can have more than one, then you some over all the log messages that come into why k from other factors. So it's this sum here. Then you take the exponential, uh, the exponential of those, some of luck factors and log messages, and then we normalize to ensure that we get a valid distribution. Okay, so this sums up the general belief propagation algorithm for general conditional random fields.