Speaker 1:          00:00          In this video, we'll discuss the relationship between the linear chain conditional random fields with the hidden Markov model.

Speaker 2:          00:09          Okay.

Speaker 1:          00:09          So, uh, we've, uh, in a previous video talked about whether there were alternatives to the linear chain conditional then field for sequence. So classification a of alternatives that might be more appropriate when we have a lot of training data.

Speaker 2:          00:23          Yeah.

Speaker 1:          00:24          This has led us to talk about the maximum entropy Markov model. And A, I've mentioned why, uh, it suffered from a problem known as the labeled bias problem where observation for the wind, the sequence would actually not have any impact on the prediction and the marginal probability for the label at positions early in the sequence.

Speaker 1:          00:48          So in other alternative to the MDM EMM, which is well known that you, uh, and we've actually, uh, meant mentioned in passing in previous videos is the hidden Markov model. So a, in the hidden Markov model, instead of assuming a generative started where we are given some inputs and those inputs in the sequence and then generated the labels, we have an inverted generative story where we, uh, the labels at first been produced. And then from the labels, the sequence of observations, I've actually been produced from this soul. And instead of of producing the inputs and then the labels, it's the other way around. And actually in Hmms, we usually define a, uh, uh, something know that as the, uh, uh, uh, emission distribution, which tells us how the inputs were generated from the labels. So p of x, given why. So, uh, from this, we actually get a full joint distribution over both y and x, both the sequence of labels and the sequence of, uh, uh, of inputs.

Speaker 1:          01:56          And if we have a joint probability from this, then we can just compute p of y given x by, uh, just dividing by the marginal probability of x, which means that we can have an expression for minus the log of the conditional probability of y given x. And we could try to optimize that. And so I essentially optimize our model to assign high conditional probability on the labels, uh, given the inputs. So, uh, uh, this training of Hmms is a, actually used a lot in speech recognition. It's known as maximum Mitchell information training. Uh, and, uh, we can show that we don't have the same label bias problem anymore. So that's an advantage. However, the director nature of the hmm can make it a bit more complicated to train. So optimization the optimization problem associated with training, uh, it can become a bit more complicated because if you write down the, hmm as a factor, a model where it's a product of factors, as we've discussed before, the factors now need to be normalized probabilities.

Speaker 1:          03:04          And so this has certain consequences in the optimization of the model. And a, it can also be a somewhat difficult to define a good distribution over the sequence of inputs x if the vectors x one x two. And so on. Our real valued vectors, defining distribution of a rail real value vector is, is actually not a trivial thing to do. Whereas in crs, because their conditional, we totally sidestep this problem and a yet we don't actually suffer from the label pious problem, much like in a descriptive. Hmm. So, uh, one can think of a conditional random fields and interchain conditional random feel as the sort of best of both worlds solution between a discriminative hidden Markov model and the maximum entropy markup model. So, um, so to sum up a while, the maximum entropy Markov model can be perhaps easier to train than the hidden Markov model. It suffers from the label bias problem and the end of the mark of model, if which words you're training descriptively though it's possible, and people do it. And speech recognition, uh, it can be harder than the linear chain conditional and the Va..