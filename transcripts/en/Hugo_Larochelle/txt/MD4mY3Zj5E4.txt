Speaker 1:          00:01          And this be new, we'll discuss and contrast two different types of learning, a discriminative and generative learning. So we'll say that a, a learning algorithm corresponds to a discriminative learning algorithm. If it's trying to optimize a conditional likelier that is if it's trying to optimize the last function, which looks at a minus the log of the probability of the true target given x. So for instance, conditional random fields are usually train in the conditional way and the actually always trained conditionally because a conditional random fields only defines a conditional distribution. It actually, uh, doesn't really look at defining a distribution just over x or John Distribution for y and X. Um, and other alternative actually would be to do generative learning. That is to optimize the joint log likelihood. So in other ways optimize minus the log of the joint probability of observing why and x as opposed to the probability of observing why, given that I'm observing x.

Speaker 1:          01:15          So for instance, Hmms, uh, are usually trained generatively. So if you know about Hmms, what you've probably seen is, uh, the learning algorithm for training. Then generatively, uh, though we could also train it. It's in this way because if I give you a joint distribution of our y and x, I can then, uh, do the inference of what is p of y given x. And then I could try to chain train it a discriminative Lee. But, uh, but initially hmms I've mostly been developed and, and propose in the context of generative training and a, So let's exploit that formula a little bit. So we have that, the joint distribution, it's just a, it can be written down as the product of the conditional p of y given x times the, uh, marginal p of x. And because the log of a sum is the sum of logs, then, uh, this is also equal to minus the log of p of y given x minus the log of p of x.

Speaker 1:          02:11          So we see that the difference is this term here. So we have this here and this here, but we have this term that is added when we were performing generative learning. And, uh, so one way we're thinking about this is that perhaps this storm is kind of acting like a regular riser. That is, it's we're adding another term which, uh, is, does not involve why. So it's not a term that encourages the model to assign high probability to the true target given x. It's just a model that requires to do something else. It's going to favor models that also explained well the marginal, so I assigned high probability to observing true data. So the data, just the inputs that we see from the training set. And for that reason, a good intuition is to think that does go to the learning will tend to fit the data better while generative learning will be more regularized because of this term here.

Speaker 2:          03:11          Yeah.

Speaker 1:          03:13          Actually we can show that, uh, there are two scenarios, uh, that we can observe in terms of how genitive versus of learning will compare ugly, uh, uh, comparatively come, uh, compare. So if a model is well specified, so that means that the data in our training set is that it is, is a actually was actually generated from the model from the same class as our model. So there's a sending of our parameters for our model that corresponds exactly to the true model that January, that the data, so if the mother as well specified, then we can show that generative learning essentially always better. That is for any size of training sets that we use, there's always going to be a gap between what generative learning, uh, uh, gets in terms of generalization performance compared to a descriptive training. So generative training is it's going to perform better. They'll, uh, they should both eventually converged to the, uh, to essentially an error Zeros or, uh, a perfect performance. But genitive learning is, uh, going to get there much faster.

Speaker 1:          04:22          However, if the model is not well specified, which is what he most of the time. So if data was generated by some experts that labeled some data, then this is a process that that is almost most, uh, almost certainly more complicated than the model we've written down in math. Uh, so, so really this is essentially for all this interesting problem. Uh, the, uh, model will tend to be, uh, uh, will not be well specified. Then, uh, the picture is not as clear. So, uh, what we'll see is a picture that is consistent with the view that genitive learning is just more corresponds to more regularization. So if the training set is small, then, uh, we should observe that generative learning. We'll get a better performance, uh, in terms of generalization. But as we increase the size of it, our training set, eventually we should reach a point where this learning we'll catch up with generative learning and, uh, we'll start being actually better in terms of, of generalization performance. I get lower generalization, uh, error then a generative learning.

Speaker 2:          05:31          Okay.

Speaker 1:          05:31          Um, and in fact, more specifically, uh, what we get is that discreted of learning will converge to a smaller syntonic error, a synthetic error being the error. Your model would get if you had infinite amount of training data. So we'll, uh, it has a smaller I synthetic error, then generative learning, which won't be able to do as well, even with an infinite amount of training data. Okay. So again, here, uh, a nice way of, a simpler way of interpreting this is that generative learning means more regularization. So if there's a small amount of training data generative learning, uh, we'll probably do better. But if I have quite a bit of training data, then now we can expect this kind of active learning to eventually catch up and do better. So that's actually a good thing to know about if you're tackling a problem for which you either have a lot of data or not a lot of data. So you know, which kind of learning you should prefer. And a far more theoretical details on this invite you to consult this paper on descriptive, Asus generative classifiers. Uh, so this is a paper by enjoying and Michael Jordan in 2001.