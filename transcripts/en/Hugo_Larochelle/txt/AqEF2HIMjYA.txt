Speaker 1:          00:00          Yeah.

Speaker 2:          00:00          In this video will introduce the different type of neural network for natural language processing, uh, known as the recursive neural network.

Speaker 2:          00:10          So here we'll start, uh, interesting ourselves with the problem of learning representations, but not just for single words, but also for multiple words, same phrases or sentences. Specifically. We like to be able to learn, uh, representation, uh, of phrases of essentially arbitrary length. So for instance, could we model the relationship between individual words and multi word expressions, which are equivalent. For instance, you could have a model that I learned a representation for the word consider and also can output a vector representation of the phrase take into account, which is similar to that a consider because effectively to consider or to take into account is essentially the same thing. They're, they're related essentially synonyms, uh, with respect to each other. So could we learn representation with a model learning route, presentation for arbitrary length, uh, phrases that allows us to compare full sentences and that actually is able to maintain as much information about it. Semantic meanings. So fun. Since uh, a model that would upload the vector representation for the sentence, word representations were learned from our corpus, that would be very close to an equivalent sentence, which is a, which as we train word representations on a text dataset. So that's the question we're going to be interested in, in, in trying to address, see how well we could learn he fixed size representation of, of these felony long sentences and still maintain as much semantic information as possible.

Speaker 2:          01:50          So to tackle this problem, Richard Socher and his colleagues introduced a different type of neural network today called eight recursive neural networks. So first describe the general idea and the next videos we'll see it, the look at the different components and how they work. So what they propose to do, if we had a sentence, say a small crowd quietly enters the historic church, uh, then to get a representation of the full sentence, they would first take the sentence and extract from it a, a syntactic tree, a binary syntactic tree, which essentially identifies the internals and tactic structure of the sentence. So I'm not gonna say much about what syntactic tree our tweets are. Um, but effectively essentially we have data sets that identify four different training sentences. What is the syntactic tree? And we were both to train models for, for performing that. And actually we'll be able to use a recursive neural network for performing that task.

Speaker 2:          02:48          But essentially what the syntactic tree gives you is that it identifies essentially the meaningful, uh, phrases within the sentence and how relate with each other. So the syntactic tree here is saying that there's a sentence which is composed of in noun phrase and then the verb phrase where the noun phrase covers a small crowd. So normally here there'd be a bigger tree, but since we don't have enough space, you should imagine that there, there, there would be a small tree here and, and so that forms a noun phrase. And then we have a verb phrase, which essentially covers all of these words here. And this verb phrase is actually a smaller verb phrase covering quietly enters and a followed by a noun phrase. Just probably the object of quietly enters and a, this non phrase covers the historic church and this non phrase, it's actually a determinant followed by a smaller noun phrase.

Speaker 2:          03:45          And this smaller numb phrase is an objective followed by her now specifically historic following church. So that's the kind of information that is contained in these barn recent tactic treats. And now to get a representation in vector representation of the full sentence, a d idea and recursive neural net is to recursively merge pairs of representations of smaller segments to get representation is not cover bigger segments. Specifically we'd start with the word representations themselves and then going up in the tree, we will take the two parents of a note and merged our representations, uh, using a neural net, which will take as argument the two representations and now put a new merged representation so far. For instance, the first thing you could do is take the word representation of the word historic and then the vector representation of the word church. Feed that to our neural network, which would then output a new vector, which would be the, uh, which we use as the representation for the noun phrase historic church.

Speaker 2:          04:49          And then to get the representation at that level here, vector representation that we'd merged the representation of its to children. That is the word, the, and then the noun phrase for which we just extracted, computed. It's a merge representation. So now we'd feed to the same neural net, this vector and this factor, and then it would output the vector representation at that level. And we continue like this for each node in the syntactic tree until we reached the route. And that vector would be, uh, considered in our model as the vector representation of the full sentence. So what we need to specify at this point is what shape will take this model that merges pairs of representations. And, uh, also all we going to model actually the tree structure. And specifically how are we going to extract this? Uh, this street's a structure from the sentences. And these are the things we're going to discuss in the following videos.