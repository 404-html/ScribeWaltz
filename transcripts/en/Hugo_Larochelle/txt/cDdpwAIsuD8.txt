Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video, we're ready to put all the previous pieces together and they find a full, a convolutional neural network. So having now these three ideas together and how we should implement it in computing the hidden layers of a commercial neural network, we are now ready to see what the full forward propagation in a conversional neural network corresponds to. And then we'll talk about, uh, what are the missing pieces for training a convolutional neural network.

Speaker 1:          00:33          Okay.

Speaker 2:          00:33          So to get the convolutional neural network, we'll just alternate between these conversional operations and pooling operations or, uh, if you want to call them instead layers. Um, so when I say convolutional layer, I'm referring to the, uh, performing all the convolutions for each input channel, uh, and then aggregating them. So summing them together and applying the nonlinearity. So this convolutional plus nonlinearity operation will be the convolutional layer, and then the pooling operation is the pooling followed by these sub sampling. Uh, so that's what I mean here by pooling layer. And what we do is that we alternate between the two. So I mentioned we had an input image which had just in this example a single channel. Then we could compute a hidden layer that corresponds to 60 for a feature maps and uh, that where each of the unit would have a nine by nine receptive field.

Speaker 2:          01:33          So that would give us 64 75 by 70 75, uh, feature maps. So there are 64 feature maps and each are Maitri matrices of 75 by 75. Uh, so this is just the result for each of taking a single convolution because we have just one input channel and then applying some nonlinearity. Then we could apply some pooling. So we could for each feature map each 64 feature map, take some say Max pooling and uh, uh, uh, none overlapping regions and then doing the sub sampling by retaining all leader maximizing values. So then we'd get a, again, 64 feature maps on art that are smaller. That is that our 14 by 14, and then we could apply a new convolutional layer. Now in this case, the 64 feature maps at layer two are going to act as the input channels for layer three and a. In this example, they use again nine by nine receptive fields on these, uh, 14 by 14, uh, input channels, which are the 14 by 14 feature maps of layer two.

Speaker 2:          02:42          And so notice that in this case computing, say that feature map requires performing 64 convolutions on all of the, uh, 14 by 14, uh, feature maps here, which are acting as it put channels for that convolutional layer here and that feature here. So that's means I'm going to have a nine by nine by, I'm going to have 64, nine by nine matrix cs for each feature map here. So that can be a lot. Uh, and, uh, actually, uh, uh, in some work, people instead randomly choose between the 60 for just a subset of a, a few feature maps that say each feature map is going to be connected through. So you sort of randomly assign to each of the feature maps in the three, a third hidden layer, a subset of the 64. And, uh, uh, pull the and subsample second hidden layer here. So that's one way of reducing the number of parameters in the number of computations, uh, that can be used.

Speaker 2:          03:44          So after we've done this, we get, uh, so in this case was chosen to have 256 feature maps at this level and performing the convolutions. Uh, we were left with six by six, uh, uh, it made me sees for these feature maps. And then after some a six by six pooling, uh, then we get 256, uh, uh, feature maps, although actually one by one that are just a single that, and to obtain a final output, we would just treat this as a factor and connected, fully connected to some output layer, uh, where the nonlinearity at. The output would be a softmax if we're doing, say, object recognition or a character classification. So a hundred, a hundred, 101 is taken for the Caltech, one o one dataset where we'd have 101 different categories, so 101 different classes for the classification problem. So here it wouldn't have any convolutions. It would just be this factor of 10 56 multiply by Matrix of weights, less some bias passed into a soft Max. Uh, you might have noticed also that here, uh, for the pooling layers, they actually used a bigger pooling, uh, neighborhood, uh, Dan d a sub sampling neighborhood. Um, and so we can actually use different sizes for either the pooling of sub sampling if we want a, but often people actually use the same neighborhood size, so they have the same uh, uh, sub sampling and pulling neighborhoods for, uh, in computing a pooling layer.

Speaker 1:          05:20          Okay.

Speaker 2:          05:21          So like I said, the output layer is irregular, fully connected softmax layer and the output does, gives us an estimate of the conditional probability of each class of each category of object. You want to detect, a one to recognize and then to train it. We can do as we did for regular feed forward Nola at work. That is use the negative log likelihood conditional love. Like if the class, given the input image as our training loss and perform stochastic gradient descent, um, backpropagation is similar as the fully connected network is. We do first a Ford pass, which computes the output and we get a loss from this. And then we get a great end of the output layer and then we have to back propagate the gradient towards the input, uh, in a way that we can thus get the, with respect to all the parameters. And in this case, the parameters are going to be, uh, the biases. If you have some biases. We have the output connection Matrix, but we also have the, uh, colonels or the, the, uh, Maitri sees a fill or filters that we have for each feature map. Uh, so we have to train these parameters now, uh, when we're doing backpropagation.

Speaker 1:          06:34          Okay,

Speaker 2:          06:34          so we only know how to pass gradients through, say, Ellenton wise activation function. For instance, when we're computing the, uh, once we've computed the reactivation using the convolutions, we do an element wise activation function. Uh, we all need to know how to pass a gradient through that. So we'll go to that part. We just look at the missing pieces for implementing, uh, backpropagation, which is how to back propagate through the convolution operation and, uh, the pooling and sub sampling operation.

Speaker 1:          07:06          Okay.

Speaker 2:          07:07          Now let l B, uh, my short notation for the loss function for some given a training example. And just to simplify now why Jay is going to be the pre activation, uh, as computer based on the IAFF, uh, input channel, uh, for some given layer. And kig is going to be their conventional colonel associated with my matrix of connections for my JF, uh, JFE feature map.

Speaker 1:          07:36          Okay.

Speaker 2:          07:36          Now, if I have, uh, the gradient of the full a result of the convolution, uh, for the GF feature map, so I'm calling this a Nebula Yj l. So that's the greening of the last week, respected the full feature map, a computed based on some input channel. I, so I'm actually here assuming there's only one, uh, essentially there's only one input channel.

Speaker 1:          08:04          Okay?

Speaker 2:          08:04          Uh, generalizing to multiple in per channel would be simple. So I just take that gradient, which is itself an image. And then I do deconvolution, uh, over, uh, uh, I do the convolution with zero panning with the weight matrix. W Ij. So no, this, now I'm doing the convolution, not using the kernel, which is just the flipped rows and columns version of Wij. I'm actually doing the convolution with WWE Ij and I'm also doing it with zero padding, so I won't go over the details for why I have to do this, why this is the correct thing. You might want to sit down and do a little toy example to figure out why this is the right thing to do.

Speaker 1:          08:48          Okay.

Speaker 2:          08:48          And now, because the IIF input channel is used by all feature maps, uh, I have to sum over all of these, uh, gradients that come from Eaton Bgf, uh, feature map and sum them together. So I have a, some over j of the BACP propagated gradient from DGF feature map, which, uh, to back propagate it, I have to do this convolution with zero patty, with my Matrix Wij and now I [inaudible] so I want to learn Wij. So at this point I can also compute the gradients with respect to wag my matrix, my filter matrix. Uh, in this case, what did corresponds to it's again, taking, uh, my, uh, great in retrospect too. I, it should be, there should be a j here and, uh, doing a convolution with the uh, version. So my input channel x Xy, but where I flipped the rows and columns. So that's why I have to tilt here. Okay. And that's doing that operation gives me the gradient retrospect to double my matrix w Ij for the, the parameters of the feature map that connects the input, um, that connects the input channel. I with the, uh, feature, uh, fi, uh, feature map Jane. So we see that much like the forward propagation can be, uh, when we apply and conclusions, I can actually invoke some code that does either zero padding or without zero patty convolution operations. The green ants can also be computed using these operation, uh, these convolutional operations.

Speaker 2:          10:31          And as for the gradients, uh, passing through the pooling and sub sampling layer before Max pooling, um, well the gradients. So if I have the gradients with respect to why I J K, so, uh, this, uh, this would correspond to a for some given a feature map. I the uh, gradients, uh, the, uh, yes, the, the gradient with respect to the element I Jane the, uh, that feature map for their a pooling layer. Now the gradient retrospect to the uh, uh, value of the input channel, which is the previous layer feature map. Uh, so the ingredients for each of its elements is actually going to be zero for everything except the positions which I'm calling p prime in Q prime, which corresponded to the maximizing of values. Um, so visually it means that, uh, imagine I had a neighborhood like this, a three by three over which I did pooling and that the maximizing element was this one.

Speaker 2:          11:40          And now, uh, and so when I did for propagation, I took this value and put it at the pooling, uh, layer, uh, physician corresponding to that, uh, that computation. Nice have the gradients with respect to add elements. Well that green is going to be copied and consider as decree of that element directly and everything else is going to have a gradient of zero. Okay. So that's how we would just back propagate the gradients, uh, for uh, through a, uh, uh, Max pooling operation. And for the average fooling operation where we're just taking the average over a local neighborhood, well we actually just uh, take, uh, we just upped sample the gradients which respect to the pool and sub sample a layer and a then we multiply by this scalar here, a one over m squared. So going again over my example here, if at this neighborhood here and uh, I had the gradient of the, with respect to the pool version of that neighborhood, which say was, um, just to make up an example, 0.5, then the great end here would all be 0.5 for everything.

Speaker 2:          12:58          So here, here, here, here, here, but I would also multiply by one over night. So he 0.5 divided by nine. That would be the gradient of the pre pooling and sub sampling, uh, values of the layer here. And that's then what I would back propagate a continuing back propagating through the network. All right. So using these definitions of the gradients, uh, of each outputs of either the convolutional operation or conventional layer or the pooling layer, then I can implement backpropagation by going in the reverse order to the Ford propagation and calling these backpropagation, uh, sorry, these great and propagating a operations which I've defined. And this will give me the parameters with respect to my, uh, the green and respect to my parameters. And then I do a great end up update as in regular stochastic gradient descent.