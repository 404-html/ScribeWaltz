Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video, we'll introduce the concept of free energy in a restricted posts for machine.

Speaker 1:          00:06          Okay.

Speaker 2:          00:08          So we've seen that a restricted Boltzmann machine, uh, defines a distribution over the inputs and a latent factor of binary hidden units. Now in this video, we'll look at what the actual marginal distribution over a x onlys over our inputs that we're interested in modeling. Uh, it looks like to try to gain some intuition as to how restrictive Bolsa machine can make certain types of values for the input vector, more likely and other vectors, that's likely. So in other words, high, it can actually model a distribution of refinery back.

Speaker 1:          00:43          Good.

Speaker 2:          00:47          So what does p of x look like? Uh, so p of x, if we wanted to write it down, uh, well we can write it down that it's just the sum over all potential binary vectors. So binary patterns we could get at the hidden layer of the probability of observing that hidden layer with the given value of x. And, uh, now we can just write down what p of x and h is just the exponential of the negative energy divided by d partition function. And what we can show is that this some of an exponential number of binary vectors, the, so all the exponential number of patterns in the hit in the hidden layer can actually be written down in the more efficient expression and more efficient to compute. So we can write it down as the exponential of a bias term that involves on the x and it's biases plus a sum over all hidden layers of the logarithm logarithm of one plus the exponential of a Lennar transform of the input, uh, and the looking at the linear transfer for each hidden a hidden unit.

Speaker 2:          02:00          So in other words, we can actually write it down as a linear term plus. So the exponential of an inner term plus the sum over all hidden units, Avi nonlinearly transform a version of the Lennar transformation associated with each hidden unit. And a, if you look at the literature and restricted motion machine, if are you see mentioned the notion of a free energy. Well we just refereeing to minus this. So the negative of this whole term here. So that would be the free energy in a restricted posted machine. So that's first actually do this derivation here, how we go from that to this and notice again that you know, this might be a surprising result because we've transformed this summation over an exponential number of term of terms and to an expression that requires a uh, some over a linear number of terms, linear in the number of hidden units.

Speaker 1:          02:58          Okay? Yeah.

Speaker 2:          02:59          Okay. So p of x is, uh, the, uh, marginalization of the joint distribution by summing over all values of age. And so that here, this is just p of x and y.

Speaker 2:          03:19          Now, uh, let's actually notice that the exponential of that does not depend on the hidden layer so we can actually put it in front of the sun. So he goes here, I'm running explicitly that this some of our, uh, all binary vectors is we can write it down to z, a series of nested, some over [inaudible], up to age, capital, age. And then I'm writing down this term here in this term here. Uh, similarly when like we've done in the demonstration for the conditional distribution and writing it as a sum over all hidden unit, uh, indices of, uh, a term that involves on the a single

Speaker 1:          04:02          in unit HJ,

Speaker 2:          04:08          then I'm using, uh, the same, uh, steps that have used in the derivation of the conditional distribution that essentially this is the exponential of a sum. So that's the product of the exponentiate it values for these terms here. And, uh, so in this product of terms, each factor on the depends on one hidden unit, which means that this nested sums cannot become a product of individual, uh, sums over each individual

Speaker 1:          04:41          heating unit here

Speaker 2:          04:47          and now, uh, because this sum is over just a two terms, so a term where the hidden units is equal to zero and another one where it's equal to one, then, uh, I can just write this down and explicitly. So when h one is equal to zero, then this is the exponential of zero, which is one. And when it's equal to one, then it's just a, the exponential of B one plus the product of x with the first, uh, row of w. And then I get a similar expression for all a hidden units until the last hidden unit. So the hidden unit, uh, capital age. Then I'll also write this term here as an exponential, an expert, the exponential of a term. So I'm just going to add the exponential of the log of the term that I hear. I'll do that for this one too. So I have the expansion of the luck of the original term.

Speaker 1:          05:50          Okay.

Speaker 2:          05:50          This allows me to write the product of all of these exponentials here as a single, where I'm summing the terms inside the exponential. So I'm summing you buy a sturm and then I'm summing all of the logs of one plus the exponential of the linear transformations of x for each hidden unit, which I'm finding no

Speaker 1:          06:15          here.

Speaker 2:          06:20          So we finished our demonstration that p of x can be written down like this. So it's just the exponential of a, a sum of a bunch of terms and there's on the capital h number of such terms, I'm not in this form. What's nice is that we'll be able to get a little bit of intuition for how we can adjust the parameters, uh, here to make the probability for a given input vector x, uh, to increase. So I can we change it so that we increase the probability of a given input vector X. First, I actually introduced a, a notation for the, uh, nonlinear already log of one plus x. It's known as the sub plus function, which we're seeing in this plot here. So first consider the dotted line, which is just the maximum between zero and the input, which I'll just call x.

Speaker 2:          07:17          That's all. So assuming that this access is x, uh, and uh, so we see that indeed it's linear. If the input of the nonlinearity the argument is greater than zero and otherwise it's clam at zero, now stop. Plus you can think of it as a smooth version of that function. So we see that as the argument becomes greater and greater and greater, then it becomes closer and closer, closer to a linear function. And if it's actually quite negative, then it's going to be a lower bounded by zero. And as we go towards minus the infinity, uh, we are getting closer and closer to zero. Okay. So it's kind of a soft version of the, of this function here. Sometimes known as the, uh, sometimes, uh, I've actually, it's actually corresponds duty, rectify linear function that we've seen when we described a activation functions for neuro networks. Okay. So now what does it take? What can we do to make p of x big?

Speaker 2:          08:20          Well, we need that term to be big. So, uh, if the input vector x is well aligned with the bias vector, see that is if the elements of XR positive, when a, our one Wentz, the terms of the associated terms in the sea vector are positive and zero, when they're negative, then you know, this, uh, I can become great if this can become a bit. Now we also need these terms to become paying. So we, there's that tiered, this influences, this only looks at individual values of x. So we can't really model, um, uh, dependencies between pairs of, uh, inputs or even just the joint, a configuration of the elements in the input vector. So this term here is not going to model dependencies between the elements in the input vector. So for that to become big, then, uh, we want the input vector to, uh, be well aligned with the rows of w so far given, uh, hidden unit j.

Speaker 2:          09:23          Uh, if the dot product between DGF row and the vector x is big, then this term is going to become big. So far, given hidden unit, um, d f x is a positive for the inputs to which the JFN in layer is positive. We connected, and if it's zero for the other, uh, uh, input neurons to which it's negatively correlated, connected, then this has a chance of, of being a bigger. Um, and then we have the bias of the, uh, hidden unit, uh, which essentially controls how big this needs to be for the soft plus to, uh, be a much bigger than a zero. So, uh, another way of seeing this is that we can think of each hidden unit that's acting as a feature where we look at whether a feature is present in the input x by multiplying by its associated weight. And, uh, so it's associated row in the matrix w and then the bias just determines what it means to four features actually be present.

Speaker 2:          10:31          So how big must this term be for us to consider that, uh, in the, this particular pattern associated with that feature is a meaningfully present in the input. So for instance, imagine that Bj is quite negative. Like for instance, imagine that it's minus four, that it, it means that once I've completed that, then the selfless is actually quite close to zero. And so it means that if I wanted the soft plus to be none, zero and actually increase the probability, uh, for the observation x, then I need to fight the contribution from the bias and um, be actually much greater than zero. Then say if the buyers had been zero. And I started my activation competing, my activation here. And, um, and, and so that's the role that each different parameters are playing. And now it makes it more explicit that really the hidden units, we can really think of them as features.

Speaker 2:          11:30          And so if we want the probability of all of our training examples to be high, then we want the different roles of w to be well aligned and represent meaningful features that are present, that are often present in all of our training data sets. And so the more these terms are going to be positive, then the greater the probability associated with a given input is going to be. And so training will correspond to finding these rows of w along with their corresponding biases, uh, which are such that this term tends to be quite high for the inputs that we have in our training set. So we would want these roads to essentially look at different, uh, uh, specific features of inputs that we tend to see for our, in our training center.

Speaker 1:          12:18          Okay.

Speaker 2:          12:19          All right. So, um, that's, so we've introduced the notion of a free energy, which is involved in computing p of x. We can compute p of x, uh, attractively in the general case because again, of the partition function. However, introducing the free energy has allowed us to, uh, get an expression where it becomes more obvious what an RBM needs to do to make certain inputs, uh, values of input vectors be, uh, have high probability. Uh, and so in this case, supports your training or the training set, how, what it needs to do to make the training set more likely. Yeah.