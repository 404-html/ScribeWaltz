Speaker 1:          00:00          In this video, we'll see how we can perform certain types of inference, uh, in a restricted Boltzmann machine. And specifically we look at conditional inference.

Speaker 2:          00:12          Yeah,

Speaker 1:          00:12          we've seen the previous video, the definition of restricted fossil machine. It's an undirected graphical model with the following, uh, energy function and a, we convert that into a probability distribution by just exponentiate the negative energy and re normalizing. So actually computing this joint probability is generally going to be intractable in a respectable some machine because of the normalization constant, which requires an, uh, an exponential, some over the numerator. So that particular inference of the, this probability is generally going to be intractable. It will need to be approximated if really want to actually compute that number. However, there are other types of inferences that are actually tractable in a restricted bolsa machine. And one of the most important is conditional inference of either ex given a value for the age factor or h, the hidden layer given the visible layer value of divisible, layer x and a. So what we'll see in the following slides is a, how to perform that particular inference.

Speaker 1:          01:23          Indeed in the restrictive Bolsa machine, uh, the conditional distribution of either h given x or x given h is actually very simple and easy to compute. So the first fact to know is that the, uh, full conditional distribution over the whole vector h the hidden layer given the visible layer x actually in fact risers. Uh, so, uh, it turns out that it can be written as the product of each, a conditional distribution of each individual hidden unit given the full vector x. So in other words, all the hidden layer units here are conditionally independent given the value of the visible layer x.

Speaker 2:          02:11          Okay.

Speaker 1:          02:11          The other interesting fact is that, uh, these distribution here, the individual individual distribution of h j each hidden unit, HJ given x, uh, so each day is a binary value random variables. So it's a Bernoulli and it's a Bernoulli, uh, which is such that the probability of HD being equal to one also has a very simple form. It's actually just the sigmoid of the linear transformation of x as multiplied by the JFF row vector of the Matrix of connections. W plus the bias, uh, of the Neuron Hg. So that speeching here. So here we have the more explicit a formulation where I've just written down with the sigmoid is, so we see that computing the perimeter of the Bernoulli, uh, of each Bernoulli here in this conditional distributions or the probability of HIV being called to one given x, uh, is actually very simple. It's quite similar actually to a computations we performed frequently in the feed forward neural network. It's the linear transformation of the input vector x. A parent tries by the rows of the matrix w plus the biases of the hidden units.

Speaker 2:          03:27          It's okay.

Speaker 1:          03:28          Also notice that the restricted was machine is actually a symmetric, uh, so it's an art director at graphical model. So are really the form of uh, the distribution of X. Given h is not really different from age given x because it's not directed graphical model. And so similarly we can then show that each x element of the x factor, uh, are conditionally independent given the value of the hidden there. And similarly, it's going to be the sigmoid of a linear transformation of my hand and layer h a and now it's going to be parent tries by the columns of a w. That is to get the probability of x gave equal to one. We'll just do the sigmoid of Ck, the bias value for the key input plus, uh, the, uh, product of the hidden layer with the KF column of a Matrix. W. Okay, so let's not actually derive this result and see how this is true. So I'll, I'll do the derivation for, uh, the condition over h given.

Speaker 2:          04:37          Okay.

Speaker 1:          04:39          Alright. So, um, first of all, it's just write p of h given x. Uh, so we can write that as the joint over x and h divided by the marginal over just, uh, x. So that's the summation over all possible hidden layer vector, which I'm calling h prime here of the joint p of x and h prime. So this whole some here, that's just p of

Speaker 2:          05:05          x.

Speaker 1:          05:08          So next, let's just actually write the a what p of x and h is a well, that's just the exponential, the negative energy, which is that, uh, divided by the normalization constant or the partition function and similarity here. And also I've just written an explicitly over what we're iterating where we're performing the sun here. So we're iterating over all, uh, vectors that are boundary of size, capital h, capital nature's, the number of hidden units.

Speaker 2:          05:37          Okay,

Speaker 1:          05:38          so we noticed that first we can cancel out the partition function at the numerator and denominator, and we can also cancel out this term here and here. And that's because it does not depend on h two. It doesn't the print he at the numerator on h prime. And so we can actually take that out. So we could multiply here by the exponential of see transpose x and remove it from here. And then it would cancel out with, what's that? The numerator.

Speaker 2:          06:10          Okay.

Speaker 1:          06:11          Next we'll just write down this whole term here into an explicit, some over, uh, all indices in the, uh, hidden layer. So in, uh, this, uh, well first off, um, this, uh, be transpose times h is that's just a sum over j of Bj Times h j and, uh, also this, we can actually write it down as the sum over the rows of w of x times the roof w times the value of the JFF unit. So if you don't see this, I recommend you just sit down and see how this is equivalent to that. And, uh, then I'm just writing exactly the same thing at the denominator. I've also written an explicitly that this sum here can be written down as a sequence of sums over the first hidden unit, uh, till the last second unit. So a sequence of nested sums over each individual hidden units.

Speaker 2:          07:20          Yeah.

Speaker 1:          07:21          Then because the exponential of a sum is the product of exponentials. I have that this is equivalent to that. We have the product of the exponentiate terms that were in today, some here. And, uh, I have exactly the same thing here.

Speaker 2:          07:38          Okay.

Speaker 1:          07:40          Then I know this, that in this product, in the denominator, uh, each factor in the product actually depends on a single hidden unit. H J o h Prime Jane. It doesn't depend. This factor here doesn't depend on the other age, uh, and other elements of h prime. Um, and so what this means is that if I'm summing say over h prime capital h, all of the terms here in this product are constant with respect to h prime age. Uh, Kevin will age except for the last one except for age prime age. So the, the last term for j Cole's capital h. So it means that all of the other factors in this product, I can actually put them in front of the sun and just perform the sum over the last, um, hidden unit of the corresponding factor in this whole product here. And once I compute this, well, then, uh, this is a constant with respect to all of their hidden units.

Speaker 1:          08:42          So I can actually put it in front of this whole sun. And in this way, I could actually, uh, write down this nested some here as just a product of the sum over, um, the hidden units times the sum over the second hidden it and so on. So what I've done is that I've converted this, uh, nested some over a holiday units as just a linear product of the, uh, as just a product of some over in individual hit an unit for each hidden unit. So this sum here was a sum of an exponential number of terms. And now here we have actually a product over a capital h number of factors. And each factor in the product is this here, it's just there's some over an individual

Speaker 2:          09:31          it in unit.

Speaker 1:          09:35          So now here I'm just writing that as a, uh, a product with the product sign over j of the sum over the JFF hidden unit.

Speaker 2:          09:47          Okay.

Speaker 1:          09:47          That's just the definition of, of that symbol really. Um, and then I can actually write down what the sum is. It's only over two terms. Four h prime, Jake walls zero or h prime j equals one. So if h prime Jay is equal to zero, then that's zero, that's zero. So the exponential zero is one. And then if h prime Jay is equal to one, then that will be the exponential of just Bj, which is here plus this term here, because this would be one next, I can just write this as a single product instead of the fraction of two products since both products or over, uh, the same indices, uh, j, uh, then that's just a product over j of the fraction of the exponential of the, uh, factor that involves h j, uh, divided by one plus the exponential of Bj plus, uh, the g eighth row of w times X. Okay.

Speaker 1:          10:56          And I noticed that this is actually a probability. So if a H J was equal to one, then I'd get this term. If HD was equal to zero, I'd get this term. So if I see some over this, if I sum over this expression for each values of, uh, that HJ can take, which probably the age you can take actually sum to one. So that's actually a, uh, distribution. And so, and this it turns out is just then the probability, it must be the probable the of, uh, age j, uh, taking it's given value, um, condition on x. So now we've shown you finally shown that p of age, given x or the full conditional over all the elements in age is just a product of the elements per element, conditional probability of the hidden units in my, uh, hidden layer. Now let's actually show that this is just a sigmoid of a linear transformation of X. So, uh, we had before that, uh, this is equal to that. That's just the last part of our derivation and the previous slide.

Speaker 2:          12:15          Okay.

Speaker 1:          12:15          And now I noticed that if I multiply here by the exponential of minus BJ minus W. Dot. X here and here as well. So essentially multiplying the whole expression by one, then this would cancel out with that. Uh, this here would cancel out that and then one, he would be replaced by the exponential of the negative term. And so that's exactly what we have here. We have one over one plus the exponential of the negative, the linear transformation of X. Okay. And by definition one over the exponential, uh, sorry, one over one plus the exponential of minus something that's the sigmoid. So that's just really the sigmoid of the lean transformation of x. So now we've shown that this conditional distribution over a single, uh, hidden unit is the sigmoid of the Lennar transformation of x when it's multiplied by the connections between Hj and all elements in x. And then we add the bias of, uh, the JF hidden units.

Speaker 2:          13:30          Okay.

Speaker 1:          13:31          And, uh, just a final remark that, uh, really here I've done the full demonstration of this conditional, uh, distribution in an RBM and in particular the a really useful property that we get some conditional independencies between the values of in one layer, given all the values in the other layer. But in fact, we could have used a general result note as the or property, uh, known as the local mark of property for a mark of network and, uh, uh, for an undirected graphical model, which simply states that if I haven't conditioned on, uh, if I have a conditional Savia undirected graphical model and its associated mark of network over a vector of random variable zed one up to zed, uh, zed v. And so in here, I notice here, uh, it should be said, I'm minus one, said I plus sorry. Plus one. So here's zed. Uh, I should not appear because it's here.

Speaker 2:          14:36          Yeah.

Speaker 1:          14:37          So the local market property says that this conditional distribution of one run the variable given all the others, it actually reduces to just the, uh, uh, conditional probability of Zen. I given only its neighbors. So in other words, a p of a zed, I, uh, sorry, a zed. I is conditionally independent of all other, uh, elements in the, uh, uh, zed variables given its neighbors in the, uh, mark of network. And, uh, so then we have that, this conditional distribution here. It can be written as the, uh, joint here. Sorry. The, yes, the joint over is that I and it's neighbors divided by, uh, the marginal over just as neighbors where we've marginalizing overs that I, and because this is a, uh, uh, undirected graphical model, which we can write down as a product of factors like this. Um, then we get the product of all it's factors, but the factors that involve, um, uh, either only the neighbors or any other ads that I variables in the, uh, full a vector are actually going to cancel out in the numerator and denominator.

Speaker 1:          15:56          So in the end, this reduces, uh, as just the, uh, uh, form, which is the product of all the factors that involve, uh, zed, I and any of the neighbors in the neighborhood or is that I in the mark of network divided by the normalization constant. So the same thing where we're normalizing by summing over all potential values of set and um, and in fact, so we could have used that formula, which I've sort of shown just by explaining it. You can do as an exercise, try to see how we can get from this to that for general, uh, undirected graphical model that we write down as a product of factors. Um, but so that's just a general property for uh, uh, the uh, for a mark of network and its associated undirected graphical model. So we need the mark local mark of property, just a shows.

Speaker 1:          16:51          So I'll remove some ink. The local market property really only refers to this and then we can exploit the fact that we have on deck undirected graphical model that separates out as a product of factors like that. It's actually gets this slightly simplified form. And by doing this we could actually have derived a, the previous result. So for the restricted Boltzmann machine, they said I variables really correspond to either the ex gay or has or either the, uh, visible layer variables are the hidden layer variables and the neighbors would be the neighbors in the mark of network. So, uh, and this, uh, case for a given x gay, this would be all the hidden units age j four j equals one, two capital h. And similarly for fixed h j, this would be all the, uh, ex, uh, ks in the visible layer four k equals one, two, uh, the size of the input layer.

Speaker 1:          17:50          And that's because as we remember, the mark of Network for an Rbm, uh, looks something like this where we have connections for a given input to all hidden layers. And similarly for a given hidden there we have connections with all inputs. In this case, the neighbor of this guy is the whole visible layer. Well, the neighbor of this guy is the whole hidden there. All Right, so this completes our description of conditional, uh, uh, inference in a restricted Boltzmann machine. Uh, it's very efficient and, uh, it has a simple form and we'll be able to exploit this a further to perform efficient learning or restricted Boltzmann machine, which we'll describe, uh, the next videos.