Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video we'll see another procedure for addressing the overfitting issues in deep neural networks known as dropout training.

Speaker 1:          00:09          Okay.

Speaker 2:          00:10          So we saw previously that if we are having over fitting issues in deep neural networks, we could use unsupervised retraining. And now we'll look at the other uh, uh, procedure for finding over fitting a known as to gastric dropout training.

Speaker 1:          00:25          Okay.

Speaker 2:          00:27          So to avoid overfitting, um, again, the idea is going to be that instead of directly optimizing what we, uh, would normally optimize with backdrop, uh, at our training loss, we'll try to somehow, uh, diversity, uh, training procedures. So I did it does something else. So for advice, we training, we had to do some unsupervised learning as an initialization phase and for dropout where in a sense going to cripple the neural network. Why it's training by removing stochastically hidden units. Um, so in the basic version of dropouts, uh, what we do is that for each hidden unit, once we've computed it, we also independently, we'll set it to zero. So essentially a throw away the actual value, the hidden units had by say, multiplying it by zero, sending it to zero with a probability of 0.5. So for instance, if we add this neural network and uh, imagine that, uh, we computed the first hidden there, uh, then once we do that, we actually for each of the new unit, uh, we'll set it to zero with a probability of 0.5 and we're going to sample this possibility of sending it to zero independently for each hit in unit.

Speaker 2:          01:49          And then, uh, so then some hidden hidden units will be set to zero. And then we repeat this procedure for computing. The second hidden layer, we compute how hidden units and then against the Catholic, we will set some of them actually to zero instead. And we continued this until we reached the output layer where we don't apply this sort of masking noise. And so for instance, one result of this sampling procedure could be that we would be sending this one to zero at the first layer and this one to zero at the second layer, which means really that we would, uh, be computing this subset of hidden units at the end. And the connections that are really used during for propagation would correspond to those that do not go into or out of the mass or dropped out hidden units. As you can see, this is similar to, uh, the type of noise we would add in denoising auto in quarters.

Speaker 2:          02:45          Uh, one difference is here that we're doing this at the hidden a unit level and said that the input unit level, and the other difference is that we're not training on autoencoder here. So what, when we're doing this, we're actually doing it while we're doing backpropagation in supervised learning with a neural network. So the impact of this is that now the hidden units cannot assume that all hidden units in the same hidden there are going to be present. And so they cannot code that do these other units. They cannot sort of collaborate to uh, generate these complicated patterns, uh, that might be useful for fitting the training data because now they cannot expect that all hidden units in the same hidden there are going to be present. And so by breaking this a coaptation, uh, a behavior that the hidden units could have, um, the idea is that now he cheated in unit will be forced to sort of focus on extracting a feature which is useful in general and not just when used in combination with all hidden units.

Speaker 2:          03:50          They still need to somehow do things that are different. So there, there is not, uh, no, uh, uh, no sort of a collaboration between the hidden units, but they cannot develop as complicated, quite updation patterns between themselves because they're not, they're knocking out as soon that aisle or the other hidden units are going to be present in some of the other hidden units will be set to zero stochastically and a and the basic version, we use 0.5 as the probability of sending two zero. We could use another probability, this could be hyper parameter, but in practice it seems like 0.5 works really well.

Speaker 2:          04:27          So how does that change the regular forward and backward propagation? I'll go to them for training a neural network. Um, well let's assume that for each year than they are. We've sampled these binary masks, so we could do this before doing for propagation. And I'll call these binary mask. M a k work gay in Eh, is an index for the hidden layer will compute our pre activation like we do bef, uh, normally. But now what we'll do is that will change the definition of the hidden layer by incorporating this multiplication with a zero. If a, with properties open five decided to set the head unit to zero. And otherwise, by multiplying by one, it means we retain the value of the hidden there. So this idea of dropping out hidden units, we can just implement it by multiplying by disbarring when you factor that we sample every time we do for propagation. And next action, you Borden's. If we put the same training inputs, uh, here, uh, uh, at two different for propagation steps, we will sample different mass, or at least we'll sample individual masks, which will almost certainly be different. Okay. So that's very important. The mask always changes. It is a samples to gasoline and that's really the only thing that changes in the algorithm before when we compute the hidden layer value of hidden unit, now we compute the activation back there and then we multiply it by this binary mass.

Speaker 1:          05:59          Yeah.

Speaker 2:          06:00          And when it comes to doing backpropagation, again, the only thing that's going to change actually are two things that are going to change. The first one is that because in the definition of h came on a swan or Hca, we were taking the activation, what used to be the activation vector and multiplying it by a mask. Then when we're back propagating, uh, the gradient up to the pre activation, uh, we also need to multiply by the chain because of the chain rule by the mask, a vector. And so this means that at this level, many of the gradients, we'll be actually set to zero. So there won't be a backpropagation, uh, that flows through a, so like no brainer. We'll file, we'll go through a hidden unit that has been dropped out. So this will be the consequence of this. And we can derive this.

Speaker 2:          06:51          Just buy regular the regular chain role for, uh, in the context of a neural network. So that's the first thing that changes. And the other thing, uh, which doesn't really change but we should take into account is that this vector now incorporates the mask. So this is the mass as the activation patterns. That's what we should not forget that here, this is not the, uh, this is a vector that contains a, a lot of zero. On average, 50% of its entries are going to be zero, because it's the what used to be the activation vector multiplied by the mask. Okay. But that's all that changes. Otherwise, the regular backpropagation algorithm, uh, remains, uh, essentially the same. It's just the incorporation of these masks that drops out. He didn't units.

Speaker 2:          07:38          So that's for training. When we're training, uh, we have to sample these mass for every training example and a yet a slightly adapt the forward and backward, uh, equations. Uh, but at test time, instead of getting something that's to castic, what we will do is that we will replace the masks by their expectation. And so for instance, if we sampled republic the 0.5, we will actually set the hidden unit to zero and drop it out. Then this, the, the mask at test time would be a vector filled with the value of 0.5 because that's the expectation of that binary vector. Um, so actually in the single hidden layer case, we can show that this is equivalent to taking a decision at test time, which is based on the geometric average of all neural networks, single hidden layer neural networks with uh, all these, uh, plus possible.

Speaker 2:          08:35          Uh, Byron, we masks. So it's as if we're building this hand symbol of an exponential number of neural networks that way combining together through each or geometric average. So, just as a reminder, if we have a number, a and a number be, if we're taking the geometric average, it would just be described route up the product. And just in general, if you have say three numbers, then it would be, uh, the product to the one over three. So that's the geometric average. And so we can think of it as an average in the log space. Or if I took the log of this would just be a one over three of the, some of the lug of these values. And then we take the exponential back and that gives us our geometric average. And so, uh, intuitively we in machine learning, we know that the best systems that rely on machine learning, uh, or if you want to improve the, the qualities of the system, one fairly raw, reliable way of doing that is to construct and then assemble of men.

Speaker 2:          09:37          We train models. Well, we can think of dropout training as doing this, uh, in a very efficient way. We don't actually have to train an exponential number of models. We just get them as a byproduct of the training procedure. Uh, at test time, uh, by just taking a, replacing the mask by this 0.5 probable that this factor of, of value with the value of 0.5 in every entry. That's only true in the single hidden layer case for multiple hidden there. This story doesn't really follow through, but we still see in, uh, that it, uh, we get much better generalization with this we could combining with combined this with unsupervised training so we could do unsupervised for training and then instead of using regular backup proper would use dropout backpropagation. And, um, in general, this procedure, uh, either with unsurprised retraining of also alone, uh, allows us to get very, very good data results in many datasets.

Speaker 2:          10:37          I won't go over them, but you should consult this paper by Jeff Hinton and colleagues, uh, that provide a lot of evidence that this is a very good, better way of training, uh, of, uh, training a neural network to get good a generalization. And, um, uh, and I would also add that one disadvantage of dropout is that training can be slower. Uh, but it again, it as, uh, if you're willing to put more computational time in training, then Eh, what we find is that often it will indeed yield with much better results at a, at test time. Alright. So that's it for dropout training.