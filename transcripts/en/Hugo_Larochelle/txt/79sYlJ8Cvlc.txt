Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video will introduce it. Another alternative to the regular auto encoder and the dinos in auto encoder, no one as the contractive or twin quarter. So again, here we're trying to fix this problem with a regular auto encoders with over complete hidden layers, uh, uh, which can actually do a perfect job by just copying, copying individual units in the input factor.

Speaker 2:          00:28          So when did the nausea on top quarter of the way we fix that is by adding noise before feeding an input, uh, to deal to a quarter with instead, uh, uh, feed it a noisy version of the original input, but we'd asked you to recover the original input and sort of remove the noise, the noise from the original input. Now in the contractive autoencoder. Instead, what we'll do is that we're actually going to try to explicitly avoid this uninteresting a solution by essentially explicitly, uh, by adding an explicit term that penalizes that solution. Um, the way we're going to do that is based on this motivation. So the types of features would like to extract are really features that are, that reflect only the types of variations that we observe in our training set and from the training distribution in general. Uh, and otherwise we actually like hidden you install are invariant to other types of variations, variations that are not meaningful for that type of data. Uh, so if we have a solution which corresponds to copying each of the inputs individually, then this is really reflecting any possible variation. So any type of very variation here will be perfectly reflected in the hidden layer. Now here what, that's something we'll try to avoid. We try to encourage the neural network to only have hidden units that reflect variations that are specific to our training set and others that are otherwise, uh, as much as possible in barrier to any other types of variation.

Speaker 1:          02:09          Okay.

Speaker 2:          02:09          So the way we'll do this is design this new loss function where in addition to the reconstruction era from the auto quarters, such as this one for Barnaby observations, the uh, cross entropy one, we'll add this term, which is going to be weighted by some hyper parameter lambda and uh, which multiplies the Cobian of the in quarter four, which we take the square of. It's a Furby this north. So the new term we add is a way to a squared for Venus norm of the Jacoby Ian of the quarter, which is this thing here. So, uh, we remember that teacher Cobian, uh, of uh, functions. In this case, the function is the encoder function is going to be the matrix of all partial Dale lives, abandonment of that factor with respect to all of the minutes of um, the uh, variable with respect to which we're competing to Jacoby in. And in this case it's the input vector x.

Speaker 2:          03:11          And so if we want the square of the venous norm, then it means that, uh, we would be, uh, this here would correspond to the sum over all elements of the, at the output of that function and the hidden there, a nested with the, some of overall inputs. So that's the variable that's which I'm computing the joke Cobian of the squared of the partial derivative of that element in the in quarter. Uh, with respect to that element k in the input vector. So what do we have here? We have first a term which says I want the encoder to keep all the good information that's necessary for me to have a good reconstruction of the original input. And then I have another term which says, well, try to actually do, I want an encoder that throws away all information. So why is that? That's because a way to minimizing the squared of the partial derivative live is to have a partial derivative of zero.

Speaker 2:          04:17          If the partial derivative is zero, it means that if I change the value of x, it actually won't change the value of the hidden unit. So, in other words, it doesn't contain information about this particular, uh, uh, value of, uh, this particular element from the input. And so if I wanted this to be zero for all hidden units, it means I would have a, uh, a hidden vector, which does not vary. Uh, if I change the input x, now there's a caveat to this. It's, it doesn't vary. Uh, so the partial data is really what happens around, uh, points in this case at a particular value of x of x t. So for, at that training example, what we're saying is that we don't want the hidden units to change. If I add a little bit of Santa's little bit of random noise to it, I want the hidden layer to stay the same.

Speaker 2:          05:13          Now I can't have a hidden there, which is in variants to what the input value is. And that also reconstruct, well, I can't have an encoder that keeps information and throws information away at the same time. But if I combine both, then we'll, I'll get is that an encoder, which we'll try to have hidden units that only keep the quote unquote good information about the input, the sufficient information for reconstructing the input while being as much as possible in variant, disaffected, the input for any other directions, any other types of information in the input layer that that's not useful at all. So to gain a little bit of a intuition for how this can work, we'll do a little bit a cartoon illustration of, uh, of this.

Speaker 2:          06:02          So again, imagine that, um, uh, actually in two D, well actually I'll be in the very high dimensions, but I project back the, uh, assume I'm projecting back the, uh, inputs into a two dimensional plane here and the imagine I have data which lies on this one dimensional manifold like this. So it actually imagined that my data is this training example here, this too, and any rotation of that too. So this would be a rotation, uh, counter clockwise and or rotations clockwise. And so any of these xs is just a rotation of that. Now, um, our objective, what kind of hidden units would it prefer? Well, if we had a hidden unit which was such that it's value changed in that direction. So if we went from that example to that example, that example, imagine a hidden unit which would take larger and larger value.

Speaker 2:          07:00          Well that hidden unit is useful for reconstruction for instance. It would allow us to distinguish between that input, that input in Davenport. So I feel it this, then there presumably there'd be a way of from the value of that hidden unit to say, I should reconstruct something like this and not something like that or something like that. However, this hidden unit will be, um, uh, we'll have ego colbian which is a, or a partial derivatives, which is not zero because it does vary. It doesn't have a partial dividends of zero. If it varies its value by changing the value of the input. However, if, uh, the lambda that waits to Jacoby and is not too big, then, um, if this hidden unit is really useful for reconstruction, then the loss in terms of, uh, so the fact that we're doing worse in terms of the Jacoby, Ian should be compensated by the fact that we were doing much better in terms of reconstruction.

Speaker 2:          07:55          So that would be a hidden unit, uh, which would be useful. However, that hidden unit would not be useful because for one thing, if it's value changes like this and uh, well, you know, a variation in this direction, this local variation here, it means that this hidden unit and Decision Unit and decision unit would essentially have the same value. Like, you know, a value that if, uh, if I project all values of that hidden unit on this slide there, we'll all essentially have this value here. So this is one unit would not help me to, uh, distinguish between that input, that input and that input. However, also this hidden unit is, uh, has partial dividends that are not zero because it does value its output. It's value. If I changed the value of the input. And so we've, uh, made the Jacoby and terminal the loss worse, it's increased, but also this union that does not help us actually getting better there.

Speaker 2:          08:56          So I hit in unit that behaves like this is a, would actually be discouraged when we optimizing it and the optimization would tend to not find hidden units like this and instead tried to discover these hidden units. And so again, we have a, we should have hidden units that learn something about the, uh, lower dimensional manifold, uh, behind our data. So learns about the structure, the meaningful structure that characterizes our training distribution. Uh, so I'm not being very specific in my details. I'm sort of trying to give intuition here and I encourage you to look at the paper for getting perhaps a more, uh, mathematically, mathematically grounded intuition for how this is, but that's essentially what contractive two in quarters, uh, do.

Speaker 1:          09:43          Okay.

Speaker 2:          09:44          Now, uh, in practice we find that the nosing auto encoders are contractive. Autumn quarter, both performed well. They outperform regular until quarter. In terms of the quality features they extract, uh, they have different advantages. Disadvantages. The advantage of a denosumab don't quarter, uh, is that, well one could argue it's simpler to implement. Really if you have auto encoder, call code merely corresponds to adding one or two lines to your code. The lines, I would add the noise before feeding it to the, uh, before computing. The headquarter function does not require a computing, uh, the Jill Cooper, the Chekhovian of the hidden layer. Uh, so the, and that this is this number of hidden units by number of inputs matrix. So this has some implications in terms of how slow the, uh, uh, how still the method could be. And that's the last function. So now we have to take the phone a degree in, in the center.

Speaker 2:          10:41          That objective, we have to do, uh, compute the gradients of the parameters with respect to the Jacobian Matrix, which is itself a matrix of derivatives. So that's the math can easily quickly become hairy. Though there are some libraries that facilitate this with automatic differentiation. The advantage of contractive, uh, auto encoder is that we actually have a gradient which is deterministic. There's no sampling involved. When we're training a contractive or towing quarter, so we can more easily, indirectly use second order optimizers like country get graded and there will be fts and so on. Um, there could be ways of sort of playing with the nosing or towing quarter and still trying to, um, uh, use country good gradient and that'll bfgs uh, but just generally speaking, it might be more stable if we do this in the context of contractive autoencoder because the grading is deterministic. So, uh, then it means that we don't need any sampling for getting the grading. And so we're getting an exact rating for what we're optimizing so we can expect perhaps a more stable performance. Um, so really it's just a matter of choice which one you prefer new one implement. Um, and again, so I refer, anyone wants to learn more about contractive auto encoders to look at this paper here by, uh, these, uh, researchers at the University of Montreal. And that's about it for a contractive or throwing quarters.