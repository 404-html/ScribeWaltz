Speaker 1:          00:01          In this video, we'll look at and derive the gradient of the last function, uh, with respect to the output layer of a neural network. Now, in our derivation of stochastic gradient descent for neural network training, we are now at this part where we need a procedure for computing the parameter gradients. This is actually a, so making the whole derivation of the gradients is, uh, actually a complicated step. So, Eh, we'll take more than one video is to make the full duration. And in this video we'll first look at, uh, the writing their gradients of the loss function at the output of the neural network.

Speaker 1:          00:41          All right, so will compute the partial dividends. And then what we'll do is that we'll combine the partial derivatives with respect to all output units, uh, into a vector farms or to get the actual ingredient. Now, uh, this is our loss function here, the negative log likelihood for the true target. Why? And now we're interested in what is the partial derivative with respect to DCF output. Okay, so in this illustration here, um, the true class might be this neuron. Uh, and now we're looking at the partial, they're there, which is back to the, uh, output of the neuron. It's activation. So this is restricted by orange here on the upper part of the circle. And, uh, and this neurons see might be any of the output neuron. They could be, could be the first one, which would be in, if this was the label with the incorrect one or it could be the last one, which would be the correct one. This made up example.

Speaker 2:          01:40          All right,

Speaker 1:          01:42          so taking the partial liver div minus the log simply gives us minus one divided by the quantity question f of x a y. So that's because there are different, the log is one over the input of the log. And, but now notice that this here affects of why DYF output. It might be a different output than the CF output. So actually this might be a constant, if c is not equal to y. So for this reason, we also multiply by one, uh, the indicator function that looks at whether y is equal to c and outputs one if y is equal to c and otherwise it outputs zero. So we'll get a partial liver zero if why is not, see and otherwise it's going, this is going to be one. So we'll get a partial derivative of one over f of x

Speaker 2:          02:32          of uh, uh, uh, of why.

Speaker 1:          02:36          And now if we put this into vector form to get the gradient, so the full gradients of dislikes function with respect to output units, then we noticed that we have minus one divided by f of x of y. Uh, no, no matter which output a neuron we are looking at, no matter what's the value of c. And the only thing that changes between different values of c is we have a one here or zero right or the indicator function outputs one or zero. So then we can factor this out and get a instead write this into vector form by constructing a vector which contains weather. Uh, the indicator function before c equals zero c equals one. And so I'm up to c goals. See my capitol c minus one. So I'll just assume at this point that I'm a indexing the labels from zero to uh, the number of classes minus one.

Speaker 2:          03:30          Okay.

Speaker 1:          03:30          And so this vector actually noted e of why, uh, we call it the a one hot vector or a one out of sea vector. So it's vector that has a bunch of zeroes except at a position and the position in, uh, and question here is why. So the, uh, at the position where we have y equals two, its actual value. We'll get a one and otherwise it's going to be zero elsewhere. Okay? So our gradient of the output is going to be minus the, uh, one heart vector for y divided by f of x of Y.

Speaker 2:          04:12          Okay.

Speaker 1:          04:12          Now let's look at what's the uh, gradient, but for the pre activation at the output layer, so now we want the partial derivative of the loss function with respect to the CF pre activation at the output layer. So that's a albus us one of x, and then the CF element of that factor. Now we'll look at the duration in the second, but it turns out it has a very simple form. It's just minus whether c is the true target or not, minus what's the probability that our neural network has assigned to Class C? So what's the CFG output at the output layer? That's actually a very simple form, if only right into a vector form to get the gradient. Well, uh, this term in vector form would become the one heart factor. And then this term would just be a, would become the vector of probabilities. So the output vector, ef bold of X. Okay, so this is actually a great Ian for the reactivation, which actually very, very simple, uh, mathematically. Now we'll look at the derivation of this partial derivative. Uh, so the, uh, I like doing this derivation because it's, uh, uh, if that's necessarily the result is a simple, but getting there is not as simple and it's uh, uh, we'll go through a bunch of mathematical steps that are useful in general for understanding and working with neural networks. Okay. So what we're interested in is computing this here.

Speaker 2:          05:52          Okay.

Speaker 1:          05:52          Now let's first do the narrative with respect to minus log. Uh, so let's pass it. They're the two minus log. So it means that we would get minus one divided by f of x, y. So that's the derivative a of minus log of something. And now using the chain rule out where I have left to do is to multiply by the partial derivative of f of x, y with respect to our pre activation. I guess that's just the application of the chain will now let's replace f of x, y by uh, what it actually corresponds to. It's the softmax applied on the reactivation vector and we just look at the y

Speaker 2:          06:36          Alan of that factor.

Speaker 1:          06:40          And that's actually replace a softmax with its definition. We remember that soft Max is the exponentiate it a pre activations. And if we will look at the wife element, then we have the Wyeth reactivation normalize by dividing by four. All possible classes see prime. So I'm using c prime here, four s as my index for summing over all classes, uh, of d exponentiate a pre activations for all output units for all values of c prime.

Speaker 2:          07:10          Yeah.

Speaker 1:          07:10          So I've just, now we have this partial there live here to do. Um, so this is, uh, we have the directive to do for a ratio of two function and now we remember that the partial derivative of the ratio of to function with respect to x is the partial derivative of g of x with respect to x times one over h of x minus g of x divided by h of x squared. So the numerator divided by the denominator, but squared times, uh, the, uh, partial derivative with respect to h of X. Okay. So this is a fairly simple,

Speaker 2:          07:48          uh, uh,

Speaker 1:          07:50          fact about the partial there, the, with respect to a ratio. So we'll in in our case, g of x would be the numerator here. H of x would be the denominator here. An x would actually be deep reactivation for the CF output a unit.

Speaker 2:          08:08          Okay.

Speaker 1:          08:09          So here I'm just replacing, I'm using this formula here and uh, uh, so replacing the whole partial narrative, uh, by this term here where I'm replaced g evacs h of x and x by the actual term. And so now we just have to do this partial they would have here and this partial derivative here.

Speaker 1:          08:32          All right, well the partial, they live of the exponentiate id a why if pre activation, which respected, uh, the CFP activation, it's going to be zero if why is not equal to c and otherwise the partial, they were live with the exponential function is the exponential function. So we'd get the exponential of the YFP activation, which is exactly this term here. Now if we do the partial derivative, which respected you see it output of the sum over all outputs of the exponentiate a pre activation. So we see, we think the exponential of all, uh, the c prime a pre activations, well if c prime is not, see in this some Dan, this is, we're doing the derivative with respect to a constant. So the term is zero and otherwise for c prime equal to see, well then we add the derivative of the exponential, which is the exponential.

Speaker 1:          09:29          Okay. So they're partially, we give up this whole sun, uh, reduces to just the exponential of the CF pre activation. Okay. Because because it's the only term that's uh, uh, uh, that depends on the reactivation, uh, uh, the CFP activation. And so the other thing that I've done is that I've taken this part here, which is squared. I just wrote that as the denominator times itself. So I can actually separate that into a first fraction here, which takes essentially, so let me remove some ink which takes, that's with one times the denominator. And then, uh, this other term is just a, we'll use the other, um, denominator there. The other part of this square here. Now let's use again the definition of soft Max. We actually noticed that this here is the why if, uh, element of the softmax under p activation vector. Uh, same thing can be said for this term here. And now this term is actually not the wife, but the sea of element of the softmax of their p activation. So I have this year here, and again, we realize that while the sea itself, Max of the activation is just the outputs, uh, so the activation of the output layer, so that's f of X. Second, replace the softmax by f of x here, here, and here,

Speaker 2:          11:06          okay.

Speaker 1:          11:07          And now we know this, that we're multiplying in front by minus one over F of x, Y I have have a f f of X. Well, why here f of x, y here is, I can actually cancel that with both these factors.

Speaker 2:          11:22          Okay?

Speaker 1:          11:23          And so this gives me indeed what's left, which is minus the indicator function of whether y is equal to c minus F of x a seat. So the CFO, uh, output value at the output of layer, okay? So that's how you can derive the partial derivative of the loss function with respect to the pre activation for any output neuron. See, and we see how that actually reduces by reusing the definition of the soft Max actually reduces into an expression that only depends on the output activation. So we get a very simple expression,

Speaker 2:          12:02          okay.