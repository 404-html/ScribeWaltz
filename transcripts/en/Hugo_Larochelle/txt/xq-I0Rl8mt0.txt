Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video, we'll look at a surprising result that states that under certain conditions a linear autoencoder, it's actually as powerful as in nonlinear roto in quarter.

Speaker 1:          00:12          Okay?

Speaker 2:          00:13          So I know Tom Quarter, as we said before, is a part of a neural network that first encodes the input into a hidden there and then the codes it to obtain a reconstruction we call x hat. And uh, in the previous videos then I was suggesting we use in the, uh, Indian quarter in nonlinearity, uh, applied on the linear transformation of the input for feed forward neural network for classification. We also justify using a nonlinearity because it would yield a nonlinear classifier, which would be more powerful than a linear classifier. So that's considered a question. Um, for auto encoders, do we actually gain something by having a nonlinearity or would removing it? So not having this or this here and just having a linear transformation for the June quarter, what, uh, how powerful would that be? How good would this auto encoder be at reconstructing it? It's imperative. We trained it on some data.

Speaker 1:          01:17          Okay.

Speaker 2:          01:18          So what we'll actually show is a pretty surprising result. It turns out that linear autoencoder is actually optimal under certain conditions. That is, it will give you a reconstruction error that for a given number of hidden units is a small as it can be. And, uh, this result applies if we consider the square difference error and it applies to, we assume that the decoder is linear. So, uh, for the squared difference error, uh, or the square difference loss, I suggested we use a Lennar decoder. So in that case we can actually show that a linear autumn quarter, uh, will be optimal. We will give you the best training or it can for a given number of hidden units. So we'll, we'll show us sketch up that, uh, uh, actual proof. And for that we'll need this particular result here, which I'm describing. So if we have any matrix a and which we can write into a singular value decomposition.

Speaker 2:          02:18          So that is, we can write a, as the product of a u n a r u matrix, which, uh, has, uh, it's columns, which is Ortho normal. So it's columns are, uh, uh, our Ortho normal vectors, a sigma, which is a diagonal matrix. And on the diagonal of the Matrix, you have the singular values of the matrix and then be transposed would be another, uh, Ortho, Ortho normal matrix. So this is what is being stated here. So imagine this matrix a and imagine that we write it's a single value, the composition like this here. And now consider the, uh, a particular case where we'd be self selecting only the key, largest singular value. So imagine the diagonal of the Sigma Matrix. We'd actually had ordered the singular values in, uh, uh, uh, in decreasing order. So the first one on the top, uh, top left of the Matrix would be the largest singular value and then a decrease as we go along the diagonal.

Speaker 2:          03:24          And a, so imagine then that we just select this sub matrix by selling thing, the k first, uh, rows and columns. And then we'd do, we'd select the left and right, uh, uh, Ortho, normal vectors in a u and v. So that's what I'm noting here. Then we can actually show that, uh, imagined for this matrix a we wanted to find in other Matrix B A, which is up rank k, so it's a, and k would be presumably smaller than the rank of uh, of a. So imagine we want to find this sort of approximation B of a, but, and it's an approximation that's less complicated in the sense that it's a matrix for the lower rank and want it and want to find the approximation that minimizes the for beenus norm between a and B, sort of the difference between a and B that makes, uh, the approximation B that is closest to a according to the for Venus norm.

Speaker 2:          04:19          So that's what is stated here. Amen. So the Matrix B with the constraint that [inaudible] needs to be a rank k, uh, of this expression here. Well, it turns out that, uh, the solution to that optimization problems, so the Matrix be star, uh, that optimizes different being a storm here is actually just a matrix who's singular value decomposition is compose is made of. Um, or sorry, the matrix speed trend star that can be written as the product of the k, uh, the selected first k columns of you, the a sub matrix. And I select the k first rows and columns of a sigma. And then, uh, we transpose where V we've also selected selected the k, uh, um, the key, uh, first, uh, Ortho, normal vectors, capers, columns. So in other words, by arranging the matrix like this, then it's actually easy to get an approximation of it that is as best as possible for some giving ranks. We just select the part of that expression that corresponds to the k largest, a singular values. Okay? So this is a theorem that we won't show this Durham, but we'll use it to show the proof that the linear term quarter is optimal.

Speaker 2:          05:43          Okay, so now let's do the proof of optimality of the linear a to encoder. It's truly just a sketch because I'm going to be making some assumptions. Um, one assumption that I'm going to make is that the number of training, uh, examples in a number of these x tees here, uh, is going to be the same as a DEA number of dimensions in, uh, in the input x. So essentially the sum here is over the same number of elements than this. Some here.

Speaker 1:          06:16          Yeah.

Speaker 2:          06:17          So when we're training and not one quarter, that's what we want to do here. Want to find the parameters such that we minimize the average or the sum that's equivalent of the reconstruction era. There's a, so the square difference and uh, we'll um, uh, in, uh, let's assume that this here was computed by a linear and quarter. So we want to be able to analyze that case.

Speaker 1:          06:42          Okay.

Speaker 2:          06:42          So another way of writing this expression would be as follows. So we want to minimize this expression instead. So this whole expression here, we can write it as half times to for Venus norm squared of this difference. Um, so in particular in this expression, capitol x here is going to be the matrix, uh, where the columns of that matrix are the different training examples. And so in other words, because in I'm assuming we have the same number of trainee examples as we have dimensionalities a number of, um, sort of the size of the, uh, so the name number of training examples as the size of these vectors. Then this matrix x is going to be square and a, that's just going to facilitate the proof. We can generalize the proof to the general case, but I'll just for simplicity, consider that specific case. So that's what x here is going to be.

Speaker 1:          07:39          Okay.

Speaker 2:          07:39          And uh, then h of x, I'll call that the matrix who's, um, columns are going to be the, uh, encoder outputs for each training example. [inaudible]. And then if we multiply by the, uh, decoder weights w star, then by doing this, uh, so by doing this, then, uh, we would obtain our reconstruction because a WWE star, the first column of that expression here, that would be the result of multiplying up. You start by the first column of h of X. And uh, and so all the columns of those matrix, that matrix would be all the reconstructed factors. And then if we take the square, the for Venus norm, we do get the uh, a double some. So a nested, some, uh, over, uh, the squared of the reconstruction of each element in the inputs, training examples and their reconstruction that all this here that we have a greater or equal, we don't have an equal. And that's because I'm also going to assume that age of x here is actually any matrix. Uh, so any matrix, a whatsoever of a size number of hidden units by a number of training examples. So we're not actually yet, it's going to constrain it to be the result of a linear encoder. Well, just assume it's any matrix. And then after that we'll actually show that, uh, if we minimize this, that the greater or equal is actually an equal, uh, if we assume that this was the result of a linear in quarter.

Speaker 1:          09:25          Okay.

Speaker 2:          09:25          But for now, let's just look at this particular problem where h of x can be any matrix.

Speaker 1:          09:34          Okay?

Speaker 2:          09:36          Okay. So now what is the result of that minimization here, which is here? Well, from the theorem and just saw before. Then if we take x and we consider it's singular value decomposition like this and well we know this, that this here is actually a low rank matrix and the rank of that Matrix is going to be at most the number of hidden units. So if we think of k as being the hidden layer size and number of heating units and the encoder, then the previous theorem tells us that a solutions for this would be to have w star, uh, be equal to say the, um, uh, d, uh, part of the, you may tricks that considers all the, the first k columns times the a sub matrix in sigma that considers only the first key rows and columns. And then h of x could be this, it could be d, the rest of the, uh, expressions. So it could be a V transpose wherever you we've taken the k first columns. We could have put a sigma here as well. That's another solution. Uh, but let's just, you know, this is one about a valid solution and it does minimize that expression based on the previous, uh, theorem.

Speaker 2:          10:55          All right. No, let's see whether this particular h of x, which states that farm and minimize this, this can actually be expressed as a linear in quarter.

Speaker 1:          11:09          Okay.

Speaker 2:          11:09          And then if we do that, we finished our proof. So h of x is equal to that. That's exactly what we have here. And now I'll just essentially multiply by the end entity y d I a, uh, because this times that is the identity. So this matrix times it's inverse here. Uh, that's just the identity. So I'm not changing anything in the equation.

Speaker 2:          11:37          And I'm going to replace x here by, it's a singular value decomposition. So x transpose, it's going to be the transpose of that expression if we put it in the singular value decomposition forms. So that's fee times sigma transpose that. You transpose. Thanks. You thanks to sigma, like times be transposed. So that's, it comes from this, this uh, sorry, this is that here and this is here, this whole thing here, this whole thing here. And then similarly for that matrix and that Matrix, which, uh, uh, actually I'm just going to take this singular value decomposition of that Matrix, which we find here now. Uh, then we, uh, no status this times that that's just the identity matrix because you is oracle normal. So we just get this expression, these two cancel out. And then I'm going to write this, uh, inversion here as that. So indeed, we know this, that if I take this, am I multiplied by what's within the inversion. So this times that, that's just the identity matrix cause this cancels out with that, uh, this trends, this matrix times the inverse of that same matrix that's going to be the identity. And then v Times v Transpose, that's also going to be the identity. Okay. So, uh, I've just rewrote this part into that form instead.

Speaker 2:          13:22          Next, uh, we noticed that this times that that's also the identity because again, the, is Ortho normal so we don't see it here anymore. Uh, we apply the same thing here, but now, because this is the transpose of only the k first columns of v, uh, then we get the identity matrix, but where we've selected only the k first rose up that identity matrix. Okay. So this will be a matrix of size, k times number of a number, a dimensionality of the input space. And it's going to be to have ones on the diagonal n zero elsewhere. So that's this matrix here. Uh, next I'll just apply the inversion and notice that if I inverse the product with Matrix, then it's the product of the inverses. But, uh, the order has been inverted. Okay. So I, that's why this is equal to that matrix inverted times, that matrix inverted. Now I know this, that's this matrix times it's inverse. That will be a day identity so they can sort it out. So I get this expression

Speaker 2:          14:46          and then, um, now I have this, the result a, that's matrix times stat matrix. I multiplying on the left by, uh, this identity matrix, which, uh, for which we consider only the first k roads. So what this matrix is going to do here, this matrix here is that what is happening here, it's only going to select the first key roles of the result of that expression and uh, and in particular the entries in that Matrix. And so the k first roles, uh, rose up that matrix is really can really be computed only by considering the sub matrix of a sigma in verse, uh, that corresponds to their first key roles in the first k columns and multiply by the U Matrix transpose. But where we've taken a little bit k first columns at that you matrix, do you think about it a little bit, uh, doing that is equivalent to this because this selects only the k first roles, rows of that Matrix. And these roles can be obtained by multiplying these two sub maitri season, some injuries of this and that.

Speaker 1:          16:03          Okay.

Speaker 2:          16:04          And then we know this, that we have and and quarter that was computed by taking all the inputs and then linearly transforming them. Because this is just a linear matrix. So this could be our matrix w in an encoder. So h of x can be expressed as a linear and quarter h of x is also, which has this form is also the optimal solution for minimizing this. So if we want to optimize that with, uh, uh, uh, if we want to optimize this thing here, then, uh, the best we could do would be this. And this could be obtained by having this be a linear in quarter. So the best reconstruction we could ever get if we have a linear decoder is to have this thing, this matrix here being a to that. And this expression here can be obtained by a linear and quarter. So that's, we've showed that the best autoencoder here, uh, with the linear decoder is a achievable by Elinor in quarter.

Speaker 1:          17:06          Okay.

Speaker 2:          17:07          All right. So, um, so this is just what I've said in h of x, which should, uh, we should take x and multiplied by that matrix. So that could be w and w star. It could be the other part of the expression in the, uh, svd a theorem that you reuse for the low rank approximation. Um, and, and with these two expression, then we've reached the best we can in terms of reconstruction era. That's what we've just shown. It's the best in terms of the square difference error or squared difference loss. Uh, it's also the best if we have a linear the quarter, there's where the two assumptions we were making, what we're making, that proof and also it's the best. So here means the lowest training reconstruction error. It doesn't necessarily say something about generalization error, but if we have more and more and more training data, uh, then eventually, uh, training error convergence to generalization there. So it's still, that's uh, it would mean that for a large amount of training data we can expect that this linear and tone quarter we actually do really, really well in terms of reconstruction there.

Speaker 1:          18:13          Okay.

Speaker 2:          18:13          Um, so that's kind of a surprising result and that perhaps suggests that there's something sort of broken about the auto encoder, a formulation that we've presented so far. If we want to consider non linear features, non linear encoders and uh, I'll find you two to finish up. I'll just mentioned that if we normalize the inputs such that we take each input and then we subtract the average of all training inputs, and then, uh, if we also divide by this factor here, then we get actually show that the encoder occurs funds to principal component analysis. And, uh, uh, so indeed, if we using this, then, um, uh, the, uh, singular values and the left vectors, I actually associated with the Indian values and the vectors of the covariance matrix of the training examples. By doing that transformation, I won't make the demonstration. You can try it out. It's not too complicated. But, uh, so in other words, there's a relationship between on tone quarters and principal component analysis. And with that preprocessing principal component analysis is really optimal in terms of encoding data into a, uh, uh, a, a hidden layer size of CISE gay, uh, uh, and, and, uh, in such a way that we get a really good reconstruction there.

Speaker 1:          19:40          Okay.

Speaker 2:          19:40          Right? So that's it about this surprising result about the neuro tone quarters.