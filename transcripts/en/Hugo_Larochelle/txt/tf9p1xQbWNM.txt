Speaker 1:          00:00          In this video, we'll look at the directives of different activation functions now or previously, the rive, the gradients of, uh, the last function with respect to the pre activation vectors. And, uh, we saw that actually involved the narrative of the activation function. So let's look at what those are. Four different choices of activation functions. If we're using a linear activation function, then, uh, the partial they live or did it, there it is actually very simple. So if you take the derivative of Gfa, which is back to a, uh, then, uh, we easily see that it only corresponds to one. So in this case, actually, we just multiply by one. So we don't even have to take into consideration where we do our computations of the gradient of the reactivation function, the reactivation of the neurons,

Speaker 1:          00:53          more common choice as we've seen as the sigmoid activation function. And so it takes this form, it's one over one plus the exponential of minus the pre activation. Now, if we take the devil develop that with respect to the pre activation function, the reactivation, sorry, a a, then it turns out if we make the derivation that uh, it has a fairly simple form, it's simply the value of uh, the activation function, the sigmoid times one minus the value of a, again, a gov of the sigmoid of the reactivation. And, and so it we see here that it makes sense indeed. If we there, there would be the slope of the function and uh, we see that if either gov or a one minus g is equal to zero, that is if Giovanni zero or if [inaudible] is equal to one, in which case we have one minus one.

Speaker 1:          01:50          So we get zero. So, uh, then we'd get a derivative of zero. And so we can only get gov equal zero if the reactivation is uh, as a minus, uh, is close to minus infinite and a or is minus infinite, and then Jay is going to be equal to one if a is equal to infinite. And that indeed we see that as we increase the value of the pre activation, as we decrease it, then we see that the slope is, is becoming, uh, the shape of the curve is becoming flatter and flatter and flatter. So indeed we see that the gradient, the derivative, as we increase the activation or decrease it becomes closer to zero. And so this actually means that when we compute the gradients, we'd be multiplying by zero. If the, uh, a hidden unit has its pre activation, a very large and either a negative or positive.

Speaker 1:          02:48          So, uh, the grain, it is going to be close to zero. If a hidden unit is a saturated, we'll say that the value is saturated because it's close to one. If it's a, uh, pos potential value add, uh, uh, in its range. So this means that, uh, actually training a neural network when hidden inside saturated is hard. And that's because the gradients are going to be very, very small. And so we'll get very little feedback. The, uh, uh, hidden units that are below that hidden, it's all going to get very little feedback from that, a hidden unit. Because it's essentially going to propagate a partial derivative of zero and other activation function that is popular is the tension or a hyperbolic tangent. Uh, it has this form here that we've seen, or we can write it this way. And again, from a derived the a narrative, we again can write it in a fairly simple form is just one minus the activation.

Speaker 1:          03:50          So the activation function applying on the reactivation square, and again, we get the same behavior. Uh, if, uh, the neuron is close to it's saturated value, which is either minus one or one, then this term will be squared would be one. So he had one minus one. So we get a derivative of zero. So again, if this neuron is saturated, then a, when we propagate the greatest, when you compute the gradients with respect to the hidden layers below, it would actually be multiplied. Uh, the, the partial, then that goes through, that neuron will be multiplied by zero if the neuron is saturated. Okay? So those are the different, uh, there this for the different activation functions that we have, the news in neural networks.