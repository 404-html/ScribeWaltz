Speaker 1:          00:00          In this video, we'll formerly introduced the multi layer neural network. We seen previously that there are certain problems that a single artificial neuron can not model. Well, uh, because, uh, it's a problem that which is saying a classification setting that is not linearly separable. However, however we've seen that we can actually take a, our input representation and try to instead, uh, transform it in some way by applying some perhaps simple transformation like applying he and in this case here, the end function over, uh, to transform version of the input to obtain the new representation of our input vector, which is now linearly separable. And if this, the computation of this representation can be made by artificial neurons, then this is suggesting a more complicated model for more complicated problems, which would consistent first repor first computing a set or a, a number of artificial neurons that will compute this new representation. And then connecting those neurons to an output neuron, which would do the rest of the job and, uh, finished a computation of are more complicated function. And so this will be the inspiration behind, uh, the use and development of multi layer neural networks.

Speaker 1:          01:25          So let's look at this idea more formally in the case of a single hidden layer. Uh, so, uh, what we mean by the hidden layer is the computation of this representation, which will make the problem more, uh, linearly separable. And so in this single layer neural network will, we'll have a, the first part of the computation which will compute the representation. And then finally the computation of our output unit, which will, uh, perform the, um, the computation of our, say, our binary classification. The more formally when we're computing the hidden layer, we will compute again, a pre activation function. And not in this hidden layer we see here we might have several, uh, neurons, not just a single neuron. And so, uh, each of the IAF neuron, which will I will call a WHO's activation function, the outfit activation will be h of X. I will be connected to all potential inputs.

Speaker 1:          02:26          And so the connection with between the, uh, I have hidden a neuron with the JF input will be a contained in some matrix, which we will call w and tickets opponent in parenthesis one. So this is to refer to the case where we have just a single, a hidden layer. So doubly one is the weight Matrix, uh, for the same, uh, the first hidden layer. And similarly we will have biases and so bias of the iff hidden unit will be B I and uh, uh, the one here in the exponent is to refer to the first hidden layer again. So whenever we are computing the pre activation, we will actually have a vector of reactivation, uh, where the IFL element of that vector will be the IFP, uh, activation for the IMF neuron and that a hidden layer. And so for the I of effector, sorry, dif pre activation will get the IMF, uh, uh, bias plus the weighted combination of all inputs j and where x j will be multiplied by its connection with the of a neuron and a if we want to perform this computation to want to perform this computation for each hidden unit.

Speaker 1:          03:57          So in fact, we can write it into matrix form to obtain directly deep reactivation vector. And, uh, this would only essentially correspond to taking the vector x, multiplying it by our matrix w one, and then adding the whole vector of biases B one. So indeed, if we take x and multiply it by W, um, we'll, uh, we'll, we'll, we'll get this, that for the iff role, we'll get a term which corresponds to the sum of all the elements over that role of matrix w one multiplied by the corresponding elements in the vector X. Okay. So we see that this, and this is actually equivalent, uh, then to compute the hidden layer activation, uh, we simply apply our activation function over all the elements in the vector. So this, uh, here in my notation, I'm assuming we are doing an element wise application of the activation function, Jay, that we've chosen chosen either the sigmoid or the Tange or rectified, linear and so on.

Speaker 1:          05:06          And then finally we get an output activation, which I will call or I will note f of x by taking the hidden layer. So this hidden layer, so does a parenthesis missing missing here, a, we'll call it h one of x again one, because we have a single hidden layer. So we'll take that vector of hidden units and then we'll multiply it by double u two, which is going to be the vector of activations between the output and all the hidden units. So w to y here is the weight of the connection between the output unit and the hidden units. So we compute that, then we add the bias of the opportunity and then we will also have a uh, activation function. And I'm going to know that o for output activation function. And this is because we will typically make different choices for the output activation function. Then, uh, for the hidden units in the neuron

Speaker 2:          06:13          network.

Speaker 1:          06:18          So, uh, if we want to do binary classification, we can just use as the output activation, the sigmoid activation function. And so this would correspond to the, this means that at the output we would get the probability of, uh, the input belonging to class one. But if we have multiple classes, if I have more than two classes, uh, then, uh, what we'll need to do is have multiple outputs. And specifically we want to have one output unit at the output layer, uh, per class. And, uh, now we'll still want to keep this idea that our neural network will estimate the a conditional probability. So we'll like the output layer to give us the probability of a, the input belonging to each potential class, which I note seats. So, um, see what a typically belong in one up to a certain number of classes. Capitol C. Okay. So if we had 10 classes, then capitol c would be equal to 10 and a, so we would have indexed all of our classes, uh, from one to capitol seat one to 10.

Speaker 1:          07:33          And so, uh, to get an output, which corresponds to a valid conditional probability. What we'll use at as the output activation function is what is called the softmax activation function. And what it takes is that what it does is that it takes the pre activation and it simply takes the exponential of all M and elements of that reactivation vector. So it's a vector now because we have multiple classes. So we have multiple, uh, output neurons. And so we are multiple reactivation, uh, values. So the soft Max takes the exponential of all of those numbers and then divides each of these numbers by the sum of the vector, by the, some of the exponential of the pre activation for all possible classes. See, that's why we have this some old receipt here. Okay. So because we are normalizing our vector like this because we're dividing by the sum of the numerators here, then we are guaranteed that it will sum to one.

Speaker 1:          08:38          And uh, that's because we're divided by the sun, the, some of the numerators. And also we are guaranteed that it's going to be strictly positive. And that's because the exponential of any number is necessarily greater than zero. So we will have a valid, uh, probably the distribution, the numbers, the probabilities are positive and there's some two more. And then if you want to predict an actual class, so if he get an input x and one to actually assign it to a particular class a, then the natural thing to do is to use the class with the highest estimated probability. That is again, uh, we just look at all the output units and we output the class associated with the upper cabinet that has the largest value.

Speaker 2:          09:25          Yeah.

Speaker 1:          09:27          And finally, uh, in the previous example, we had only a single hidden layer, but there's no reason for not using multiple hidden layers. And, uh, we will discuss in other videos, uh, why we might want to do that. But to get this generalization to multiple hidden layers, say l hidden layers, uh, we'll have now a activation function as, sorry, a p activation value at any hidden layer. Kay. So we'll call that a k of x. And as this will just correspond to the bias of that hidden layer. Plus the Matrix of connections for that key. If hidden layer multiply by the activation values at the previous hidden there. So at the hidden they are k minus one. And if we use a for our definition, uh, if we assume that h zero of x is just x, so age zero would be the zero with in an layer.

Speaker 1:          10:29          So this will correspond to the input layer here in our definition then that this, uh, correctly defines the pre activation value at all. Then there and then to get the hidden layer activation. So the output of the hidden units at each hidden layer, what we do is that we simply take, apply again the activation function on the reactivation of the curse finding hidden there. And then to get our output layer activation, which corresponds to the index of the layer cake, uh, equal to l plus one where l is the number of hidden layers. Then, uh, what we could, if we use these definitions here for the activation function, uh, so for the layer l plus one h l plus four on the x is going to be again the activation function. Uh, but pass through an activation function for the case. Uh, we have the, uh, output layers.

Speaker 1:          11:34          So the, uh, uh, index alk plus one, we'll use our ever, not the g activation function, but the output activation function that we're noting. Oh, and, uh, typically instead of using this for the notation of the output layer, uh, often referred to it instead as f of x, which is to be the output of the neural network. Now if you're wondering why we say that these layers here are hidden and this is the output. So this is the output for the obvious reason that this is the actual value that is you say by our classifier, which is trying to classify some input. And these are hidden because we don't know what's the actual right value for these hidden units. So the neural network, uh, when we talk, when we talk about learning and the parameters of a neural network, it will try to find what's the correct behavior for these neurons here.

Speaker 1:          12:30          However, for the output, we know what's the right answer. So we're normally, we'll have some label data and for some given input, we will actually know which output we want to see, have it's value be the biggest amongst all output. So that's why these guys are actually, these hidden units are actually hidden. They're hidden in the sense that we don't know what's the actual best value for them to take for some given input. We only know what value would like to see, uh, for the output. If we actually know, uh, say in some data, if we actually know that certain inputs are classified in some, uh, certain classes.