Speaker 1:          00:00          Okay.

Speaker 2:          00:01          In this video we'll finally look at the derivation for the pre training procedure and a deep belief network.

Speaker 1:          00:09          Okay.

Speaker 2:          00:10          So, uh, now we seen the version all bound. We're ready to see the detail of the derivation for how we can, uh, pre train a deep belief network by first training in Rpm. Then taking it Suedes as is as the initialization of the first uh, layer sitting my belief net of a two hidden layer at DPN and then pretrained only the top RBM part. And then once that's finished, we use the uh, weights of the VPN here as the initialization for this thing might belief net, uh, for the two hidden layers in three hidden there. Dbn and then we are trained only the top RBM part and so on for as many layers as we want. Um, in what follows? Uh, I'm going to focus on the case where we start from a, uh, first, uh, one he didn't layer DPN or just an RBM and then we use it to initialize a, uh, two hidden, they are DPN where we take these weights, initialize the sigma belief net part here. And then a train the top a restricted Boltzmann machine here. So keep this a specific case in mind in what follows.

Speaker 2:          01:26          So we saw the various snowbound which uh, gives us for any model with uh, some latent variables, a lower bound here on the actual log probability for some given observation x. And uh, when we've, when we've seen in particular is that in this version Nobel and we introduce a variational posterior Qa h one given x and that, uh, if Q h one is equal to p of h one, then it's actually very tight. We get inequality between this and that. And uh, the courses actually cue is going to be with respect to p uh, the pursue your p of h one given x, then the tighter the bound here is going to be. Okay. So that's something we're going to exploit in our derivation of the pre training algorithm.

Speaker 1:          02:15          Okay.

Speaker 2:          02:16          All right. First thing we're going to do is that will, uh, instead of writing this term here as like a p of x, give p of x and h one we had to write it as the sum of a log of p of x given h one plus logo, p of h one. So that's because, uh, p of x given h p of x and h one is a product of this times. That is what we think the log, it's just a, some of them. Okay. Now imagine that, uh, so in this, sorry, in the single hidden layer DPN, which really is just an RBM p of x given h one and p of h one, uh, depending on the same parameters of the same model. So this is just the regular conditional in the risk. Most machine and P of h one would actually curse fun to, um, sorry. Poh One would be equal to the sum over all values of x of p of x and h one. So that's how we could compute p of h one in one hidden layer DPN or just a, which is just a regular restricted posts from Michigan.

Speaker 1:          03:31          Okay.

Speaker 2:          03:32          So when we jump to the two hidden there DPN case, uh, what we will do as we've described before is that to model p of h one we'll use a separate model with its own set of parameters and the model we'll use is going to be an RBM which has its own additional hidden layer h two. So in other words, p of h one once we go from a single rpm to a uh, to hidden there, DBN is going to curse fund to the marginalization of that second hidden layer in this, uh, top most RBM that we've introduced. So pfh one is going to be the sum over h two of a Pov h one and h two where this is an RBM with its own set of parameters that are not the same parameters as in a p of x given h one. So this is to, this part is going to have its own set of connections between h x and h one. And this is going to have its own connections between h one and h two.

Speaker 1:          04:38          Okay.

Speaker 2:          04:39          So if we do this, uh, it means that once we've added this second layer, we've not untied the parameters with respect to which we have to do an optimization where we're training a between this part of the bound and this part. So now this part is a free with respect to changes to this part here because they have different parameters. And in the Katrina peer training case, actually we were just going to fix that part. We'll just assume that this is not moving anymore. We're using the same parameters for p of x given h one, and they're going to be a set to the uh, uh, parameters of the single layer RBM that we've pretrained initially. And so when we get to train the second layer, we only need to change. We're only changing this part here because the predators aren't involved in that section.

Speaker 2:          05:30          So if we train our, uh, second hidden there to maximize the bone, it's, it means that, um, the only terms here that are dependent on the parameters of that second hidden there are here. So in other words, we can ignore that because it doesn't depend on the second hidden their parameters. And we can ignore all of this because to have a h one given x, that's a separate posterior model and the approximate exterior, uh, which we'll talk about in the next slide, but it's, it's separate that has its own set of parameters which, which are not adjusted. And so if we're maximizing this, then it's the same thing as minimizing minus that which is here. So minus the sum over each one of Qa, h one given x times log of p of h one. And um, and because this is an RBM, it actually corresponds to minimizing the negative log likelihood of, um, examples or samples from queue of h one given x.

Speaker 2:          06:35          So it's like we're training in RBM on data that was generated for, uh, by Q of h one given x by our approximate mysteria. Okay. So when we're maximizing dispositional bound, it's now been converted to the problem of training in Rpm on some of the observations that are generated by this approximate the mysterium. So for a given training example x, what we could do is generate a bunch of samples from Q of h one given x and then use those, provide those to the learning algorithm of the RBM, which is going to, uh, optimize the, its parameters to minimize the negative log likelihood of these parameters. And if we have a whole training, a set of examples x that it means that we would actually, for each training example, we'll need to provide a set of samples from Q of h one given x t.

Speaker 2:          07:30          So now we'd have separate training examples. We'd collect this big training set of uh, a sample hidden layers for a where we get a few samples from each training example and putting all those together would feed that to the learning algorithm or restricted poster machine. And that's how we could, uh, try to optimize this lower bound here. So this starts to look a lot like a, the training procedure we seen before where to train the second layer of a feet for no network with Matt, transform all the data from the, uh, of its input representation x to the latent representation h of x. And then we train, uh, an additional second hidden layer on top of that representation. What has changed is that in this context, what is different is that we actually sampling from a posterior to get these observations were not performing it just a Ford Past.

Speaker 2:          08:28          Now what should we use for this approximate posterior cue of h one given x? This bound applies in any case, but we know that it's tight. If Qa h one given x is actually close to the, uh, posterior peak of h one given x in this two hidden layer DBM. So what we'll use is actually the posterior of the first layer RBM. So, uh, in other words, uh, if you remember, if you have a single layer, RBM, um, computing the getting the per is equivalent to or requires to do it four feet forward, sigmoidal computing a feet forward sigmoidal layer, which gives us all the probabilities of each hidden units. And then to sample from that, mysteria would just look at each value, uh, in the Avi Chit in unit. And then this would give us the probability of each in an unit being sampled to one.

Speaker 2:          09:26          So to get a two sample from that choice of posterior, which would be the posterior of are a model that has just one hidden layer and corresponds to an RBM, we would just compute if feed forward pass and then sample. Now what's interesting here is that if we initialize the weights of the second layer RBM in our second, uh, in our two hidden their deep belief network, then we will start our optimization problem, uh, at a point where this is exactly equal to that. Um, so, and, and that's because a to hidden layer a db n is actually equivalent to one hidden there RBM. So let's look at that in more detail.

Speaker 1:          10:13          Okay.

Speaker 2:          10:14          So imagine we have a one hidden layer RBM. So here to simplify to illustration, just assume x and h one are vectors. And now we want to actually initialize the, uh, we're going to start training

Speaker 1:          10:28          a

Speaker 2:          10:30          two hidden there. DBN. So where we have directed edges here, and we've added this a second hidden there, here, and here we have undirected edges. So this farms in RBM.

Speaker 2:          10:45          So the first thing to notice if, uh, these connections are parent tries by w and if we take them this same w and we fix it to the parameters here, and then for this layer we actually use w transpose. So in other words, we set the parameters here to be w transpose the transpose of this matrix here. Then this model and that model are exactly equivalent. So one way to think, uh, to see this is that if were to sample from, uh, this RBM, we would say initialized x to some random values and we'd perform gib sampling. So we sample h one and an x and an h one and an excellent, continue like this. Uh, infinitely. And then at some point, um, you know, as if we get close to, uh, the equal in distribution, we could just observe x in, this would be a sample from, uh, from the distribution.

Speaker 2:          11:35          Now if you want it to sample from, uh, two hidden layer DBN, uh, w we do is that we could say we have to do Gibbs sampling here and eventually sample x. But because this is w transpose and this is w performing gib sampling here is equivalent to performing gib sampling here. Uh, so in other words, when for instance, we're sampling from h one to x discourages funds to taking each one and sampling age too. It's the same exact same procedure. And once we get an h one and we simple x instead, then, um, this actually again, is exactly the same thing as taking each one and sampling each do instead. So in other words, if we take all of the Gibbes sampling steps here, they could equally, uh, have been derived by doing gib sampling here. And then the very last step of good sampling, going from age one to x instead and observing x.

Speaker 2:          12:36          So from that argument, these two models are exactly the same, which means that initially if we've initialize the parameters of the second layer RBM as the transpose of the first hidden layer, then it means that the posterior, uh, as we computed in a first hidden layer, one hand them their RBM is the trooper steer or also for that model. And so if to sum up, if we initialize the two hidden layer DBM by first training in RBM taking it swayed w and initializing the weights here, two w and the way it's here to w transpose, we have that by using this posterior a of h one given x, uh, we are starting optimization at a point where this is exactly equal to or lower bound here. And then as we're going to train in our greedy peer training procedure, only those weights here. So we're only changing the weights here.

Speaker 2:          13:35          So they're going to depart from w transpose then our approximation here, which is only going to use w, uh, where w is kept fixed. Uh, as we're changing these values and we keep those fixed, then it's not going to be true anymore. Cue of h one given x is not going to be equal to the troop posterior appeal of h one given x in this two hidden there. Dbn but what's nice is that at least we have a good initialization starting point where we are tightly uh, close to what's the true luck probability of the data. And as we move along well we'll keep pushing uh, from that point on the lower bound up and which means that we're guaranteed to at least improve over a log of p of x a little bit and uh, uh, as uh, optimization progresses. Um, then we'll all be optimistic optimizing a variational bounds or a lower bound the log of p of x, but at least we'll have started at a point where we were at the first step will be guaranteed to improve it. And so that's the justification for providing using this person and layer with its suedes w to initialize the to hit and they're at DBN. That's because in this bearish no bound setting, we actually gets a tight correspondence between the version of bound of the two hidden there DBN and its actual log probability for some given example X. Okay.

Speaker 2:          15:06          So, um, so I've just described how we set u h one given x, we just used the previously pretrained weights to derive ARPA steer over the uh, hidden layer h one and then we'll use that posterior to train an RBM, which is the second layer RBM at corresponds to p of h one here. And the way we do that is that we take each training sample XD, we sample a hidden there for it. We construct a training set like this and then we feed it to a learning algorithm for a restricted Boltzmann machine, which is going to train the weights of that second hidden layer. Um, in practice people often instead of sampling from Qa h one given x, they will actually use the factor of probabilities as if we were doing for propagation, their regular neural network. This is really an approximation. There's not a lot of justification for doing that, but that's what people do in practice. If most of the units are either zero and one, it's going to be very close to the actual thing. Uh, and uh, and practice it works well. And now if we do this, then we get exactly the pre training procedure for feed forward neural network where we use, because this vector of probabilities is this h of x that we've, uh, uh, defined in the context of a feed forward no network. So then we get exactly the pre training procedure.

Speaker 2:          16:32          Now we can repeat this procedure, uh, by adding more and more hidden layers. So we started with a first hidden there, uh, one hidden there, RBM and then we use these weights to initialize the, do a hidden there RBM. And then again, uh, what we do for a three hidden there. DBN sorry. Yeah, I meant to hit in there DBN here. And then to get a three hidden in there at Dbn, we just think the prior here and convert it into an RBM. And then we use the weights that were pretrained here as the fixed value for these weights here. And then to get our posterior up to this layer layer here, we actually go up by using the same weights. Okay. So we using the same weights to go up in the hidden there. Now at this point, once we are at three hidden layers, we don't get a title rebound over a log of p of x.

Speaker 2:          17:26          So this argument that I said where, uh, we get an equivalence, uh, and a tight bound doesn't apply once we reach three hidden theirs. But still it does, the procedure is still corresponds to maximizing a bound on the actual log likelihood of the data for that dbs. It's just, we're not starting at a point where we actually are exactly equal to log of p of x. So in theory, the, uh, as we go on, we're not guaranteed that, uh, our approximation as we are training the Dbn, uh, for the posterior Q of h one given x or a Qa h two given x, once we at a training, a third hidden layer, uh, we're not guaranteed. It's going to be close to the troop exterior, it might actually get very far. So this only means that we're improving. Um, uh, we might not be improving on the look like you and we might be going to be improving on the no bound, but uh, still we might be extracting a better features by doing this.

Speaker 2:          18:27          And uh, as we've seen, uh, if we think of this procedure as a good initialization four feet for room feed forward neural network, we actually do get a good initialization. So a good set of initial features, uh, that are implemented by each hidden layer and that after some supervised by tuning, uh, uh, provide good results. Now in the context of a deep belief network, we do have a specific fine tuning algorithm for a fine tuning and this case, the, uh, value of the sum over our training example of log of p of x t. Uh, this algorithm known as the uptown algorithm is described in the original reference for deep belief networks. I'm not going to talk about it here, but you can go look it up. It would be useful if we wanted to do a train this deep belief networks in an unsupervised way and train all hidden layers at the same time as opposed as to just adding layers one after another. And if we do this retraining procedure, uh, first before doing the up down algorithm, we get better results. And that's how this particular procedure was. Why was first proposed? All right. So, uh, for more details on deep belief networks, I really encourage you to look at this paper in a sense. It's really the paper that started the whole deep learning, uh, uh, recent advances in the literature. So it's a really good read that's worth taking a look at. And that's about it for a deep belief networks.