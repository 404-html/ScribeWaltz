Speaker 1:          00:00          Yeah.

Speaker 2:          00:00          In this video, we'll look at how representations are merged in a recursive neural network. So we briefly described the recursive neural network in the previous video where after we've extracted a sink tactic, a parse tree of a sentence, then we essentially merge representations by following the structure of this syntactics parse tree. And, uh, these representations are merged by a neural network. It's so now we'll see specifically what kind of structure wood was used by which are social and his coauthors for extract, for merging the, uh, vector representations of the words and the phrases in the sentence.

Speaker 2:          00:44          So the neural network that they use is actually quite simple. Uh, it essentially corresponds to a neural network where we have, uh, just a hidden layer, uh, which is called P, which is connected to the concatenation of the two input vector representations, c one and c two c one and c two might be either the representation of, of some words or have some, a phrase which is covered by an internal node in the syntactic tree. And P is going to correspond to the new vector representation for a d note, uh, that has c one and c two as as children in the syntactic a treat. And so specifically, this representation will be obtained by taking the concatenation of the vector c one and c two multiplied by a matrix w adding a bias and then passing that through a nonlinearity, for instance, a Sigmoidal nonlinearity.

Speaker 2:          01:42          And so p will then become either see one of c to have a, a new call to the same neural net with the same weights, W and B, which will be merging this p and then perhaps, uh, some other vector representation in this intact if we as we move along towards the root of, uh, the syntactic tree. So that's the representation, that's the parametrization of the neural net that does the merging of the vector representation. And another thing that this neural network will do is actually compute a score of, uh, how the, the essentially the quality of the merge. How appropriate is it to take c one and C two and merge it into a vector, uh, which corresponds to pee. And to get this cord essentially only model it as a linear transformation of the merge representation piece. So you take the vector of B multiplied by w scored at different, uh, in this case, not a matrix, but in factor that's multiplied by P and gives a real value add score.

Speaker 2:          02:50          And this core is actually going to use that as a sort of estimate of the quality of the merge. And, uh, essentially it's going to be used to decide which pairs of representations, uh, uh, should we merge first as we construct is syntactic trees. So in other words, it's going to be used to determine what is the best syntactic tree to use, uh, in order to extract the vector representation of the different phrases and then ultimately the sentence. Uh, so I had to get this vector representation and so that part would be useful, uh, in the next video when we'd start talking about how to extract the same tactic tree from it given sentence.