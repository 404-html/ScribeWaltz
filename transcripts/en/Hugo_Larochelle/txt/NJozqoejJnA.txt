Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video, we'll talk about how we can infer the syntactic tree of a sentence using a recursive neural network.

Speaker 1:          00:08          Okay.

Speaker 2:          00:09          So in the previous video, we discussed how, uh, in the recursive neural network approach, we, uh, had the neural network, which could merge the Paris of, uh, a pair of representations of different nodes in the syntactic tree and then output the representation of a parent node. And we also discussed that we, uh, in that neural network, we would also have a part of the neural network, which would also output a score for the quality of that merged. And what we see now is that we could use that score in order to score entire trees. And then it's essentially a search in the space of trees to find a, uh, the trio would, which would be most appropriate as the, the, the best thing tactic tree, uh, to explain a given sentence. And at which point it means that we can just perform syntactic parsing using a recursive neural network.

Speaker 2:          01:06          So this is the neural net we saw before. It takes a, this is emerging neural net, which takes representation of two, uh, children in a tree, uh, representing a, there were a single word or a phrase in that tree and then outputs a merge representation for the parent in the tree. And then it also gives us a, a score for the quality of that merge as just the linear transformation of the merged representation. Now let's assume for now that this, uh, this, this prediction score is actually a good score for identifying where there, uh, these two parts of the sentences, these two phrases or two words or pair of words and phrase, uh, we're actually, uh, appropriate for being merged and be part of, uh, uh, a more global, a sink tactic tree. Now how can we use that in order to assign scores to a entire trees? And then how could we try to at least approximately search for the best treat? It has the biggest score.

Speaker 1:          02:11          Okay.

Speaker 2:          02:13          Well, the simplest approach in terms of scoring an entire tree is to just assign it the score, which is going to be the sum of all of the merging scores. So essentially the is going to be as good as the, some of the qualities of each mergers that are required. Then I that are done by the recursive neural network for that part street. So for instance, imagine that we had the small sentence, the cat is here and that uh, we, uh, considered this particular part story then the merchants we'd have to do is we'd have to merge the representation of the and cat to get the representation here and then are merging neural network would also give us as the output a score, which I'm calling s one, two, one two, because uh, it's essentially a scoring this subtree which merges one with two. Uh, we'd also have to emerge is, and here this would give us this word representation from a, sorry phrase representation from these two word representations.

Speaker 2:          03:15          And again, we get a score for that merge. And then following, this part's true. You have to merge these two notes to get the parent node for the full sentence. So we're merging this representation and that representation by concatenating them multiplying by w applying a sigmoid, which gives us this factor. And from that vector multiplying by w score gives us the score for that merge, which I'm calling s one, two and three four. So we see that the index is essentially representation with parent thesis of the actual tree. That's the notation and I'm using here. And then the score for the full tree is just going to be s one, two plus s three four plus s one, two, three, four as written here. So in other words, the some, uh, the, the score of the tree is just a sum of all the scores. I computed as I was, uh, performing all the, uh, representation mergers, uh, as I was going up towards the root of the corresponding part street.

Speaker 2:          04:20          Now if we were considering instead this parse tree, then I'd have to merge the with gap giving us this and this score here. Uh, then I take that representation and this word representation, merging it to get the representation for their cat is which is obtained by, uh, taking this representation here concatenated with this one multiplied by w by sigmoid, then multiplying by w score, we get the score for that particular emerge. And so on to get the score also for the merge at the root of the tree. And then this syntactic true would have instead of score s one to s one, two with three and s, uh, one, two with three, and then one, two with three merged with one. Okay, so now the question is for a given [inaudible] neural net that performs these merges. So you know, this, this, uh, neural network that would, uh, the W's and the w score and the biases as well. These are the parameters of their recursive neural network. It corresponds to the recursive neural network really. And, uh, so far a particular Chris neural network, what is the tree that has the highest score? Um, how do we find this particular tree? Unfortunately, we can't do this exactly. However, Richard Socher and, uh, his colleagues propose and approximate algorithm, which they claim works well, uh, uh, in practice and indeed, uh, the report good results by using this particular inference procedure. So let's look

Speaker 2:          05:58          at a disenfranchise procedure. I'm, I'm going to describe it essentially using an example to give you an intuition for how it works.

Speaker 2:          06:08          So we want to find the tree which has the highest sum of all of the merging scores. And so because the sum of a bigger tree is going through a, we can think of it as essentially the composite as the sum of, uh, a d score associated with it sucked trees. Then that's essentially how will obtain an algorithm, which will first look at the best a sub trees and then move toward, uh, move to, uh, trees that cover more and more words to eventually reach, uh, Eh, an inference of the tree, which would cover all of the words. So specifically we'll have, and the rea, which I'm calling eight here, which is indexed by the first and last word that the, uh, tree for this particular selling dra covers. So for instance, this cell would correspond to, uh, trying to infer the treat that covers the words from position one, two, three.

Speaker 2:          07:06          So the cat and it's a two four would be covering instead, cat is here and one, two would just be the tree that covers just the end cap and so on. And, uh, as we'll see as I described the algorithm out, I'll be able to reuse what I'm looking at, the scores of the trees that cover one and three, I'll be able to reuse these scores of the [inaudible], uh, below in the Ndra. And uh, and so this will be able to reuse the computations that I've done for a smaller sub trees to, uh, performing maximisation for larger centuries.

Speaker 2:          07:46          So, um, let's first consider, uh, what is the best subtree that covered just two words. So one, two and to three and three, four. And this gives it, its just one tree. The tree that merges the two corresponding words. So from that we would get then compute what is the score associated with each such subtree. So in this case, this is a score of merging one with two, which could, could be computed using our recursive neural network there. The part that merges the scores and then computes a, the emerges that word representations and then computes a score. Uh, we also probably have a, uh, another [inaudible] which would actually store the representation for, uh, for that tree, which is going to be used later on. But for now it's consider computing the finding the, uh, maximizing value for the sum of scores. Next we can move to tweeze of size two. So this covered trees of size one. Now let's look at trees of size too. So for the tree that would, um, and by size too, I mean the number of internal notes. So these trees covered, uh, you of size do with cover, uh, three words.

Speaker 2:          09:03          Um, so now imagine the tree that covers, uh, one, uh, the first word, the second word. And the third word, uh, with are two such trees. There's the tree which merges one, two first, and then merge is the representation of one two with the third word or the other way around. Merges two, three together first, and then merges the first word with their result of merging two and three. So the sum of the scores in this case would be the, uh, score of the first merge here. And then the numeral that corresponds to the root of this sub tweet that covers a words from one, two, three. So I'll read the computed the emerge up one and two. That's a one too. So I can just take that score and then add a new score of merging the result of emerging one two with uh, the third word, which is it's, so that's why we need that corresponding word fee. Uh, we're a site, an upward vector representation somewhere too. We use it in order to compute that score.

Speaker 1:          10:08          Okay.

Speaker 2:          10:09          So then I could compare that with the possibility of merging two and three first a witch and the score for that as being computed already. It's in this cell of the array. And then I would just add the score of then merging one with the result of having merged two and three. I'm assuming the APP that already somewhere that I can pass the word representation for one, the representation for two and three and then again, query mind neural nets to get a score, which would be this and then add that to the value in the cell two and three. And then I do a comparison. And then let's imagine that in this example, this sub tree is actually more likely a den. That's the value that I would keep in this area. And also somewhere I would keep what is the resulting vector at this node in that sub tree so I can reuse it later.

Speaker 1:          11:02          Okay.

Speaker 2:          11:02          Similarly for the tree that covers, uh, words from two to a position to, to visit for af two possibilities. Either I reuse the subtree covering two n three and then I merged the result of that with the fourth word in the sentence. Or I do the other way around you as a, assuming I've merged three and four together and then merging the result of that with the second work. And so similarly, Zach do a comparison. Maybe this is the most likely, uh, sorry, the, the, the, the, some of scores that is highest. And so I started that instead in the Dra and uh, and then also take the result of this, merge the vector representation and put it somewhere so I can reuse it later.

Speaker 1:          11:52          Okay.

Speaker 2:          11:52          Not, if I reach that part of the IRA, then now I have to consider all the trees that, uh, cover, uh, forwards. And this gives does own the one. Uh, so in this case it can, if we're coming forwards, it can only cover from the first the forward. So all words in the sentence and then there are three possibilities. Uh, so what can I do? I can either assume that, um, I have already merged all of the words from the first two, the third position, and then all that's left is to merge the fourth word with my representation of, uh, with, uh, uh, of the, uh, phrase from the covers were a position one to position three. So, uh, so this would be the score of that sub for you. Plus the new score from merging the fourth word with the representation of the uh, phrase from one to three, the cat is, and now I want to compare that with instead of having merge one to two and also having merged three to four separately, and then just taking [inaudible] and joining them with a parent node and, and merging the representation of these two separate treats.

Speaker 2:          13:08          So one and two and three and four, uh, there, uh, here. And here's, I can reuse those computations again. And uh, so these are raise the cells in these arrays contain the scores of these two sub trees. And I just need to increment the score of a so and incorporate the score of merging the representation of the phrase from one to two. So the phrase the cat with the representation of the phrase three for the phrase is here. Um, and then finally I can instead assume that I've merged already. Everything that's from two to force. I have a, I think the tree that, the best subject I found for covering a words at position two to four. And then I add to this, the, uh, remaining merge, which is to take the representation of the first word and merge it with the representation of the phrase from the second position to the fourth position.

Speaker 2:          14:08          So now I have three things to compare. And now let's imagine that in this case, uh, it's this option here that is most likely, that is assuming I've already covered, uh, one to two and three to four and then just merging these two subjects together. And now in that case, um, if I then wanted to infer what is the syntactic tree that corresponds to that when in this case it would correspond to disinfect Dick Tree. So indeed here we've made the decisions that we wanted to have merged. Uh, the, uh, have, uh, use the subtree between one and two and uh, the and joined with the subtree between three and four. And then the subject between one and two, well, there's only one, it's the one that merges the first word, the second word. So it's that sub tree and then is here the century that the only possible separately is this one and a here at the decision that have made was to merge these two.

Speaker 2:          15:05          So we get the global tree, which is like this. So that's a very simple, uh, uh, example for this, this, uh, our with them. Uh, I just wanted to give you an intuitive, uh, impression of, I actually works for more details. I encourage you to look at the original Richard social paper and the references and contains to know more about how you could actually perform this for general length sentences. But effectively the general procedure is you try to infer the best subtree for each possible span, uh, or phrase, uh, uh, lengths starting from, uh, phrases that covered two words as in here. And then phrases that cover three words as in here. And then phrases that cover forwards as in here. And by moving like this, you can always use the computations that you've performed previously for a, where you inferred the subsidiary that was best, uh, um, that was best, uh, for these, uh, these, uh, smaller phrases. Also, I'm not going to explain why, but this is actually not returning the best. Subtree. Uh, so this is actually an approximation of the best tree. Uh, and, um, I courage you to think more about this and, uh, to see why you might have to look at the original algorithm that stews for performing syntactic inference of, uh, a syntactic parsing. But, uh, for now you need to know really is that this is the general procedure that, that is used with recursive neural networks for inferring the syntactic tree structure of a given sentence.