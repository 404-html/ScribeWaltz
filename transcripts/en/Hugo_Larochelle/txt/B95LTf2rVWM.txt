Speaker 1:          00:00          Okay.

Speaker 2:          00:01          In this video we'll discuss the different type of output layer, which is going to be more efficient to use in the context of language modeling. Uh, and we will call that out type of output layer, a hierarchical output layer.

Speaker 2:          00:14          So we've seen a previous video how to define the neural network language model, which is essentially a neural net that takes as input all of the word representations of the words in some given conditioning context. And then we concatenate those representation and feed that to regular neural network where at the output we get a softmax layer, which will define the and give us the probability of observing each possible word in our vocabulary following our context. Now one problem with this approach is that the softmax layer here is actually really, really big. If we have say a 250,000 words in our vocabulary, that it means that this layer has to 250,000 different units, one for each possible word. And when performing the Softmax, we does have to compute all the reactivation. So these 250,000 activations and then we have to perform the softmax known in the charity. So that's going to be very expensive to do if we have to do this for every training example, every example of context and the word that follows it in the text. So what we'll talk about here is a way of getting a much better, much more efficient, uh, have a defining a distribution. The distribution offered the next word, given the previous word, uh, which in practice is very efficient and gives us good performance as well.

Speaker 1:          01:42          Okay.

Speaker 2:          01:42          The main idea is to, instead of modeling the probability of the next word, using a flat layer, we use a hierarchical layer where our wheel decompose their probability of observing the word into a sequence of probabilities, which corresponds to choices as to uh, to which group, uh, does the next word is more likely to, to belong, to, to organize. These groups will actually take all of the words in our vocabulary. And let's work with an example to make this more concrete. So imagine we have eight words in our vocabulary and the old fee work. Now we will do is that will place all of these words into a treat and sensei complete tree like here, uh, which is uh, which is balanced and essentially the probability of a, of a given word following the context is going to be given by the probability that from the route we would actually follow the path to the word cat in the tree.

Speaker 2:          02:42          That is, we would go left here, right here and then right here, so left at the note labeled one, right that the node label too. And then right at the node level five. So if we model probabilities this way, we noticed that we actually only need the probabilities for these three nodes and we don't need the probabilities for say the nodes here or this note. So if we look at the probability of one specific word, then actually computing probabilities is going to be a, it's going to be a, I have a computational burden proportional to the depth of the word in the tree. And if we have a balanced tree is going to be logarithmic. So it's going to be much more efficient to compute one specific probability. So more specifically what we'll do is that we will, uh, essentially have all of these nodes in the tree, correspond to a sigmoidal.

Speaker 2:          03:38          Uh, I'll put you in, it's in the neural nets, will have connections from the hidden layer. So imagine we had this context and then from this context, the neural net would extract the word representations, concatenate them, and compute the hidden layer off the neural net. So that's what this corresponds to here. And now this hidden layer, we'll connect it to all of the nodes in the internal notes in our trees and not the leaf nodes, which aren't the words, but the internal notes. We'll connect them with the matrix of parameters, which I'm calling v here. And so like I said before, now say we take this example where we want the probability that cat would follow the context, which is made up the word that dog. And that in this case, all I need is really the probability of, uh, taking the right decision, the right, uh, branches, uh, at each of the internal notes, one, two, and five. So at one specifically I want the probability of branching left. And then at two I want the probability of branching, right? And then five, the probability of a branch, Shane, uh, right. So this, uh, this shouldn't be three. They should be, they should be fine. And so, uh, no, what we need is for the neural net to output us this probability to provide us a model for these probabilities of, of branching.

Speaker 2:          05:00          So like I said, we could use sigmoidal units and then, uh, so essentially for this note, we would take the product of the row in Matrix v that corresponds to the parameters of this note here, when multiply that by the hidden layer and then with applied a sigmoid. And then we could interpret this as the probability of say a branching, a right, it could be branching left, it doesn't matter. This is an arbitrary choice. As long as we're consistent with the way we interpret the probabilities. So for this example, we're assuming that taking v multiply by the hidden layer and applying to sigma, it gives us the probability of branching right at a given a note. So it means that while initially the probability of cat of reaching cap would be to, they're probably the brunching right? A left then, right then, right

Speaker 2:          05:52          then if I want to use my model to validate that, probably I'll convert the probability of branching as one minus the probability of branching. Right? And then like I said before, I'll, uh, use my model to compute these probabilities and they're going to be computed as the hidden layer multiplied by the associated row in my matrix of connections feet. So we see here that will be taking the first row for the first note here, taking the second row for the second node here and in the fifth row for the fifth, a node here in my, in my tree. And we were doing the same thing for the [inaudible], the first bias. Second bias if fifth bias in my bias vector B. So by taking the product of all of this, then that would give me the probability of reaching cap. And I've been able to compute it in this model using only, uh, uh, using computation that scale, not linearly in the number of words in my vocabulary, but logarithmically and that's because I'm using a balanced tree where the depth of the tree is, uh, uh, in the order of the logarithm of, uh, the number of words in the vocabulary.

Speaker 2:          07:01          Okay. So that's the article. That's the general idea for the article output layer. Uh, that is very useful for, uh, in the neural network language model.

Speaker 2:          07:14          So now how do we actually define this? This tree, this word Harkey, uh, that organizes the words and two different leaves of a, of a treat. Uh, the simplest thing to do probably is to just generate this trees randomly. So we'd read, we'd a essentially generate a complete tree, so make it balanced so that at least the computations and the tree is logarithmic. And, um, uh, and then we just assigned the leaves of that tree randomly, two different words. So we take each word and we assigned randomly to a different leaf in industry. Uh, this is likely to be suboptimal for language modeling. It, it actually is. We get a worse performance by doing this then, uh, by using the softmax layer. Uh, that being said, it's, it's, it's not an entirely terrible, so, um, uh, it's, uh, it against it. It'll give a decent results and other approaches to actually use something good stick resources, say a we're net, which is, uh, uh, essentially a, a data structure.

Speaker 2:          08:15          Four words were the words organized in some tree. And, uh, essentially the, uh, words that are semantically related, uh, share certain sensors are, are closer in, in that tree. So I won't go into detail of how we're networks. You can look at this paper for how in particular you can use where net to derive a tree for organizing all the words and the, uh, into a tree. But in this paper, they actually reported really good speed ups that you'd expect from using a, a tree, and they only saw a slight decrease in performance compared to using a softmax layer. Uh, and then finally, more recently, uh, in this paper by me and Hinton, uh, a learning approach and machine learning approach for trying to derive what this word Harkey should be, uh, was proposed. It was based on a recursive partitioning strategy. Uh, so the way they would actually learn the trees that they would first learn irregular model.

Speaker 2:          09:14          So a, if I wanted that models, if you trained it with a softmax layer, that would be slow. So perhaps you could use instead a a random tree and train a first model. Uh, that model will have trained some word representations and then we'll try to use these word representations in order to infer what the tree should be. And the way we'll do that, this, that we'll use some hierarchical clustering approach where first weed at the root note of our tree, that would correspond to a cluster which contains all of the words, uh, in, uh, in our vocabulary. And then I'd use the word representations a and run the a clustering algorithm where I would extract all the two clusters. So for instance, my clustering algorithm could identify that this forms a cluster. And this forms and other cluster. Uh, and then I would recursively continue doing these binary clusterings, uh, but over the partitioning is that I discovered that the previous stage.

Speaker 2:          10:11          So in this case I would rerun and other clustering algorithm, uh, within this Gloucester and then within also discussed or separately. So maybe I'd get these two clusters here and then here I'd get these two other clusters. And then I continue like this until in, uh, my clusters, our only have at most a two words. Then for cluster, it has two words in it. Then that defines the two leaves for that particular part of the tree and to extract the tree. Then I just go from the general clusters or this one here. That would be the leaf. Uh, sorry, the root of the tree, it's two leaves would be this cluster and this cluster. And then, so for this cluster it's two leaves would be this cluster and this cluster. So all of the words here would be saying on the left, uh, part of, uh, that, uh, that tree accorded corresponding to that cluster here.

Speaker 2:          11:06          And this would, these two words would be on the right. And then I kicked, did you like this? So this node in the tree would have one of its a leaf be this node here and in another leaf would be an internal node of crisp corresponding to that cluster, which has two leaves, which are these two words here and so on for the other part of the treat. So we see that from this, we can extract, uh, our Harkey, uh, for the words, and then we could use that Harkey, uh, for prediction. And so the advantage of doing this is that if we have good word representations that were pretrained using a random tree or some other, uh, procedure, then this tree will more directly reflect the similarity between these words putting clothes together and the tree words to have close a word representations. And then I could retrain a new model, a new neural network language model that uses this tree now instead of c of random tree in order to perform a neural network language modeling.

Speaker 2:          12:04          So for more details, I encourage you to look at this paper, but they show that they get similar kind of speed ups you would get with other types of trees and uh, uh, but without a performance decrease. So that's very encouraging. Uh, it's a, it's a very good result. And before I end, I just want to say that this hardcore output layer is useful where for given contexts, we want to evaluate the probability of just one word or a few words. If we actually wanted the full distribution over all the words, uh, we wouldn't gain anything because we would still need to get to the probability of branching left and right for every node within the tree and the number of nodes, the total number of internal nodes in the tree is going to be linear in the number of words. Uh, and so we won't actually have gained anything if far giving context.

Speaker 2:          12:53          We have to compute the probability of all the words. So it's really when for, since we were training, we know what is the next word. Then we only need to evaluate the probability of that next word. Then in this case, we get a speed up. But otherwise, if we need the full distribution, then, uh, we don't sell it. Get a speed up in that case, and we'll still scale linearly in the number of words in the vocabulary. All right, so that's it about how I call output layer. And now most modern no network language models use some sort of hierarchical output layer. I've described the binary layer where at every node you make a binary decision benching left and right. You could have other versions of that where you would actually branch mall multi-way not just left and right. Uh, but in general, the idea is that by doing a hierarchical approach, we can get good speed ups in and be able to apply neural nets, uh, for language modeling unfair logical capillaries.