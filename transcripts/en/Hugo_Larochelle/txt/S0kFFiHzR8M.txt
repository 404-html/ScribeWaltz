Speaker 1:          00:01          In this video, we'll discuss a variant of the contrasted average in south with them. No one has a persistent contrasted divergence. So we've seen that the contrasted divergence divergencies actually a fairly simple algorithm for training restricted Boltzmann machine. It requires that for each training example, we sample in a what we call a negative sample instilled, which is generated by performing a just a few steps of Gib sampling, which is initialized at the training observation x, t and then the update of the parameters as a very simple form which involves a computations both at the training example and at the negative sample X. Still, I've also discussed at the end of the previous video that this, uh, this algorithm CDK, uh, where, uh, Kay is the number of gifts step, uh, was actually a very good algorithm for training a restricted Boltzmann machine to extract some useful features, uh, which is represented by the vector h of X.

Speaker 1:          01:07          So, um, h of x is the vector of hidden units, probability and a, what we could do is train a restricted Boltzmann machine on some unlabeled data and then we could change our training set or our data set in general, all of our data so that instead of representing it by the x factor, we could represent it as the h of x factor. And then use that representation to train, uh, say a classifier safe. We're classifying images of digits. We could feed that to a, uh, that new representation to a linear classifier or another neural network. Uh, we'll discuss also, we can use that to initialize a neural network. But the CD a algorithm with gay couples, one actually works really well for extracting a good set of features. However, when it comes to evaluating, say, a validation set, our test said, what's the value of the average negative log probability or in the log likelihood?

Speaker 1:          02:09          Uh, it turns out that it's not doing that well. Uh, it tends to perform a not so great. Um, the reason for that is, uh, so let's look back at our intuition for what city does. So imagine have an energy function which looks something like this where this would be a linear, uh, organization of all the values for the x factor. And Imagine I have a training example here. So what that would be one training example. So what city does to get an extended is that it starts give sampling there. And what Gibbs sampling does is that essentially jumps like this and say I do case steps. Then maybe, uh, give somebody with a essentially fall say on that point here. Uh, it doesn't go much beyond that point because the energy is much higher here. So the probability of sampling in that, uh, in these points here is actually quite small.

Speaker 2:          03:06          Okay.

Speaker 1:          03:06          Now because we do on the case steps of give sampling, actually the probability of going well over here or here is actually quite small because I'm doing just a very few steps of give sampling. And each step really goes a, it doesn't go very far away from its previous point. I imagine that my energy function is such that there's then the cliff. And then there are values for x here, which actually do have very small energy. But because I'm using a few steps of give sampling, uh, it's actually impossible for, uh, the, uh, gift sampling change actually go like this. And I said eventually reached these points. So this might going from here to here might require a lot of steps of Gif sampling for, uh, such a, uh, a trajectory to be likely. And because of this, it means that there will be regions with CD, a one or a CD five, uh, which, uh, will not get their energies raised.

Speaker 1:          04:06          And because these are regions of the input space, which doesn't have training data, then it means that when we were normalized the exponential in a negative energies, the relative probability of this is going to be higher. I see. Going to be lower, sorry, because we've assigned low probability, uh, elsewhere and because everything needs to sum to one that it means that if I'm assigning probably here, then I need to, to, uh, assign less probability here. So, uh, we'll see a very simple modification of the contrast of diverging salary with them, which was essentially partly solve this problem and a yield, a much better estimator of the, uh, in a much better model of the, a probability distribution that generated our training set.

Speaker 2:          04:56          Okay.

Speaker 1:          04:56          So the idea is actually quite simple. Um, because we're performing few steps of GIB sampling in between updates, it means that we can never wander too far away from our training example. However, in the previous iteration, uh, so before our previous update, we actually had performed some gifts, a sample, and it's actually kind of the same, a shame that we can't reuse this sampling to generate our next sample of our next update. And the idea of persistency is actually quite simple. Instead of using of initializing I Gibbs, Shane to the current training example.

Speaker 2:          05:35          Okay.

Speaker 1:          05:36          We will actually use,

Speaker 2:          05:38          okay,

Speaker 1:          05:38          the, uh, exterior that we sample at the previous iteration. So before our updates, uh, uh, uh, based on the previous training example, and, and that's it, the rest of the algorithm stays exactly the same. So this means that if I've performed, say, 10 parameter updates and I'm performing just step of Gif sampling for every update, x stilled is going to be fairly close to the result of performing 10 steps of good sampling instead of just a single step. It's not exactly this because in between, so during the gifts chain, uh, the parameters of the RBM will have changed, uh, because I'm updating the RBM as I'm doing this, uh, but still it will allow for x still to wonder further away, uh, much more, uh, far away from the training examples. Then if I keep re initializing the gifts chain, uh, to the training example and the in practice, what we see is that indeed by performance is very small change.

Speaker 1:          06:37          Uh, we get a much better estimator of the distribution and we got much better performance in terms of a negative log likelihood of the validations or that that's set. So of new examples. Also, I should mention that in practice, often instead of performing this in the stochastic, we will actually use a mini batch variants of this algorithm. And also, instead of maintaining just a single gifts chain that keeps being, uh, updated. So instead of having just one sequence of x, still, we actually often maintain, uh, a mini batch of x stills. Uh, and so in other words, there will be many gifts change that will be may Gibbs, uh, sorry, Gibbs change that will be maintained in parallel during training. So if you want to know more, you can look at the paper by Telemann ICML in 2008 and a, so that concludes our explanation for persistency.