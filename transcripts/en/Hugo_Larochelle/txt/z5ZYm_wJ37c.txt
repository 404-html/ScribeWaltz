Speaker 1:          00:01          In this video, we'll talk about the popular model in deep learning, the deep auto encoder. So here we're considering the case where a instead of training is single hidden there or throwing quarter, we would actually want to train a multilayer or two quarter in particular, a deal don't quarter will be interested in is in its own corner where the encoder gradually decreases the dimensionality of the input. So it extracts a 2000 hidden units, a vector representation. So 2000, and that's just an example, could have chosen other numbers, but then we will gradually decrease the representation as we move along in the computations made by Dee and quarter. And so this would allow us to reach eventually a very small dimensionality, even like a a two, a two d representation, which we could for instance, visualize, uh, uh, in, uh, as an image to do some data visualization.

Speaker 1:          01:02          And then the decoder will not just be a single linear plus nonlinearity reconstruction operation. They will actually again, gradually increased the dimentionality, uh, from the encoded representation into a 500 like here. And then a thousand like here. And then eventually 2000 like here, and then ultimately go back into the input space and do a full reconstruction of the original input here. So training such a deeper tone quarter is actually very hard. And in this case, the problem we face is not really under a, it's not really overfitting but more under fitting. It's actually a hard to reach very good training error, uh, especially if you have a large training sample. And, uh, it turns out that if we use the pre-training with unsupervised learning procedure where we really greedily trained, say a first RBM and then a second RBM and so on, uh, so actually this would be the first RBM then get another RBM here.

Speaker 1:          02:07          Now they're RBM and so on, until we reach a hidden, there are 30 hidden units. If you used this to initialize a deeper tone quarter, well actually get some, uh, quite good results. So this was a result presented by Jeff Hinton and Russell is adequate enough in science in 2006. Um, and that's somewhat surprising because I've argued thus far that pre training with unsupervised learning was useful to obtain a good regularization. So to fight over fitting and here it seems we actually fighting under fitting. So that's somewhat surprising. Um, so this perhaps means that unsupervised speech training doesn't play only a role of regularization, but perhaps it can be sometimes useful for uh, uh, improving the quality of the optimization here. It could be due to the fact that we're actually not performing classification. We all are doing unsupervised learning training. Deep auto encoder is unsupervised learning where that pretty thing a labeled.

Speaker 1:          03:02          We really reconstructing the input. So perhaps there's a relationship between unsupervised pre-training and just doing better at global joint unsupervised, uh, uh, learning. So that's perhaps part of the reason. Uh, but that being said, this is a procedure in the case of the depot tone quarter were on spy speed training actually helps fighting, uh, uh, under fitting, uh, also better optimization algorithms will help. Uh, there's this paper deep learning via Estrin free optimization by James Martin's in 2010 that, uh, show that, uh, instead of using stochastic gradient descent, if you use a more sophisticated second order optimization, uh, we could actually get really good results even without pre-training with unsupervised learning. Um, so I mentioning this because this is a model, the deeper tone color, which is often used for evaluating better, uh, optimization algorithms in the context of, uh, of neural networks in the context of deep learning.

Speaker 1:          04:07          So one application for it is, uh, to uh, reduce the dimensionality of some data set. If we wanted to, uh, for instance, uh, take these big images and actually store them in a much smaller dimensional vector so that we can actually store this on this more easily. And then if we keep the decoder around them, we can easily reconstruct the original input. Uh, and if we compare that with the, uh, type of, uh, representation would get if we did PCA and then from PC we provided a reconstruction. We see that the reconstruction, we get as much better with the deeper tone quarter then with PCA where this is the original data, uh, notice that here because we're talking about a depot in quarter. So we showed that PCA was in some sense optimal, uh, compared to, uh, training a single hidden layer, auto encoder with some nonlinearity.

Speaker 1:          04:59          So essentially we a re a PCA will do better than any auto encoder, a single hidden layer and the squared output a reconstruction error. But because here in the deep auto encoder, we have a nonlinear decoder, a then essentially the demonstration for PC being optimal does not apply for the case of a depot twin quarter. So a deeper tone color could improve over a PCA because it has a more complicated, a nonlinear decoder multilayers that sequentially reconstruct or increased the dimensionality until we reached the, uh, original input space. Okay. So this is why this result is not really a contradiction with, uh, with respect to what we've seen before in the single layer. A auto encoder case.

Speaker 2:          05:47          Okay.

Speaker 1:          05:48          And other application is just visualization. So if we make the middle hidden there, a two dimensional vector, then we can plot this by, uh, taking all of our data and, and placing a point in two d at the position given by the two dimensional vector. And so this is an example from a, again, the same science paper where, uh, uh, [inaudible] was trained on documents where a document is represented by a deer frequency of each possible word or each possible word that we can recognize in a document is a different dimension. So words are pixels. Essentially if we want to make a curse funds with images, and then the intensity of Pixel is just how many times we see the, the, the given the word, uh, in that document. Um, and uh, and we see that it seems to learn, uh, to, uh, essentially, uh, put together, uh, documents that talk about similar things.

Speaker 1:          06:48          So say disasters and accidents, legals, DCO, government borrowings, account earnings, and so on. So the color represents these different classes and we see that they are close together in this two d visualization of the original data. Okay. So that's a deeper tone quarter. It can help us get a very small dimensional representation. Um, some data that still maintains quite a bit. The original information from the, from the original data can help us with two d dimensions, uh, to the, uh, visualization of the data. And, uh, there's quite a bit of research, uh, research being done over these models because they're a good benchmark in particular for evaluating a better optimization algorithms for neural networks.