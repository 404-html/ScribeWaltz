Speaker 1:          00:00          In this video, we're going to see another type of deep neural network, uh, which is at the origin of the recent advances in, uh, training deep neural networks. Uh, this deep neural network is known as the deep belief network. So the deep belief network is I think, particularly important to, uh, look at, uh, because it's actually at the origin of the unsupervised layer wise be training procedure that we've seen for a deep feet forward, no networks. Um, and so let's look at what it is. It's going to, uh, have a see a lot of important concepts in a training deep neural networks that are probabilistic in nature. So a deep belief network or DBN is a generative model. Uh, it can be an unsupervised, entirely on supervised generative model, which is going to mix undirected and directed interactions between the variables that constitute either the input layer or visible layer, uh, or all the hidden layer.

Speaker 1:          01:06          So here we have an example of a DBM with three hidden layer. And as we see, we have undirected connections here, but we have directed connections or interactions here. So in a DBN, the top two layers will always form an RBM. So in other words, uh, in this case, the distribution over h two and h three is going to be an RBM with undirected interactions. And now the other layers are going to form a Bayesian network instead with directed interactions, specifically the conditional distributions of the units in these layers, given the layer above is going to take this form. Uh, so this is going to correspond to the probabilistic model associated with the logistic regression model. That is the probability of say, a hidden unit in the first hidden layer in this case or a visible layer, uh, visible unit. And the visible layer is going.

Speaker 1:          02:10          So the probability of either of these units to be equal to one is going to be the sigmoid applied on a linear transformation of the layer above. So for each one that's going to be a lane and transformation of h two and four x, the visible layer, it's going to be a linear transformation of a h one. So, uh, and when we have units that interact in this way, uh, we call such a model as sigma it belief networks. So that's a model that was known before dbs and a which was called a similar belief network. So to sum up in this model, the top two layers are going to form an RBM. While the bottom layers are going to form a sigma it belief network. And think about how this model works. For instance, generate data to generate data from a DBN, you will need to do gift sampling between the top two layers, uh, for a very long time. Uh, and then, uh, once you've converted it, equilibrium distribution and h two is sample from, uh, it's uh, the actual, uh, prior, uh, p of h two and h three. Then from age two, you would directly generate h one using this, the gastric process, these equations. And then from h one you would generate directly x and that would give us an observation or a sample of the input layer x, uh, from a deep belief network.

Speaker 2:          03:36          Okay.

Speaker 1:          03:36          Okay. So, so sampling from this model requires a lot of good sampling. Add the top two layers. And then once we've reached equivalent and distribution, then we can a sample down, then obtain a sample from the visible layer. So that's generative process associated with a deep belief networks. So notice that a DBF is not a feed forward network. So sometimes in the literature you see some people calling, uh, you know, stacking different rbms, uh, a deep belief networks. I prefer to, uh, and, and, and if you just do that to initialize a feed forward neural network, I would not personally called this a deep belief network. I would just call this unsupervised free trading four feet forward, no network. So a DBN is really a specific model where the hidden units are stochastic, there are random variables. They're binary and uh, they have this special structure where the top two layers are an RBM formula in RBM and the bottom layers, uh, for may submit belief network.

Speaker 1:          04:38          So more specifically then the joint distribution over the input layer and our three hidden layers for three hidden, they were a case is going to be a prior over h two and h three which is going to take this form here. So that's, we recognize here the probability distribution of a restricted Boltzmann machine where we have our weights between h two and h three and then the biases for the um, uh, two layers. And then we have this marginalization as this a normalization constant. So we have this prior and then uh, we have a times p of h one given h two.

Speaker 2:          05:16          Okay.

Speaker 1:          05:16          So that's the part of the sigma belief network that uh, corresponds did it conditional of h one given HD. So we notice here for instance, that each one is independent of h three, given h two in this model. And then we multiply by p of x given h one. So x is a sample from each one using the same weight belief network, conditional distribution we've seen in the previous slide. And so again, x is only dependent on each one. Uh, if, if we know h one, it does not depend on either h two or h three.

Speaker 2:          05:49          Okay.

Speaker 1:          05:49          Okay. And also in the, uh, as sig my belief network, uh, the full distribution of h one given each to, uh, um, in fact rises into individual conditional. So in other words, given age, do, each of the hidden units in the first hidden layer are independent. And similarly for a divisible layer given h one, if one of to look to get perhaps a more intuitive idea of how a DBM models the data. You can look at this demo here online where a deep belief networks was trained on emptiness digits. It's a special type of deep belief network where the, um, prior here actually also involves a label. Why? So in this case, instead of just being unsupervised, uh, where we only model x, we would also model a labeled y and the labeled why would be introduced by connecting it to the top most hidden, they are age three.

Speaker 1:          06:46          So you can look at that at this link here. It's pretty interesting and actually quite impressive how good the samples can be from this. Uh, DBM a model train on feminist. And, um, what makes this model interesting is that much like four deep p four node network, uh, training at DBN is a hard problem. And A, actually, it turns out that, uh, a good initialization can play a really crucial role in the quality of results we can get. And this is in fact how the, uh, greedy layer wise be training procedure was developed as a way of initializing better the parameters of a deep belief network. And then after that it was noticed that we could just use the same initialization four feet for neural networks. And that was also working really well for, uh, on, uh, initializing a feed forward neural networks. So that's why we're going to talk about deep belief networks mostly in, in this course so that we get an idea of where this came about. Uh, how this came about, this procedure of stacking restrictive bolsa machines for a pre training, a neural network.

Speaker 1:          07:57          So the idea for this initialization procedure was that, uh, in order to train, say a three hidden there DBN we would actually go from a one hidden layer DPN. So I want him later. DBN would just be an RBM because if we take the top two layers and make them an RBM that there are no other layers left to form a sigma belief networks. So this would be just the RBM part of the DBN. So that's a one hidden layer VPN and then we train that for a while and then we would use it to initialize a two hidden layer DBM where would take the parameters here, would use those to initialize the parameters here in the two hidden layer deep belief network. And then we would be left at a training this top part of the DPN, the RBM part of the two hidden layer BBN.

Speaker 1:          08:53          Um, so, uh, and by keeping these weights fixed and that's how we would sort of initialize the parameters of this layer. And then to go from a to B, a hidden layer, two or three hidden there, DBN would again use these weights to initialize that part of the three hidden there at DBN. These weights to initialize that part of the, uh, 300 layer DBN and then we would be left with a training these weights here, um, uh, to get a good initialization for these weights. And it would repeat that for a little while for as many hidden layers as well. So if there was three, would just stop there and then there would be a fine tuning procedure, which is not backpropagation. It's known as the up down Algorithm, which we won't see, but you can consult in the original publication. So that's just the high level, uh, description of the pre training procedure, uh, how it applies in the context of a deep belief network.

Speaker 1:          09:53          And, uh, before we look at the details of this procedure, I just want to give a quick intuition, uh, of, of what we're actually going to try to achieve with this procedure. So the idea is that if we've trained a one a hidden, they are [inaudible] or in other words in Rbm, then we have a model on p of x, which uh, it's just the marginalization of the hidden layer for the joint p of x and h one which is defined by an RBM. Now one thing we could do at this point is to notice that, well, okay, this p of x and h one that's really p we can write it as p of x given h one times p of h one and in an RBM, the parameters that are involved in the formula for this conditional are also involved in the prior over h one they're both present in both this conditional and the prior over h one.

Speaker 1:          10:54          And so the idea when we were passing from one hidden layer to 200 layers is to say, well, let's separate these two parts. Let's make the parameters here and the parameters here different and actually let's keep those parameters fixed to the values we found by training a one hidden layer RBM. And then that's only adapt the parameters dot are now and p of h one and four P of h one we'll use an RBM, uh, which corresponds. So this would be this part here. This would be a new RBM which with its own weights and a to get p of h one and it would need to marginalize over age two. So that here would be p of h one and but it would be when we're at the two hidden layer DBN, it would be an RBM with its own parameters that are separate from the parameters that RP and p of x given h one unlike in the RBM where the parameters between the two parts are actually tied.

Speaker 1:          11:51          And then we repeat this procedure for p h one and h two we also noticed that while Pfh one given h two that's p of h one, sorry poh one Nah Dude, that's poh one given h two times a prior of HD. If this as in this two hidden layer RBM, if the DPN, if this corresponds to an RBM, then these parameters would be tied. Well, once we fall into the three hidden layer DBN we actually untied those parameters. We're going to have different parameters here. And, uh, these are going to be different parameters which are going to occur as fun to the, um, parameters of an RBM, which has its own third hidden there and where we marginalize it out. That's how I would define pure h two in this model here. And it's going to have its own parameters that are different from those here.

Speaker 1:          12:42          Okay. So that's sort of the high level idea behind stacking rbms in the context of the belief networks. And then the following Dubios we'll look at more of the details for, uh, how we can actually perform this and guarantee that we're doing effective training. We are effectively improving the model. And so in the next video specifically, we'll look at the general tool for training, uh, graphical models with a hidden variables known as the variational bound, which is going to be a tool very useful for, uh, designing this and, and, uh, properly, uh, designing this greedy layer wise pre-training with rbms.