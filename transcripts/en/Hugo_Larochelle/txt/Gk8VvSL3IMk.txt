Speaker 1:          00:00          Okay.

Speaker 2:          00:00          In this video we'll see an example of the results one can get with a commercial loan network on an actual object recognition dataset. So the experiments we're going to look at are taken from this paper by Jared, that all in 2009, uh, there researchers at Nyu. Uh, and uh, what's really interesting is that they're, uh, they tested different variations on the commercial loan network. Uh, one thing that it tested is the actual impact of using or not using the rectification layer or using or not the contrast normalization. There

Speaker 2:          00:37          other thing that they looked at is the impact of initialization of the parameters in the convolutional neural network. Uh, so that's something we haven't discussed thus far. Um, but like in the regular kids wore a neural network, we could have initialized to parameters before training randomly. Uh, but given what we know about unsupervised speed training, that's also an approach we might want to try, uh, uh, for the conditional neural network case. Uh, now notice that in this case, the hidden units in each of the feature maps are only connected to a small receptive field within the whole image. And, uh, so now we need to initialize this sort of smaller matrix, which is not of size, size of the image, uh, height of the image by with the image. And so a procedure for performing unsupervised, uh, pre training of couple additional neural network is to generate these data sets of small patches that fit the size of these, uh, receptive fields that are hidden units are going to have.

Speaker 2:          01:42          So, uh, essentially the, uh, we need to convert pre-training into a patch patch wise learning problem. And we would do that is that we take some, uh, images and then we extract patches of the same size as the receptive fields of the hidden units in our first layer, uh, in nonconventional neural network. And we extract a bunch of patches at random positions. So we collected a big data set like this. Then we trained some unsupervised neural network and RBM and not throwing quarter or as far as the, as far as scoring model on those patches. And then we use the weights, the connections between the input patch and each hidden units to get the initial value of uh, the uh, filter or the, a matrix of connections for a different feature maps in our neural network. So the number of hidden units of say the RBM should be the same as the number of feature maps in the conventional neural network and its first hidden layer.

Speaker 2:          02:42          And then to get initialization of subsequent hidden layer, uh, hidden layers, we would need to map the images through the feature maps and a and perform pulling if want to perform bullying. And so get up to the uh, feature maps right before the next convolutional layer and then repeat this procedure of extracting patches and doing free training using St Rbm and a to get an initial value of the feature maps for parameters for that convolutional layer. Okay. So that's all we would perform on speed training in this case and also in the eye experiments from Jared at all. We'll also look at, uh, so we'll look at the impact of using a random initialization versus a using unsupervised pre training. And we'll also look at what's the impact of using some fine tuning. So performing gradient descent, uh, to train all the parameters of, uh, the neural network after initialization. So that's going to be noted with a little plus or a only training, the output layer. So without any plus in the table that we'll see a, it means that all the units, uh, the feature maps are left with their initial values of their parameters and only the output layer is being trained with it's fully connected, uh, connections. And so for random initialization, we'll have the later letter r and foreign supplies be training. We'll have the letter u and the tables.

Speaker 1:          04:13          Okay,

Speaker 2:          04:13          so here's the table summarizing all of the results that they perform on the Caltech one on one data set. Um, so when they're writing f a c g, that means the convolution layer r is for the rectification there and is for the local contrast normalization layer. P M is for the Max pooling layer and p a four, the average pooling layer. And so here we have a network with a single, uh, what they call stage. So that means a single sequence of convolutional layer, potentially some rectification, potentially some, uh, local contrast, normalization. And then finally some pooling. And then the two stage, uh, sequence two stage system has, uh, uh, twice this sort of alternation between a convolution rectification, contrast, normalization and pooling. So we have essentially a total to pooling layers. Uh, and then the last pooling layer is fed to the output layer. So that's essentially the sort of equivalent of a single layer if though it's multiple layers. But if you want to consider all of these, the sequence of the latest layers and one actual layer and we, uh, sort of a shallow version of a convolutional net. And so this would be a deeper version of it.

Speaker 2:          05:35          So if we just use convolutional with average pooling, their results are actually really, really bad. So this is, uh, accuracy. So 14% accuracy for, uh, here. Uh, so for each line we have unsupervised learning plus fine tuning, random plus fine tuning, unsupervised learning without fine tuning. So just training the top connections up, the connections and just random and we can't ignore, we won't talk about this one here or, or this one. You can look at the original paper, uh, if you want more details about these experiments. But so just conversion on average pooling roles, results are really bad. If we introduce normalization, local contrast, normalization, then we get a little bit of a boost in terms of performance. But the biggest boost comes from just adding rectification. Absolute. So computing adding a layer that computes just the absolute value of the feature. The previous layer feature maps, we see that we go from something like 15% to 50, 47% and so on.

Speaker 2:          06:37          So really much better results. Part of the reason for this is that when we computing average bullying, we're summing together, uh, all the activations, uh, within some, uh, local neighborhood. Now if we have a, um, if say a, a feature is detecting a particular edge, if we move across the neighborhood that we might actually, because the edge is sort of gonna change a polarity as we move, uh, we might actually see that the activations will cancel out if we sum them together because we'll have some negative activations canceling the positive activations just because we have, as we move in the image, a sort of a change of the polarity of have an edge. If we have an edge detector, if we take the absolute value, we don't get this problem anymore. And, and that's, uh, the, uh, suggestion, the explanation that's suggested by Jared at all for why we get a big boost here.

Speaker 2:          07:38          This is partly confirmed by the fact that if we actually use Max pooling instead, we get a actually a pretty good result with a contrast formalization much better than if we use average. Uh, pulling has we see here. So that's part of the explanation for why average pooling can really fail if we don't, uh, make sure that, uh, we don't get a cancellation. In fact, by averaging, uh, where, uh, pull the polarity changes in the, uh, contrast or the edges in the image would provoke these cancellations. And if we combine both a absolutes, a rectification, uh, with absolute value and, uh, uh, local contrast normalization that we get even better results, that's the best result with a single stage system. The picture is more or less the same for the two stage system, the sort of two hidden layer system. Uh, but the results are better.

Speaker 2:          08:33          So we get an improvement by using a deeper competitional of neural network, which again, uh, goes to say that a deep neural network tends to a, is able to extract more useful features. Um, and uh, we actually also know there's something perhaps a bit strange. Uh, so this means that convolutional layers I've been initialized. The filter maps have been initialized with random parameters and because there's no plus, it means that we're training only the output layer. So really this is using random features. They are not trained at all. The only thing that is training is the top most layer. And yet the results I actually really, really good. So that seems counter intuitive. And, uh, I believe that this was even discovered by the authors sort of by chance. They, they thought they had regularized correctly the neural network, but it turns out that, uh, was not training the other layers and yet they were getting good results. So they started investigating this, uh, particular effect here.

Speaker 2:          09:38          And so random filters are a working surprisingly good. And the reason for this is that they actually do tend to, uh, extract something like edge and formation. So if we have, so the experiment that they did is as follows, the a initialize, a layer, a convolutional layer using some random filters. And then what they did is that it took a hidden unit and they perform great in the sense on the, uh, original, uh, input, uh, so as to maximize the activity of that hidden unit. So the idea is to see what's the input pattern that activates to sit in unit as much as possible. And so for these, all of these different squares here correspond to different feature maps with different filters of parameters. And here is the associated best input pattern. And we see that for a lot of them. Say for instance, this one or this one, you actually get an input pattern, which as a particular, uh, uh, spatial orientation.

Speaker 2:          10:43          So, uh, it kind of looks like a sequence of edges in a particular orientation. They, it seems like they can actually be tuned to particular orientation of edges and for extracting and being able to detect certain objects. Uh, useful information for doing that is actually orientation of the edges. So our random initialization within the convolutional neural network, a yields these hidden units that are not doing something necessarily totally random, it's actually extracting, um, and usually get to hit. And Nene and star are tuned to particular, uh, spatial orientations of, of edges. Whereas if you learn the filters use and use an unsupervised pre-training, we get things that really looked like edge detectors and the maximum, the input pattern step, maximize the activation, uh, are not that different from the ones we get. Uh, if you use a random initialization, they're a bit more localized and it's more obvious that they're tuned to particular spacial orientation. Uh, but they're in a sense somewhat similar. So this would be the explanation for why random initialization, uh, actually works well.

Speaker 2:          11:59          And so this really shows that choosing the right architecture is really important in this specific setting. If a almost more than the learning algorithm unsupervised and utilization, it's not still the, uh, helping that much. It's helping a little bit, but not that much. Uh, but using rectification or not using it is very important. Same thing with local gunshots, normalization, which will give some boosts and actually the how important this is is depending on the how much training data, if you have very little training data per category, which is true for Caltech one on one, uh, we see this effect quite a bit, but did this experiment on the NORC data set where we have a much fewer training classes, there are actually a little toys, a film, uh, that film. But, um, uh, that, uh, for which we've taken pictures of different orientations eliminations. And, um, in this case we have many more samples per class.

Speaker 2:          12:54          And so on this data that said that were able to increase the number of training examples per class and see, uh, what's the relative performance of the different architecture choices. So this is the one that would work really badly without any the rectification or local a normalization. So that's the Blue Line here. And we see that with little training data, it does indeed perform really badly. But as we increase the amount of training data, it actually starts performing, uh, fairly similarly to say the best choice which would, which would be here, which corresponds to rectification and a normalization and using some fine tuning. So using backpropagation to train all the parameters, whereas the random purely random filters their work well initially. But eventually when you have a lot of training data that they don't perform nearly as well as the case where we actually train these filters using some backpropagation. So with the plus here, okay. So part of that, also, the random filters I worked really well is sort of an artifact of the fact that we don't have a lot of training data for each objects that we want to, we want to detect.

Speaker 1:          14:05          Okay.

Speaker 2:          14:06          All right. So, uh, for more information about these experiments are really, really enlightening and helpful. See this paper by Jared at all in 2009 so that you can see the reference on the website for this course.

Speaker 1:          14:18          Okay.