1
00:00:00,300 --> 00:00:05,300
Dr Peterson thank you for coming.
We appreciate it a lot.

2
00:00:05,660 --> 00:00:10,660
Uh,
I wanted to get your opinion on 

3
00:00:10,660 --> 00:00:11,010
censorship that we're seeing on the web.
Uh,

4
00:00:11,011 --> 00:00:13,500
it's accelerating.
You were a very notable example.

5
00:00:13,501 --> 00:00:16,500
You were locked out of your g mail in 
the interim account.

6
00:00:16,690 --> 00:00:18,770
Pardon me?
Oh yeah.

7
00:00:18,780 --> 00:00:21,510
Trump just got a deleted by an errand 
person,

8
00:00:21,511 --> 00:00:23,610
you know,
now they're saying that perhaps this was

9
00:00:23,611 --> 00:00:25,890
just a contractor and,
you know,

10
00:00:25,891 --> 00:00:26,960
maybe,
uh,

11
00:00:27,110 --> 00:00:32,110
someone from twitter who's gone into 
very far left direction a youtube has 

12
00:00:32,461 --> 00:00:34,730
gone very far left direction.
Uh,

13
00:00:34,790 --> 00:00:39,790
I'm just wondering,
I've started an alternative to youtube 

14
00:00:39,790 --> 00:00:40,800
called Pew Tube.
What kind of,

15
00:00:40,880 --> 00:00:42,150
um,
what,

16
00:00:42,230 --> 00:00:46,160
what do you see for possible solutions 
and just your thoughts in general.

17
00:00:47,430 --> 00:00:51,390
Here's a crazy thought,
but I'm going to tell it to you anyways.

18
00:00:51,690 --> 00:00:54,480
So I was just reading one of Ray 
Kurzweil's books,

19
00:00:54,481 --> 00:00:56,130
I think it was called how to make a 
mind.

20
00:00:56,190 --> 00:01:01,190
I really liked it actually.
It helped me understand how the brain 

21
00:01:01,190 --> 00:01:03,660
compresses information because the world
is really complicated,

22
00:01:03,700 --> 00:01:07,380
so you have to make a low resolution 
representations of it to live in it.

23
00:01:07,860 --> 00:01:12,860
And he actually explained to me in a way
that I hadn't really understood how the 

24
00:01:12,860 --> 00:01:13,740
brain might do that neurologically.
So that was cool.

25
00:01:13,920 --> 00:01:18,920
But you know,
Kurtzweil is this guy who thinks that 

26
00:01:18,920 --> 00:01:19,260
he's a smart guy,
very smart guy,

27
00:01:19,410 --> 00:01:24,410
and he's invented a fair bit of high end
technical technological software and 

28
00:01:26,281 --> 00:01:31,281
hardware and he's the guy that thinks 
that we're heading towards the 

29
00:01:31,281 --> 00:01:31,500
singularity.
And so the singularity is,

30
00:01:31,730 --> 00:01:36,730
you know,
how processing speed doubles every 18 

31
00:01:36,730 --> 00:01:39,981
months and like hard disk capacity every
year and there's a bunch of doublings 

32
00:01:39,981 --> 00:01:44,661
going on,
a huge number of them and they 

33
00:01:44,661 --> 00:01:44,661
accelerate exponentially.

34
00:01:44,661 --> 00:01:48,570
And so it's probably,
we're probably three years away,

35
00:01:48,630 --> 00:01:53,630
maybe even less than from building a 
computer that has the capacity to make 

36
00:01:54,271 --> 00:01:59,271
as many calculations as reasonable 
estimates of the calculating capacity of

37
00:01:59,731 --> 00:02:03,630
the human brain are currently set out.
Eighteen months away,

38
00:02:04,710 --> 00:02:06,330
two years away,
something like that.

39
00:02:08,110 --> 00:02:11,310
And then we're 18 months away from 
having one that's twice that fast.

40
00:02:11,311 --> 00:02:14,190
And then 18 months away from having one 
that's twice as fast as that.

41
00:02:14,191 --> 00:02:19,191
So that's like say six years and we've 
got something that's eight times as 

42
00:02:19,621 --> 00:02:24,621
smart as a human being,
but there's a twist on that and this is 

43
00:02:24,621 --> 00:02:28,661
Kurt swells twist,
which is as soon as you make a machine 

44
00:02:28,661 --> 00:02:28,661
smart enough to make the next machine 
that's smarter than it,

45
00:02:28,860 --> 00:02:33,860
which is sort of what we're doing 
because computers are so fast that that 

46
00:02:33,871 --> 00:02:38,871
will scale up to near infinite computing
power computing power almost 

47
00:02:38,871 --> 00:02:41,931
instantaneously now me think no,
probably not.

48
00:02:42,450 --> 00:02:47,450
And Alan Gates partner has written 
critiques of Kurtzweil and you know,

49
00:02:48,810 --> 00:02:53,810
you might think if something's 
impossible then it won't happen even if 

50
00:02:53,810 --> 00:02:53,910
you don't know why.
And there's reasons to,

51
00:02:53,970 --> 00:02:58,970
to not think that that will happen.
But Kurzweil's traced to back the of 

52
00:02:59,321 --> 00:03:04,321
computing power way before the existence
of the transistor and it's been 

53
00:03:04,321 --> 00:03:05,830
ridiculously stable,
crazily stable.

54
00:03:06,030 --> 00:03:09,300
So God only knows what we're coming up 
with here.

55
00:03:10,020 --> 00:03:15,020
You know,
and you don't know what something have 

56
00:03:15,020 --> 00:03:15,020
infinite computing power.
It might be like,

57
00:03:15,020 --> 00:03:19,600
like you seriously don't know.
And there are serious people who are 

58
00:03:19,600 --> 00:03:19,840
very,
very,

59
00:03:19,841 --> 00:03:21,850
very worried about that.
They're very worried,

60
00:03:21,851 --> 00:03:26,851
for example,
that companies like facebook and Google 

61
00:03:26,851 --> 00:03:29,521
will manage that first and you know,
those companies are already making 

62
00:03:29,521 --> 00:03:32,560
censorship,
ai bots and that's what that smart.

63
00:03:32,590 --> 00:03:35,350
It's sorta like making really fast 
robots that can shoot people.

64
00:03:35,351 --> 00:03:39,190
It's not that smart and we're doing that
to very rapidly and you know,

65
00:03:39,191 --> 00:03:41,670
I know some guys who work in advanced 
Ai,

66
00:03:41,940 --> 00:03:44,680
you know how you look,
you watch the term terminator movies and

67
00:03:44,681 --> 00:03:46,990
you see the robots that miss when they 
shoot at you,

68
00:03:47,470 --> 00:03:52,470
like they're not very bright because the
bright ones not only shoot at where you 

69
00:03:52,470 --> 00:03:56,431
are,
but they estimate where you're going to 

70
00:03:56,431 --> 00:03:58,021
be when you make your escape moves and 
they shoot their simultaneously and 

71
00:03:58,021 --> 00:03:58,021
their death rate is 100 percent.

72
00:03:58,620 --> 00:04:02,880
And so there's no war against the 
robots.

73
00:04:02,881 --> 00:04:04,170
I mean,
when those things get going,

74
00:04:04,171 --> 00:04:09,171
they're going to be so much faster than 
us that will look like we're moving 

75
00:04:09,171 --> 00:04:09,171
through molasses to them.

76
00:04:09,171 --> 00:04:09,600
So

77
00:04:13,500 --> 00:04:18,500
you know,
so maybe what we're deciding now with 

78
00:04:18,500 --> 00:04:20,481
all of our individual decisions about 
censorship and the way that we're going 

79
00:04:20,481 --> 00:04:23,321
to construct the world and all that is 
exactly what kind of super intelligence 

80
00:04:23,321 --> 00:04:25,710
we're going to bring into being.
And I would suggest that we try to bring

81
00:04:25,711 --> 00:04:29,340
one in that's good and moral rather than
one that's evil and demonic.

82
00:04:31,050 --> 00:04:35,880
Right?
So what can we do about that?

83
00:04:39,900 --> 00:04:40,350
The.

84
00:04:40,680 --> 00:04:45,680
There's only one answer to that.
As far as I know that that works is gay 

85
00:04:45,680 --> 00:04:49,671
rock together.
You're going to be the person who's 

86
00:04:49,671 --> 00:04:49,830
working in Ai,
right?

87
00:04:50,520 --> 00:04:55,520
I know some of these people,
they better be good people because 

88
00:04:55,520 --> 00:04:56,550
they're going to build whatever they're 
like into their machines.

89
00:04:57,420 --> 00:05:02,420
They better have their head screwed on 
straight because they're going to get 

90
00:05:02,420 --> 00:05:05,421
amplified like mad.
And I don't like what's happening with 

91
00:05:05,421 --> 00:05:08,841
Google and facebook and youtube.
They're building censorship bots 

92
00:05:08,841 --> 00:05:09,760
predicated on a certain kind of 
ideology.

93
00:05:09,790 --> 00:05:13,950
The ideology that we outlined today.
It's a very bad idea.

94
00:05:19,180 --> 00:05:22,790
Hopefully good people will stop that.
So then that.

95
00:05:22,870 --> 00:05:26,920
What that means is that your moral 
obligation is to be good and the way you

96
00:05:26,921 --> 00:05:31,921
do that is first by stopping being bad 
and everyone can do that a little bit.

97
00:05:33,280 --> 00:05:38,280
So I hope that's what everyone does 
because the consequences of not doing it

98
00:05:38,710 --> 00:05:40,960
are not going to be pleasant.
They never are.

99
00:05:42,580 --> 00:05:45,580
Thank you.
Hi,

100
00:05:45,990 --> 00:05:48,220
this question is about your biblical 
lecture series.

101
00:05:48,970 --> 00:05:53,970
Um,
I like that one because it's about 

102
00:05:53,970 --> 00:05:56,501
genesis,
which is usually ignored as being where 

103
00:05:56,501 --> 00:05:58,490
this post enlightenment society.
We need these ancient creation myths.

104
00:05:59,030 --> 00:06:04,030
And also I thought revelations kind of 
gets the same treatment as being 

105
00:06:04,030 --> 00:06:07,760
dismissed because it's the crazy 
hallucinogenic trip trip,

106
00:06:08,280 --> 00:06:10,700
some isolated madman in the middle of 
the Mediterranean.

107
00:06:11,360 --> 00:06:13,520
So I was wondering if after you're done 
with genesis,

108
00:06:13,521 --> 00:06:15,440
if you were thinking about doing 
revelation's,

109
00:06:15,740 --> 00:06:19,670
not without traversing the Geo geography
in between.

110
00:06:20,150 --> 00:06:21,710
Okay.
Yeah.

111
00:06:21,711 --> 00:06:24,890
I want to walk through the whole thing 
if I can do that before they expire.

112
00:06:27,290 --> 00:06:31,220
So I mean I've read it and thought about
it and it like,

113
00:06:31,370 --> 00:06:36,370
it's such a strange book,
a because it's really big among the 

114
00:06:36,370 --> 00:06:37,580
evangelical Republican types.
And you'd think really,

115
00:06:38,210 --> 00:06:40,240
really,
that's the book that's.

116
00:06:40,300 --> 00:06:42,380
But you're relying.
Have you read it?

117
00:06:43,100 --> 00:06:43,910
It's,
it's,

118
00:06:44,380 --> 00:06:47,270
it's,
it's a crazy hallucinogenic trip.

119
00:06:47,271 --> 00:06:49,070
That's what revelation is.
Now.

120
00:06:49,130 --> 00:06:54,130
That's not to play it down because God 
only knows about crazy hallucinogenic 

121
00:06:55,161 --> 00:06:57,080
trips,
that's for sure.

122
00:06:57,170 --> 00:06:59,840
I mean there's,
there's accruing evidence.

123
00:06:59,841 --> 00:07:04,841
I would say that a tremendous amount of 
the religious orientation of human 

124
00:07:04,841 --> 00:07:06,770
beings,
deep mythological,

125
00:07:06,771 --> 00:07:11,771
symbolic orientation is in no small part
a consequence of humanities 

126
00:07:11,771 --> 00:07:13,400
experimentation with psychedelics 
substances.

127
00:07:13,910 --> 00:07:18,910
I think that's,
that the evidence for that I think has 

128
00:07:18,910 --> 00:07:21,521
become virtually an overwhelming.
So anyways,

129
00:07:22,521 --> 00:07:24,130
I will get there.
Maybe.

130
00:07:24,290 --> 00:07:27,740
Probably not because at the rate I'm 
going through the first stories,

131
00:07:27,741 --> 00:07:30,740
it'll take me forever to get there,
but that's okay.

132
00:07:31,190 --> 00:07:32,000
So.
All right,

133
00:07:32,210 --> 00:07:32,740
thank you.
Yep.

