1
00:00:00,520 --> 00:00:04,300
Big Data is not an unmitigated good like
many things in society,

2
00:00:04,301 --> 00:00:05,680
in fact,
probably all things,

3
00:00:05,860 --> 00:00:09,610
it comes with risks as well and it comes
with a dark side and one dark side of

4
00:00:09,611 --> 00:00:13,720
course is privacy that exists today.
It'll exist tomorrow.

5
00:00:13,780 --> 00:00:15,760
Maybe it gets bigger with big data as
well,

6
00:00:16,150 --> 00:00:19,000
but there is something else to play for
something else that's a little bit more

7
00:00:19,001 --> 00:00:22,770
troubling still and that is if you will
propensity it,

8
00:00:22,771 --> 00:00:26,680
is big data algorithms making a
prediction of what you are likely to do

9
00:00:26,830 --> 00:00:30,490
before you've actually done it.
Now the criminal justice system has

10
00:00:30,491 --> 00:00:33,190
never really dealt with this sort of
problem before.

11
00:00:33,400 --> 00:00:37,630
Typically you have to commit a crime
before you were penalized for that

12
00:00:37,631 --> 00:00:40,690
crime,
but what if it is simply a prediction

13
00:00:40,691 --> 00:00:42,730
that you have a likelihood of committing
a crime?

14
00:00:43,210 --> 00:00:46,330
Would society be remiss not to
intervene?

15
00:00:46,330 --> 00:00:51,330
If I could tell with a 98 percent
statistical accuracy that you are likely

16
00:00:51,430 --> 00:00:56,430
to shoplift in the next 12 months?
Public Safety requires that I interact

17
00:00:57,221 --> 00:01:00,190
and maybe I don't put you into jail.
It's not minority report.

18
00:01:00,191 --> 00:01:03,490
It's not pre crime.
I have a social worker knock on your

19
00:01:03,491 --> 00:01:05,830
door and say,
we'd like to help you.

20
00:01:05,831 --> 00:01:09,040
We'd like to get you an afterschool job.
If you're a teenager,

21
00:01:09,160 --> 00:01:11,350
we'd like to sort of support you.
Well,

22
00:01:11,590 --> 00:01:13,990
that sounds like it's a benefit,
but in reality,

23
00:01:13,991 --> 00:01:16,660
if you think about it,
this person is going to be stigmatized

24
00:01:16,661 --> 00:01:20,650
in the eye of his peers,
school teachers,

25
00:01:20,651 --> 00:01:21,730
parents,
in fact,

26
00:01:21,731 --> 00:01:25,000
he'll probably feel stigmatized in his
own eyes and feel badly,

27
00:01:25,001 --> 00:01:28,210
and we might even encourage the tort
sort of behavior that we want to

28
00:01:28,211 --> 00:01:31,240
prevent.
The point is that he will have been a

29
00:01:31,241 --> 00:01:34,660
victim of a prediction about him and he
could rightly say,

30
00:01:35,140 --> 00:01:39,640
I will be the two percent that will not
a shoplift that I'll exercise my moral

31
00:01:39,641 --> 00:01:42,460
choice.
So the solution seems to be in a big

32
00:01:42,461 --> 00:01:45,640
data world.
We want to send how sanctify the notion

33
00:01:45,641 --> 00:01:50,641
of the human volition of human free will
and to preserve that as a central

34
00:01:50,921 --> 00:01:51,490
attribute.

