1
00:00:05,510 --> 00:00:09,710
Hollywood movies make people worry about
the wrong things in terms of

2
00:00:09,711 --> 00:00:11,850
superintelligence,
well,

3
00:00:11,860 --> 00:00:14,240
we should really worry about it.
It's not malice,

4
00:00:14,720 --> 00:00:18,770
but competence
where we have machines that are smarter

5
00:00:18,771 --> 00:00:22,010
than us,
whose goals just aren't aligned with

6
00:00:22,011 --> 00:00:26,520
ours for example.
I don't hate that.

7
00:00:26,521 --> 00:00:30,590
So I don't go out of my way to stop
button and if I see one on the sidewalk,

8
00:00:30,920 --> 00:00:34,370
but if I'm in charge of this
hydroelectric dam construction and just

9
00:00:34,371 --> 00:00:36,890
as I'm going to flood this value of
water,

10
00:00:36,891 --> 00:00:39,470
I see an ant until very no tough luck
for the ads,

11
00:00:39,471 --> 00:00:41,510
right?
Their goals weren't aligned with mine

12
00:00:41,840 --> 00:00:44,780
and because I'm smarter,
it's going to be my goals,

13
00:00:44,870 --> 00:00:49,610
not the ants goals that get fulfilled.
We never want to put humanity and the

14
00:00:49,611 --> 00:00:52,730
role of those ads.
On the other hand,

15
00:00:52,731 --> 00:00:54,830
it doesn't have to be bad if you solve
the goal.

16
00:00:54,830 --> 00:00:58,910
Layman problem.
My little babies tend to be in a

17
00:00:58,911 --> 00:01:03,320
household surrounded by human level.
Intelligence is the smarter than the

18
00:01:03,321 --> 00:01:05,150
babies and they leave their parents
right.

19
00:01:05,390 --> 00:01:09,650
And that works out fine because the
goals of the parents are wonderfully

20
00:01:09,651 --> 00:01:13,190
align with the goals of the child.
So the chaplain just all good.

21
00:01:13,820 --> 00:01:16,520
And this is one vision that a lot of ai
researchers have.

22
00:01:16,840 --> 00:01:20,810
The friendly ai vision that we will
succeed and not just making machines

23
00:01:20,811 --> 00:01:25,310
that are smarter than us,
but also machines that then learn,

24
00:01:25,550 --> 00:01:28,250
adopt,
then retain our goals as they get ever

25
00:01:28,251 --> 00:01:31,400
smarter.
It might sound easy to get machines to

26
00:01:32,990 --> 00:01:37,990
learn and adopt and retain our goals,
but these are all very tough problems.

27
00:01:38,360 --> 00:01:41,540
First of all,
if you take yourself driving taxi and

28
00:01:41,541 --> 00:01:44,780
tell it in the future to take you to the
airport as fast as possible and then you

29
00:01:44,781 --> 00:01:47,390
get there covered in vomit and chased by
helicopters and you said,

30
00:01:47,570 --> 00:01:47,841
no,
no,

31
00:01:47,841 --> 00:01:48,410
no,
no,

32
00:01:48,410 --> 00:01:49,610
that's not what I wanted.

33
00:01:50,030 --> 00:01:54,470
And it replies.
That is exactly what you asked for.

34
00:01:55,040 --> 00:01:59,270
Venue have appreciated how hard it is to
get a machine to understand your goals,

35
00:01:59,271 --> 00:02:00,650
your actual goals.
You know,

36
00:02:00,651 --> 00:02:05,150
a human cab driver would have realized
that you also had other goals that were

37
00:02:05,151 --> 00:02:09,200
unstated because she was also a human
and has all this shared reference frame.

38
00:02:09,440 --> 00:02:13,060
But a machine doesn't have that unless
we explicitly teach it that.

39
00:02:13,070 --> 00:02:15,740
Right.
And then once the machine understands

40
00:02:15,741 --> 00:02:17,690
our goals,
there's a separate problem getting them

41
00:02:17,691 --> 00:02:21,850
to re adopt the goals.
Anyone who has had kids knows how hard

42
00:02:21,851 --> 00:02:24,860
or how big the difference is between
making the kids understand what you want

43
00:02:25,100 --> 00:02:29,300
and actually adopt your goal to do what
you want and,

44
00:02:29,301 --> 00:02:31,520
and finally,
even if you can get your kids to adopt

45
00:02:31,521 --> 00:02:32,780
your goals,
that doesn't mean they're going to

46
00:02:32,781 --> 00:02:34,130
retain them for life.
Right?

47
00:02:34,400 --> 00:02:38,660
My kids are a lot less excited about
lego now than they were when they were

48
00:02:38,661 --> 00:02:43,370
little and we don't want machines as
they ever get ever smarter to gradually

49
00:02:43,371 --> 00:02:47,030
change their goals away from being
excited about protecting us and thinking

50
00:02:47,031 --> 00:02:51,800
of this thing about taking care of
humanity is just a little childhood

51
00:02:53,060 --> 00:02:55,190
thing like Lego is that they get bored
with.

52
00:02:55,190 --> 00:02:58,280
Eventually.
If we can solve all three of these

53
00:02:58,281 --> 00:02:59,440
quests,
tech challenges,

54
00:02:59,950 --> 00:03:01,690
getting machines to understand our
goals,

55
00:03:01,810 --> 00:03:03,280
adopt the man,
retain them,

56
00:03:03,520 --> 00:03:07,210
then we can create an awesome future
because everything I love about

57
00:03:07,570 --> 00:03:09,610
civilization as the product of
intelligence.

58
00:03:10,420 --> 00:03:13,210
Then if we can use machines to amplify
our intelligence,

59
00:03:14,050 --> 00:03:18,340
then we used to have this potential to
solve all the problems that are dumping

60
00:03:18,341 --> 00:03:22,750
us today and create a better future than
we even dare to dream of.

61
00:03:23,320 --> 00:03:27,190
If machines ever surpass us and can
outsmart us at all tasks,

62
00:03:27,340 --> 00:03:32,170
that's going to be a really big deal
because intelligence is power.

63
00:03:32,410 --> 00:03:36,730
The reason that we humans have more
power on this planet than tigers is not

64
00:03:36,731 --> 00:03:40,710
because we have larger muscles are
sharper clause,

65
00:03:40,711 --> 00:03:42,160
right?
It's because we're smarter than the

66
00:03:42,161 --> 00:03:46,450
Tigers and in this exact same way,
it's machines are smarter than us.

67
00:03:47,950 --> 00:03:52,950
It becomes perfectly plausible for them
to control us and become the rulers of

68
00:03:53,410 --> 00:03:55,210
this planet and beyond.

69
00:03:55,990 --> 00:03:59,830
And um,
when I j good made this famous analysis

70
00:03:59,831 --> 00:04:02,230
of how you could get an intelligence
explosion,

71
00:04:02,390 --> 00:04:06,310
we're intelligence just kept creating
greater and greater intelligence leaving

72
00:04:06,311 --> 00:04:09,880
us far behind.
He also mentioned that this super

73
00:04:09,881 --> 00:04:14,410
intelligence would be the last invention
that men need ever make.

74
00:04:14,830 --> 00:04:16,660
And what he meant by that,
of course,

75
00:04:16,661 --> 00:04:20,170
was it so far,
the most intelligent being on this

76
00:04:20,171 --> 00:04:21,820
planet that's been doing all the
inventing,

77
00:04:22,120 --> 00:04:25,870
it's been us,
but once we make machines that are

78
00:04:25,871 --> 00:04:30,871
better than us at the inventing all
future technology that we ever need can

79
00:04:31,751 --> 00:04:36,160
be created by those machines.
If we can make sure that they do things

80
00:04:36,190 --> 00:04:41,190
for us that we want and to help us
create an awesome future where humanity

81
00:04:42,251 --> 00:04:43,720
can flourish like never before.

82
00:04:44,390 --> 00:04:44,980
Mm.

