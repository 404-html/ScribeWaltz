1
00:00:06,710 --> 00:00:07,543
Attended

2
00:00:08,660 --> 00:00:13,660
about a month ago,
a meeting in Japan in Kyoto organized by

3
00:00:15,800 --> 00:00:20,800
a remarkable individual named a Koji Omi
who was among other things or finance

4
00:00:21,771 --> 00:00:25,880
minister of Japan for awhile and I only
saw one,

5
00:00:25,910 --> 00:00:30,380
has been holding this annual conference
on science,

6
00:00:30,890 --> 00:00:35,360
technology and society and he calls it
light and shadow,

7
00:00:36,470 --> 00:00:41,270
light and shadow,
a very Japanese way of saying science

8
00:00:41,271 --> 00:00:46,271
can shed light to extraordinary positive
things that it also has a dark side.

9
00:00:47,271 --> 00:00:52,271
It creates shadows.
This is true of anything economic power,

10
00:00:53,750 --> 00:00:58,160
military power and scientific power have
a bright side.

11
00:00:58,190 --> 00:01:02,090
They have a dark side.
We must be very aware of that.

12
00:01:03,260 --> 00:01:08,260
I'm a great believer that
we should have full freedom as

13
00:01:09,741 --> 00:01:14,741
conceivable for the human mind to
explore nature and understand.

14
00:01:16,520 --> 00:01:21,520
However,
we have to be aware of the consequences

15
00:01:21,770 --> 00:01:26,770
of some of the kinds of knowledge that
we are beginning to develop and I

16
00:01:27,711 --> 00:01:31,700
suspect that over this coming decade or
so,

17
00:01:32,540 --> 00:01:36,950
that we're going to face a lot of very
deeply ethical questions.

18
00:01:37,040 --> 00:01:42,040
As life science becomes more and more
the basis of technology and action.

19
00:01:45,290 --> 00:01:48,470
We should never be afraid of learning of
discovery,

20
00:01:49,370 --> 00:01:54,370
but we need to be cautious as we move
into new technological areas because

21
00:01:56,691 --> 00:01:59,270
they move so fast,
you know,

22
00:01:59,271 --> 00:02:02,270
in the old days,
you could generally develop technology.

23
00:02:02,271 --> 00:02:04,030
It took a lifetime,
uh,

24
00:02:04,390 --> 00:02:07,070
you know,
for the automobile to reach 25 percent

25
00:02:07,071 --> 00:02:09,950
of the public,
whereas the worldwide web did it and

26
00:02:09,980 --> 00:02:12,230
seven and a half years.
So this,

27
00:02:12,280 --> 00:02:17,090
this pace
doesn't always give us the time to think

28
00:02:17,091 --> 00:02:21,410
through before we moved.
And we saw that in genetically modified

29
00:02:21,411 --> 00:02:26,411
foods which created a cultural figure in
Europe that the people who were doing

30
00:02:27,501 --> 00:02:31,520
the original marketing really hadn't
stepped back and thought through.

31
00:02:32,420 --> 00:02:37,420
So the two areas that I think we're
going to have to think deeply about are

32
00:02:38,690 --> 00:02:40,280
certainly,
uh,

33
00:02:40,350 --> 00:02:45,350
the whole world that's beginning to
evolve of synthetic biology and the

34
00:02:46,131 --> 00:02:47,870
increasing genetic knowledge.

35
00:02:47,870 --> 00:02:50,150
We're going to have a very selves and
others.

36
00:02:50,840 --> 00:02:53,240
What really is going to constitute
wisdom?

37
00:02:53,241 --> 00:02:57,710
How are we going to decide what one
wants to know?

38
00:02:58,430 --> 00:03:01,210
Uh,
how do we start thinking about things

39
00:03:01,211 --> 00:03:04,870
when we begin creating life forms which
were doing,

40
00:03:05,350 --> 00:03:08,410
you know,
you walk up and down the halls of a

41
00:03:08,411 --> 00:03:12,190
place like mit,
hear the kids talking about biohacking.

42
00:03:12,820 --> 00:03:14,080
That means,
you know,

43
00:03:14,081 --> 00:03:17,930
we're taking organisms,
we're taking the stuff of life and we're

44
00:03:17,950 --> 00:03:21,660
mixing it up and playing around.
We're not creating monsters when we're

45
00:03:21,670 --> 00:03:26,670
doing things at a molecular level that
we have to think through what's going to

46
00:03:26,921 --> 00:03:30,640
happen.
The whole field of genetic counseling,

47
00:03:30,641 --> 00:03:32,050
what do you want to know?
Should you know,

48
00:03:32,051 --> 00:03:36,930
should you know that you've got an very
high probability of having Alzheimer's,

49
00:03:36,970 --> 00:03:37,930
uh,
later on.

50
00:03:38,320 --> 00:03:43,320
So I think most of the areas we have to
think most deeply about are going to be

51
00:03:43,451 --> 00:03:48,451
driven by the infusion of life science
into things that directly affect us into

52
00:03:48,641 --> 00:03:51,820
medicine,
into the production and materials and so

53
00:03:51,821 --> 00:03:52,654
forth.

54
00:03:52,840 --> 00:03:56,140
At the same time,
I don't want to slow that because I

55
00:03:56,141 --> 00:03:59,590
believe within,
it belies a lot of the resolution of

56
00:03:59,980 --> 00:04:04,980
environmental problems and so forth
because we learned from nature to design

57
00:04:05,471 --> 00:04:09,670
and grow as opposed to physically
manufacture materials.

58
00:04:09,671 --> 00:04:12,610
We can do it generally with a lot less
energy,

59
00:04:12,611 --> 00:04:15,340
with more uses of natural materials and
so forth,

60
00:04:15,700 --> 00:04:20,700
but we do have to think that through a
more understandable example because

61
00:04:21,251 --> 00:04:25,600
we're in some ways further down the path
is people will have some legitimate

62
00:04:25,601 --> 00:04:30,601
concerns about nanotechnology and it's a
complicated area because a nano just

63
00:04:32,831 --> 00:04:37,831
really refers to building things out of
extremely small molecules and particles

64
00:04:39,450 --> 00:04:41,680
and we've been doing this forever on one
hand,

65
00:04:42,040 --> 00:04:44,050
but on the other hand,
we're starting to,

66
00:04:44,350 --> 00:04:47,110
uh,
take metals and various materials and

67
00:04:47,111 --> 00:04:51,880
put them in this very small form that
can enter directly into cells,

68
00:04:51,881 --> 00:04:53,710
can be breathed in,
in different ways.

69
00:04:54,100 --> 00:04:58,630
Chances are 99 percent of it is not
going to be dangerous,

70
00:04:59,020 --> 00:05:03,850
but we have to be willing to kind of
make the investments as we go along to

71
00:05:03,851 --> 00:05:08,851
engage bright people who by the way
cannot all be professional scientists

72
00:05:09,311 --> 00:05:13,060
and engineers.
We need lay people and thinkers engaged

73
00:05:13,660 --> 00:05:16,090
and just think airway through some of
these issues.

74
00:05:16,660 --> 00:05:18,730
At the same time,
you know,

75
00:05:18,760 --> 00:05:21,790
I believe in boldness and I believe in
taking risks.

76
00:05:22,380 --> 00:05:26,170
It's just that we don't want to take
risks on scales and with people who

77
00:05:26,171 --> 00:05:28,450
don't know they're taking risk and so
forth.

78
00:05:28,720 --> 00:05:33,700
We just need deeper thought and these
newer areas because they're moving so

79
00:05:33,701 --> 00:05:38,701
rapidly and I think we're going to face
some really tough ethical decisions in

80
00:05:39,310 --> 00:05:43,900
these areas and they're not gonna all be
easily resolved and going back to

81
00:05:43,901 --> 00:05:48,550
something you asked about earlier,
it can't just be science and

82
00:05:48,551 --> 00:05:49,970
engineering.
You know it.

83
00:05:50,510 --> 00:05:54,220
We live in a democracy.
We have political processes for making

84
00:05:54,221 --> 00:05:58,730
decisions.
We just want those decisions to be truly

85
00:05:58,760 --> 00:06:03,760
well informed and questions shaped in
ways that really make sense and are

86
00:06:05,751 --> 00:06:06,320
appropriate.

