1
00:00:03,540 --> 00:00:07,830
If you think much about physics and
cognition and intelligence,

2
00:00:08,100 --> 00:00:12,930
it's pretty obvious the human mind is
not the smartest possible general

3
00:00:12,931 --> 00:00:16,980
intelligence and you more than humans
are the highest jumpers are the fastest

4
00:00:16,981 --> 00:00:19,860
runners.
We're not going to be the smartest

5
00:00:20,070 --> 00:00:25,070
thinkers if you are going to work toward
agi rather than focusing on some narrow

6
00:00:26,911 --> 00:00:30,960
application.
There's a number of different approaches

7
00:00:30,961 --> 00:00:35,961
that you might take and I've spent some
time just surveying the Agi field as a

8
00:00:37,321 --> 00:00:40,380
whole and organizing an annual
conference on their gi.

9
00:00:40,680 --> 00:00:45,680
And then I've spent a bunch of more time
on a specific agi approach,

10
00:00:46,560 --> 00:00:51,560
which is based on the open cog open
source software platform in the big

11
00:00:52,201 --> 00:00:55,170
picture.
One way to approach Agi is to try to

12
00:00:55,171 --> 00:00:59,070
emulate the human brain at some level of
precision,

13
00:00:59,310 --> 00:01:01,950
and this is the approach I see.
For example,

14
00:01:01,951 --> 00:01:03,630
Google deep mind taking.

15
00:01:03,960 --> 00:01:08,580
They've taken deep neural networks,
which in their common form are mostly a

16
00:01:08,581 --> 00:01:13,410
model of visual and auditory processing
in the human brain.

17
00:01:13,740 --> 00:01:18,740
And now in the recent work such as the
DNC differential neuro computer,

18
00:01:19,320 --> 00:01:24,320
they're taking these deep networks that
model visual or auditory processing and

19
00:01:24,691 --> 00:01:28,230
their coupling that with a memory matrix
which models some aspect of what the

20
00:01:28,231 --> 00:01:31,230
hippocampus does,
which is the part of the brain that

21
00:01:31,231 --> 00:01:35,190
deals with working memory,
short term memory among other things.

22
00:01:35,310 --> 00:01:40,310
So this illustrates an approach where
you take neural networks emulating

23
00:01:40,381 --> 00:01:43,830
different parts of the brain and maybe
you take more and more in the neural

24
00:01:43,831 --> 00:01:46,380
networks and building different parts of
the human brain.

25
00:01:46,620 --> 00:01:49,380
You try to get them to all work
together,

26
00:01:49,560 --> 00:01:52,500
not necessarily doing computational
neuroscience,

27
00:01:52,710 --> 00:01:57,480
but trying to emulate the way different
parts of the brain are doing processing

28
00:01:57,481 --> 00:01:59,130
and the way they're talking to each
other.

29
00:01:59,590 --> 00:02:04,590
A totally different approach is being
taken by a guy named Marcus hooter in

30
00:02:05,160 --> 00:02:10,160
Australian National University.
He wrote a beautiful book on Universal

31
00:02:10,230 --> 00:02:14,730
Ai in which he showed how to write a
superhuman infinitely intelligence

32
00:02:14,731 --> 00:02:17,130
thinking machine and like 50 lines of
code.

33
00:02:17,970 --> 00:02:22,710
The problem is it would take more
computing power than there is in the

34
00:02:22,711 --> 00:02:24,780
entire universe to run.
So it's so,

35
00:02:24,781 --> 00:02:28,680
it's not practically useful,
but they're then trying to scale down

36
00:02:28,681 --> 00:02:33,681
from this theoretical Agi to find
something that that will really work.

37
00:02:34,830 --> 00:02:39,830
Now the approach we're taking in the
open cog project is different than

38
00:02:40,470 --> 00:02:45,120
either of those we're attempting to
emulate at a very high level.

39
00:02:45,660 --> 00:02:50,130
The way the human mind seems to work as
an embodied social,

40
00:02:50,430 --> 00:02:55,380
generally intelligent agent,
which is coming to grips with hard

41
00:02:55,381 --> 00:02:59,680
problems in the context of coming to
grips with itself and its and its life

42
00:02:59,681 --> 00:03:00,514
in the world.

43
00:03:01,120 --> 00:03:05,760
We're not drawing the model the way the
brain works at the level of neurons and

44
00:03:05,761 --> 00:03:09,640
neural networks.
We're looking at the human mind more

45
00:03:09,641 --> 00:03:11,470
from a high level cognitive point of
view.

46
00:03:11,471 --> 00:03:13,630
What kinds of of memory are there?
Well,

47
00:03:13,631 --> 00:03:18,631
there's semantic memory about abstract
knowledge or concrete facts.

48
00:03:18,971 --> 00:03:23,080
There's episodic memory of our other
biographical history,

49
00:03:23,081 --> 00:03:27,400
their sensory motor memory.
There's associative memory of things

50
00:03:27,401 --> 00:03:29,770
that have been related to us in our
lives.

51
00:03:30,010 --> 00:03:34,390
There's procedural memory of how to do
things and we then look at the different

52
00:03:34,391 --> 00:03:39,190
kinds of learning and reasoning the
human mind can do.

53
00:03:39,191 --> 00:03:43,300
We can do logical deduction sometimes.
We're not always good at it.

54
00:03:43,330 --> 00:03:46,900
We date,
we make emotional intuitive leaps and

55
00:03:47,110 --> 00:03:52,110
strange creative combinations of things
we learned by trial and error and habit.

56
00:03:52,690 --> 00:03:55,540
We learned socially by imitating,
mirroring,

57
00:03:55,570 --> 00:04:00,250
emulating or opposing others,
these different kinds of memory and

58
00:04:00,251 --> 00:04:02,710
learning that the human mind has.

59
00:04:03,160 --> 00:04:08,050
One can attempt to achieve each of those
where the cutting edge computer science

60
00:04:08,051 --> 00:04:13,051
algorithm rather than trying to achieve
each of those functions and structures

61
00:04:14,530 --> 00:04:17,920
and the way the brain does.
So what we have in open cog,

62
00:04:17,950 --> 00:04:22,950
we have a central knowledge repository,
which is very dynamic and lives in ram

63
00:04:25,241 --> 00:04:29,020
on a large network of computers,
which we call the atom space.

64
00:04:29,050 --> 00:04:33,100
And for,
for the mathematicians or computer

65
00:04:33,101 --> 00:04:37,570
science in the audience,
the Adams space is what you'd call a,

66
00:04:37,870 --> 00:04:39,970
uh,
weighted labeled hypergraph.

67
00:04:40,090 --> 00:04:42,730
So it has nodes,
it has links.

68
00:04:42,970 --> 00:04:46,420
A link can go between two nodes or a
link ego between three,

69
00:04:46,421 --> 00:04:48,220
four,
five or 50 nodes.

70
00:04:49,120 --> 00:04:53,950
Different nodes and links have different
types and the nodes and links can have

71
00:04:54,040 --> 00:04:57,730
numbers attached to them and know your
link could have a weight indicating a

72
00:04:57,731 --> 00:05:01,510
probability or a confidence.
It could have a weight indicating how

73
00:05:01,511 --> 00:05:06,300
important it is to the system right now
or how important it is in the longterm,

74
00:05:06,460 --> 00:05:11,460
so it should be kept around and the
systems memory on this Adam space this

75
00:05:12,780 --> 00:05:15,130
way did labeled hypergraph.

76
00:05:15,460 --> 00:05:20,460
We can have a lot of different ai
processes working together

77
00:05:20,620 --> 00:05:23,890
cooperatively,
so the the atom space,

78
00:05:24,130 --> 00:05:27,970
the memory stores,
what we would call neural symbolic.

79
00:05:28,390 --> 00:05:33,370
That means we can represent nodes and
links that are like neurons in the

80
00:05:33,371 --> 00:05:35,800
brain,
which is fairly low level,

81
00:05:36,010 --> 00:05:41,010
but we can also represent nodes and
links that are higher level representing

82
00:05:41,141 --> 00:05:46,141
pieces of of symbolic logic expressions.
So we can do explicit logical reasoning,

83
00:05:47,110 --> 00:05:50,770
which is pretty abstract and low level
neural net stuff.

84
00:05:51,070 --> 00:05:56,070
In the same hypergraph the same items
space acting on this Adam space.

85
00:05:56,930 --> 00:06:01,820
We have deep neural networks for visual
and auditory perception.

86
00:06:02,420 --> 00:06:06,710
We have a probabilistic logic engine
which does abstract reasoning.

87
00:06:07,370 --> 00:06:11,470
We have an evolutionary learning
algorithm that uses genetic algorithm

88
00:06:11,480 --> 00:06:16,480
type methods to try to evolve radical
new new ideas and concepts and look for

89
00:06:17,151 --> 00:06:22,151
data patterns and we have a neural net
type dynamic that spreads activity and

90
00:06:23,331 --> 00:06:25,790
importance throughout the network.

91
00:06:25,910 --> 00:06:29,100
A few other algorithms,
a pattern mining algorithm that just

92
00:06:29,510 --> 00:06:34,280
scans through the whole admin space
looking for surprising stuff and the

93
00:06:34,281 --> 00:06:39,281
trick is all these different cognitive
algorithms have to work together

94
00:06:40,640 --> 00:06:44,360
cooperatively to help each other rather
than hurt each other.

95
00:06:44,540 --> 00:06:49,460
See the bottleneck in essentially every
ai approach ever taken,

96
00:06:49,910 --> 00:06:52,160
be it a neural net,
a logic engine,

97
00:06:52,161 --> 00:06:56,180
the genetic algorithm,
whatever the bottleneck in every ai

98
00:06:56,181 --> 00:07:00,380
approach ever taken has been what we
call common tutorial explosion.

99
00:07:00,710 --> 00:07:03,920
And what that means is you have a lot of
data items,

100
00:07:03,921 --> 00:07:07,910
you're a lot of perceptions coming into
your eye or you have a lot of possible

101
00:07:07,911 --> 00:07:12,350
moves on the chessboard or a lot of
possible ways to move the wheel of the

102
00:07:12,351 --> 00:07:17,351
car and there's so many combinations of
possible data items and possible things

103
00:07:18,501 --> 00:07:21,920
you could do.
Sifting through all those combinations

104
00:07:22,550 --> 00:07:24,980
becomes an exponential problem.

105
00:07:25,040 --> 00:07:27,410
I mean if,
if you have a thousand things,

106
00:07:27,680 --> 00:07:31,730
there's two to the 1020 a combined them
and that and that's the way to money.

107
00:07:31,820 --> 00:07:36,820
So how the sift through common tutorial
explosions is the core problem everyone

108
00:07:37,341 --> 00:07:42,341
has to deal with in a deep neural
network as currently pursued it solved

109
00:07:43,251 --> 00:07:48,251
by making the network have a very
specific structure which reflects the

110
00:07:48,321 --> 00:07:51,140
structure of visual and auditory
streams.

111
00:07:51,440 --> 00:07:56,150
And in a logic engine you don't have
that sort of luxury because a logic

112
00:07:56,151 --> 00:07:57,770
engine has to deal with anything.
Not,

113
00:07:57,771 --> 00:08:02,771
not just sensory data,
but what we do in open cog is we've

114
00:08:03,111 --> 00:08:08,111
worked out a system where each of the
cognitive processes can help the other

115
00:08:08,451 --> 00:08:11,810
one out when it,
when it gets stuck in some communist

116
00:08:11,811 --> 00:08:15,500
oriel explosion problems.
So if if a deep neural network turning

117
00:08:15,501 --> 00:08:18,800
the perceived things gets confused
because it's dark or it's looking at

118
00:08:18,801 --> 00:08:22,640
something it never saw before,
well maybe the reasoning engine can come

119
00:08:22,641 --> 00:08:25,580
in and do some inference to cut through
that confusion.

120
00:08:25,940 --> 00:08:30,710
If logical reasoning is getting confused
and doesn't know what step to take next

121
00:08:30,711 --> 00:08:33,470
cause there's just so many possibilities
are there.

122
00:08:33,471 --> 00:08:38,180
And not much information about the mall.
Maybe you fish into your sensory motor

123
00:08:38,181 --> 00:08:42,440
memory and you use deep learning to
visualize something you saw before.

124
00:08:42,680 --> 00:08:47,230
And that gives you a clue of how to pair
through the many possibilities that the,

125
00:08:47,231 --> 00:08:52,231
that the logic engine is seeing.
Now you can model this kind of cognitive

126
00:08:53,421 --> 00:08:58,421
synergy mathematically using a branch of
mathematics called category theory,

127
00:08:59,220 --> 00:09:01,560
which is something I've been working on
lately.

128
00:09:02,100 --> 00:09:07,100
But what's really interesting more so is
to build a system that manifests this

129
00:09:08,671 --> 00:09:12,570
and achieves general intelligence as a
result.

130
00:09:12,571 --> 00:09:15,030
And that's what we're doing in the open
card project.

131
00:09:15,031 --> 00:09:17,790
We're not there yet to general
intelligence,

132
00:09:17,791 --> 00:09:22,791
but we're getting there step by step.
We're using our open source open cog

133
00:09:23,371 --> 00:09:27,480
platform to control David Hanson's
beautiful,

134
00:09:27,540 --> 00:09:31,620
incredibly realistic humanoid robots
like this Sophia robot,

135
00:09:31,650 --> 00:09:35,220
which has gotten a lot of media
attention in the last year.

136
00:09:35,580 --> 00:09:40,580
We're using open card to analyze
biological data related to the genetics

137
00:09:40,741 --> 00:09:42,810
of,
of longevity.

138
00:09:42,811 --> 00:09:46,680
And we're doing a host of other
consulting projects using this.

139
00:09:46,681 --> 00:09:49,770
So we're,
we're proceeding on an r and d track and

140
00:09:49,771 --> 00:09:52,380
an application track at at the same
time.

141
00:09:52,800 --> 00:09:57,800
But our end goal with the system is to
use cognitive synergy on our neural

142
00:09:59,341 --> 00:10:04,341
symbolic knowledge store to achieve
initially human level Ai.

143
00:10:04,771 --> 00:10:09,771
But that's just nearly stage goal and
then ai much beyond the human level.

144
00:10:10,590 --> 00:10:15,150
And that is another advantage of taking
an approach that doesn't adhere

145
00:10:15,151 --> 00:10:19,980
slavishly to the human brain.
The brain is pretty good at recognizing

146
00:10:19,981 --> 00:10:24,600
faces because millions of years of
evolution when into that part of the

147
00:10:24,601 --> 00:10:27,870
brain.
But for doing science or math or logical

148
00:10:27,871 --> 00:10:29,610
reasoning,
your strategic planning,

149
00:10:29,970 --> 00:10:33,420
we're pretty bad.
And these are things that we started

150
00:10:33,421 --> 00:10:38,421
doing only recently in evolutionary time
as a result of of modern culture.

151
00:10:39,450 --> 00:10:44,450
So I think actually opened cognitive,
other AI systems have potential to be

152
00:10:45,150 --> 00:10:49,320
far better than,
than human beings at the sort of logical

153
00:10:49,321 --> 00:10:53,700
and strategic side of things.
And I think that's quite important

154
00:10:53,701 --> 00:10:58,701
because if you take a human being and
upgrade them to like 10,000 Iq,

155
00:10:59,440 --> 00:11:03,360
the the,
the outcome might not be what you want

156
00:11:03,361 --> 00:11:06,150
because you,
you've got a motivational system and an

157
00:11:06,151 --> 00:11:11,151
emotional system that basically evolved
in prehuman animals.

158
00:11:11,680 --> 00:11:16,680
Where as if you architect a system where
rationality and and empathy play a

159
00:11:17,551 --> 00:11:22,551
deeper role in the architecture,
then as its intelligence ramps way up,

160
00:11:22,650 --> 00:11:25,260
we may find the more beneficial outcome.

