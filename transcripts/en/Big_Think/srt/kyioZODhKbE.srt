1
00:00:05,660 --> 00:00:10,190
So most of us think that information is
the best way to convince people of our

2
00:00:10,191 --> 00:00:15,191
truth and in fact it doesn't work that
well and we see that all the time,

3
00:00:15,771 --> 00:00:18,740
right?
We see it where with climate change,

4
00:00:18,741 --> 00:00:22,040
where there's tons of data suggesting
that climate change is manmade,

5
00:00:22,041 --> 00:00:26,420
but about 50 percent of the population
doesn't believe it or with people

6
00:00:26,421 --> 00:00:29,690
arguing about things like how many
people were in the neck in the

7
00:00:29,691 --> 00:00:32,750
presidential inauguration.
So we have facts,

8
00:00:32,900 --> 00:00:35,360
but people decide which facts they want
to listen to,

9
00:00:35,361 --> 00:00:38,810
which facts they want to take and change
their opinions in which they want to

10
00:00:38,811 --> 00:00:43,430
disregard.
And one of the reasons for this is when

11
00:00:43,431 --> 00:00:47,840
something doesn't confirm to what I
already believe,

12
00:00:48,830 --> 00:00:53,830
what people tend to do is either
disregard it or rationalize it away

13
00:00:54,350 --> 00:00:58,340
because information doesn't take into
account what makes us human,

14
00:00:58,341 --> 00:01:00,080
which is our emotions,
our desires,

15
00:01:00,081 --> 00:01:01,880
our motives,
and our prior beliefs.

16
00:01:02,300 --> 00:01:05,020
So for example,
in one study my colleagues and I tried

17
00:01:05,021 --> 00:01:09,260
to see whether we could use science to
change people's opinions about climate

18
00:01:09,261 --> 00:01:12,230
change.
The first thing we did was ask people,

19
00:01:12,290 --> 00:01:14,360
do you believe in manmade climate
change?

20
00:01:14,510 --> 00:01:17,510
Do you support the Paris agreement?
And based on their answers,

21
00:01:17,511 --> 00:01:20,990
we divided them into the strong
believers and the weak believers.

22
00:01:21,620 --> 00:01:24,140
And then we gave them information for
some people.

23
00:01:24,141 --> 00:01:29,141
We said that scientists have reevaluated
the data and now conclude the things are

24
00:01:30,051 --> 00:01:33,590
actually much worse than they thought
before that the temperature would rise

25
00:01:33,591 --> 00:01:36,920
by about seven degrees to 10 degrees.
For some people,

26
00:01:36,921 --> 00:01:41,420
we said the scientists have reevaluated
data and they now believe that actually

27
00:01:41,421 --> 00:01:43,400
the situation is not as bad as I
thought.

28
00:01:43,870 --> 00:01:46,010
Um,
it's much better and the rise in

29
00:01:46,011 --> 00:01:51,011
temperature would be quite small.
And what we found is that people who did

30
00:01:51,201 --> 00:01:53,900
not believe in climate change when they
heard that the scientists are saying,

31
00:01:53,901 --> 00:01:55,130
actually it's not that bad.

32
00:01:55,660 --> 00:01:57,980
Um,
they changed their beliefs even more in

33
00:01:57,981 --> 00:02:00,260
that direction.
So they became more extremists in that

34
00:02:00,261 --> 00:02:01,460
direction.
But when they hear that,

35
00:02:01,461 --> 00:02:03,830
the scientists think it's much worse,
they didn't nudge.

36
00:02:04,250 --> 00:02:06,930
And the people who already believe that
climate change is manmade.

37
00:02:06,950 --> 00:02:08,600
When they heard that things are
actually,

38
00:02:08,601 --> 00:02:10,700
scientists are saying are much worse
than they did before.

39
00:02:10,880 --> 00:02:14,150
They moved more in that direction,
so they became more polarized,

40
00:02:14,420 --> 00:02:17,030
but when they heard that the scientists
are saying it's not that bad.

41
00:02:17,031 --> 00:02:20,390
They didn't touch much.
So we gave people information and as a

42
00:02:20,391 --> 00:02:25,391
result it caused polarization.
It didn't cause people to come together.

43
00:02:26,510 --> 00:02:31,100
So the question is what's happening
inside our brain that causes this?

44
00:02:31,370 --> 00:02:35,690
And in one study,
my colleagues and I scanned brain

45
00:02:35,691 --> 00:02:39,560
activity of two people who were
interacting and what we found was when

46
00:02:39,561 --> 00:02:42,920
those two people agreed on a question
that we gave them,

47
00:02:43,400 --> 00:02:47,780
the brain was really encoding what the
other person was saying,

48
00:02:47,840 --> 00:02:49,250
the details that they gave.

49
00:02:49,610 --> 00:02:54,350
But when the two people disagreed,
it looked metaphorically as if the brain

50
00:02:54,351 --> 00:02:58,040
was switching off and not encoding what
the other person was saying.

51
00:02:59,140 --> 00:03:01,210
And as a result,
when the two agreed,

52
00:03:01,270 --> 00:03:04,300
they became even more confident.
But when they disagreed,

53
00:03:04,390 --> 00:03:07,870
there wasn't as much of a change in
their confidence in their own view.

54
00:03:08,230 --> 00:03:12,820
What has been shown by [inaudible] and
colleagues from Yale University is that

55
00:03:13,120 --> 00:03:16,780
the more intelligent you are,
the more likely you are in fact,

56
00:03:16,900 --> 00:03:21,550
to change data at will.
So what they did is they first gave

57
00:03:21,730 --> 00:03:25,000
participants in their experiment,
I'm analytical,

58
00:03:25,001 --> 00:03:30,001
a math questions to solve,
and then they gave them data about gun

59
00:03:30,341 --> 00:03:32,710
control,
is gun control actually reducing

60
00:03:32,711 --> 00:03:36,700
violence.
And they found that more intelligent

61
00:03:36,701 --> 00:03:41,701
people actually were more likely to
twist data at will to make it conform to

62
00:03:41,741 --> 00:03:44,800
what they already believed.
So it seems that people are using their

63
00:03:44,801 --> 00:03:48,220
intelligent,
not necessarily to find the truth,

64
00:03:48,550 --> 00:03:53,550
but to take in the information and
change it to conform to what they

65
00:03:53,681 --> 00:03:55,000
already believe.

66
00:03:55,510 --> 00:03:56,260
So,
um,

67
00:03:56,260 --> 00:03:58,690
yeah,
so that suggests that just giving people

68
00:03:58,750 --> 00:04:03,750
information without considering first
where they're coming from may backfire

69
00:04:03,911 --> 00:04:05,070
at us.
Um,

70
00:04:05,830 --> 00:04:10,830
but we don't always need to go against
someone's conviction in order to change

71
00:04:12,071 --> 00:04:14,230
their behavior.
And let me give you an example for this

72
00:04:14,231 --> 00:04:17,560
is a study that was conducted at Ucla,
um,

73
00:04:17,620 --> 00:04:21,100
were what they wanted to do is convince
parents to vaccinate their kids.

74
00:04:21,460 --> 00:04:25,540
And some of the parents didn't want to
vaccinate their kids because they were

75
00:04:25,541 --> 00:04:29,650
afraid of the link with autism.
So they had two approaches.

76
00:04:29,740 --> 00:04:31,120
First they said,
well,

77
00:04:31,210 --> 00:04:33,700
the link with autism is actually not
meal.

78
00:04:33,701 --> 00:04:36,460
Here's all the data suggesting there
isn't a link between vaccines and

79
00:04:36,461 --> 00:04:38,920
autism.
And it didn't really work that well,

80
00:04:39,070 --> 00:04:42,970
but instead they use another approach.
So instead of going that way,

81
00:04:42,971 --> 00:04:46,780
they used another approach which is,
let's not talk about autism.

82
00:04:46,780 --> 00:04:50,200
We don't necessarily need to talk about
autism to convince you to vaccinate your

83
00:04:50,201 --> 00:04:51,430
kid.
Instead they said,

84
00:04:51,550 --> 00:04:55,480
well look,
these vaccines protect kids from deadly

85
00:04:55,481 --> 00:04:59,230
diseases right from the measles.
And they showed them pictures of what

86
00:04:59,231 --> 00:05:02,110
the measles are.
Because in this argument about vaccines,

87
00:05:02,111 --> 00:05:04,630
people actually forgot what the vaccines
are for,

88
00:05:04,810 --> 00:05:09,190
what are they protecting us from?
And they highlighted that and didn't

89
00:05:09,280 --> 00:05:14,280
necessarily go on to discuss autism.
And that had a much better outcome for

90
00:05:15,040 --> 00:05:17,380
the parents were much more likely to
say,

91
00:05:17,620 --> 00:05:19,990
yes,
we are going to vaccinate our kids.

92
00:05:20,080 --> 00:05:24,490
So the lesson here is that we need to
find the common motives.

93
00:05:24,640 --> 00:05:27,790
The common motives in this case was the
health of the children,

94
00:05:27,880 --> 00:05:31,490
right?
Not necessarily going back to the the

95
00:05:31,570 --> 00:05:33,910
thing that they were arguing about that
they disagreed about.

