1
00:00:04,560 --> 00:00:09,270
What we mean by valuing our elderly has
probably changed a lot over time.

2
00:00:09,940 --> 00:00:13,200
In traditional societies,
we look to our elderly as the

3
00:00:13,201 --> 00:00:17,910
repositories of our society's history.
It's tradition and it's knowledge.

4
00:00:18,150 --> 00:00:23,150
They were our computer memory banks.
They were the ones who had experienced

5
00:00:24,360 --> 00:00:28,710
and understood all of society's
intricacies.

6
00:00:29,580 --> 00:00:33,930
As time went on,
the elderly were less and less important

7
00:00:33,931 --> 00:00:38,250
for that purpose,
so books first became available and in

8
00:00:38,251 --> 00:00:42,120
books you could write down a scientific
facts,

9
00:00:42,390 --> 00:00:44,700
traditions,
society's history,

10
00:00:45,000 --> 00:00:49,290
and you didn't need the memory of the
elderly as much to remind you of those

11
00:00:49,291 --> 00:00:51,750
things.
And of course when we get to the modern

12
00:00:51,750 --> 00:00:55,530
day,
we seem not to need the elderly for that

13
00:00:55,531 --> 00:00:59,640
at all.
We have enormous repositories of

14
00:00:59,700 --> 00:01:04,700
information in computers and books,
and so we ask ourselves,

15
00:01:05,310 --> 00:01:07,740
what does it mean to value the elderly
today?

16
00:01:08,340 --> 00:01:13,340
And aside from the simple important of
the of elderly being human beings who

17
00:01:16,051 --> 00:01:20,130
have wants and needs and desires and
feelings that we as other human beings

18
00:01:21,750 --> 00:01:24,720
need to respect and help nurture.

19
00:01:25,230 --> 00:01:27,600
And aside from the fact that when we say
the elderly,

20
00:01:27,750 --> 00:01:31,170
we're really talking about our parents
and grandparents and our love for them,

21
00:01:32,490 --> 00:01:37,490
the social question of their function,
whatever that would mean the society may

22
00:01:37,711 --> 00:01:42,480
have changed.
And my suggestion is that though the

23
00:01:42,481 --> 00:01:47,481
elderly may have lost their traditional
role as the repositories of knowledge,

24
00:01:49,320 --> 00:01:52,800
they still have their traditional role
as fonts of wisdom,

25
00:01:53,670 --> 00:01:58,090
their experience,
their lived experience of,

26
00:01:58,440 --> 00:02:00,720
of the decades.
And,

27
00:02:00,750 --> 00:02:05,750
and I'm a great events and social
moments,

28
00:02:06,300 --> 00:02:07,710
uh,
that proceeded.

29
00:02:08,580 --> 00:02:13,260
The birth of the people who could
benefit by them are extraordinarily

30
00:02:13,261 --> 00:02:18,261
important for us to respect.
Because out of that experience comes a

31
00:02:18,991 --> 00:02:23,991
perspective that youth doesn't have.
And so my suggestion is we need to value

32
00:02:26,491 --> 00:02:31,410
our elderly for the wisdom that they can
give us to guide us through some of the

33
00:02:31,411 --> 00:02:34,950
really important decisions that we have
to make in modern society.

34
00:02:36,810 --> 00:02:41,810
Just because my mother or grandmother
May need some help operating her

35
00:02:43,981 --> 00:02:48,981
computer does not mean that she doesn't
have a deeper understanding of how

36
00:02:51,120 --> 00:02:54,720
sitting in front of a computer,
of how interacting with others through a

37
00:02:54,721 --> 00:02:59,721
computer screen
may not be the productive way to,

38
00:03:02,110 --> 00:03:04,960
um,
develop oneself as a human being.

39
00:03:05,230 --> 00:03:10,230
We can draw from our elderly enormous
value through depending on them as a

40
00:03:14,261 --> 00:03:17,620
check and a balance against the excesses
of youth.

41
00:03:17,950 --> 00:03:19,440
And so I think,
um,

42
00:03:19,930 --> 00:03:23,080
that's how we value our,
that's how we should value our elderly.

43
00:03:23,590 --> 00:03:28,420
Uh,
we've changed remarkably in recent times

44
00:03:28,720 --> 00:03:32,110
in our willingness to cater to the
elderly.

45
00:03:32,111 --> 00:03:36,670
We have retirement communities and
activities that we never had before.

46
00:03:36,880 --> 00:03:41,880
But with that tends to do often is to
isolate them from interactions with the

47
00:03:42,941 --> 00:03:45,490
younger generation,
especially in are incredibly mobile

48
00:03:45,491 --> 00:03:48,490
society.
And so I think as a society we do need

49
00:03:48,491 --> 00:03:53,491
to make an effort to mix the general
generation's a little more so that the

50
00:03:53,800 --> 00:03:58,120
younger generations can benefit from the
wisdom of their elders

51
00:04:01,180 --> 00:04:02,020
in bioethics.

52
00:04:02,020 --> 00:04:07,020
The term dignity is bandied about a lot
without people really thinking deeply

53
00:04:07,510 --> 00:04:09,470
about what it means.
No.

54
00:04:09,550 --> 00:04:14,550
Historically,
a French medieval scholar and historian

55
00:04:16,120 --> 00:04:20,530
named Felipe Irys wrote a book about the
stages of death throughout history.

56
00:04:20,880 --> 00:04:24,730
A tamed death is what he called the
first stage in ancient times when people

57
00:04:24,731 --> 00:04:29,530
expected death to come and it was
incorporated into the society.

58
00:04:30,280 --> 00:04:34,990
Then there was a stage called my own
death when people discovered selfhood in

59
00:04:34,991 --> 00:04:38,590
a sense historically individuality,
and for the first time we see grave

60
00:04:38,591 --> 00:04:43,591
markers with individuals' names on them
and there were other phases to the last

61
00:04:43,811 --> 00:04:46,240
phase before.
The one we're in now was called hidden

62
00:04:46,241 --> 00:04:50,170
death where we put death in hospitals.
When we closed the curtain and death was

63
00:04:50,171 --> 00:04:53,590
hidden away from society.
The stage I think we're in now,

64
00:04:53,591 --> 00:04:56,020
I call controlled death.
Today,

65
00:04:56,021 --> 00:05:00,280
our goal is to decide how we're going to
die when we're gonna die under what

66
00:05:00,281 --> 00:05:04,690
circumstances in some cases even
initiating our own death through

67
00:05:04,691 --> 00:05:06,970
physician assisted suicide or
euthanasia.

68
00:05:08,350 --> 00:05:11,800
So what we mean by a dignified death has
changed over time.

69
00:05:12,220 --> 00:05:14,920
And today.
What we tend to mean by it is a death

70
00:05:14,921 --> 00:05:19,660
that I control in my own way where I'm
not stuck in a hospital bed with needles

71
00:05:19,661 --> 00:05:23,140
and tubes going into me under the
control of someone else.

72
00:05:25,000 --> 00:05:29,380
Is that really what we want a dignified
death to be?

73
00:05:30,070 --> 00:05:32,140
That is what society is deciding right
now.

74
00:05:32,141 --> 00:05:36,400
That's what the debate about physician
assisted suicide are about right now.

75
00:05:36,670 --> 00:05:38,680
What does it mean to have a dignified
death?

76
00:05:38,890 --> 00:05:42,970
Does that mean just a death where others
treat you with deference or does it mean

77
00:05:42,971 --> 00:05:46,030
to death where you get to decide the
nature,

78
00:05:46,300 --> 00:05:48,670
timing and circumstances of your own
death?

79
00:05:53,320 --> 00:05:58,320
Researchers are making enormous strides
and brain computer interfaces and there

80
00:05:58,851 --> 00:06:03,650
are two major kinds.
One is called augmented cognition,

81
00:06:03,890 --> 00:06:08,360
and that involves
creating computers that can actually

82
00:06:08,390 --> 00:06:12,470
interact with human beings.
We have very limited memories.

83
00:06:12,471 --> 00:06:17,471
Attention desertion.
When we try to make decisions when we're

84
00:06:18,531 --> 00:06:22,640
piloting a plane or when we're working
on a complex task,

85
00:06:22,910 --> 00:06:26,930
our brains can only incorporate finite
amounts of information.

86
00:06:26,960 --> 00:06:31,430
The idea of augmented cognition is to
supplement that for a computer to be

87
00:06:31,431 --> 00:06:36,431
able to see how a human being is acting
and then reacting to that human being's

88
00:06:36,621 --> 00:06:39,800
needs.
So I don't think there's a major issue

89
00:06:39,801 --> 00:06:44,801
with augmented cognition,
but the second piece of BCIS or brain

90
00:06:46,491 --> 00:06:50,660
computer interfaces is what we call
neuroprosthetics and that involves

91
00:06:50,661 --> 00:06:54,080
actually using computers to supplement,
um,

92
00:06:54,140 --> 00:06:59,140
damage into the function of the brain
and they were actually talking about in

93
00:06:59,511 --> 00:07:03,200
many cases,
implanting computer chips into the brain

94
00:07:03,410 --> 00:07:08,060
or creating an interface,
a physical hard wired interface between

95
00:07:08,061 --> 00:07:11,960
the brain and a computer through
implanting electrodes that are connected

96
00:07:11,961 --> 00:07:16,961
to a computer and that can be used to
help paraplegic and paraplegics and

97
00:07:17,031 --> 00:07:19,160
quadriplegics and manipulate their
environment.

98
00:07:19,430 --> 00:07:23,750
It can be used for or they hope it will
be able to be used for people who have

99
00:07:23,960 --> 00:07:26,720
lesions or tumors in the brain that
disrupt function.

100
00:07:27,260 --> 00:07:29,960
Then we're talking about for the first
time in history,

101
00:07:30,380 --> 00:07:35,380
having brains and information technology
working together in an integrated way.

102
00:07:37,010 --> 00:07:41,810
The good news about that is that brain
plasticity allows that and we may find

103
00:07:41,811 --> 00:07:45,560
that these neural prosthetics helped
people in profound ways.

104
00:07:45,740 --> 00:07:48,230
An example of that would be cochlear
implants.

105
00:07:49,400 --> 00:07:54,400
The problem is once you begin to
supplement the natural function of our

106
00:07:54,531 --> 00:07:58,890
brains with artificially created
information technologies,

107
00:07:59,070 --> 00:08:01,700
you,
you're risking a whole host of problems

108
00:08:01,970 --> 00:08:06,970
from competition between the two forms
of information to malfunction of the

109
00:08:09,351 --> 00:08:13,220
information technology to kind of deep
psychological,

110
00:08:14,390 --> 00:08:18,800
um,
difficulty in understanding that my own

111
00:08:18,801 --> 00:08:23,270
thought processes are no longer only
mine,

112
00:08:23,600 --> 00:08:28,520
but involve some sort of,
of silicon chip that has now been fully

113
00:08:28,521 --> 00:08:32,360
integrated into me because after all,
unlike genetics,

114
00:08:32,361 --> 00:08:34,730
even though we spend a lot of time
thinking about that,

115
00:08:35,240 --> 00:08:40,240
we think of our brains as uniquely us.
My identical twin can share my genetics,

116
00:08:41,240 --> 00:08:42,890
but my brain is mine.
And when we,

117
00:08:42,920 --> 00:08:46,970
when we begin to manipulate the brain,
whether it's through bcis or

118
00:08:47,240 --> 00:08:51,920
pharmaceuticals or in any way,
we are beginning to alter in some sense

119
00:08:51,921 --> 00:08:55,160
that which we think of as most uniquely
ourselves.

120
00:08:55,530 --> 00:08:57,570
And we have to do it with great caution.

121
00:09:00,960 --> 00:09:05,960
Brain imaging research is advancing at a
startling pace for a long time.

122
00:09:06,961 --> 00:09:10,620
I kept predicting that this research was
going to hit a wall and stop and it

123
00:09:10,621 --> 00:09:12,930
hasn't.
So I've stopped thinking that there's a

124
00:09:12,931 --> 00:09:17,750
natural boundary to this research.
What they are doing now is

125
00:09:19,430 --> 00:09:22,820
having people think of objects,
uh,

126
00:09:22,910 --> 00:09:27,910
intentions and and other normal
functions of the brain and through brain

127
00:09:28,311 --> 00:09:31,250
imaging,
they are able to predict what that

128
00:09:31,251 --> 00:09:34,430
person is thinking of what sometimes
with startling accuracy,

129
00:09:34,820 --> 00:09:39,020
certainly if you have someone think of
one object rather than another,

130
00:09:39,021 --> 00:09:42,560
a hammer or a chair.
Brain imaging can now after getting a

131
00:09:42,561 --> 00:09:47,210
baseline predict with great accuracy
which one you're thinking of.

132
00:09:47,690 --> 00:09:52,690
There's research that showed a out of
Berlin that if you asked someone to add

133
00:09:52,731 --> 00:09:55,730
or subtract two numbers together and you
brain image them,

134
00:09:55,790 --> 00:09:59,270
walk once they've decided,
but before they've revealed it to you,

135
00:09:59,510 --> 00:10:03,080
you can predict with great accuracy
which one they have done,

136
00:10:03,680 --> 00:10:08,680
which one they've decided to do.
So what we have now is the possibility

137
00:10:10,910 --> 00:10:15,910
that brain imaging will at some point be
able to actually get fairly detailed and

138
00:10:16,221 --> 00:10:17,054
robust

139
00:10:19,780 --> 00:10:24,220
the predictions or recognitions of what
a person is thinking of.

140
00:10:24,640 --> 00:10:26,830
Now,
this has profound implications for

141
00:10:26,890 --> 00:10:30,220
forensics.
Courtrooms would love this kind of

142
00:10:30,290 --> 00:10:34,690
ability.
They are working very hard on a brain

143
00:10:34,691 --> 00:10:39,610
imaging lie detector and there are now
two companies that actually offer brain

144
00:10:39,611 --> 00:10:43,020
imaging lie detection to the public.
Um,

145
00:10:43,690 --> 00:10:48,690
the question of putting someone in a
scanner to try to determine guilt or

146
00:10:49,361 --> 00:10:52,960
innocence or to try to decide if they
were at the scene of a crime.

147
00:10:52,961 --> 00:10:56,560
They are also using eeg besides brain
imaging for that,

148
00:10:56,900 --> 00:11:00,250
um,
has become a very controversial issue in

149
00:11:00,340 --> 00:11:03,820
illegal channels.
And I don't think that there really is

150
00:11:03,821 --> 00:11:07,300
any consensus about how this can and
should be used.

151
00:11:07,630 --> 00:11:10,390
But even outside of the courtroom,
there are some very interesting

152
00:11:10,391 --> 00:11:13,450
implications of this.
I have two teenage daughters.

153
00:11:13,451 --> 00:11:16,810
If I come home one day and my car is
dented and both of them said they didn't

154
00:11:16,811 --> 00:11:20,440
do it,
will I be able to drag them off to a one

155
00:11:20,441 --> 00:11:22,690
of these new lie detection companies and
say,

156
00:11:22,691 --> 00:11:23,650
get in the scanner.

157
00:11:23,650 --> 00:11:25,540
I want to know which one of you dented
the car.

158
00:11:25,870 --> 00:11:28,450
Would schools be able to use that for
drug searches?

159
00:11:29,560 --> 00:11:33,520
Will companies be able to use that?
We all saw minority report when Tom

160
00:11:33,520 --> 00:11:37,060
Cruise walks into the store and they
scanners in the store and know who he

161
00:11:37,061 --> 00:11:38,200
is,
what he likes to buy,

162
00:11:38,201 --> 00:11:39,880
what he bought the last time he was
there.

163
00:11:40,300 --> 00:11:45,300
There really is a a privacy issue here
that we as a society are going to have

164
00:11:46,061 --> 00:11:50,590
to make a decision about.
One quick example that I think will

165
00:11:50,890 --> 00:11:54,760
illustrate how different this is.
Similar issues that came before.

166
00:11:55,360 --> 00:11:58,810
According to our constitution,
I have a right against self

167
00:11:58,811 --> 00:12:03,010
incrimination.
What that means legally though is not I

168
00:12:03,040 --> 00:12:07,060
have a right to deny you my DNA.
If a court orders it or,

169
00:12:07,150 --> 00:12:11,230
or a hair sample,
the court can take those from me even

170
00:12:11,231 --> 00:12:14,950
though they may incriminate me because
the constitution gives us a right

171
00:12:15,430 --> 00:12:16,730
against,
uh,

172
00:12:16,731 --> 00:12:20,830
the fifth amendment actually against
testimonial self-incrimination.

173
00:12:21,040 --> 00:12:23,440
You can't force me to testify against
myself,

174
00:12:23,770 --> 00:12:26,830
which includes pointing at something
gesturing.

175
00:12:26,890 --> 00:12:31,120
Any action that I do that involves
revealing my thought to you is

176
00:12:31,121 --> 00:12:33,820
considered testimonial.
Well,

177
00:12:33,821 --> 00:12:38,290
what if you could read my brain and get
a piece of evidence that could be used

178
00:12:38,291 --> 00:12:41,440
against me?
Is that testimonial evidence?

179
00:12:41,500 --> 00:12:42,910
Well,
I haven't testified.

180
00:12:43,300 --> 00:12:45,820
Or is that more like blood,
semen,

181
00:12:45,880 --> 00:12:48,640
DNA,
which they can take from me even though

182
00:12:48,641 --> 00:12:51,880
I don't want them to.
The answer is it's not like either of

183
00:12:51,881 --> 00:12:55,270
those things.
It's a new ability that we have as human

184
00:12:55,271 --> 00:12:58,180
beings and at some point the courts are
going to have to decide,

185
00:12:58,600 --> 00:13:03,600
is brain imaging of protected a kind of
testimonial evidence or is it a picture

186
00:13:07,541 --> 00:13:12,541
on a scanner that should be considered
the same way we consider other physical

187
00:13:13,541 --> 00:13:15,550
evidence that I don't have a right to
withhold.

188
00:13:15,820 --> 00:13:20,820
That's one example of the thorny kinds
of questions that this technology is

189
00:13:21,071 --> 00:13:21,904
going to bring.

190
00:13:25,360 --> 00:13:29,950
The stem cell debate has been a
fascinating debate in our society for a

191
00:13:29,951 --> 00:13:31,660
number of reasons.
For example,

192
00:13:32,410 --> 00:13:36,490
we always thought that the abortion
debate was pretty set.

193
00:13:36,491 --> 00:13:40,120
There was a group pro choice group,
pro life,

194
00:13:40,450 --> 00:13:45,450
and it was very easy to determine which
groups were in which camp.

195
00:13:47,440 --> 00:13:51,610
Then stem cells came along,
which are really based on the same moral

196
00:13:51,611 --> 00:13:55,780
objection that an embryo should be
treated as a human being and that human

197
00:13:55,781 --> 00:14:00,220
being should not be destroyed.
But what you saw for the first time was

198
00:14:00,280 --> 00:14:03,190
people crossing camps.
So you had a number of,

199
00:14:03,320 --> 00:14:05,140
of,
of people in Congress,

200
00:14:05,200 --> 00:14:07,660
Orrin Hatch,
and Strom Thurmond and others who are

201
00:14:07,661 --> 00:14:12,661
very strongly in the anti abortion camp
who were also very strongly in the pro

202
00:14:13,210 --> 00:14:15,010
stem cell camp.
Uh,

203
00:14:15,011 --> 00:14:20,011
you had some religious groups that were
against abortion when the fetus was in a

204
00:14:21,491 --> 00:14:26,491
woman's uterus.
But who said a cell in a dish is not the

205
00:14:27,101 --> 00:14:27,934
same thing.

206
00:14:28,000 --> 00:14:33,000
So you had important a schisms there in
this endless abortion debate that we've

207
00:14:34,661 --> 00:14:38,320
been having for over 30 years in this
society,

208
00:14:38,560 --> 00:14:42,520
and that is actually a hopeful thing.
It shows that perhaps by breaking

209
00:14:42,521 --> 00:14:44,800
through some of those traditional
barriers,

210
00:14:44,801 --> 00:14:49,000
we can begin to talk more about
compromise not only on stem cells,

211
00:14:49,240 --> 00:14:51,710
but also on abortion.
However,

212
00:14:51,830 --> 00:14:56,690
the bottom line on the stem cell debate
is unlike the abortion debate,

213
00:14:56,740 --> 00:15:01,740
it is going to go away and it's going to
go away because science is in the

214
00:15:01,761 --> 00:15:04,130
process of,
and there's some successes in this

215
00:15:04,131 --> 00:15:08,670
already being able to create stem cells
without having destroy,

216
00:15:09,110 --> 00:15:12,320
destroy the embryos.
And since the destruction of the embryos

217
00:15:12,321 --> 00:15:15,520
was really the only objection,
uh,

218
00:15:15,570 --> 00:15:18,200
against stem cell research,
one science masters,

219
00:15:18,201 --> 00:15:23,201
the ability to create stem cells that do
everything embryonic stem cells do

220
00:15:23,691 --> 00:15:25,500
without destroying embryos.
Uh,

221
00:15:25,610 --> 00:15:30,410
the debate that debate will go away
assuming that science achieves that

222
00:15:30,411 --> 00:15:32,540
they're very close.
They haven't quite gotten there yet.

223
00:15:36,260 --> 00:15:40,190
Ethics involved scholars from across the
intellectual spectrum,

224
00:15:40,191 --> 00:15:44,480
so philosophers and social scientists.
And then in bioethics you have

225
00:15:44,481 --> 00:15:47,480
clinicians,
you have lawyers and nurses,

226
00:15:47,481 --> 00:15:50,810
and you've got economists and you've got
political activists.

227
00:15:52,250 --> 00:15:57,140
Ethics involves people who approach it
from a scholarly perspective,

228
00:15:57,141 --> 00:15:59,780
those who approach project from a very
sort of pragmatic,

229
00:15:59,781 --> 00:16:02,850
social perspective.
So what ethics really is,

230
00:16:02,851 --> 00:16:07,851
is a conversation and to point to a few
things that are somehow innovative and

231
00:16:09,051 --> 00:16:13,220
ethics is to ask the question of what's
innovative in our society that has

232
00:16:13,221 --> 00:16:15,860
ethical implications.
So for example,

233
00:16:16,310 --> 00:16:21,310
there's a lot of ethical discussion
right now about the proper use of the

234
00:16:21,771 --> 00:16:23,510
web,
twitter,

235
00:16:23,750 --> 00:16:25,670
facebook,
and other kinds of information

236
00:16:25,671 --> 00:16:30,671
technologies and the question of whether
that should be government control.

237
00:16:31,001 --> 00:16:33,830
Should there be regulation there?
Should it be controlled by the people.

238
00:16:34,160 --> 00:16:36,830
There are ethical questions around our
economy.

239
00:16:36,831 --> 00:16:39,140
Of course,
the big one we're dealing with as we

240
00:16:39,830 --> 00:16:42,020
tape this is healthcare reform.

241
00:16:42,530 --> 00:16:46,910
And there are some very innovative kinds
of suggestions that have been made by

242
00:16:46,970 --> 00:16:51,620
ethicists and others about how we might
get around the roadblocks of healthcare

243
00:16:51,650 --> 00:16:53,210
reform.
And in fact,

244
00:16:53,440 --> 00:16:55,940
uh,
the president's bioethics commissions,

245
00:16:55,941 --> 00:16:59,960
both President Clinton and president
bushes have worked hard on these kinds

246
00:16:59,961 --> 00:17:03,530
of issues.
One of the things that interests me of

247
00:17:03,531 --> 00:17:06,620
course,
is biotechnologies and there we have

248
00:17:06,621 --> 00:17:11,570
some fascinating issues and questions
about how we are going to change

249
00:17:11,571 --> 00:17:15,170
ourselves.
A human enhancement has become a very

250
00:17:15,171 --> 00:17:20,171
big
question for those of us thinking about

251
00:17:20,630 --> 00:17:25,280
how is technology going to alter the
nature of being human,

252
00:17:25,700 --> 00:17:27,680
what it's going to mean for our
abilities?

253
00:17:27,920 --> 00:17:31,580
How are our children as they go through
school best,

254
00:17:31,690 --> 00:17:32,970
um,
uh,

255
00:17:34,100 --> 00:17:39,100
considered with attention enhancing
drugs available now with memory

256
00:17:39,501 --> 00:17:44,180
enhancing drugs coming out soon.
Is it a boon to our children or is it

257
00:17:44,181 --> 00:17:49,181
somehow diminishing their developmental
capabilities to give them drugs that

258
00:17:49,591 --> 00:17:51,630
will enhance their capabilities in
school?

259
00:17:52,100 --> 00:17:57,030
Uh,
we now have abilities to predict the

260
00:17:57,031 --> 00:17:58,140
onset,
for example,

261
00:17:58,141 --> 00:18:00,210
of mental illness in ways we haven't
before.

262
00:18:00,211 --> 00:18:05,211
Some work being done here at emory has
suggested that even in children as young

263
00:18:05,521 --> 00:18:10,350
as elementary school,
we might be able to see some beginning

264
00:18:10,351 --> 00:18:13,230
signs of susceptibility to
schizophrenia.

265
00:18:13,710 --> 00:18:16,260
And then the big ethical question is
what do we do about it?

266
00:18:17,070 --> 00:18:21,420
Do we give them a kind of prophylactic
psychiatric medication?

267
00:18:21,750 --> 00:18:25,770
All the medications for schizophrenia?
While they are very important for people

268
00:18:25,771 --> 00:18:30,060
who have it are not nice medications and
you don't want to give them to children,

269
00:18:30,240 --> 00:18:33,990
especially without being able to say,
this child will definitely become

270
00:18:33,991 --> 00:18:38,580
mentally ill,
so what we have now is an explosion in

271
00:18:38,581 --> 00:18:42,390
biotechnology.
We are able to manipulate the plasms and

272
00:18:42,391 --> 00:18:46,380
the functions of the body in a way that
just a few years ago,

273
00:18:46,650 --> 00:18:51,000
we're almost inconceivable and we have
to ask ourselves,

274
00:18:51,540 --> 00:18:54,720
should we do it?
How should we do it and under what

275
00:18:54,721 --> 00:18:56,850
circumstances?
And that by the way,

276
00:18:56,851 --> 00:19:01,470
is where the question of wisdom versus
knowledge actually comes in because the

277
00:19:01,471 --> 00:19:05,400
answers to these questions don't involve
gathering more facts.

278
00:19:05,670 --> 00:19:10,650
They involve thinking deeply about those
things that are of most importance to us

279
00:19:10,950 --> 00:19:15,950
and deciding how we can sustain and
maintain those things even as we use

280
00:19:16,051 --> 00:19:18,750
biotechnology for its best purpose.

