WEBVTT

1
00:00:00.090 --> 00:00:04.020
<v Speaker 1>I see no obstacle to computers</v>
<v Speaker 1>eventually,</v>

2
00:00:04.320 --> 00:00:07.140
<v Speaker 1>uh,</v>
<v Speaker 1>becoming conscious in some sense.</v>

3
00:00:07.920 --> 00:00:10.950
<v Speaker 1>That'll be a fascinating experience.</v>
<v Speaker 1>Um,</v>

4
00:00:10.980 --> 00:00:13.980
<v Speaker 1>as a physicist,</v>
<v Speaker 1>I want to know if those computers do</v>

5
00:00:13.981 --> 00:00:18.870
<v Speaker 1>physics the same way humans do physics</v>
<v Speaker 1>and there's no doubt that those machines</v>

6
00:00:18.871 --> 00:00:21.390
<v Speaker 1>will be able to,</v>
<v Speaker 1>um,</v>

7
00:00:23.130 --> 00:00:26.400
<v Speaker 1>evolve computationally potentially at a</v>
<v Speaker 1>faster rate than humans.</v>

8
00:00:26.580 --> 00:00:28.800
<v Speaker 1>And in the long term,</v>
<v Speaker 1>the,</v>

9
00:00:28.890 --> 00:00:29.723
<v Speaker 1>the,</v>
<v Speaker 1>um,</v>

10
00:00:30.180 --> 00:00:35.100
<v Speaker 1>ultimate highest forms of consciousness</v>
<v Speaker 1>on the planet may not be purely</v>

11
00:00:35.101 --> 00:00:38.550
<v Speaker 1>biological,</v>
<v Speaker 1>but that's not necessarily a bad thing.</v>

12
00:00:39.120 --> 00:00:44.120
<v Speaker 1>We always present computers as if they</v>
<v Speaker 1>don't have capabilities of empathy or a</v>

13
00:00:45.550 --> 00:00:48.210
<v Speaker 1>emotion,</v>
<v Speaker 1>but I would think that any intelligent</v>

14
00:00:48.211 --> 00:00:52.500
<v Speaker 1>machine would ultimately have experience</v>
<v Speaker 1>that it's a learning machine and</v>

15
00:00:52.501 --> 00:00:54.510
<v Speaker 1>ultimately it would learn from its</v>
<v Speaker 1>experience like a,</v>

16
00:00:54.720 --> 00:00:58.080
<v Speaker 1>like a biological conscious being.</v>
<v Speaker 1>And therefore it's hard for me to</v>

17
00:00:58.081 --> 00:01:03.081
<v Speaker 1>believe that it would not be able to</v>
<v Speaker 1>have many of the characteristics that we</v>

18
00:01:03.601 --> 00:01:07.740
<v Speaker 1>now associate with being human.</v>
<v Speaker 1>Elon Musk and others who have expressed</v>

19
00:01:07.741 --> 00:01:12.060
<v Speaker 1>concern and Stephen Hawking are friends</v>
<v Speaker 1>of mine and I understand their potential</v>

20
00:01:12.061 --> 00:01:15.750
<v Speaker 1>concerns,</v>
<v Speaker 1>but I'm frankly not as concerned about</v>

21
00:01:15.751 --> 00:01:17.640
<v Speaker 1>ai in the near term,</v>
<v Speaker 1>at the very least,</v>

22
00:01:17.641 --> 00:01:20.250
<v Speaker 1>as many many of my friends and</v>
<v Speaker 1>colleagues are.</v>

23
00:01:20.420 --> 00:01:22.440
<v Speaker 1>It's far less powerful than people</v>
<v Speaker 1>imagined.</v>

24
00:01:22.441 --> 00:01:25.140
<v Speaker 1>I mean,</v>
<v Speaker 1>you try to get a robot to fold laundry</v>

25
00:01:25.141 --> 00:01:29.160
<v Speaker 1>and I've just been told you can't even</v>
<v Speaker 1>get full robot to fold laundry at.</v>

26
00:01:29.161 --> 00:01:31.140
<v Speaker 1>Someone just wrote me.</v>
<v Speaker 1>They were surprised when I said an</v>

27
00:01:31.141 --> 00:01:34.380
<v Speaker 1>elevator is an old example of the fact</v>
<v Speaker 1>that when you get an elevator,</v>

28
00:01:34.381 --> 00:01:36.750
<v Speaker 1>you're,</v>
<v Speaker 1>it's a primitive form of a computer and</v>

29
00:01:36.751 --> 00:01:41.070
<v Speaker 1>you're and you're giving up control of</v>
<v Speaker 1>the of the fact that it's going to take</v>

30
00:01:41.071 --> 00:01:43.200
<v Speaker 1>you where you want to go.</v>
<v Speaker 1>Cars are the same thing.</v>

31
00:01:43.710 --> 00:01:47.760
<v Speaker 1>Machines are useful because they're</v>
<v Speaker 1>tools that help us do we want to do,</v>

32
00:01:48.270 --> 00:01:51.630
<v Speaker 1>and I think computation machines are</v>
<v Speaker 1>exempt,</v>

33
00:01:51.660 --> 00:01:56.660
<v Speaker 1>are good examples of that.</v>
<v Speaker 1>One has to be very careful in creating</v>

34
00:01:56.761 --> 00:02:00.330
<v Speaker 1>machines to not assume they're more</v>
<v Speaker 1>capable than they are.</v>

35
00:02:01.410 --> 00:02:04.050
<v Speaker 1>That's true in cars.</v>
<v Speaker 1>That's true in vehicles that we make.</v>

36
00:02:04.051 --> 00:02:06.390
<v Speaker 1>That's true in weapons.</v>
<v Speaker 1>We create that's true in defensive</v>

37
00:02:06.391 --> 00:02:11.391
<v Speaker 1>mechanisms we create and so to me,</v>
<v Speaker 1>the dangers of Ai,</v>

38
00:02:11.880 --> 00:02:16.410
<v Speaker 1>what are mostly due to the fact that</v>
<v Speaker 1>people may assume the devices they</v>

39
00:02:16.411 --> 00:02:19.980
<v Speaker 1>create are more capable than they are</v>
<v Speaker 1>and don't need more control and</v>

40
00:02:19.981 --> 00:02:22.590
<v Speaker 1>monitoring.</v>
<v Speaker 1>I guess I find the opportunities to be</v>

41
00:02:22.591 --> 00:02:26.100
<v Speaker 1>far more exciting than the dangers.</v>
<v Speaker 1>The unknown is always dangerous,</v>

42
00:02:26.520 --> 00:02:27.353
<v Speaker 1>but</v>

43
00:02:29.700 --> 00:02:34.700
<v Speaker 1>ultimately machines and computational</v>
<v Speaker 1>machines are</v>

44
00:02:37.560 --> 00:02:42.560
<v Speaker 1>improving our lives in many ways.</v>
<v Speaker 1>We of course have to realize that the</v>

45
00:02:43.051 --> 00:02:48.051
<v Speaker 1>rate at which machines are,</v>
<v Speaker 1>are evolving in capability may far</v>

46
00:02:48.901 --> 00:02:51.630
<v Speaker 1>exceed the rate at which society is able</v>
<v Speaker 1>to deal with them.</v>

47
00:02:51.840 --> 00:02:54.810
<v Speaker 1>The fact that teenagers aren't talking</v>
<v Speaker 1>to each other,</v>

48
00:02:54.990 --> 00:02:57.450
<v Speaker 1>but always looking at their phones,</v>
<v Speaker 1>not just teenagers.</v>

49
00:02:57.451 --> 00:03:01.150
<v Speaker 1>I was just in a restaurant here,</v>
<v Speaker 1>New York this afternoon and half the</v>

50
00:03:01.151 --> 00:03:02.920
<v Speaker 1>people were not talking to people they</v>
<v Speaker 1>were with,</v>

51
00:03:03.130 --> 00:03:07.160
<v Speaker 1>but we're staring at their phones.</v>
<v Speaker 1>While that may be not a good thing for</v>

52
00:03:07.180 --> 00:03:10.630
<v Speaker 1>societal interaction and people may have</v>
<v Speaker 1>to come to terms with that,</v>

53
00:03:11.260 --> 00:03:13.720
<v Speaker 1>but I don't think people view their</v>
<v Speaker 1>phones as a danger.</v>

54
00:03:14.110 --> 00:03:16.510
<v Speaker 1>They view their phones as a tool that in</v>
<v Speaker 1>many ways,</v>

55
00:03:16.870 --> 00:03:19.660
<v Speaker 1>um,</v>
<v Speaker 1>allow them to do what they would</v>

56
00:03:19.870 --> 00:03:21.490
<v Speaker 1>otherwise do more effectively.</v>

