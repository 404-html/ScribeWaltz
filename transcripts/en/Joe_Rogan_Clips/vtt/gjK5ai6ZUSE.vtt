WEBVTT

1
00:00:00.240 --> 00:00:05.160
Do you guys have conversations about trying to shift the public perception of

2
00:00:05.161 --> 00:00:08.340
having this left wing bias,
maybe possibly addressing it?
Yeah,

3
00:00:08.750 --> 00:00:10.500
that's what they're doing right now,
right?
Yeah.

4
00:00:10.501 --> 00:00:14.810
I mean I went on the Sean Hannity Show,
right?
You know,
we,
we,

5
00:00:15.000 --> 00:00:17.820
how was that we brought ourselves before.
It was pretty a lot of sunlight.

6
00:00:17.850 --> 00:00:20.610
It was short and he,
well it was short and um,

7
00:00:20.940 --> 00:00:24.390
the run a lot of really tough questions and that was a feedback as well.

8
00:00:24.391 --> 00:00:28.800
And yeah,
I get it.
Like,
look again,
I'm also,
I'm from Missouri.

9
00:00:29.340 --> 00:00:32.940
My Dad is a Republican.
He listened to Hannity,
he listened to rush Limbaugh.

10
00:00:32.960 --> 00:00:37.960
My mom was a Democrat and I feel extremely fortunate that I was able to first

11
00:00:39.150 --> 00:00:43.080
see that spectrum but also feel safe enough to express my own point of view.

12
00:00:43.500 --> 00:00:47.180
But when I go on some of them like Hannity,
I'm not talking to anybody.

13
00:00:47.190 --> 00:00:49.210
I'm talking to people like my dad who listened to him.

14
00:00:49.650 --> 00:00:52.590
And I want to get across how we think and,

15
00:00:52.860 --> 00:00:56.490
and also that are thinking of all these and here's the challenges we're seeing.

16
00:00:56.491 --> 00:01:00.840
And like,
this is our intent.
This is what we're trying to protect and we're,

17
00:01:00.841 --> 00:01:03.750
we're gonna make some mistakes along the way and we're going to admit to him,

18
00:01:03.751 --> 00:01:07.980
we didn't admit to them in the past week and middle to a lot more,
uh,

19
00:01:08.010 --> 00:01:11.730
over over the past three years.
Um,
but you know,

20
00:01:11.880 --> 00:01:15.390
I don't know any other way to address some of these issues.
It all,

21
00:01:15.450 --> 00:01:17.040
it all goes back to trust.
Like our,

22
00:01:17.370 --> 00:01:20.760
one of our core operating principles is earning trust.
How do we earn more trust?

23
00:01:20.761 --> 00:01:25.410
And I,
you know,
the people in the world who do not trust us at all,

24
00:01:25.440 --> 00:01:28.050
and there are some people who trust us a little bit more,

25
00:01:28.051 --> 00:01:29.820
but this is the thing that we want to measure.

26
00:01:29.821 --> 00:01:31.530
This the thing that we want to get better at.

27
00:01:31.610 --> 00:01:36.060
<v 1>I saw you get a conversation with I think Katie Hertzog.
No,
no,
no.
Who Was it?
Um,</v>

28
00:01:36.410 --> 00:01:40.140
as the wrong person.
You had a Twitter conversation with Kara Swisher.
Wow.

29
00:01:40.310 --> 00:01:43.940
Wrong person,
but someone's got to shut up.
Uh,
and,
and you know,

30
00:01:44.240 --> 00:01:47.300
I see that the left goes at you in the opposite direction.
They want more,

31
00:01:47.840 --> 00:01:50.420
they want more banding,
they want more restrictions.

32
00:01:50.540 --> 00:01:52.720
And then looked at the right is saying less.
Right.

33
00:01:53.180 --> 00:01:55.910
So I mean in terms of solving the problem,
you uh,

34
00:01:55.970 --> 00:01:58.180
tell us what their conversation was about.
Uh,

35
00:01:58.280 --> 00:01:59.900
I would you want to summarize it cause it,
cause my,

36
00:01:59.930 --> 00:02:03.050
the thing I was pointing out specifically was that you were being asked to do

37
00:02:03.051 --> 00:02:06.140
more in terms of controlling it wasn't just more,

38
00:02:06.141 --> 00:02:08.300
but to be a lot more specific about what

39
00:02:08.400 --> 00:02:10.950
<v 0>actions we've taken to promote more health on the platform.</v>

40
00:02:10.951 --> 00:02:15.360
Like what products did we change,
what policies did we introduce in the past,
uh,

41
00:02:15.510 --> 00:02:19.560
two years.
Um,
uh,
so she was asking questions.

42
00:02:19.860 --> 00:02:23.850
Every question she asked.
She wanted me to be a lot more specific.

43
00:02:23.851 --> 00:02:27.540
And some of these things have something that is very specific,

44
00:02:27.630 --> 00:02:32.610
some are directional right now because like we,
we have to prioritize,
you know,

45
00:02:32.611 --> 00:02:35.160
the,
the direction.
And,
and I talked about like,
you know,

46
00:02:35.161 --> 00:02:38.460
we've decided that physical safety is going to be a priority for us.

47
00:02:38.461 --> 00:02:42.030
And to us that means like being a whole lot more proactive around things like

48
00:02:42.031 --> 00:02:42.960
doxing.
So

49
00:02:43.370 --> 00:02:43.950
<v 1>he was a guest means,</v>

50
00:02:43.950 --> 00:02:47.040
I guess I'm not going to imply that you have unlimited funding,

51
00:02:47.130 --> 00:02:50.460
but we did mention the peer review.
Oh,
right,
right.
And you had,

52
00:02:51.060 --> 00:02:54.570
you mentioned earlier with layoffs and retraction,
uh,
peer review,

53
00:02:54.571 --> 00:02:57.330
which we mentioned,
but have you considered opening an office,

54
00:02:57.331 --> 00:03:01.510
even a small one for trust and in an area that's not predominantly blue so that

55
00:03:01.511 --> 00:03:05.260
at least you have like you can have some pushback and what is learned to code

56
00:03:05.261 --> 00:03:08.830
mean and then they could tell you.
Absolutely.
That's it.
That's great feedback.

57
00:03:08.831 --> 00:03:09.664
And just seeing,
you know,

58
00:03:09.670 --> 00:03:13.120
to trust and safety team is also global team and the enforcement team is global

59
00:03:13.121 --> 00:03:13.840
team.

60
00:03:13.840 --> 00:03:17.710
So it's not like people from California who are looking at everything making

61
00:03:17.711 --> 00:03:19.000
decisions there are global.

62
00:03:19.270 --> 00:03:22.760
Now I hear your point about who trains them and the materials they have and all

63
00:03:22.761 --> 00:03:25.700
of that.
And like we have to think about that.
And um,
that's,

64
00:03:25.750 --> 00:03:28.330
that's one thing that Jack has really been pushing us to think about is how do

65
00:03:28.331 --> 00:03:32.530
we decentralize our workforce?
Because out of San Francisco in particular,

66
00:03:32.560 --> 00:03:34.210
so this is something he's very focused on.

67
00:03:34.330 --> 00:03:38.530
What about publishing evidence of wrongdoing and abandon.

68
00:03:38.590 --> 00:03:40.870
So when people say,
you know,
what did Alex Jones really do?

69
00:03:40.900 --> 00:03:43.690
Maybe a lot of people didn't realize what you,
what you saw.
And again,

70
00:03:43.691 --> 00:03:48.310
it's an issue of trust.
Yeah,
I love this,
Tim.
I'm a lawyer.
So by training,
um,

71
00:03:48.311 --> 00:03:50.680
we're thinking of doing something called we call case studies,

72
00:03:50.681 --> 00:03:53.890
but essentially like this is our case law.
This is what we use.

73
00:03:54.190 --> 00:03:56.980
And so high profile cases,
cases,

74
00:03:56.981 --> 00:04:00.640
people ask us about like to actually publish this so that we can go through,

75
00:04:00.970 --> 00:04:02.710
you know,
tweet by tweet just like this.

76
00:04:02.711 --> 00:04:06.400
Because I think a lot of people just don't understand and they don't believe us

77
00:04:06.401 --> 00:04:09.470
when we're saying these things.
So to put that out there so people can see it,

78
00:04:09.471 --> 00:04:11.650
and again,
they may disagree with the calls that we're making,

79
00:04:11.651 --> 00:04:15.670
but we at least want them to see why we're making these calls,
I think.
And that,

80
00:04:15.671 --> 00:04:18.940
that I do want to do,
I want to at least start that by the end of this year.

81
00:04:19.480 --> 00:04:20.710
So I think,
you know,

82
00:04:20.830 --> 00:04:24.280
ultimately my main criticism stands and I don't see a solution to in that

83
00:04:24.520 --> 00:04:28.540
Twitter is an unelected,
unaccountable as far as I'm concerned.

84
00:04:28.541 --> 00:04:29.830
When it comes to public discourse,

85
00:04:30.190 --> 00:04:33.670
you have rules that are very clearly at odds as we discussed.

86
00:04:33.700 --> 00:04:34.750
I don't see a solution to that.

87
00:04:34.751 --> 00:04:38.380
And I think in my opinion we can have this kind of like we've tone things down.

88
00:04:38.381 --> 00:04:40.300
We've had some interesting conversations,
but ultimately,

89
00:04:40.780 --> 00:04:44.050
unless you're willing to allow people to just speak,
speak entirely freely,

90
00:04:44.560 --> 00:04:45.430
you are in,

91
00:04:45.970 --> 00:04:49.870
we have an unelected group with a near monopoly on public discourse in many

92
00:04:49.871 --> 00:04:53.740
capacities.
And I understand it's not everything rented.
It is big too.
And it's,

93
00:04:53.770 --> 00:04:54.071
you know,

94
00:04:54.071 --> 00:04:58.210
what I see is you are going to dictate policy whether you realize it or not and

95
00:04:58.211 --> 00:05:00.820
that's going to terrify people and it's going to make violence happen.

96
00:05:00.821 --> 00:05:04.570
It's going to make things worse.
You know,
the,
the,
I,
I,

97
00:05:04.580 --> 00:05:06.340
I hate bringing up this example on the,

98
00:05:06.341 --> 00:05:08.140
on the rule from mis-gendering because I'm actually,

99
00:05:08.141 --> 00:05:11.860
I understand it and I can agree with it to a certain extent.
Um,
and I've,

100
00:05:11.890 --> 00:05:14.440
you know,
nothing but we're nothing but respect for the trans community.

101
00:05:14.441 --> 00:05:17.350
But I also recognize we've seen an escalation and street violence.

102
00:05:17.650 --> 00:05:21.680
We see a continually a disenfranchised large faction of individual in this

103
00:05:21.681 --> 00:05:24.370
country.
We then see only one of those factions,
band.

104
00:05:24.760 --> 00:05:28.810
We then see a massive multinational billion dollar corporation with private and

105
00:05:28.840 --> 00:05:33.460
Wioa foreign investors.
And it looks to me like if you hold,
if you know,

106
00:05:33.790 --> 00:05:35.950
foreign governments are trying to manipulate us there.
I don't,

107
00:05:35.951 --> 00:05:39.100
I don't see a direct solution to that problem that you do have political views,

108
00:05:39.101 --> 00:05:40.030
you do enforce them.

109
00:05:40.390 --> 00:05:43.270
And that means that Americans who are abiding by American ruler being excised

110
00:05:43.271 --> 00:05:48.080
from political discourse,
and that's the future.
That's it.
We do have views on,

111
00:05:48.081 --> 00:05:51.040
on the approach.
And,
and again,
uh,
we,

112
00:05:51.640 --> 00:05:55.870
we ground this in creating as much opportunity as possible for the largest

113
00:05:55.871 --> 00:05:58.190
number of people.
Right?
That's where it starts

114
00:05:58.250 --> 00:06:02.090
<v 0>and where we are today,
um,
we'll certainly evolve,
but like that,</v>

115
00:06:02.091 --> 00:06:05.550
that is what we are trying to base our,
uh,
rules and judgment.
It's not,

116
00:06:05.980 --> 00:06:09.710
and I get that.
That's an ideology.
I completely understand it.

117
00:06:09.890 --> 00:06:12.530
But we,
we also have to,

118
00:06:13.610 --> 00:06:18.610
we also have to be free to experiment with solutions and experiment with

119
00:06:20.120 --> 00:06:24.770
evolving policy and putting something out there that might look right at the

120
00:06:24.771 --> 00:06:27.830
time and evolving.
I'm not saying this is it,

121
00:06:27.831 --> 00:06:32.450
but like we,
we look to research,

122
00:06:32.451 --> 00:06:35.460
we look to our experience and data on the platform and,

123
00:06:35.980 --> 00:06:39.170
and we make a call and if we get it wrong,
we're,

124
00:06:39.350 --> 00:06:41.990
we're going to admit it and we're,
we're going to evolve it.

125
00:06:42.200 --> 00:06:45.110
<v 1>But I guess,
do you,
uh,
do you understand my point?</v>

126
00:06:45.260 --> 00:06:46.660
I understand the point that they are,

127
00:06:46.670 --> 00:06:49.310
there are American citizens abiding by the law who have a right to speak and be

128
00:06:49.311 --> 00:06:51.680
involved in public discourse that you have decided aren't allowed to.

129
00:06:52.120 --> 00:06:55.630
<v 0>Yeah.
And,
and I think we've discussed,
um,
like we,
we,</v>

130
00:06:55.631 --> 00:06:58.760
we don't see that as a win.
Uh,
we,

131
00:06:58.820 --> 00:07:02.080
we see that as not promoting health ultimately over time.
Right.

132
00:07:02.081 --> 00:07:04.450
But it's ultimately what is your priority?

133
00:07:04.540 --> 00:07:08.140
Do you have it prioritized in terms of what you got w what you guys would like

134
00:07:08.141 --> 00:07:10.720
to change?
I think Jack has said it a couple of times,

135
00:07:10.721 --> 00:07:13.750
but the first thing we're going to do is prioritize people's physical safety

136
00:07:13.870 --> 00:07:17.650
because that's gotta be understanding.
You already have done that pretty much.

137
00:07:17.651 --> 00:07:21.480
Right?
When you do that more,
we've prioritized it.
Okay,
we're,

138
00:07:21.530 --> 00:07:22.271
we're doing the work.

139
00:07:22.271 --> 00:07:27.271
I don't think companies like ours make the link enough online and offline

140
00:07:28.090 --> 00:07:31.660
ramifications.
What's the main criticism?
What's the main criticism you guys,

141
00:07:31.690 --> 00:07:34.810
is it censorship that you guys experience?
Is it censorship?

142
00:07:34.811 --> 00:07:37.240
Is abandoning like what does it,
what does ends?
What do you get the most?

143
00:07:37.390 --> 00:07:40.380
It depends on who.
Every single person has a different criticism,

144
00:07:40.381 --> 00:07:42.160
so I don't think there's a universal opinion.

145
00:07:42.190 --> 00:07:45.020
I mean you're just painted the picture right between the,

146
00:07:45.740 --> 00:07:48.360
the left end of that spectrum is asking for more.
Sure.

147
00:07:48.670 --> 00:07:53.080
The ride is asking for less.
That's very simplified just for this country,

148
00:07:53.590 --> 00:07:56.440
but at a high level.
Yeah,
that's consistent.

149
00:07:56.520 --> 00:07:59.820
<v 1>I mean,
my opinion would be,
as much as I,
I don't,</v>

150
00:08:00.000 --> 00:08:02.070
I don't like a lot of what people say about me,
what they do.

151
00:08:02.430 --> 00:08:04.980
The rules you've enforced on Twitter have done nothing to stop harassment

152
00:08:04.981 --> 00:08:08.100
towards me or anyone else,
right?
I swear to God my Twitter,

153
00:08:08.130 --> 00:08:11.580
I mean my reddit is probably,
you know,
50 messages from various,
you know,

154
00:08:11.610 --> 00:08:16.240
far left and left wing sub.
It's lying about me.
It calling me horrible names.

155
00:08:16.241 --> 00:08:18.630
The quote tweeting me and these people are blocked,
right?

156
00:08:18.940 --> 00:08:21.570
And I never used to block people because I thought it was silly because I can

157
00:08:21.571 --> 00:08:24.960
get around in any way.
But I decided to at one point,
because out of sight,

158
00:08:24.961 --> 00:08:27.720
out of mind,
if they see my tweets last,
they'll probably interact with me last.

159
00:08:27.721 --> 00:08:30.030
But they do this and they lie about what I believe.

160
00:08:30.031 --> 00:08:32.730
They lie about what I stand for and they're trying to destroy everything about

161
00:08:32.731 --> 00:08:36.360
me and they do this to other people.
I recognize that.
So ultimately I say,
well,

162
00:08:36.361 --> 00:08:38.310
what can you do?
It's going to happen.
I'm one of these platforms.

163
00:08:38.311 --> 00:08:41.640
The Internet is a thing,
as they say on the Internet.
Welcome to the Internet.

164
00:08:42.030 --> 00:08:44.940
So you know,
to me,
I see Twitter trying to enforce all these rules to,

165
00:08:44.960 --> 00:08:48.450
to maximize good.
And all you end up doing is stripping people from the platform,

166
00:08:48.780 --> 00:08:50.880
putting them in dark corners of the web where they get worse,

167
00:08:51.240 --> 00:08:53.240
and then you don't actually solve the harassment problem.

168
00:08:53.280 --> 00:08:56.990
Retard the dark corner of the web.
Right,
right.
No,
I'm not talking to him,

169
00:08:57.390 --> 00:09:00.690
but there are dark corners.
I've read it.
There were alternatives.
I mean,

170
00:09:00.930 --> 00:09:03.440
the Internet isn't going to go away and people have found alternative.

