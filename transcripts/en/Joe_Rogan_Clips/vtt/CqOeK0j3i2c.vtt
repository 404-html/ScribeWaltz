WEBVTT

1
00:00:01.000 --> 00:00:02.860
Hello Freak bitches.

2
00:00:05.740 --> 00:00:10.480
When you see some of the emerging technologies like CRISPR in some of these,
uh,

3
00:00:10.510 --> 00:00:15.510
genetic engineering technologies where they're starting to use non-viable human

4
00:00:15.671 --> 00:00:19.780
fetuses and run some tests on them,
are you concerned at all about that?

5
00:00:19.781 --> 00:00:23.230
Are you concerned about,
or I shouldn't need to use this term,
the term concern.

6
00:00:23.231 --> 00:00:28.030
Cause obviously you're here to that mindset.
No,
no,
no,
no,
I'm good.
Things happen.

7
00:00:28.120 --> 00:00:29.710
Oh No,
it's not just let things happen.

8
00:00:30.730 --> 00:00:35.170
Let them watch what's going to happen.
Trying to anticipate the results,

9
00:00:35.680 --> 00:00:37.360
understand them in detail,

10
00:00:37.570 --> 00:00:41.050
anticipate what the results are and avoid negative ones to the extent you can.

11
00:00:41.470 --> 00:00:43.600
But it's,
life is all about how things are going to change,

12
00:00:43.690 --> 00:00:48.490
but accept the fact that things are gonna change and,
and,
and that's not,
that's,

13
00:00:48.550 --> 00:00:52.180
that's are we happy that the world is different than it was during medieval

14
00:00:52.181 --> 00:00:57.050
times?
Sure.
I mean except for,
you know,
Mike Pence and other people were,

15
00:00:57.070 --> 00:01:01.000
the rest of us are happy or you know,
I can pick a lot of radio commentators,

16
00:01:01.270 --> 00:01:04.900
but most of us are happy that we,
that the world has gotten a more open,

17
00:01:04.901 --> 00:01:05.734
more interesting.

18
00:01:05.830 --> 00:01:10.830
And so that's part of the human drama is that it's going to go places and we

19
00:01:11.231 --> 00:01:13.330
don't know where it's gonna go and that's okay.

20
00:01:13.331 --> 00:01:17.500
But we should all work as much as we can to try and make sure to the extent that

21
00:01:17.501 --> 00:01:21.130
we can,
that the direction of heads is a good one.
Is,
is beneficial.

22
00:01:21.220 --> 00:01:24.010
More interesting,
more exciting,
more possibilities,

23
00:01:24.070 --> 00:01:27.260
more fun for everybody and,

24
00:01:27.270 --> 00:01:30.820
and maybe even more sustainable because it seems reasonable that it should be

25
00:01:30.821 --> 00:01:34.240
sustainable if we think we care about not just our children but our

26
00:01:34.241 --> 00:01:35.590
grandchildren and their grandchildren.

27
00:01:36.040 --> 00:01:39.670
And so that it's self interest in some sense to be interested in,

28
00:01:39.940 --> 00:01:44.410
in conservation and sustainability instead of immediate profit if you really

29
00:01:44.411 --> 00:01:45.430
care.
Of course,
you know,
might say,

30
00:01:45.431 --> 00:01:48.820
if I amass enough wealth and my children will be fine forever and who gives a

31
00:01:48.850 --> 00:01:51.670
damn about the rest of the People's children?
But you know,

32
00:01:51.671 --> 00:01:55.420
we can decide that maybe it's in the best interest of,

33
00:01:55.440 --> 00:01:59.500
of everyone if human societies sustainable because there'll be less likelihood

34
00:01:59.501 --> 00:02:01.840
for extreme war,
extreme violence,
blah,
blah,
blah.
You can,

35
00:02:02.260 --> 00:02:07.260
I would argue that we behave well in large part because of reason and,

36
00:02:07.920 --> 00:02:09.460
and,
and my point is,
and I've had this though,

37
00:02:09.461 --> 00:02:11.800
we had a session in my origins project,

38
00:02:12.070 --> 00:02:14.050
a whole meeting on the origins of morality.
And,

39
00:02:14.360 --> 00:02:16.870
and I've had this debate with a number of colleagues who point out,

40
00:02:16.871 --> 00:02:20.740
I think it was [inaudible] who said you can't get ought from is okay,

41
00:02:21.070 --> 00:02:24.310
you can't get off from is just by rationality.
You can't decide how to behave.

42
00:02:24.790 --> 00:02:28.000
Maybe,
maybe.
But here's the point.
Without,

43
00:02:28.001 --> 00:02:32.320
is you can never get dog without knowing the consequences,

44
00:02:32.321 --> 00:02:34.540
your actions,
which is what science is all about.

45
00:02:34.960 --> 00:02:36.520
You can't decide what's good and bad.

46
00:02:37.300 --> 00:02:42.300
And so science and reason is an essential part of any progress because we can't

47
00:02:43.751 --> 00:02:48.751
possibly decide what economic policies to enact or what or what social policies

48
00:02:49.391 --> 00:02:53.170
are,
what it technological policies if we don't know the consequences of actions.

49
00:02:53.171 --> 00:02:55.570
That's why,
for example,
she was an example.

50
00:02:55.571 --> 00:02:59.410
It was so stupid for the Republicans to design this healthcare policy and

51
00:02:59.411 --> 00:03:02.980
promoted before anyone analyze,
say the economic impact of it.

52
00:03:03.580 --> 00:03:06.790
I mean they could have still decided to do it.
It's not as if but,

53
00:03:06.820 --> 00:03:11.170
but at least that data would have been useful for making a final decision.

54
00:03:11.710 --> 00:03:15.680
It's that simple.
Hmm.
What,
what,

55
00:03:15.760 --> 00:03:17.500
I guess getting back to that CRISPR thing,

56
00:03:17.770 --> 00:03:22.770
if that becomes available and if it advances to the point where it's available

57
00:03:22.841 --> 00:03:26.500
to people that are alive today,
would would you,
would you give it a shot?

58
00:03:26.550 --> 00:03:29.410
Would you change anything about yourself?
Would you become Thor?

59
00:03:29.980 --> 00:03:32.000
I mean if it really gets to that point when we start,

60
00:03:32.080 --> 00:03:35.800
we can worry about a lot of things when I'm not with me anywhere near as worried

61
00:03:35.801 --> 00:03:38.190
about that as I'm hacking.
Right,
right.
Because if you know,

62
00:03:38.200 --> 00:03:41.920
we can hack computers and if you can hack the DNA as a lot of kids want to do.

63
00:03:41.921 --> 00:03:43.570
In fact,
I was told years ago,

64
00:03:43.780 --> 00:03:45.880
I'm chairman of the board of something called the golden age of atomic

65
00:03:45.881 --> 00:03:49.300
scientists that have the word of sponsors that sets the doomsday clock every

66
00:03:49.300 --> 00:03:52.060
year.
And so we have to think about existential threats to mankind.

67
00:03:52.330 --> 00:03:54.070
I remember about seven or eight years ago,
we had a,

68
00:03:54.470 --> 00:03:58.480
a presser from MIT who said his computer science students were most interested

69
00:03:58.481 --> 00:04:02.410
in hacking DNA,
much more interested than hacking because it's just an ad.

70
00:04:02.620 --> 00:04:03.780
It's just a code,
right?

71
00:04:03.810 --> 00:04:08.770
And so if you could manipulate arbitrarily in a very precise way,
DNA,

72
00:04:09.370 --> 00:04:12.250
then of course there are many good things that can come and maybe you can make

73
00:04:12.251 --> 00:04:15.310
yourself stronger,
bigger,
whatever you want.
Maybe we're not you,

74
00:04:15.311 --> 00:04:18.850
maybe your children,
whatever.
And maybe you can overcome genetic diseases,

75
00:04:18.851 --> 00:04:22.660
which of course would be great,
but you can also with,
you know,

76
00:04:22.690 --> 00:04:25.870
with great power comes great responsibility.
And with that,

77
00:04:25.871 --> 00:04:27.850
you can also imagine hacking,
right?

78
00:04:27.851 --> 00:04:30.870
And creating new viruses or whatever you want.
And so,
yeah,
it's,

79
00:04:30.900 --> 00:04:34.210
it's any new technology is terrifying.

80
00:04:34.540 --> 00:04:37.930
Does that mean we shouldn't create new technologies?
I mean,
cars are terrifying.

81
00:04:38.290 --> 00:04:40.420
Cars kill.
Look how many people cars can kill.

82
00:04:40.421 --> 00:04:43.090
Now maybe you will have self driving cars,
maybe fewer people die.

83
00:04:43.330 --> 00:04:46.840
Some people are afraid of that self driving cars because they do present moral

84
00:04:46.841 --> 00:04:51.440
problems and up as if a car is designed to minimize the number of people that

85
00:04:51.441 --> 00:04:54.040
kills.
And it can do that by killing you.

86
00:04:54.700 --> 00:04:58.120
If you're faced with running into five school children or the car turning and

87
00:04:58.121 --> 00:05:01.240
hitting a wall,
what do you want your car program to do?

88
00:05:01.910 --> 00:05:04.750
And it's fascinating questions we will have to address.

89
00:05:05.170 --> 00:05:07.030
But technology could,

90
00:05:07.031 --> 00:05:11.730
so technology can be used in many ways and it's terrifying.
But uh,
it's,

91
00:05:11.940 --> 00:05:15.910
it's trite to use this old expression,
but I do think of it at times,

92
00:05:15.911 --> 00:05:20.911
which is that the little thing I gave him a stepdaughter once it said ships are

93
00:05:20.921 --> 00:05:23.470
safe in the harbor.
But that's not what ships are meant to do.

94
00:05:25.930 --> 00:05:29.710
I mean,
you can,
you can bury your head in the sand.

95
00:05:29.711 --> 00:05:34.060
You can never go outside the house for fear of being run over by a car or being

96
00:05:34.061 --> 00:05:35.080
embarrassed or whatever.

97
00:05:35.260 --> 00:05:40.210
Or you can choose to live a life as your choice.
But to me,

98
00:05:40.600 --> 00:05:45.040
living the life is as it is more interesting.
That's why in in the,
in the,

99
00:05:45.041 --> 00:05:48.610
in the book I point out,
you can choose how to look at the world.

100
00:05:48.611 --> 00:05:50.980
You can choose to say you're the center of the universe and if that makes you

101
00:05:50.981 --> 00:05:52.990
feel better,
finding the universal was created for you.

102
00:05:53.170 --> 00:05:57.040
Or you can choose to let your beliefs conform to the evidence of reality.
And,

103
00:05:57.350 --> 00:06:00.890
and as soon the universe exists and evolved independent of your existence,

104
00:06:01.040 --> 00:06:03.890
and in that case,
you're bound to be surprised.

105
00:06:04.250 --> 00:06:07.200
Isn't it better to be able to have a life full of surprise and a life that

106
00:06:07.260 --> 00:06:09.230
doesn't have any,
no.
It's a wonderful philosophy.

107
00:06:09.530 --> 00:06:14.300
What I'm thinking is I'm wondering about these technological advancements when

108
00:06:14.301 --> 00:06:17.900
it comes to the ability to manipulate the human body and when they get to the

109
00:06:17.901 --> 00:06:19.440
point where we,

110
00:06:19.510 --> 00:06:22.820
we don't have the same issues that we have today with diseases and when injuries

111
00:06:22.821 --> 00:06:27.620
or even with a biological inferiorities with with everyone looks like Lebron

112
00:06:27.620 --> 00:06:31.790
James.
Yeah,
yeah.
Okay.
That you could imagine that's the case.
I,
I,

113
00:06:32.070 --> 00:06:36.470
I suspect people will want different things and,
but okay,

114
00:06:36.471 --> 00:06:39.920
that'll be a very different world.
But look at this way.
Um,

115
00:06:40.100 --> 00:06:44.090
you're a pretty buff guy.
Okay.
You manipulated your body,
right?
Yes.
Okay.

116
00:06:44.120 --> 00:06:47.880
What's wrong with that?
No,
nothing,
but I'm still five foot eight.
Okay.

117
00:06:47.930 --> 00:06:50.570
The Lebron James is seven feet tall and he manipulated his body.

118
00:06:50.571 --> 00:06:53.270
It's a very different deal.
If someone gives me a seven foot tall pill,

119
00:06:53.271 --> 00:06:54.060
I might take it.
Yeah.

120
00:06:54.060 --> 00:06:58.820
But you manipulate your body given the technology of the time changes and

121
00:06:58.821 --> 00:07:00.130
everyone could be seven feet tall.

122
00:07:00.160 --> 00:07:03.680
It was at worst because you can manage because you know enough physiology now or

123
00:07:03.681 --> 00:07:04.070
whatever,

124
00:07:04.070 --> 00:07:08.090
excess physiology that you can manipulate your body more efficiently now than

125
00:07:08.091 --> 00:07:08.924
you could before.

126
00:07:08.950 --> 00:07:13.950
And people can run faster miles or jump higher up because we've been sports

127
00:07:14.420 --> 00:07:17.120
forever,
and so that's okay.
You know?
That's fine.

128
00:07:17.270 --> 00:07:20.360
I'm just wondered what your thoughts are on the future.
Well,

129
00:07:21.040 --> 00:07:22.850
these physiological imbalance,

130
00:07:22.851 --> 00:07:27.380
I try to anticipate the possibilities and to the extent I can discuss what they

131
00:07:27.381 --> 00:07:32.330
are so that we as a society could address them more cogently.
I do not,
however,

132
00:07:32.331 --> 00:07:35.960
generally make predictions about anything less than 2 trillion years in the

133
00:07:35.961 --> 00:07:36.794
future.

134
00:07:45.430 --> 00:07:47.660
<v 1>[inaudible].</v>

