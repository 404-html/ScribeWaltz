WEBVTT

1
00:00:00.020 --> 00:00:04.020
Messy.
Do you see a way in which this political epic comes to an end?

2
00:00:04.920 --> 00:00:09.920
The only hope that I have is through reasonable dialog becoming an accepted and

3
00:00:14.041 --> 00:00:16.020
appreciated thing,
a celebrated thing,

4
00:00:16.620 --> 00:00:19.100
and that this is possible that people can realize that there's some,

5
00:00:19.110 --> 00:00:24.110
some stupidity to this team mentality that we have is right versus left,

6
00:00:24.391 --> 00:00:29.391
which is almost all a good percentage of it is he's assumed identities,

7
00:00:30.300 --> 00:00:31.170
right?
These,
uh,

8
00:00:31.200 --> 00:00:35.520
these predetermined patterns that get adopted in order to,

9
00:00:35.550 --> 00:00:37.470
as we first started talking about this,

10
00:00:37.540 --> 00:00:41.150
in order to establish yourself as someone who's in a group,
right?

11
00:00:41.340 --> 00:00:45.300
You get accepted by this group and you see it left and right.
I mean,

12
00:00:45.301 --> 00:00:46.410
I don't want to name any names,

13
00:00:46.411 --> 00:00:48.090
but there's a bunch of people that do it blatantly.

14
00:00:48.300 --> 00:00:52.170
You see them and I've even seen him switch teams and you see them switch teams

15
00:00:52.171 --> 00:00:55.680
and I don't buy their rationalizations when it comes to ideology,

16
00:00:55.681 --> 00:00:58.620
but I think is what they're doing is they're switching teams because they

17
00:00:58.621 --> 00:01:01.890
realize there's an in on this team and they can just say this is the problem

18
00:01:01.891 --> 00:01:04.730
with the team.
I used to be on those fucking losers and they're,

19
00:01:04.731 --> 00:01:07.150
they're really benedict Arnold.
Right.
And like,

20
00:01:07.250 --> 00:01:12.250
like they probably have as much of an affinity to the ideas of one side as you

21
00:01:12.261 --> 00:01:13.094
do the other side.

22
00:01:13.230 --> 00:01:16.470
They just go all in on one side to get acceptance from the group.

23
00:01:17.130 --> 00:01:20.700
It is no way people change their opinion that much over two years or something

24
00:01:20.701 --> 00:01:21.480
like that or you know,

25
00:01:21.480 --> 00:01:24.840
it's like they just decide this group makes more sense now and I've been

26
00:01:24.841 --> 00:01:28.170
attacked by people on the left so I'm going to go to the right or vice verse.

27
00:01:28.470 --> 00:01:29.910
And usually what it is is,

28
00:01:30.230 --> 00:01:33.240
I mean when even when they say they had been attacked like,
oh you fucking baby,

29
00:01:33.241 --> 00:01:36.840
there's 300 million people just in this country alone.

30
00:01:37.200 --> 00:01:40.740
If you put something out there publicly and a thousand people attack,

31
00:01:40.741 --> 00:01:44.520
you don't act like you're being persecuted.
Okay?
You have an idea,
you've,
you've,

32
00:01:44.550 --> 00:01:49.200
you've launched that idea out into the zeitgeist and people took a big shit on

33
00:01:49.201 --> 00:01:51.930
it.
You know,
whether it's people on the right are people on the left.

34
00:01:51.931 --> 00:01:55.140
You've got to be able to argue your 0.1 way or the other and not just

35
00:01:55.141 --> 00:01:59.850
immediately jumped ship when someone who shares ideas with you decides that your

36
00:01:59.851 --> 00:02:03.000
idea sucks and maybe they're wrong and maybe you're right,

37
00:02:03.150 --> 00:02:04.800
but you've got to argue that through.

38
00:02:04.890 --> 00:02:09.890
But this idea of these partisan patterns that people just seem to automatically

39
00:02:10.891 --> 00:02:15.570
fall into there are so detrimental to dialogue is so detrimental to us under

40
00:02:15.571 --> 00:02:19.290
really understanding each other and really having some sort of a sense of

41
00:02:19.291 --> 00:02:23.040
community,
right?
This is a giant community of 300 million people.

42
00:02:23.041 --> 00:02:25.590
That's what it's supposed to be.
And this idea that it's,

43
00:02:25.591 --> 00:02:29.040
this group is trying to fuck it up and they're trying to turn us all Muslims and

44
00:02:29.160 --> 00:02:32.550
this one wants everybody to be gay and this one wants everybody to fucking have

45
00:02:32.551 --> 00:02:36.090
free food.
And this and this.
This is nonsense.
This is nonsense.

46
00:02:36.210 --> 00:02:40.290
We need better understanding and,
and you know the word better,

47
00:02:40.291 --> 00:02:41.730
education gets tossed around a lot,

48
00:02:41.880 --> 00:02:46.650
but it also means better social understanding,
better social education.

49
00:02:46.830 --> 00:02:50.910
I got at appreciation of who we are and why we think the way we think and

50
00:02:50.911 --> 00:02:54.030
calling out weasels on both sides of the pattern,

51
00:02:54.330 --> 00:02:57.420
like calling out weasels on the right that are pandering,

52
00:02:57.421 --> 00:03:01.810
that are just trying to like get the repeating a lot of these like accepted

53
00:03:01.811 --> 00:03:04.840
beliefs cause they know that they can hit this frequency and a lot of people

54
00:03:04.841 --> 00:03:07.930
sing along or the same thing that our people are doing on the left.

55
00:03:08.140 --> 00:03:09.490
They're doing an on both sides.

56
00:03:09.670 --> 00:03:14.670
I think most reasonable people have a collection of ideas that they share from

57
00:03:14.921 --> 00:03:19.210
both the right and the left and most reasonable people are reasonably

58
00:03:19.211 --> 00:03:23.260
compassionate.
And I think that's one of the things that we're missing some,

59
00:03:23.670 --> 00:03:27.820
a reasonable sense of not just ethics,

60
00:03:27.970 --> 00:03:31.960
but an appreciation for each other,
for all of us as a group.

61
00:03:32.500 --> 00:03:34.040
And this,

62
00:03:34.360 --> 00:03:39.360
that I think if we can celebrate reasonable conversations and celebrate and

63
00:03:40.571 --> 00:03:42.700
understanding of other people's perspectives,
right?

64
00:03:42.710 --> 00:03:46.690
Like be able to just look at how you're looking at things and have empathy.
Okay,

65
00:03:46.691 --> 00:03:48.520
let me see where you're coming from with this.
Okay,

66
00:03:48.521 --> 00:03:52.300
let me put myself in your shoes.
Okay.
Instead of just immediately like,
fuck you,

67
00:03:52.301 --> 00:03:57.070
you cook and fuck you,
you this.
And instead of thinking about it that way,

68
00:03:57.190 --> 00:04:02.190
if we just tried to just everybody exercise a little bit more so we're a little

69
00:04:02.351 --> 00:04:07.351
bit more calm and come at this from a rational place and try to like realize

70
00:04:07.541 --> 00:04:08.374
like w

71
00:04:08.680 --> 00:04:12.880
<v 1>exclusion.
I've been experimenting with a very dangerous idea,</v>

72
00:04:12.940 --> 00:04:16.530
which is that I keep hearing about chief inclusion officers and you know,

73
00:04:16.540 --> 00:04:21.460
I thought about,
um,
from it,
from Ecclesiastes,
these,
you know,

74
00:04:21.461 --> 00:04:25.840
to every season there is a purpose under heaven.
So if there's inclusion,

75
00:04:25.841 --> 00:04:27.850
there also has to be exclusion,
right?

76
00:04:27.910 --> 00:04:32.680
And like deep platforming or unplanned forming somebody as an act of exclusion.

77
00:04:32.681 --> 00:04:35.620
And very often it's very interesting that the people who are for inclusion are

78
00:04:35.621 --> 00:04:39.220
very focused on the need for deep platforming,
which is an act of exclusion.

79
00:04:39.760 --> 00:04:44.760
So should we have chief exclusion officers that both monitor who is being

80
00:04:45.340 --> 00:04:49.720
excluded,
including,
you know,
somebody like James Demore,
uh,
at Google.

81
00:04:50.020 --> 00:04:55.020
Like is it ethical to exclude him or are there certain voices that need to not

82
00:04:55.391 --> 00:04:58.390
be at some tables in order for something to make progress?

83
00:04:58.391 --> 00:05:02.050
Because if you always have the voice that's the most extreme,

84
00:05:02.051 --> 00:05:04.030
that doesn't accept the game,

85
00:05:04.060 --> 00:05:07.540
then it's very hard to move forward within the game if you're constantly being

86
00:05:07.930 --> 00:05:11.070
reminded,
you know,
so we have this,
um,

87
00:05:11.620 --> 00:05:16.620
we have a series of situations in which it seems like some perspective that very

88
00:05:17.471 --> 00:05:22.471
few people hold terrorizes majorities or you know,

89
00:05:22.570 --> 00:05:27.220
group of people who sort of can more or less get along with each other and keeps

90
00:05:27.221 --> 00:05:32.210
pushing us into this very divided landscape.
And I was just curious,
you know,

91
00:05:32.710 --> 00:05:37.210
in terms of our group of people that we talk and hang out with in common,

92
00:05:37.600 --> 00:05:42.310
um,
where you see the high leverage it is that we're,

93
00:05:42.580 --> 00:05:45.640
we've just finished the midterm,
we've got this 20,
20 election.

94
00:05:45.641 --> 00:05:49.990
It looks to me like Hillary is kind of I and whether she wants to get back in

95
00:05:49.991 --> 00:05:54.490
the game,
um,
this Trump thing has completely,
uh,

96
00:05:55.030 --> 00:05:57.980
you know,
it's like,
it's like the dress is it black and blue

97
00:05:57.980 --> 00:06:02.440
<v 0>or white and gold for like could be eight years.
Yeah.</v>

98
00:06:02.630 --> 00:06:06.830
And I just have you thought about how this ends?

99
00:06:08.180 --> 00:06:08.720
Well,

100
00:06:08.720 --> 00:06:13.720
I would never be so presumptuous to think that I have any idea how this ends.

101
00:06:13.970 --> 00:06:14.630
Okay.
I have,

102
00:06:14.630 --> 00:06:19.630
I've proposed various scenarios to myself and I don't like any of them.

103
00:06:20.630 --> 00:06:23.930
I don't like where it's going because what I worry about,
and this is,
uh,

104
00:06:24.200 --> 00:06:29.200
also again hypocritical that cause I think it probably should burn down and be

105
00:06:30.291 --> 00:06:35.210
rebuilt from the ruins.
We're not going to get such a clean to be clean.
I know.

106
00:06:35.330 --> 00:06:39.230
And this isn't very clean either though.
Honestly.
It's that guy.
One,

107
00:06:39.231 --> 00:06:43.460
it's not clean,
heavy hit.
This is,
he loves Putin,
you know,

108
00:06:43.461 --> 00:06:45.740
this ain't clean.
You know,
the whole thing is weird.

109
00:06:45.741 --> 00:06:48.770
It's the banker's having the amount of influence they have.

110
00:06:48.771 --> 00:06:50.720
The fact that there's two lobbyists.
What is it?

111
00:06:50.780 --> 00:06:55.280
What's the number like two lobbyists to every member of Congress or to lobbyist,

112
00:06:55.320 --> 00:06:59.780
every senator from the pharmaceutical industry.
By the way,

113
00:07:00.140 --> 00:07:04.790
the number of people that have influence over the way,
our laws are shaped.
It's,

114
00:07:04.910 --> 00:07:08.840
it's so fucking bananas right now.
Right.
So,
so off the rails,

115
00:07:09.650 --> 00:07:13.160
is that what it is?
12 but I didn't type in specifically,

116
00:07:13.161 --> 00:07:17.900
but there's a 23 registered lobbyists for every member of,
no,
I think,
uh,

117
00:07:17.990 --> 00:07:20.600
from the pharmaceutical industry they were saying not just,

118
00:07:20.750 --> 00:07:25.280
I think it's too for every member of Congress in the pharmaceutical industry.
Um,

119
00:07:25.760 --> 00:07:29.750
yeah.
The,
the question that you started out with like d platforming people,

120
00:07:31.430 --> 00:07:35.390
I think we're impatient and I think we're,
we're,

121
00:07:35.420 --> 00:07:40.420
we really want to make sure that this vetting of ideas happens quickly because

122
00:07:42.261 --> 00:07:47.240
we see the answer.
We s we see the solution,
we,
we,
we see that this is incorrect.

123
00:07:47.510 --> 00:07:49.790
And we see these people that think the world is flat,
are idiots.

124
00:07:49.791 --> 00:07:51.170
And we think that these people that this,

125
00:07:51.400 --> 00:07:53.750
I think this and think that we think they're all wrong.

126
00:07:53.751 --> 00:07:57.500
And so we want to stop them from talking.
But that doesn't work.

127
00:07:58.040 --> 00:08:00.410
It just works for now.
It,
it,
it,

128
00:08:00.520 --> 00:08:05.240
it oftentimes feeds those ideas and it also,
it,

129
00:08:05.420 --> 00:08:08.120
it,
you have to question like,

130
00:08:08.360 --> 00:08:13.360
why are you so sure why you showed so sure that you are correct that you want,

131
00:08:14.510 --> 00:08:18.860
you don't just want your side to be heard exclusively.
You want to,

132
00:08:19.100 --> 00:08:20.790
you want to silence these,
uh,

133
00:08:20.840 --> 00:08:23.810
other people's ability to participate in this argument,

134
00:08:23.811 --> 00:08:25.010
even if they're totally wrong.

135
00:08:25.940 --> 00:08:30.940
I think that's dangerous because I think that the way to fight off ideas that

136
00:08:31.701 --> 00:08:34.490
aren't good is to introduce ideas that are good and you're gonna,

137
00:08:34.520 --> 00:08:36.980
you're gonna have a bunch of people that agree with the ideas that are bad.

138
00:08:37.610 --> 00:08:42.290
But I think that that's a part of this whole figuring things out.

139
00:08:42.500 --> 00:08:46.530
Like you need to have bad ideas floating around there to appreciate good idea.

140
00:08:46.531 --> 00:08:49.550
So if all the ideas are good,
like what do we do?
Cut it out against.

141
00:08:50.030 --> 00:08:54.020
It's not bad to have these bad ideas broadcast.
What's bad?

142
00:08:54.021 --> 00:08:56.610
To not have someone say,
Hey,
these are bad ideas.

143
00:08:57.060 --> 00:09:00.810
You need to see the pitfalls of racism.
We need to see the pitfalls of crime.

144
00:09:00.960 --> 00:09:04.020
We need to see the pitfalls of corruption.
We need to see it in action.

145
00:09:04.080 --> 00:09:06.090
I think it's like stock market,
swindling.

146
00:09:06.180 --> 00:09:07.740
I think in a lot of ways it's important.

147
00:09:07.860 --> 00:09:10.770
We need to understand that this is a pattern that people fall into continually

148
00:09:10.771 --> 00:09:13.320
over and over again when they have control over the money.

149
00:09:13.470 --> 00:09:16.260
When they have control over that we didn't move the numbers.
What do I do this?

150
00:09:16.380 --> 00:09:18.740
How about if I tell you that this is going to go down and then you invest some

151
00:09:18.750 --> 00:09:21.480
money and I put some money in your bank and we work together,

152
00:09:21.630 --> 00:09:23.760
let's make some money.
This is what people do,
right?

153
00:09:23.761 --> 00:09:26.790
If they just fucking do it over and over and over again,
should you punish him?

154
00:09:26.850 --> 00:09:28.230
Yes,
absolutely,

155
00:09:28.500 --> 00:09:33.500
but I think it's kind of important to see some fucked up behavior just because

156
00:09:34.860 --> 00:09:35.820
we're not done.

157
00:09:35.940 --> 00:09:40.940
We're still in some sort of emotional and psychological and even physical

158
00:09:41.191 --> 00:09:42.900
evolution.
We're in the middle of this thing.

159
00:09:43.350 --> 00:09:47.550
And I think that bad ideas facilitate comprehension,

160
00:09:48.000 --> 00:09:50.550
like these really shitty ideas that a lot of people have.

161
00:09:50.670 --> 00:09:54.720
What they do is they facilitate a comprehension of why we think dumb shit.

162
00:09:55.380 --> 00:09:58.500
And sometimes you don't know why people think dump shit until you see someone

163
00:09:58.501 --> 00:10:01.530
over and over again that thinks dumb shit.
And you get to see that,

164
00:10:01.620 --> 00:10:04.080
whether it's Alex Jones or whether it's to fill in the blank.

165
00:10:04.350 --> 00:10:05.940
What guy do you want the plan?
But I don't.

166
00:10:06.200 --> 00:10:10.920
<v 1>Okay,
so here's my thing.
I want a lot of our leading experts,
the platform.</v>

167
00:10:11.870 --> 00:10:13.270
Okay,
well you're going deep

168
00:10:14.490 --> 00:10:18.430
<v 0>spray paint.
I don't know,
a fucking big a on Tucker Carlson's driveway,
right?</v>

169
00:10:19.450 --> 00:10:20.810
<v 1>We mean?
Well,</v>

170
00:10:22.820 --> 00:10:27.560
if I think about who the great danger is,
uh,
is it Alex Jones,
uh,
you know,

171
00:10:27.561 --> 00:10:29.690
who veers towards tin foil hat land,
uh,

172
00:10:29.720 --> 00:10:34.490
with some frequency or is it the people who were selling weapons of mass

173
00:10:34.491 --> 00:10:38.690
destruction in Iraq as a response to nine 11,
or you know,

174
00:10:38.720 --> 00:10:39.650
the people,

175
00:10:40.190 --> 00:10:44.150
let's assume that you're a reasonable person on immigration.

176
00:10:44.510 --> 00:10:48.510
You neither think that borders should be opened or closed.
Um,

177
00:10:48.560 --> 00:10:51.050
then you start hearing professors say,
you know,

178
00:10:51.051 --> 00:10:52.330
the great thing about immigration is,

179
00:10:52.331 --> 00:10:55.490
is that it has absolutely no costs and all of them are better than all of our

180
00:10:55.491 --> 00:10:59.210
people because uh,
you know,
they're highly trained,
they're highly motivated,

181
00:10:59.211 --> 00:11:02.090
they're young.
You're thinking like,
okay,

182
00:11:02.091 --> 00:11:05.280
what kind of thing has all benefits and no costs.
You're not a,

183
00:11:05.281 --> 00:11:08.960
you're not even entering into the rational description and now we're hearing

184
00:11:08.961 --> 00:11:13.280
like all these trade deals that got negotiated.
Yeah,
that kind of wasn't true.

185
00:11:13.310 --> 00:11:14.990
All of those things that we were telling you that if you,

186
00:11:14.991 --> 00:11:18.410
if you question these things,
you are backward protectionist and you were just,

187
00:11:18.470 --> 00:11:21.940
you were stuck in the old world and you couldn't embrace the new,
yeah,

188
00:11:21.950 --> 00:11:23.860
that was all bullshit.
Um,

189
00:11:24.470 --> 00:11:29.300
what I think is we have a crisis and expertise,
institutional expertise,

190
00:11:29.880 --> 00:11:32.900
uh,
is at an all time low.

191
00:11:32.901 --> 00:11:37.901
Nobody really trusts any of our institutions to be an authoritative source of

192
00:11:37.941 --> 00:11:38.774
ground truth.

193
00:11:39.020 --> 00:11:43.040
It's not to say that everything that the institution say is wrong or everything

194
00:11:43.041 --> 00:11:44.690
the experts say is wrong,
far from it.

195
00:11:45.050 --> 00:11:48.380
It's just that there are almost no experts or institutions that aren't willing

196
00:11:48.381 --> 00:11:51.290
to distort facts in order to pursue institutional goals.

197
00:11:52.160 --> 00:11:55.570
That that's a giant issue.
Right?
Right.

198
00:11:55.600 --> 00:11:58.480
And so I don't actually want a deep platform these people,

199
00:11:58.481 --> 00:12:00.930
but I do have the very strong sense,
you know,
when,
when,

200
00:12:01.180 --> 00:12:05.650
when Ilan came on your show and Peter Teal,
um,

201
00:12:05.710 --> 00:12:07.990
my friend and boss came on Dave Rubin show,

202
00:12:08.290 --> 00:12:12.940
I thought that was quite a moment where this alternate network of distribution,

203
00:12:13.210 --> 00:12:15.400
which is not under centralized control,

204
00:12:15.940 --> 00:12:19.960
started to be seen as comparably powerful and important.

205
00:12:20.440 --> 00:12:24.880
And I think some of the noises that Tucker Carlson just made today,
Ruben about,

206
00:12:24.910 --> 00:12:25.420
well,
hey,

207
00:12:25.420 --> 00:12:29.080
you're doing this out of your garage and you have the freedom to do anything.

208
00:12:29.081 --> 00:12:33.970
And I'm,
I'm beholding to the structure in which I live,
um,
word,

209
00:12:34.000 --> 00:12:39.000
a very interesting place with respect to what is this thing and this alternate

210
00:12:41.381 --> 00:12:46.180
distribution network for,
for ideas that's an policed by the institutions.

211
00:12:46.630 --> 00:12:50.440
And you know,
I think I've been convinced in the last two days that I need,

212
00:12:50.980 --> 00:12:53.350
this is advice that I got from you at the beginning.
He said,

213
00:12:53.351 --> 00:12:54.640
you need to start a podcast.

214
00:12:55.300 --> 00:12:59.360
I think I need to start a podcast and I think you need to start a podcast just

215
00:12:59.361 --> 00:13:00.150
to keep going on,

216
00:13:00.150 --> 00:13:03.790
on about the hop thing until people figured it out just about the hot thing.

217
00:13:04.480 --> 00:13:05.380
But we have to,

218
00:13:05.410 --> 00:13:10.410
we have to return to some kind of stable sanity that I'm positive that the

219
00:13:11.411 --> 00:13:14.380
institutions can't return us to because the,

220
00:13:14.530 --> 00:13:17.740
the institutional interests,
uh,

221
00:13:17.800 --> 00:13:21.220
really have to do with the fact that certain kinds of growth on which they're

222
00:13:21.221 --> 00:13:24.310
predicated,
their existence is predicated if evaporated.

223
00:13:24.790 --> 00:13:29.790
So all of these institutions are extremely vulnerable to corruption at the

224
00:13:30.881 --> 00:13:31.880
moment.
And the,

225
00:13:31.950 --> 00:13:36.950
the real revolution as I'm seeing it is that high agency individuals are out

226
00:13:37.421 --> 00:13:42.370
competing,
traditional institutional structures.
Um,
in terms of mind share.

227
00:13:42.400 --> 00:13:46.090
And some of those high agency individuals are irresponsible,
you know,

228
00:13:46.091 --> 00:13:50.380
and they're like Milo types that are kind of trying to light things up and some

229
00:13:50.381 --> 00:13:53.700
of them are extremely responsible and some of them,
you know,

230
00:13:53.710 --> 00:13:56.380
we'll do a few irresponsible things but will self correct.

231
00:13:56.440 --> 00:14:01.440
And this new world that is being born is a huge check on the institutions,

232
00:14:01.811 --> 00:14:03.460
but it's still largely separate.
Like,

233
00:14:03.461 --> 00:14:07.240
am I right that you don't do a lot of network television?
Oh,
do ain't.
Yeah.

234
00:14:07.690 --> 00:14:11.800
Anymore.
But I used to,
I mean,
that's how I became famous in the first place.

235
00:14:11.830 --> 00:14:15.370
Right.
You know?
Um,
but yeah,
I don't do it anymore,

236
00:14:15.400 --> 00:14:17.740
but it's also because there's nothing fun out there like this.

237
00:14:18.460 --> 00:14:21.640
Like there's no place for this.
Right.
Other than this,

238
00:14:21.700 --> 00:14:23.170
this is the only place you could do this.

239
00:14:23.410 --> 00:14:26.860
But isn't it interesting to you that we still have,

240
00:14:26.861 --> 00:14:31.861
not like Jordan had to be dealt with by the mainstream because the book was too

241
00:14:33.071 --> 00:14:35.410
big,
his affect was too large?

242
00:14:36.100 --> 00:14:41.020
I think his effect on the Internet is bigger than the book.
I think the,

243
00:14:41.030 --> 00:14:44.740
the youtube videos and the debates that he has,
the one that I was telling you,

244
00:14:44.741 --> 00:14:48.070
the recent one interview with Gq,
interesting.
It's really good.

245
00:14:48.310 --> 00:14:52.190
The woman's very smart,
but she gets trounced and it's because he's been

246
00:14:52.250 --> 00:14:56.930
<v 0>in the trenches with this stuff for a long time.
I mean he,
he's,</v>

247
00:14:56.960 --> 00:15:01.490
he's fighting a very strange fight of dialogue and of interpretation and have

248
00:15:01.491 --> 00:15:03.420
discussion and,
and

249
00:15:05.020 --> 00:15:06.340
<v 1>the freedom of</v>

250
00:15:07.030 --> 00:15:09.250
<v 0>intellectual sovereignty.
You know,</v>

251
00:15:09.251 --> 00:15:13.630
there's a lot of people that want you to think a very certain way and use

252
00:15:13.631 --> 00:15:18.340
certain words and say certain things and it doesn't matter whether or not you

253
00:15:18.341 --> 00:15:22.390
are in fact racist or sexist or homophobic or whatever.

254
00:15:22.391 --> 00:15:26.620
This is a weird battle of control going on that it's the heart of it as much as

255
00:15:26.621 --> 00:15:31.621
it is a battle of inclusion and diversity and strengthening our overall

256
00:15:31.751 --> 00:15:34.480
progressive mindset.
There's a little bit of that too,

257
00:15:34.660 --> 00:15:38.710
but there's also an undeniable game that's being played and people want to win.

258
00:15:38.950 --> 00:15:40.630
There's scores that are being scored.

259
00:15:40.631 --> 00:15:43.240
There's points on the board that throwing in new agents,

260
00:15:43.241 --> 00:15:46.720
they have teams going at it and whenever Jordan goes on one of these

261
00:15:46.721 --> 00:15:47.680
conversations,

262
00:15:47.890 --> 00:15:51.490
these video interviews and as a feminist and Jordan Peterson like this is a

263
00:15:51.491 --> 00:15:54.220
fucking game going on.
We're watching a soccer match,

264
00:15:54.400 --> 00:15:57.150
we're watching a wrestling match.
This is Jujitsu there.

265
00:15:57.260 --> 00:16:01.420
They're playing intellectual Jujitsu and Jordan's really good at tapping people.

266
00:16:01.510 --> 00:16:04.450
He's really good at it and they get pissed.
They keep sending in new chicks.

267
00:16:05.770 --> 00:16:07.780
They sit in that Kathy Newman lady and she's like,

268
00:16:07.810 --> 00:16:11.590
so what you're saying is that didn't work either and she just got devastated.

269
00:16:11.830 --> 00:16:15.730
She got rocked and this is what's happening over and over and over again because

270
00:16:15.731 --> 00:16:18.940
whether you appreciate what he's saying or not,

271
00:16:19.420 --> 00:16:22.660
he has some facts that are undeniable.

272
00:16:22.810 --> 00:16:26.260
He has some positions that are based on a rich understanding of history and of

273
00:16:26.261 --> 00:16:30.190
Marxism,
communism and a lot of the problems with

274
00:16:30.420 --> 00:16:33.690
<v 1>people with compelled</v>

275
00:16:33.970 --> 00:16:37.150
<v 0>thoughts.
If you're compelling people to behave a certain way,</v>

276
00:16:37.300 --> 00:16:41.260
compelling people to talk a certain way.
And we're not talking about,
you know,

277
00:16:41.830 --> 00:16:45.280
compelling people to not commit crimes or violence.
We're talking about

278
00:16:45.510 --> 00:16:49.470
<v 1>weird things like compelled pronouns.
And so if I take,</v>

279
00:16:49.471 --> 00:16:52.120
if I take your analogy cause you brought it up,
um,

280
00:16:52.470 --> 00:16:57.150
that he's like doing Jujitsu.
So in some previous era,

281
00:16:57.210 --> 00:17:01.710
and I thought your description of the early days of MMA was fascinating that we

282
00:17:01.711 --> 00:17:04.170
just didn't know what fighting was,

283
00:17:04.550 --> 00:17:07.140
so we didn't know who would win or what systems worked.

284
00:17:07.260 --> 00:17:12.260
And if you think about the mainstream media is like Aye Ay Ay Kido.

285
00:17:13.610 --> 00:17:13.950
Yeah.

286
00:17:13.950 --> 00:17:18.950
Some system that maybe has some valid validity in some very rarefied context and

287
00:17:19.681 --> 00:17:23.490
it comes into general purpose fighting systems and it's,

288
00:17:24.000 --> 00:17:25.980
it's dismantled very quickly.

289
00:17:26.520 --> 00:17:31.520
So now we have this weird situation that we've got this new world of kind of

290
00:17:33.930 --> 00:17:35.070
rule laden.

291
00:17:35.071 --> 00:17:39.960
Anything goes discussions more or less and the mainstream world doesn't want,

292
00:17:39.990 --> 00:17:44.070
like the Ikea world doesn't want to acknowledge that this weird UFC type thing

293
00:17:44.071 --> 00:17:47.190
is happening.
How long does that go on?

294
00:17:47.760 --> 00:17:50.040
It goes on for as long as it knit.
And this is

295
00:17:50.040 --> 00:17:53.130
<v 0>similar to I think the what's happening intellectually,</v>

296
00:17:53.160 --> 00:17:56.340
and this is one of the reasons why I don't think you should stop people from

297
00:17:56.341 --> 00:17:59.460
expressing these bad ideas is one thing.
Stopping people to say,
hey,

298
00:17:59.461 --> 00:18:01.440
we need to kill black people.
Stopping people to said,

299
00:18:01.441 --> 00:18:03.600
we need to kill white people.
We need to kill,
fill in the blank.

300
00:18:03.601 --> 00:18:06.510
Whatever the group is.
Yeah,
that's,
that's different.
You're,
you're,

301
00:18:06.580 --> 00:18:11.580
you're clearly stepping outside of the realm of civilization and into war and

302
00:18:12.841 --> 00:18:16.620
violence and we could all collectively decide and we should all collectively

303
00:18:16.621 --> 00:18:19.140
decided we should have ethics together.

304
00:18:19.170 --> 00:18:21.690
Like whether it's right or left or in the middle,
we should all decide,
hey,

305
00:18:21.691 --> 00:18:24.120
you can't do that because what you're doing is you're,

306
00:18:24.300 --> 00:18:27.510
you're calling for violence against someone who's not committing any violence.

307
00:18:27.511 --> 00:18:30.240
Can I pause you right there?
I think there's a really interesting point.
Okay.

308
00:18:32.090 --> 00:18:36.600
Let's assume that we know that that behavior needs to be down regulated in some

309
00:18:36.601 --> 00:18:37.261
way.
You can,

310
00:18:37.261 --> 00:18:40.320
you can try to silence the person where we just physically duct taped them so

311
00:18:40.321 --> 00:18:44.730
they can't say anything.
Right.
You know,
we,
we put them in jail.
Uh,
we won't,

312
00:18:44.731 --> 00:18:47.880
don't give them access to the media,
et Cetera,
et cetera.
Um,

313
00:18:48.160 --> 00:18:51.570
or we can shame them or we can kind of take them aside.

314
00:18:51.840 --> 00:18:56.460
At what layer of this sort of communication stack?
It's a very good question.

315
00:18:56.461 --> 00:18:57.420
We should,

316
00:18:57.630 --> 00:19:02.370
because I think one of the things that we haven't done is to positively say we

317
00:19:02.371 --> 00:19:06.510
agree with you that the speech is offensive and it is potentially dangerous,

318
00:19:06.750 --> 00:19:10.710
but we think it should be down regulated differently than the D platforming

319
00:19:10.711 --> 00:19:13.740
option.
Well,
the D platforming option,

320
00:19:13.741 --> 00:19:18.660
the real issue is there's only a few different avenues for these people to

321
00:19:18.661 --> 00:19:21.460
express themselves publicly.
Okay.
Right.
And the,

322
00:19:21.461 --> 00:19:22.800
the argument that's really strange.

323
00:19:22.801 --> 00:19:27.780
It should these be regulated like a utility or should they be thought of as

324
00:19:27.781 --> 00:19:31.560
private businesses get to decide what's on their channel,
essentially?

325
00:19:31.620 --> 00:19:33.250
Like it's almost like a,
uh,
uh,

326
00:19:33.450 --> 00:19:38.450
a private NBC that everyone can broadcast out if it's none of the above.

327
00:19:38.791 --> 00:19:41.960
What if the problem is,
we're trying to pretend,
is it like a dinner party?

328
00:19:41.961 --> 00:19:45.300
Is that the public square?
Is that the utility?
And it's none of these things.

329
00:19:46.050 --> 00:19:49.830
I think these ideas,
what I was discussing,
that like there's,

330
00:19:49.950 --> 00:19:54.950
there's a reason why good ideas and bad ideas should go to war is the same

331
00:19:55.411 --> 00:19:56.250
reason why,

332
00:19:56.251 --> 00:20:01.251
even though I kind of knew that most Kung Fu was bullshit before the UFC,

333
00:20:01.860 --> 00:20:06.420
I want those guys to get in there and try,
oh,
you got some death touch.
Hey,

334
00:20:06.830 --> 00:20:10.500
come on in.
I want to,
I'll introduce you to a guy,
you know,
this is,
uh,

335
00:20:10.860 --> 00:20:12.940
his name's Cain Velasquez and on,

336
00:20:12.950 --> 00:20:15.870
you're going to try your death touch and now he's just going to arrest me to the

337
00:20:15.871 --> 00:20:19.320
ground and beat your fucking brains in.
Okay.
Right.
But that's not gonna happen.

338
00:20:19.321 --> 00:20:21.930
Cause you know,
death touch,
good luck.
And you let them Duke it out.

339
00:20:22.050 --> 00:20:26.370
And that is the battlefield of ideas.
But it is a little,
no,
no,
no,
no.
But,

340
00:20:26.610 --> 00:20:29.380
but when you do platform people,
that's when it's not happening with you.

