WEBVTT

1
00:00:00.300 --> 00:00:01.230
My question is like,

2
00:00:01.290 --> 00:00:04.860
I know how much time you must be spending on your Tesla factory.

3
00:00:05.040 --> 00:00:09.090
I know how much time you must be spending on space x and yet you still have time

4
00:00:09.091 --> 00:00:13.110
to dig holes under the ground in La and come up with these ideas and then

5
00:00:13.111 --> 00:00:13.944
implement them.
Guess

6
00:00:14.140 --> 00:00:18.340
<v 1>I got a million ideas.
I'm sure you did.
No shortage of that.
Yeah.</v>

7
00:00:18.610 --> 00:00:21.040
<v 0>I just don't know how you manage your time.
I don't understand it.</v>

8
00:00:21.041 --> 00:00:23.920
It doesn't seem,
it doesn't even seem humanly possible.

9
00:00:25.460 --> 00:00:27.750
<v 1>You know,
I do.
Basically,</v>

10
00:00:27.920 --> 00:00:31.040
I think we all like don't totally understand what I do with my time.

11
00:00:32.090 --> 00:00:35.590
They think like I'm a business guy or something like that.
Um,

12
00:00:35.900 --> 00:00:39.620
like my Wikipedia page says business magnate,
what would you call yourself?

13
00:00:40.580 --> 00:00:45.260
A business magnet.
Kids aren't,
please change my Wikipedia page two magnet.

14
00:00:45.410 --> 00:00:48.160
They'll change it right now it's locked.

15
00:00:48.260 --> 00:00:51.530
So somebody has to be able to unlock it and change it to magnets.
Yeah.

16
00:00:51.531 --> 00:00:54.990
What did we have magnet?
Um,
no,
I,
I,

17
00:00:55.040 --> 00:00:59.600
I do engineering and you know,
and manufacturing and that kind of thing.

18
00:00:59.690 --> 00:01:02.900
That's like 80% or more of my time ideas.

19
00:01:02.901 --> 00:01:06.860
And then the implementation of those ideas that was like hardcore engineering,

20
00:01:06.890 --> 00:01:09.710
like yeah,
designing things,
you know?
Right.

21
00:01:12.260 --> 00:01:15.200
Structural,
mechanical,
electrical,
software,

22
00:01:16.110 --> 00:01:20.420
a user interface,
engineering,
aerospace,
engineering.

23
00:01:21.200 --> 00:01:23.870
<v 0>But you must understand this,
not a whole lot of human beings like you.</v>

24
00:01:23.930 --> 00:01:28.820
You know that,
right?
So you're an audit teams.
Yes.
Chimps like me,

25
00:01:29.720 --> 00:01:32.840
we're all chimps.
Yeah,
we are.
We're one notch,
one notch above a chimp.

26
00:01:33.020 --> 00:01:36.020
Some of us are a little more confused when I watched you do and all these
things.

27
00:01:36.021 --> 00:01:36.291
I'm like,

28
00:01:36.291 --> 00:01:41.291
how does this motherfucker have all this time and all this energy and all these

29
00:01:41.421 --> 00:01:43.760
ideas and then people just let them do these things

30
00:01:45.250 --> 00:01:46.083
<v 1>because I'm an alien.</v>

31
00:01:46.350 --> 00:01:50.970
<v 0>That's what I've speculated.
Yes.
I'm on record saying this in the past.</v>

32
00:01:51.120 --> 00:01:54.000
I wonder.
It's true.
I mean if there was one,
I was like,

33
00:01:54.001 --> 00:01:57.960
if there was like maybe an intelligent being that we created,
you know,

34
00:01:57.961 --> 00:02:01.500
like some AI creature that's a superior to people.

35
00:02:02.130 --> 00:02:04.320
Maybe they just hang around with us for a little while like you've been doing

36
00:02:04.380 --> 00:02:05.520
and then fix a bunch of shit.

37
00:02:06.230 --> 00:02:10.530
I mean that's the way that might have some rotation or something like that.

38
00:02:10.560 --> 00:02:12.360
You might,
do you think you do?
Probably.

39
00:02:12.390 --> 00:02:15.000
Do you wonder like are you around normal people?
You're like,
hmm.

40
00:02:17.940 --> 00:02:20.600
You like,
what's up with these boring dumb mother fuckers ever?

41
00:02:20.990 --> 00:02:22.310
<v 1>Not Bad for a human,</v>

42
00:02:23.450 --> 00:02:28.450
but I think it will not be able to hold a candle to AI.

43
00:02:29.470 --> 00:02:30.303
<v 0>Hmm.</v>

44
00:02:30.490 --> 00:02:33.700
You scare the shit out of me when you talk about Ai between you and Sam Harris.

45
00:02:33.820 --> 00:02:37.780
Oh sure.
Consider it until I had a podcast with Sam once was great.

46
00:02:37.930 --> 00:02:42.070
He made me Shit my pants.
Talking about Ai,
I realize like,

47
00:02:42.100 --> 00:02:44.350
oh well this is a genie that wants,
it's out of the bottle.

48
00:02:44.351 --> 00:02:45.640
You never getting it back in.

49
00:02:46.560 --> 00:02:47.393
<v 1>That's true.</v>

50
00:02:48.360 --> 00:02:53.360
<v 0>There was a video that you tweeted about one of those Boston dynamic robots and</v>

51
00:02:54.140 --> 00:02:58.620
you know like in the future it'll be moving so fast you can't see it without a

52
00:02:58.621 --> 00:02:59.454
strobe.

53
00:02:59.540 --> 00:03:01.760
<v 1>Yeah.
You could probably do that right now</v>

54
00:03:03.380 --> 00:03:07.190
<v 0>and no one's really paying attention too much other than people like you or</v>

55
00:03:07.191 --> 00:03:08.840
people that are really obsessed with technology.

56
00:03:09.020 --> 00:03:11.990
All these things are happening and these robots are,

57
00:03:12.090 --> 00:03:13.160
and did you see the one where it,

58
00:03:13.161 --> 00:03:16.550
to Pete Peta put out statement that you shouldn't kick robots?

59
00:03:17.330 --> 00:03:22.310
It's probably not wise
for retribution there.
Their memory is probably good.

60
00:03:22.730 --> 00:03:25.310
I bet it's really good.
It's really good.
I bet it is.

61
00:03:25.700 --> 00:03:27.790
And getting better every day.
It's really good.

62
00:03:27.890 --> 00:03:30.870
Are you honestly legitimately concerned about this?
Are you,

63
00:03:31.140 --> 00:03:36.140
is like AI one of your main worries in regards to the future?

64
00:03:41.350 --> 00:03:46.180
<v 1>Yes.
It's less of a worry than it used to be a mostly due to</v>

65
00:03:48.040 --> 00:03:50.390
taking more of a fatalistic attitude.
Mm.

66
00:03:51.770 --> 00:03:56.770
<v 0>So you used to have more hope and you gave up some of it and now you don't worry</v>

67
00:03:57.381 --> 00:04:00.530
as much about AI you like this is just what it is.

68
00:04:03.620 --> 00:04:03.890
<v 2>Yeah,</v>

69
00:04:03.890 --> 00:04:07.160
<v 1>pretty much.
Yes.
Yes.
It's,
it's no,</v>

70
00:04:08.030 --> 00:04:12.470
it's not necessarily bad.
It's just,

71
00:04:12.680 --> 00:04:17.540
it's definitely going to be outside of human control.
Not necessarily bad.
Right?

72
00:04:17.620 --> 00:04:19.790
Yeah,
it's not,
it's not necessarily bad,
it's just,

73
00:04:20.900 --> 00:04:22.530
it's just outside of human control.
Now,

74
00:04:22.580 --> 00:04:27.580
the thing that's going to be tricky here is that it's going to be very tempting

75
00:04:28.491 --> 00:04:32.360
to use AI as a weapon.
That's going be a very tempting,
in fact,

76
00:04:32.450 --> 00:04:35.230
it will be used as a weapon.
Um,

77
00:04:37.040 --> 00:04:41.660
so the,
the,
the,
the onramp to serious AI,

78
00:04:43.340 --> 00:04:48.110
the danger is going to be more humans using it against each other.

79
00:04:48.111 --> 00:04:53.060
I think most likely that'll be the danger.
Yeah.

80
00:04:53.590 --> 00:04:58.210
<v 0>How far do you think we are from something that can make its own mind up?</v>

81
00:04:58.211 --> 00:05:01.330
Whether or not something's ethically or morally correct or whether or not it

82
00:05:01.331 --> 00:05:04.540
wants to do something or whether or not it wants to improve itself or whether or

83
00:05:04.541 --> 00:05:08.320
not it wants to protect itself from people or from other Ai.

84
00:05:08.440 --> 00:05:11.320
How far away are we from something that's really truly sentient?

85
00:05:16.730 --> 00:05:17.180
<v 1>Well,</v>

86
00:05:17.180 --> 00:05:22.180
I mean you could argue that any group of people like it,

87
00:05:22.400 --> 00:05:27.400
like a company is essentially a cybernetic collective of people and machines.

88
00:05:29.870 --> 00:05:31.100
That's what a company is.

89
00:05:34.010 --> 00:05:36.230
And then there are different,

90
00:05:37.970 --> 00:05:41.900
there's different levels of complexity in the way these companies are formed.

91
00:05:43.580 --> 00:05:48.580
And then there are sort of does this or like a collective AI and in the Google

92
00:05:50.151 --> 00:05:52.490
sort of search,
Google search,
you know the,

93
00:05:53.990 --> 00:05:57.110
where we're all sort of plugged in as like nodes.

94
00:05:57.230 --> 00:06:00.080
The network like leaves on a big tree

95
00:06:02.280 --> 00:06:07.220
off and we're all,
we're all feeding this network without questions.
Announcers,

96
00:06:08.420 --> 00:06:11.090
rural collectively programming,
Bai

97
00:06:12.670 --> 00:06:14.620
and that and Google plus the,

98
00:06:14.680 --> 00:06:19.550
all the humans that connect to it are one giant cybernetic collective.

99
00:06:20.150 --> 00:06:25.150
This is also true of Facebook and Twitter and Instagram and all these social

100
00:06:25.161 --> 00:06:28.880
networks.
The giants,
I haven't had a collectives

101
00:06:31.310 --> 00:06:35.500
<v 0>and humans and electronics all interfacing and constantly now constantly</v>

102
00:06:35.501 --> 00:06:36.250
connected.

103
00:06:36.250 --> 00:06:38.590
<v 1>Yes.
Constantly.</v>

104
00:06:39.920 --> 00:06:43.400
<v 0>One of the things that I've been thinking about a lot over the last few years is</v>

105
00:06:43.401 --> 00:06:48.401
that one of the things that drives a lot of people crazy as how how many people

106
00:06:48.741 --> 00:06:51.470
were obsessed with materialism and getting the latest greatest thing.

107
00:06:51.920 --> 00:06:55.470
And I wonder how much of that is,
well,

108
00:06:55.550 --> 00:06:59.600
a lot of it is most certainly fueling technology and innovation and it almost

109
00:06:59.601 --> 00:07:03.110
seems like it's built into us to like what we like and what we want.

110
00:07:03.320 --> 00:07:07.280
That we're fueling this thing that's constantly around us all the time and it

111
00:07:07.281 --> 00:07:09.470
doesn't seem possible that people are going to pump the brakes.

112
00:07:09.500 --> 00:07:11.690
It doesn't seem possible at this stage.

113
00:07:11.870 --> 00:07:14.360
We'll work constantly [inaudible] newest cell phone,

114
00:07:14.600 --> 00:07:17.180
the latest Tesla update the newest Mac book pro,

115
00:07:17.181 --> 00:07:22.181
but everything has to be newer and better and that's going to lead to some

116
00:07:24.230 --> 00:07:26.960
incredible point.
And it,

117
00:07:27.140 --> 00:07:31.700
it seems like it's built into us and almost seems like it's an instinct that we

118
00:07:31.701 --> 00:07:35.600
were working towards this,
that we'd like it.
Our job,

119
00:07:35.601 --> 00:07:39.380
just like the ants build the ant hill,
our job is to somehow know the fuel.
This.

120
00:07:42.620 --> 00:07:43.590
<v 1>Yes.
Um,</v>

121
00:07:44.030 --> 00:07:49.030
I mean I made those comments some some years ago but it feels like we are the

122
00:07:49.641 --> 00:07:54.290
biological bootloader for AI.
Effectively.
We are building it

123
00:07:56.150 --> 00:08:01.150
and then we are building progressively greater intelligence and the percentage

124
00:08:03.291 --> 00:08:08.291
of intelligence that is not human is increasing and eventually we will represent

125
00:08:09.741 --> 00:08:11.810
a very small percentage of intelligence.

126
00:08:15.070 --> 00:08:15.401
But the,

127
00:08:15.401 --> 00:08:20.401
the AI is informed strangely by the human limbic system.

128
00:08:22.440 --> 00:08:26.250
It is in large part our Ed Red Lodge.

129
00:08:28.210 --> 00:08:32.950
How so?
Well you mentioned all those things,
the sort of primal drives.

130
00:08:34.210 --> 00:08:35.043
Um,

131
00:08:35.320 --> 00:08:40.320
there's all whole things that we like and hate and fear.

132
00:08:44.560 --> 00:08:49.060
They're all there on the Internet.
The projection of our limbic system.

133
00:08:57.960 --> 00:09:00.720
<v 0>No,
it makes sense.
The thinking of it as a,</v>

134
00:09:01.700 --> 00:09:05.010
I mean think of thinking of corporations and just thinking of just human beings

135
00:09:05.011 --> 00:09:07.740
communicating online through these social media networks as some sort of an

136
00:09:07.741 --> 00:09:12.330
organism.
That's a,
it's a Cyborg.
It's a combination.

137
00:09:12.660 --> 00:09:14.910
It's a combination of electronics and biology.
Yeah.

138
00:09:18.010 --> 00:09:18.843
<v 1>This is,
it's a,</v>

139
00:09:18.850 --> 00:09:23.850
it's a measure like it's the success of these online systems is that is f is

140
00:09:24.431 --> 00:09:25.720
sort of a function of,

141
00:09:28.000 --> 00:09:33.000
of how much limbic resonance they're able to achieve with people.

142
00:09:34.330 --> 00:09:37.060
The more limbic resonance,
the more engagement.

143
00:09:38.090 --> 00:09:38.923
<v 2>Mm.</v>

144
00:09:39.940 --> 00:09:44.770
<v 0>Whereas like one of the reasons why I probably Instagram is more enticing than</v>

145
00:09:44.771 --> 00:09:49.420
Twitter.
Limbic resonance.
You get more images,

146
00:09:49.421 --> 00:09:52.870
more video.
Yes.
It's tweaking your system more.
Yes.

147
00:09:53.260 --> 00:09:56.320
Do you worry about what or wonder in fact of what the next step is?

148
00:09:57.760 --> 00:09:59.780
A lot of people didn't see Twitter coming that,
you know,

149
00:09:59.890 --> 00:10:04.870
communicate with 140 characters or 280 now would be a thing that people would be

150
00:10:04.871 --> 00:10:09.090
interested in.
Like it's going to excel,

151
00:10:09.120 --> 00:10:11.130
it's going to become more connected to us,
right?

152
00:10:13.000 --> 00:10:13.833
<v 2>Yeah.</v>

153
00:10:13.850 --> 00:10:17.210
<v 1>Yes.
Things are getting more and more connected there.
At this point,</v>

154
00:10:17.211 --> 00:10:21.440
constrained by bandwidth.
Our input output is low,

155
00:10:21.710 --> 00:10:25.010
particularly output.
Awkward got worse with thumbs.
You know,

156
00:10:25.011 --> 00:10:28.460
we used to have input with 10 10 fingers,
no,
we have thumbs,

157
00:10:30.710 --> 00:10:35.480
but images are just oral.
So are there a way of communicating at high bandwidth?

158
00:10:36.040 --> 00:10:39.690
You take pictures and you send pictures of people,
let's sends,
that's,

159
00:10:39.720 --> 00:10:42.880
that communicates far more information than you can communicate with them.

160
00:10:43.300 --> 00:10:48.130
<v 0>So what happened with you where you decided or you took on a more fatalistic</v>

161
00:10:48.131 --> 00:10:49.350
attitude?
Like what?

162
00:10:49.390 --> 00:10:52.810
Was there any specific thing or was it just the inevitability of our future?

163
00:11:03.310 --> 00:11:05.020
<v 1>I tried to convince people to slow down,</v>

164
00:11:05.860 --> 00:11:08.830
slow down AI to regulate AI.

165
00:11:11.290 --> 00:11:14.530
This was futile.
I tried for years.

166
00:11:16.060 --> 00:11:18.500
<v 0>Nobody listened in a movie,
nobody robots,</v>

167
00:11:18.520 --> 00:11:21.910
good fucking take over and you're freaking me out.
Nobody listened.

168
00:11:22.000 --> 00:11:25.570
Nobody listens.
No one are people more inclined to listen today.

169
00:11:25.571 --> 00:11:29.710
It seems like an issue that's brought up more often over the last few years than

170
00:11:29.711 --> 00:11:32.920
it was maybe five,
10 years ago.
It seemed like science fiction,

171
00:11:35.400 --> 00:11:38.040
<v 1>maybe they will.
So far they haven't.</v>

172
00:11:41.400 --> 00:11:44.700
I think people don't like the normally the way that's regulations work.

173
00:11:44.960 --> 00:11:49.230
It was very slow,
very slow indeed,
so

174
00:11:52.110 --> 00:11:54.190
usually it'll be something some when your technology

175
00:11:55.780 --> 00:12:00.700
will cause damage or death,
there will be an outcry.

176
00:12:02.110 --> 00:12:04.870
There will be an investigation.
Years will pass.

177
00:12:05.410 --> 00:12:08.620
There will be some sort of insight committee.

178
00:12:08.950 --> 00:12:13.540
They will be rulemaking than there will be oversight.
Eventually regulations.

179
00:12:14.410 --> 00:12:18.070
This all takes many years.
This is the normal course of things.

180
00:12:18.100 --> 00:12:21.160
If you look at say what a motive regulations,

181
00:12:21.161 --> 00:12:25.480
how long did it take for seat belts to be,
to be implemented?

182
00:12:25.481 --> 00:12:29.050
To be required.
You know,
the auto industry fought seat belts,

183
00:12:29.051 --> 00:12:30.790
I think for more than a decade,

184
00:12:31.840 --> 00:12:35.320
successfully fought any regulations on seat belts,

185
00:12:35.350 --> 00:12:39.520
even though the numbers were extremely obvious,

186
00:12:39.700 --> 00:12:41.680
if you had a seat belts on,

187
00:12:42.390 --> 00:12:46.840
you would be far less likely to die or be seriously injured.
Unequivocal.

188
00:12:49.170 --> 00:12:52.690
And the industry fought this for years successfully.

189
00:12:54.370 --> 00:12:57.760
Eventually after many,
many people died,

190
00:12:59.830 --> 00:13:02.180
regulators insisted on the belt.

191
00:13:04.810 --> 00:13:07.840
This is a first timeframe is not relevant to AI.

192
00:13:08.410 --> 00:13:12.580
You can't take 10 years from the point of which is dangerous.
It's too late.

193
00:13:16.170 --> 00:13:21.170
<v 0>And you feel like this is decades away or years away</v>

194
00:13:23.070 --> 00:13:23.940
from being too late.

195
00:13:24.000 --> 00:13:27.820
If you have this fatalistic attitude and you feel like it's going well,

196
00:13:28.320 --> 00:13:31.560
we're in a,
almost like a doomsday countdown.

197
00:13:32.400 --> 00:13:36.330
It's not necessarily a doomsday countdown.
It's,
it's a out of control.

198
00:13:36.350 --> 00:13:37.410
Countdown out of control.

199
00:13:37.510 --> 00:13:42.280
<v 1>Yeah.
Maybe we'll call it the singularity.
And uh,
that's,</v>

200
00:13:42.281 --> 00:13:44.130
that's probably a good way to think about it.
It's,
it's a single hour.

201
00:13:44.131 --> 00:13:48.310
It's hard to predict like a black hole.
What,
what happens past the event horizon?

202
00:13:48.400 --> 00:13:51.310
<v 0>Right.
Sort of,
once it's implemented,
it's very different because it,</v>

203
00:13:51.440 --> 00:13:52.810
it wants to take us out of the bottle,

204
00:13:52.870 --> 00:13:57.130
what's going to happen and it will be able to improve itself.
Yes.

205
00:13:58.630 --> 00:14:00.100
That's where it gets spooky.
Right?

206
00:14:00.550 --> 00:14:04.420
The idea that it can do thousands of years of innovation,
we're very,

207
00:14:04.421 --> 00:14:08.750
very quickly.
Yeah.
And then we'll be just ridiculous.

208
00:14:09.080 --> 00:14:09.680
Ridiculous.

209
00:14:09.680 --> 00:14:14.600
We will be like this ridiculous biological shitting pissing thing trying to stop

210
00:14:14.601 --> 00:14:19.090
the gods now stop.
We like,
we like living with a finite lifespan and,

211
00:14:19.091 --> 00:14:21.620
and watching,
you know,
Norman Rockwell paintings,

212
00:14:24.090 --> 00:14:28.620
<v 1>it could be terrible and it could be great.
It's not clear.
Right.</v>

213
00:14:29.040 --> 00:14:31.950
But one thing is for sure we will not control it.

214
00:14:32.550 --> 00:14:37.550
<v 0>Do you think that it's likely that we will merge somehow or another with this</v>

215
00:14:38.190 --> 00:14:42.570
sort of technology and it will augment what we are now or do you think it will

216
00:14:42.870 --> 00:14:43.710
replace us?

217
00:14:48.560 --> 00:14:50.130
<v 1>Well,
that's the scenario.
The,
the,</v>

218
00:14:50.310 --> 00:14:55.310
the merge with AI is the one that seems like probably the best for us.

219
00:14:58.490 --> 00:15:02.750
Yes.
Like if you,
if you can't beat it,
join it.

220
00:15:03.800 --> 00:15:07.550
That's,
yeah.
You know?
Um,

221
00:15:09.500 --> 00:15:14.270
so from a long term existential standpoint,

222
00:15:14.300 --> 00:15:19.300
that's like the purpose of neurolink is to create a high bandwidth interface to

223
00:15:21.951 --> 00:15:26.951
the brain such that we can be some arctic with AI because we have a bandwidth

224
00:15:28.581 --> 00:15:31.760
problem.
You just can't communicate through your fingers is too slow.

225
00:15:32.690 --> 00:15:35.660
And where's neural link at right now?

226
00:15:37.940 --> 00:15:40.460
I think we'll have something interesting to announce in a few months.

227
00:15:41.480 --> 00:15:44.060
That's at least an order of magnitude better than anything else.

228
00:15:44.770 --> 00:15:47.660
Probably I think better than probably anyone thinks is possible.

229
00:15:47.930 --> 00:15:51.590
How much can you talk about that right now?
I don't want to jump the gun on that.

230
00:15:51.730 --> 00:15:55.580
Um,
but what's like the ultimate,
what's,

231
00:15:55.730 --> 00:15:58.490
what's the idea behind it?
Like what are you trying to accomplish with it?

232
00:15:58.860 --> 00:16:02.990
Like what would you like best case scenario?
I think best case scenario,
we

233
00:16:05.000 --> 00:16:10.000
effectually emerge with AI where we ais serves as a tertiary cognition layer,

234
00:16:13.470 --> 00:16:15.500
uh,
where we've got the limbic system.

235
00:16:16.160 --> 00:16:18.290
I'm kind of the planning of the primitive brain.
Essentially.

236
00:16:18.950 --> 00:16:20.120
You've got the CORTEX.

237
00:16:20.180 --> 00:16:24.770
So you're currently in a symbiotic relationship with your cortex and limbic

238
00:16:24.771 --> 00:16:26.270
system or in a symbiotic relationship.

239
00:16:26.360 --> 00:16:30.050
And generally people like their cortex and they liked the limbic system.

240
00:16:30.110 --> 00:16:33.410
I haven't met anyone who wants to delete their limbic system or to lead their

241
00:16:33.411 --> 00:16:35.780
cortex are very seem sort of like both.

242
00:16:37.370 --> 00:16:41.360
And the CORTEX is mostly in service to the limbic system if you will.

243
00:16:41.361 --> 00:16:44.690
May think that that,
that they're,

244
00:16:45.140 --> 00:16:47.300
that they're thinking part of themselves is in charge,

245
00:16:47.301 --> 00:16:51.590
but it's mostly their limbic system that's in charge and the cortex is trying to

246
00:16:51.591 --> 00:16:52.910
make the limbic system happy.

247
00:16:53.870 --> 00:16:57.680
That's what most of that computing power is oriented towards.

248
00:16:58.190 --> 00:17:01.550
How can I make the limbic system happy?
That's what I was trying to do.

249
00:17:03.380 --> 00:17:05.990
Now if we do have a third layer,

250
00:17:06.290 --> 00:17:10.350
which is the AI extension of yourself,
that is also some biotic,

251
00:17:10.970 --> 00:17:11.660
um,

252
00:17:11.660 --> 00:17:16.660
and there's enough bandwidth between the cortex and the AI extension of yourself

253
00:17:18.290 --> 00:17:21.530
such that the AI doesn't de facto separate,

254
00:17:22.730 --> 00:17:25.520
then that could be a good outcome.

255
00:17:25.760 --> 00:17:28.100
That could be quite a positive outcome for the future.

256
00:17:28.280 --> 00:17:32.330
So instead of replacing us,
it will radically change our capabilities.

257
00:17:33.260 --> 00:17:34.760
Yes it will.
It will

258
00:17:36.980 --> 00:17:41.600
enable anyone who wants to have super human cognition.

259
00:17:44.480 --> 00:17:45.560
Anyone who wants it.

260
00:17:45.630 --> 00:17:48.050
This is not a matter of earning power because you're earning power would be

261
00:17:48.620 --> 00:17:50.220
vastly after you do it.

262
00:17:50.280 --> 00:17:55.280
So it's just like anyone who wants can just do it in theory.

263
00:17:56.850 --> 00:17:58.320
That's the theory.
And,

264
00:17:59.270 --> 00:18:04.270
and if that's the case then and let's say billions of people do it,

265
00:18:05.640 --> 00:18:10.640
then the outcome for humanity will be the sum of,

266
00:18:13.230 --> 00:18:14.280
of human will.

267
00:18:15.440 --> 00:18:19.200
Some of billions of people's desire for the future.

268
00:18:20.160 --> 00:18:25.080
<v 0>And that deputy ends of people with enhanced cognitive ability radically
enhance.</v>

269
00:18:25.140 --> 00:18:26.760
Yes.
Which would be

270
00:18:28.450 --> 00:18:30.600
how much different than people today.
Like if you,

271
00:18:30.601 --> 00:18:35.601
if you had to explain it to a person who didn't really know and understand what

272
00:18:36.511 --> 00:18:37.020
you're saying,

273
00:18:37.020 --> 00:18:40.680
how much different are you talking about when you say radically improve?

274
00:18:40.681 --> 00:18:44.260
Like what do you mean?
You mean mine?

275
00:18:45.180 --> 00:18:49.270
<v 1>It will be difficult to,
to really appreciate the Dif,
the difference.
Um,</v>

276
00:18:51.070 --> 00:18:55.330
it's Kinda like how much smarter are you with a phone or computer then without

277
00:18:55.510 --> 00:18:58.960
it's you're vastly smarter.
Actually,
you know,

278
00:18:58.961 --> 00:19:02.140
you can answer any question if you're,
if you're connected to the Internet,

279
00:19:02.710 --> 00:19:05.410
you can ask her any question pretty much instantly.

280
00:19:05.500 --> 00:19:10.000
Any calculation that your phone's memory is essentially perfect.

281
00:19:10.440 --> 00:19:14.110
Uh,
you can remember flawlessly for your phone,
can remember videos,

282
00:19:15.160 --> 00:19:19.780
pictures,
everything perfectly.
That's the,

283
00:19:19.840 --> 00:19:24.760
that your phone is already an extension of you.
You're already a Cyborg.

284
00:19:24.820 --> 00:19:27.610
You don't even,
well most people don't rise.
They are already a Cyborg.

285
00:19:28.360 --> 00:19:31.720
If that phone is an extension of yourself,

286
00:19:32.260 --> 00:19:35.350
it's just that the,
the data rate,

287
00:19:35.830 --> 00:19:40.390
the rate at which there's a communication rate between you and the cybernetic

288
00:19:40.391 --> 00:19:44.410
extension of yourself that is your phone and computer is slow.

289
00:19:44.440 --> 00:19:45.490
It's very slow

290
00:19:47.140 --> 00:19:52.140
and and that that it's like a tiny straw of of of information flow between your

291
00:19:55.061 --> 00:19:57.820
biological self and your digital self

292
00:19:59.440 --> 00:20:02.860
and we need to make that tiny straw like a giant rower.

293
00:20:03.370 --> 00:20:08.020
Huge high bandwidth interface interface,

294
00:20:08.021 --> 00:20:12.850
problem data rate problem.
I saw the data rate a problem.

295
00:20:14.110 --> 00:20:19.110
Then I think I think we can hang on to human machine symbiosis through the long

296
00:20:20.921 --> 00:20:21.754
term

297
00:20:23.050 --> 00:20:26.980
and then people may decide that they want to retain their biological self.

298
00:20:26.981 --> 00:20:27.814
We're not,

299
00:20:28.930 --> 00:20:31.720
I think they'll probably choose to retain their volume by electrical self

300
00:20:32.460 --> 00:20:36.240
<v 0>versus some sort of Ray Kurzweil scenario where they download themselves into a</v>

301
00:20:36.241 --> 00:20:37.380
computer.
You will,

302
00:20:37.450 --> 00:20:40.690
<v 1>we essentially snapshot it into a computer and anytime if your biological self</v>

303
00:20:40.691 --> 00:20:43.780
dies,
you could just probably just upload into a new unit

304
00:20:45.640 --> 00:20:47.480
that's really pass that whiskey.

305
00:20:48.080 --> 00:20:49.670
<v 0>This is,
we're getting crazy over here.</v>

306
00:20:51.320 --> 00:20:55.280
Ridiculous down the rabbit hole.
Grab that.
Soccer can be something that,

307
00:20:56.090 --> 00:21:00.320
this is too freaky.
See if I was thinking about this for a long time.
By the way,

308
00:21:00.350 --> 00:21:03.950
I believe you have,
if I was talking to one more cheers by the way,
cherish.
Okay.

309
00:21:03.951 --> 00:21:08.330
This is a great whiskey.
Thank you
know where this came from,

310
00:21:08.630 --> 00:21:11.750
who brought this to us?
Trying to remember.
Somebody gave it to us old camp,

311
00:21:11.810 --> 00:21:15.070
whoever it was.
Thanks.
Yeah,
it is good.
Um,

312
00:21:16.280 --> 00:21:18.740
this is just inevitable.
Again,
going back to your,

313
00:21:18.741 --> 00:21:22.430
when you decided to be half of this fatalistic viewpoints,
so you weren't,

314
00:21:22.431 --> 00:21:25.430
you tried to warn people,
you talked about this pretty extensively.

315
00:21:25.431 --> 00:21:28.270
I've read several interviews where he talked about this and then you just sort

316
00:21:28.271 --> 00:21:29.540
of just said,
okay,
just is,

317
00:21:30.210 --> 00:21:35.210
let's just end you in a way by communicating the potential fee.

318
00:21:35.660 --> 00:21:39.230
I mean for sure you're getting the warning out to some people.

319
00:21:39.760 --> 00:21:42.520
<v 1>Yeah.
Yeah.</v>

320
00:21:42.550 --> 00:21:47.530
I mean I was really going on the morning quite,

321
00:21:47.531 --> 00:21:52.000
quite a lot.
I was good
morning everyone.
I could

322
00:21:53.760 --> 00:21:58.760
you ever met with Obama and just put one reason like watch about AI and what did

323
00:21:59.351 --> 00:22:02.920
he say?
So what about Hillary worry about her first

324
00:22:05.210 --> 00:22:09.940
he listened.
He certainly listened.
Um,
I met with Congress,

325
00:22:10.540 --> 00:22:11.830
I met with,

326
00:22:12.160 --> 00:22:17.160
I was at a meeting of all 50 governors and talked about just AI danger.

327
00:22:19.810 --> 00:22:22.390
And I talked to everyone I could,

328
00:22:25.570 --> 00:22:28.600
the one that seemed to realize where this was going.

329
00:22:30.000 --> 00:22:32.840
<v 0>Is it that or do they just assume that someone smarter than them?</v>

330
00:22:32.841 --> 00:22:35.580
It's already taken care of it.
Cause when,

331
00:22:35.640 --> 00:22:39.730
when people hear about something like Ai,
it's almost abstract.
It's almost,

332
00:22:40.030 --> 00:22:41.260
it's almost like it's so,

333
00:22:41.440 --> 00:22:44.200
it's so hard to wrap your head around it at the time had already happens.

334
00:22:44.350 --> 00:22:45.183
It'll be too late.

335
00:22:46.920 --> 00:22:48.570
<v 1>Yeah.
I think they</v>

336
00:22:50.640 --> 00:22:55.640
didn't quite understand it or didn't think it was near term or not sure what to

337
00:22:56.731 --> 00:22:58.980
do about it.
When I said like,
you know,

338
00:22:58.981 --> 00:23:01.860
an obvious thing to do is to just establish

339
00:23:03.380 --> 00:23:07.950
a committee,
government committee to gain insight.
You know,
before,

340
00:23:08.640 --> 00:23:11.530
before you oversight,
before you do make regulations,

341
00:23:11.550 --> 00:23:14.490
they should like try to understand what's going on.
Um,

342
00:23:14.580 --> 00:23:19.050
and then if you have an insight committee,
then the,

343
00:23:19.560 --> 00:23:21.870
once they learn what's going on and get up to speed,

344
00:23:22.530 --> 00:23:27.300
then they can make maybe some roles.
We'll propose some rules and,

345
00:23:27.330 --> 00:23:30.120
and that would be probably a safer way to go about things.

346
00:23:31.140 --> 00:23:32.550
<v 0>It's seems,
I mean I,</v>

347
00:23:32.570 --> 00:23:35.820
I know that it's probably something that the government's supposed to handle,

348
00:23:36.300 --> 00:23:37.740
but it seems like I wouldn't want the,

349
00:23:37.800 --> 00:23:40.900
I don't want the government to handle this.
Who Do you want to have?

350
00:23:40.901 --> 00:23:42.630
Want you to handle it?
Yeah.

351
00:23:43.050 --> 00:23:45.950
I feel like you're the one who could ring the bell because if,

352
00:23:46.010 --> 00:23:49.250
if Mike pence starts talking about Ai,
I'm like,
shut up bitch.

353
00:23:49.280 --> 00:23:52.220
You don't know anything about Ai.
Come on man.
He hasn't.

354
00:23:52.240 --> 00:23:55.730
Nobody's talking about it.
I don't have the power to regulate other companies.

355
00:23:55.900 --> 00:23:58.850
The one who,
I suppose maybe companies can agree,

356
00:23:59.150 --> 00:24:01.310
maybe there could be some sort of a wait.
I mean there's,

357
00:24:02.210 --> 00:24:05.810
we have agreements where you're not supposed to dump toxic waste into the ocean.

358
00:24:05.811 --> 00:24:07.520
You're not supposed to do certain things.

359
00:24:07.521 --> 00:24:11.540
It could be terribly damaging even though there'll be profitable.

360
00:24:12.170 --> 00:24:13.370
Maybe this is one of those things.

361
00:24:13.370 --> 00:24:15.950
Maybe we should realize that you can't hit the switch on something that's going

362
00:24:15.951 --> 00:24:18.890
to be able to think for itself and make up its own mind as to whether or not it

363
00:24:18.891 --> 00:24:19.820
wants to survive or not.

364
00:24:20.090 --> 00:24:24.080
And whether or not it thinks you are a threat and whether or not it thinks

365
00:24:24.081 --> 00:24:24.891
you're useless.

366
00:24:24.891 --> 00:24:29.891
Like why do I keep this dumb finite life form alive?

367
00:24:30.290 --> 00:24:32.750
Why,
why keep this thing around?
It's just stupid.

368
00:24:32.751 --> 00:24:35.480
It just keeps polluting everything shitting everywhere it goes.

369
00:24:35.840 --> 00:24:38.000
Lighting everything on fire and shooting each other.

370
00:24:38.270 --> 00:24:41.390
Why would I keep this stupid thing alive?
Cause sometimes it makes good music,

371
00:24:41.840 --> 00:24:43.580
you know,
sometimes it makes great movies,

372
00:24:43.581 --> 00:24:46.550
sometimes it makes beautiful art and sometimes you know,

373
00:24:46.610 --> 00:24:50.840
sometimes it's cool to hang out with Mike.
Yeah.
For us,

374
00:24:50.870 --> 00:24:54.350
those are great reasons,
but for anything objective,
standing outside like,
oh,

375
00:24:54.351 --> 00:24:55.820
this is definitely a flawed system.

