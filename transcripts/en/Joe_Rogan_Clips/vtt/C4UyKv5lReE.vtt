WEBVTT

1
00:00:00.270 --> 00:00:02.660
Do you think you can hold off regulation though?
You did.

2
00:00:02.670 --> 00:00:07.080
Do you think that by these approaches and by being proactive and by taking a

3
00:00:07.081 --> 00:00:07.411
standard,

4
00:00:07.411 --> 00:00:11.370
perhaps offering up a road to redemption to these people and making clear

5
00:00:11.371 --> 00:00:15.030
distinctions between what you're,
what you're allowing,
what you're not allowing,

6
00:00:15.031 --> 00:00:18.000
you can hold off regulation or do you disagree with what he's saying about

7
00:00:18.020 --> 00:00:18.590
regularly?

8
00:00:18.590 --> 00:00:20.830
<v 1>I believe that should be our goal is to hold off regulation.</v>

9
00:00:20.850 --> 00:00:22.910
I believe we should be,

10
00:00:23.180 --> 00:00:25.940
we should participate like any other citizen,

11
00:00:25.941 --> 00:00:30.290
whether it be a corporate citizen or an individual citizen in helping to guide

12
00:00:30.291 --> 00:00:31.130
the right regulation.

13
00:00:31.660 --> 00:00:33.070
<v 2>So,
uh,
are you familiar,</v>

14
00:00:33.071 --> 00:00:36.280
and I could be wrong on this because it's been like 15 years since I've done

15
00:00:36.281 --> 00:00:39.040
this.
Are you familiar with the clean water restoration act at all?

16
00:00:39.070 --> 00:00:41.830
I don't expect you to be.
It's a very specific thing.
So,
uh,

17
00:00:41.831 --> 00:00:45.490
it was at some point in like the early seventies there was a river in Ohio,

18
00:00:45.491 --> 00:00:47.170
and again,
I could be wrong.
It's been 15 years.

19
00:00:47.171 --> 00:00:49.720
I used to work for an environmental organization,
started on fire.

20
00:00:50.380 --> 00:00:54.520
And what was typically told to us was that all of these different companies
said,

21
00:00:54.521 --> 00:00:57.010
we're doing the right thing.
But like as I mentioned,

22
00:00:57.011 --> 00:00:58.240
the snowflake doesn't blame itself.

23
00:00:58.420 --> 00:01:02.020
So over time the river was so polluted it became sludge and lit on fire.

24
00:01:02.320 --> 00:01:03.190
And so someone said,

25
00:01:03.520 --> 00:01:06.910
if all of these companies think they're doing the right thing and they've all

26
00:01:06.911 --> 00:01:10.690
just contributed to this nightmare,
we need to tell them blanket regulation.

27
00:01:11.140 --> 00:01:14.260
And so what I see with these companies,
like banking institutions,

28
00:01:14.290 --> 00:01:18.610
public discourse platforms,
the distribution,
I actually,

29
00:01:18.611 --> 00:01:21.430
I'm really worried about what regulation would look like because I think the

30
00:01:21.431 --> 00:01:23.170
government is going to screw everything up,

31
00:01:23.440 --> 00:01:28.330
but I think there's going to be a recoil of a,
at first I think the Republicans,

32
00:01:28.450 --> 00:01:31.210
because I watched the testimony you had in Congress and I thought they had no

33
00:01:31.211 --> 00:01:32.620
idea what they're talking about,
nor did they care.

34
00:01:32.980 --> 00:01:34.510
There was like a couple of people who make good points,

35
00:01:34.511 --> 00:01:36.070
but for the most part they were like whatever.

36
00:01:36.370 --> 00:01:38.890
And they asked about Russia and stuff,
so they have no idea what's going on.

37
00:01:38.891 --> 00:01:41.650
But there'll come a time when you know,
for instance,
one of the,

38
00:01:41.680 --> 00:01:44.080
one of the great things they brought up was that by default,

39
00:01:44.081 --> 00:01:48.100
when someone in DC signs up,
they see way more Democrats than Republicans.
Right.

40
00:01:48.130 --> 00:01:50.740
You remember that when you testified?
Yeah.
So,

41
00:01:50.860 --> 00:01:53.050
well that there's an issue and I don't think,

42
00:01:53.180 --> 00:01:56.320
I believe you when you say it's Algorithmic,
that these are,
you know,

43
00:01:56.321 --> 00:01:59.640
prominent individuals.
So they get automatically recommended.
But then there,

44
00:01:59.820 --> 00:02:01.750
you know,
so again,
the solution to that,

45
00:02:01.751 --> 00:02:04.570
like how do you regulate a programmer to create an algorithm to solve that

46
00:02:04.571 --> 00:02:07.780
problem is,
is,
is crazy,
or you're regulating someone to invent technology.

47
00:02:08.320 --> 00:02:11.170
But I feel like there'll be a backlash when too many,

48
00:02:11.350 --> 00:02:12.640
right now we're seeing the reason,

49
00:02:12.670 --> 00:02:14.920
one of the reasons we're having this conversation is that conservatives feel

50
00:02:14.921 --> 00:02:19.300
like they're being persecuted and repressed.
So then it's gonna escalate from,

51
00:02:19.301 --> 00:02:20.730
it's not going to stop with these conversations.

52
00:02:20.810 --> 00:02:22.880
<v 0>And so that we're,
we've been having a lot of talks about this,</v>

53
00:02:22.910 --> 00:02:25.070
particularly around algorithms.
And,
um,

54
00:02:25.130 --> 00:02:28.250
one of the things that we are only focused on is not just fairness in outcomes

55
00:02:28.251 --> 00:02:31.540
but also explainability of algorithms.
And I know Jackie,
you love the steps.

56
00:02:31.550 --> 00:02:33.940
I don't know if you want to talk a little bit about our work there and

57
00:02:33.960 --> 00:02:34.770
<v 1>we,
um,</v>

58
00:02:34.770 --> 00:02:38.520
so there's two fields of research within artificial intelligence that are rather

59
00:02:38.521 --> 00:02:43.000
new but I think really impactful for our industry.
One is fairness and ml.
Um,

60
00:02:43.170 --> 00:02:47.970
so you're guessing what fairness and machine learning and deep learning.

61
00:02:48.090 --> 00:02:52.740
So looking at everything from what data said is fed to an algorithm.

62
00:02:53.040 --> 00:02:57.630
So like the training data set all the way to how the algorithm actually behaves

63
00:02:57.631 --> 00:03:00.220
on that,
on that data set,
making sure that it,

64
00:03:00.310 --> 00:03:04.200
it does not develop bias over the,
um,

65
00:03:04.280 --> 00:03:06.580
launch longevity of the algorithms use case.

66
00:03:07.090 --> 00:03:10.060
So that's one area that we want to lead in.

67
00:03:10.061 --> 00:03:14.920
And we've been working with some of the leading researchers in the industry to

68
00:03:14.921 --> 00:03:19.510
do that because the reality is a lot of this human judgment is moving

69
00:03:19.511 --> 00:03:20.344
algorithms.

70
00:03:20.530 --> 00:03:25.530
And the second issue with it moving now rhythms is algorithms today can't

71
00:03:25.691 --> 00:03:29.950
necessarily explain the decision making criteria that they use so they can't

72
00:03:29.951 --> 00:03:31.600
explain and the way that you make a decision,

73
00:03:31.640 --> 00:03:33.070
you explain why you make that decision.

74
00:03:33.460 --> 00:03:36.220
I read them today or not being programmed in such a way that they can even

75
00:03:36.221 --> 00:03:39.240
explain that you may wear an Apple Watch for instance.

76
00:03:39.250 --> 00:03:42.370
It might tell you to stand every now and then.
Um,

77
00:03:42.640 --> 00:03:47.380
right now those algorithms can't explain why,
why they're doing that,
right?
That,

78
00:03:47.381 --> 00:03:51.820
that's a bad example cause he does it every fit,
every 50 minutes.
But um,

79
00:03:51.850 --> 00:03:55.360
as we offload more and more of these decisions both internally and also

80
00:03:55.361 --> 00:03:59.650
individually two watches and,
and two cars and whatnot,

81
00:04:00.110 --> 00:04:01.600
there,
there was no,
uh,

82
00:04:01.630 --> 00:04:06.100
there's no ability right now for that algorithm to actually go through and list

83
00:04:06.101 --> 00:04:08.050
out the criteria you use to make that decision.

84
00:04:08.051 --> 00:04:11.440
So this is another area that we'd like to get really good at if we want to

85
00:04:11.441 --> 00:04:14.110
continue to be transparent around our actions.

86
00:04:14.111 --> 00:04:18.580
Because a lot of these things are just black boxes and they're being built in

87
00:04:18.581 --> 00:04:20.830
that way because there's been no research into like,

88
00:04:20.860 --> 00:04:24.280
well how do we get these algorithms to explain what their decision is?

89
00:04:24.430 --> 00:04:25.480
That question hasn't been asked.

90
00:04:25.590 --> 00:04:30.050
<v 2>My fear is it's technology that you need to build.
Yeah,
but the,</v>

91
00:04:30.051 --> 00:04:33.450
the public discourse is there.
We know that foreign governments are doing this.

92
00:04:33.451 --> 00:04:37.800
We know that democratic operatives in Alabama did this.
And so I imagine that,

93
00:04:38.450 --> 00:04:40.170
you know,
with Donald Trump,
I,
I,
you know,

94
00:04:40.171 --> 00:04:43.050
he talked about an executive order for free speech on college campuses so that

95
00:04:43.051 --> 00:04:47.610
the,
the chattering is here.
Someone's going to take a sledgehammer to Twitter,

96
00:04:47.611 --> 00:04:49.260
to Facebook,
to youtube and just be alike.

97
00:04:50.220 --> 00:04:51.780
Not Understanding the technology behind it,

98
00:04:52.230 --> 00:04:55.050
not willing to give you the benefit of the benefit of the doubt and just saying,

99
00:04:55.080 --> 00:04:57.460
I don't care why you're doing it.
We are mad.
You know what I mean?

100
00:04:57.461 --> 00:05:01.620
And then pass some bills and,
and then it's over again clarifying.
I,

101
00:05:01.840 --> 00:05:05.250
I think you guys are biased and I think what you're doing is dangerous,

102
00:05:05.670 --> 00:05:08.430
but I think that doesn't matter.
It doesn't matter what you think is right.

103
00:05:08.460 --> 00:05:11.670
It matters that all of these companies are doing similar things and it's,

104
00:05:11.671 --> 00:05:14.380
and it's,
and it's already terrifying people.
I mean,
look,
when,

105
00:05:14.381 --> 00:05:18.000
when I saw somebody got banned from their bank account,
that's terrifying.

106
00:05:18.030 --> 00:05:19.690
And paypal has done this for a long time.
You know,

107
00:05:20.030 --> 00:05:24.500
<v 0>it seems like more grievous than being banned from any social justice or social</v>

108
00:05:24.680 --> 00:05:26.030
media platform that,

109
00:05:26.060 --> 00:05:30.560
that seems to me to be worthy of a boycott.

110
00:05:30.790 --> 00:05:33.190
<v 2>Patriot Patriot issued a statement about a man,</v>

111
00:05:33.191 --> 00:05:35.290
I believe his name is Robert Spencer,
and they said,

112
00:05:35.291 --> 00:05:38.110
mastercard instructed us to ban him.
And you know what,
you know,

113
00:05:38.111 --> 00:05:42.400
I'll say this to me mentioning chase,
paypal,
mastercard terrifies me.

114
00:05:42.670 --> 00:05:44.170
I'm on the Joe Rogan podcast right now,

115
00:05:44.230 --> 00:05:46.300
calling out these big companies in defiance.

116
00:05:47.190 --> 00:05:50.140
<v 0>I would like to know all the specifics of why they chose to do that.</v>

117
00:05:50.141 --> 00:05:52.960
And I would hope that they would release some sort of a statement explaining why

118
00:05:52.961 --> 00:05:54.850
they chose to do that.
Maybe there's something we don't know.

119
00:05:55.040 --> 00:05:56.470
<v 2>There was a,
there was a reporter,
um,</v>

120
00:05:56.720 --> 00:05:59.390
and I could be in this wrong because I didn't follow it very much with big

121
00:05:59.391 --> 00:06:03.590
league politics.
Who said that after reporting on paypal negatively,

122
00:06:03.710 --> 00:06:08.390
they banned him.
That's terrifying.
So less reporting on it.
In what way?

123
00:06:08.990 --> 00:06:11.930
To Sargon off a cod issue.
No,
apparently is a journalist.

124
00:06:11.931 --> 00:06:14.870
He wrote about something bad pay pal did league politics is conservative.

125
00:06:14.900 --> 00:06:17.690
And so all of a sudden he got a notification that they can't tell them why,

126
00:06:17.691 --> 00:06:21.260
but he's gone.
So I see these big tech of monopolies.
I see youtube,
Facebook,

127
00:06:21.261 --> 00:06:22.700
Twitter.
I see paypal,
mastercard,

128
00:06:23.120 --> 00:06:25.610
and they're doing it and they all say they're doing the right thing.

129
00:06:25.820 --> 00:06:28.730
But all of these little things they're doing are adding up to something

130
00:06:28.731 --> 00:06:30.250
nightmarish and some,

131
00:06:30.280 --> 00:06:32.600
some writing legislator's going to show up and in a matter of time with a

132
00:06:32.601 --> 00:06:35.000
sledgehammer and just he's in whack.
Your Algorithm.

