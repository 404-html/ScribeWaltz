WEBVTT

1
00:00:01.830 --> 00:00:05.970
I have tried over the years,
whether it's conspiracy theorists,
community is,

2
00:00:05.971 --> 00:00:10.080
or terrorists or um,
or uh,
you know,

3
00:00:10.081 --> 00:00:14.350
Russia are on state,
the state sponsored actors,
the domestic ideologues.
Um,

4
00:00:15.060 --> 00:00:16.530
I have tried to always say like,

5
00:00:16.620 --> 00:00:21.620
here's the specific kind of forensic analysis of this particular operation and

6
00:00:23.251 --> 00:00:27.420
then here is what we can maybe take from it and make,
make changes.

7
00:00:28.140 --> 00:00:28.980
Um,
we've,

8
00:00:29.100 --> 00:00:34.100
we've seen some of that begin to take shape and so I feel grateful to

9
00:00:36.030 --> 00:00:39.810
have had the opportunity to work towards connecting those dots and work towards

10
00:00:39.811 --> 00:00:41.730
having this conversation.
Um,

11
00:00:42.000 --> 00:00:46.050
meaning meaning helping people understand what's going on.
I think,

12
00:00:46.920 --> 00:00:47.850
I am not,

13
00:00:49.140 --> 00:00:53.730
I am most concerned about the,

14
00:00:55.620 --> 00:00:58.100
as this gets increasingly,
uh,

15
00:00:58.230 --> 00:01:03.000
easy to do through things like chat bots.
Um,
you know,
now there's these,

16
00:01:03.060 --> 00:01:07.740
um,
you've seen the,
uh,
the website,
uh,
this person does not exist.com.

17
00:01:08.370 --> 00:01:11.850
So it's a,
there's a technology called,
it's a machine learning technique,

18
00:01:11.851 --> 00:01:14.060
generative adversarial networks.
And they,
um,

19
00:01:14.070 --> 00:01:19.050
it's these basically there in this particular application working to create

20
00:01:19.290 --> 00:01:24.180
pictures of people,
faces of people.
And so this website is,
when you go to it,

21
00:01:24.181 --> 00:01:27.300
it pulls up a,
yeah,
there you go.
So this person does not actually exist.

22
00:01:27.301 --> 00:01:30.990
This is not a religion.
Yeah.
And so these are all um,

23
00:01:31.470 --> 00:01:34.990
these are computer generated.
Yup.
So and you can see created by,

24
00:01:35.020 --> 00:01:36.990
again it says it down at the bottom there.

25
00:01:36.991 --> 00:01:41.340
So these are not real people and so we have increasingly sophisticated chat

26
00:01:41.341 --> 00:01:42.001
technology.

27
00:01:42.001 --> 00:01:45.540
We have increasingly sophisticated like you're not going to detect that image

28
00:01:45.541 --> 00:01:46.380
somewhere else,

29
00:01:46.410 --> 00:01:49.260
that old trick of Lake right click and look and see if you're talking to someone

30
00:01:49.261 --> 00:01:52.710
with a stock photo that goes right out the window is stuff like this gets easier

31
00:01:52.711 --> 00:01:56.970
and easier to do.
Deep faking.
Yeah,
the deep fakes on the video front.

32
00:01:57.300 --> 00:01:59.220
I think that it does change.

33
00:01:59.910 --> 00:02:04.910
I think we haven't quite adapted to what does it like to live in a world where

34
00:02:04.921 --> 00:02:06.810
so much of the Internet is fake.

35
00:02:07.200 --> 00:02:11.580
And I do think per your point about identity that there will be groups of people

36
00:02:11.581 --> 00:02:15.720
that self select into communities where identity is mandatory.
You know,
we're,

37
00:02:15.721 --> 00:02:15.871
we're,

38
00:02:15.871 --> 00:02:19.950
this is who you are and you have some sort of verification versus people who

39
00:02:19.951 --> 00:02:23.640
choose to live in the world of,
um,
you know,
drink from the fire hose,

40
00:02:23.641 --> 00:02:26.940
take it all in and try to filter it out yourself.
Um,

41
00:02:27.900 --> 00:02:32.900
so we look at these evolving technologies and I don't necessarily feel,

42
00:02:35.690 --> 00:02:38.160
you know,
particularly optimistic in the short term.

43
00:02:38.161 --> 00:02:39.870
I think that ultimately it does,

44
00:02:40.140 --> 00:02:44.430
like we change as a society to a large extent in response to this.

45
00:02:44.431 --> 00:02:48.000
We think about,
um,
you know,

46
00:02:48.030 --> 00:02:50.240
they're going to be some fixes that the platforms are going to be able to

47
00:02:50.400 --> 00:02:53.370
undertake.
They're going to be,
we're going to get better at detecting the stuff.

48
00:02:53.400 --> 00:02:55.440
Maybe,
you know,
the adversary will evolve.

49
00:02:55.441 --> 00:02:58.350
Hopefully we get better at detecting it as it evolves,

50
00:02:58.351 --> 00:03:00.520
but it's,

51
00:03:00.760 --> 00:03:03.680
I think we fundamentally ultimately change.

52
00:03:03.681 --> 00:03:07.210
Like people become more aware that this is a thing.
They are more skeptical.
Uh,

53
00:03:07.240 --> 00:03:11.890
that does change our,
the,
our ways of interacting with each other.
Uh,
but I,

54
00:03:11.900 --> 00:03:15.430
I feel like that is going to be the direction that this goes.

55
00:03:15.980 --> 00:03:18.580
There's the more like,
you know,

56
00:03:18.581 --> 00:03:21.630
the thing that keeps me up at night would be more the,
um,

57
00:03:23.560 --> 00:03:24.393
the ease of,

58
00:03:24.420 --> 00:03:28.660
of turning this from a social media problem until like a real world war problem.

59
00:03:28.900 --> 00:03:31.720
Meaning,
um,
as an example,

60
00:03:31.721 --> 00:03:35.440
back in 2014 when the first things Internet research agency dead,

61
00:03:35.920 --> 00:03:39.820
September 11th,
2014,
they created a hoax,
um,

62
00:03:40.300 --> 00:03:43.630
saying that isis had attacked a chemical plant down in Louisiana.

63
00:03:44.440 --> 00:03:48.220
It's called the Columbia.
The Columbia chemical plant hoax is,

64
00:03:48.221 --> 00:03:50.380
I think there's a Wikipedia article about it now.

65
00:03:50.830 --> 00:03:55.600
But what happened was they created a collection of websites.

66
00:03:55.601 --> 00:03:58.300
They created fake CNN mockups,
Twitter accounts,

67
00:03:58.301 --> 00:04:02.260
text messages that went to local people,
radio station,
Collins,
you name it,

68
00:04:02.261 --> 00:04:07.261
everything to create the impression that a chemical factory had just exploded in

69
00:04:08.260 --> 00:04:10.960
Louisiana.
And there was some attribution to isis.

70
00:04:10.961 --> 00:04:12.580
And this was done on September 11th.

71
00:04:12.970 --> 00:04:15.760
So this is the kind of thing where this actually did go viral.

72
00:04:15.790 --> 00:04:18.580
Like I remember this happening not as a social media researcher.

73
00:04:18.581 --> 00:04:21.760
I just remember it actually being pushed into my social media feed.

74
00:04:22.120 --> 00:04:23.980
So you have these,
um,

75
00:04:23.981 --> 00:04:27.250
and we didn't know that it was in and a research agency for a year and a half

76
00:04:27.251 --> 00:04:28.084
after.

77
00:04:28.510 --> 00:04:33.510
But this is the kind of thing where you look at parts of the world that aren't

78
00:04:33.581 --> 00:04:37.120
the u s like the recent,
um,
drama between India and Pakistan.

79
00:04:37.570 --> 00:04:40.210
And you can see how these kinds of things can go horribly,

80
00:04:40.211 --> 00:04:45.040
horribly wrong if throng person is convinced that something has happened or if

81
00:04:45.041 --> 00:04:45.910
there's a,
you know,

82
00:04:45.940 --> 00:04:50.050
or if this leads to our riot or if this leads to real world action.

83
00:04:50.051 --> 00:04:54.400
I think that's,
um,
one of the main fears.

84
00:04:54.401 --> 00:04:57.490
As this gets better and better,
the video fakes get better the.

