WEBVTT

1
00:00:00.670 --> 00:00:04.470
Don't feel bad.
I get you one.
Hold on a second.
Um,
who's,

2
00:00:04.540 --> 00:00:08.020
who's seen all this stuff?
Is this stuff mean?
Uh,
it has,

3
00:00:08.290 --> 00:00:11.710
obviously Facebook has a check this out.

4
00:00:12.040 --> 00:00:13.870
I'm sure Twitter's aware.

5
00:00:14.530 --> 00:00:18.130
What has the reaction been and is there any sort of a concerted effort to

6
00:00:18.131 --> 00:00:21.250
mitigate some of some of the impact of these,
these sites of

7
00:00:21.440 --> 00:00:26.360
<v 1>yeah,
lots of it actually.
So I think,
um,
in,</v>

8
00:00:29.560 --> 00:00:33.380
in 2017

9
00:00:34.880 --> 00:00:37.550
was when we started,
like we being independent researchers,

10
00:00:37.551 --> 00:00:41.690
I guess people on the outside of the company's academics,
um,

11
00:00:41.900 --> 00:00:44.710
began to find the content,
you know,

12
00:00:44.720 --> 00:00:49.040
really began to investigative journalists would identify the name of a page and

13
00:00:49.041 --> 00:00:52.100
then me and people like me would go and we would scour the Internet looking for

14
00:00:52.130 --> 00:00:56.240
evidence of what was on that page.
So I found a bunch of the stuff on Pinterest,

15
00:00:56.241 --> 00:01:00.740
for example,
wrote about it.
Um,
guy by the name of Jonathan Albright,
uh,

16
00:01:00.741 --> 00:01:02.360
found a crowd tangle data cache.

17
00:01:02.480 --> 00:01:05.570
And with that we got the names of a bunch more pages,
bunch more posts,

18
00:01:05.571 --> 00:01:08.570
and we had some really interesting stuff to work with originally.

19
00:01:08.571 --> 00:01:11.570
The platforms were very resistant to the idea that this had happened.

20
00:01:13.250 --> 00:01:17.900
And so as a result of that,
they were,
um,

21
00:01:20.150 --> 00:01:22.400
in,
you know,
there was a,

22
00:01:22.410 --> 00:01:27.410
the first thing that Zach said in 2016 when Trump gets elected Twitter,

23
00:01:28.461 --> 00:01:32.840
it goes crazy that night with people who work at Twitter saying,
oh my God,

24
00:01:32.960 --> 00:01:37.260
where are we responsible for this?
Which is very silicon valley thing to say.
Um,

25
00:01:37.280 --> 00:01:41.300
but what I think they meant by that was their platform had been implicated as

26
00:01:41.301 --> 00:01:45.140
hosting Russian bots and fake news and harassment mobs in a number of other

27
00:01:45.141 --> 00:01:47.780
things.
And there was always the sense that it didn't have an impact on it didn't

28
00:01:47.781 --> 00:01:51.830
matter.
And so this was the first time that they started to ask the question,

29
00:01:51.831 --> 00:01:56.290
did it matter?
And then Zach made that statement.
Um,

30
00:01:56.330 --> 00:01:59.150
fake news is a very small percentage of,
you know,
whatever on Facebook,

31
00:01:59.151 --> 00:02:02.670
the amount of information on Facebook and the idea that it could have swung on

32
00:02:02.671 --> 00:02:07.400
election was ludicrous.
So you have the platforms kind of,

33
00:02:07.540 --> 00:02:07.970
uh,

34
00:02:07.970 --> 00:02:12.770
the leaders at the platforms digging in and saying it's inconceivable that,

35
00:02:12.830 --> 00:02:15.710
that,
that this,
you know,
could have happened.

36
00:02:16.580 --> 00:02:21.580
And as the research and the discovery begins to take place over the next nine

37
00:02:21.981 --> 00:02:26.600
months or so,
you,
you,
you get to when the tech hearings happen.

38
00:02:26.600 --> 00:02:30.860
So I worked with,
um,
guy by the name of Tristan Harris.

39
00:02:30.861 --> 00:02:35.720
He's the one who introduced me to Sam.
Um,
and he,

40
00:02:36.410 --> 00:02:41.240
he and I started going to DC with a third fellow,
Roger Mcnamee and saying,

41
00:02:41.241 --> 00:02:43.160
Hey,
there's so much,

42
00:02:43.190 --> 00:02:47.780
there's this body of evidence that's coming out here and we need to have a

43
00:02:47.781 --> 00:02:48.614
hearing.

44
00:02:48.740 --> 00:02:53.270
We need to have congress ask the tech companies to account for what happened,

45
00:02:53.750 --> 00:02:55.670
to tell the American people what happened.

46
00:02:56.030 --> 00:02:58.700
Because what we're seeing here as outside researchers,

47
00:02:59.470 --> 00:03:01.420
what investigative journalists are writing,

48
00:03:01.840 --> 00:03:05.440
the things that we're finding just don't line up with the statements that,

49
00:03:05.590 --> 00:03:07.780
that nothing happened.
And was,
this was all no big deal.

50
00:03:08.320 --> 00:03:12.940
And so we start asking for these hearings and actually,

51
00:03:13.590 --> 00:03:17.620
um,
myself and a couple of others then began asking them in the course of these

52
00:03:17.621 --> 00:03:18.454
hearings,

53
00:03:18.460 --> 00:03:22.030
can you get them to give you the data because the platform's hadn't given the

54
00:03:22.031 --> 00:03:22.660
data.

55
00:03:22.660 --> 00:03:27.580
So it was that lobbying by concerned citizens and journalists and researchers

56
00:03:27.581 --> 00:03:30.040
saying,
we have to have some accountability here.

57
00:03:30.041 --> 00:03:32.500
We have to have the platforms account for what happened.

58
00:03:32.710 --> 00:03:35.110
They have to tell people.
Um,

59
00:03:35.290 --> 00:03:38.710
because it had become such a politically divisive issue,
did it even happen?

60
00:03:39.190 --> 00:03:44.190
And we felt like having them actually sit there in front of Congress and account

61
00:03:44.231 --> 00:03:48.130
for it would be the first step towards,
um,

62
00:03:48.790 --> 00:03:51.580
towards moving forward in a way,
but,
but also towards,
um,

63
00:03:52.510 --> 00:03:56.290
changing the minds of the public and making them realize that what happened on

64
00:03:56.291 --> 00:03:58.090
social platforms matters.

65
00:03:58.960 --> 00:04:03.060
And it was,
it was really interesting to,

66
00:04:03.250 --> 00:04:06.730
to be part of that as it,
as it played out.
Um,

67
00:04:07.420 --> 00:04:10.000
because one of the things that Senator Blumenthal,
one of the,

68
00:04:10.030 --> 00:04:12.010
one of the senators said was actually said,
um,

69
00:04:12.580 --> 00:04:15.400
Facebook and Twitter have to notify people who engage with this content.

70
00:04:15.790 --> 00:04:19.210
And so there,
there was this idea that,
um,

71
00:04:20.230 --> 00:04:24.610
if you are engaging with propagandas content,
you should have the right to know.

72
00:04:25.150 --> 00:04:26.890
And so they started to push messages.

73
00:04:26.980 --> 00:04:31.980
Twitter sent out these emails to all these people saying you engaged with this

74
00:04:32.381 --> 00:04:34.160
Russian troll.
And,
um,

75
00:04:34.960 --> 00:04:37.930
Facebook created a little field,

76
00:04:38.020 --> 00:04:41.680
a little little page that told people if they had liked or followed a troll
page.

77
00:04:42.160 --> 00:04:46.540
So it was really trying to get at making the platform's accountable with they it

78
00:04:46.541 --> 00:04:48.340
outside the platform through email.
Huh.

79
00:04:49.250 --> 00:04:52.220
Twitter is interesting because I would never read an email.
The Twitter sends me,

80
00:04:53.430 --> 00:04:57.130
this has just gotta be nonsense.
I didn't get one.
So I maybe,

81
00:04:58.750 --> 00:05:00.730
I guess I just got lucky,
but,
um,

82
00:05:00.880 --> 00:05:03.940
I might've had a multiple day back and forth and some Russian,

83
00:05:05.590 --> 00:05:08.140
but that was,
I think one of the first steps towards saying like,

84
00:05:08.200 --> 00:05:09.910
how do we make the platforms accountable?

85
00:05:10.900 --> 00:05:15.220
Because the idea of that platform should be accountable,
was not a,

86
00:05:15.550 --> 00:05:18.790
a thing that everybody agreed on in 2015 when we were having this conversation

87
00:05:18.791 --> 00:05:23.190
about isis.
And that's where there's the,
the through line here,
which is,
um,

88
00:05:23.260 --> 00:05:25.510
and it does connect into some of the speech issues too,

89
00:05:26.200 --> 00:05:31.180
which is what kind of monitoring and moderation do you want the platforms to do.

90
00:05:31.870 --> 00:05:36.610
And when we were having this conversation about Isis,
there was a,
um,

91
00:05:37.600 --> 00:05:42.600
not insignificant collection of voices that were really concerned that if we

92
00:05:43.811 --> 00:05:48.010
moderated isis trolls on Twitter and then not the beheading videos,

93
00:05:48.011 --> 00:05:50.380
there were sort of universal agreement that the beheading videos should come

94
00:05:50.381 --> 00:05:54.820
down.
But if we took out what were called the Isis fanboys,
which were like 30,

95
00:05:54.821 --> 00:05:59.360
40,000 accounts at their peak,
um,
that we would,
yeah,

96
00:05:59.361 --> 00:06:02.150
there's a document called the isis Twitter census for anyone who wants to

97
00:06:02.151 --> 00:06:06.740
actually see the research done on understanding the Twitter network in 2015

98
00:06:07.190 --> 00:06:10.580
there was a sense that like one man's terrorist is another man's freedom
fighter.

99
00:06:10.700 --> 00:06:13.940
And if we took down Isis fanboys,
um,

100
00:06:14.000 --> 00:06:15.710
where we stifling their freedom of speech,

101
00:06:15.711 --> 00:06:18.320
freedom of expression and like goodness,
what would come next.

102
00:06:19.220 --> 00:06:22.760
And that when you,
when you look at that,
um,

103
00:06:22.880 --> 00:06:27.710
that fundamental swing that has happened now in 2018,

104
00:06:27.711 --> 00:06:29.880
20,
19 or those,

105
00:06:29.890 --> 00:06:34.220
there's that same narrative because originally no moderation was taking place

106
00:06:34.250 --> 00:06:37.010
and then now there's a feeling that it's kind of swung too far in the other

107
00:06:37.011 --> 00:06:40.760
direction.
But the original conversations were really,

108
00:06:42.110 --> 00:06:45.890
how do we make Twitter take responsibility for this?

109
00:06:46.460 --> 00:06:50.660
And legally they aren't responsible for it.
All right?

110
00:06:50.690 --> 00:06:53.630
They are legally indemnified against the,

111
00:06:54.350 --> 00:06:56.540
they're not responsible for any of the content on their platforms.

112
00:06:56.541 --> 00:06:57.560
None of the platforms are.

113
00:06:58.040 --> 00:07:01.790
There's a law called communications decency act section two 30,

114
00:07:02.240 --> 00:07:04.310
and that says that they're not responsible.

115
00:07:04.311 --> 00:07:08.420
They have the right to moderate but not the obligation to moderate because they

116
00:07:08.421 --> 00:07:12.380
are indemnified from responsibility.
So the question becomes,

117
00:07:12.381 --> 00:07:15.890
now that we know that these platforms are used for these kinds of harms and they

118
00:07:15.891 --> 00:07:19.790
are used for this kind of interference,
um,
where is that balance?

119
00:07:19.791 --> 00:07:24.620
What do we want them are responsible for monitoring and moderating?

120
00:07:25.430 --> 00:07:27.180
And how do we,
um,

121
00:07:27.230 --> 00:07:32.090
how do we recognize that that is occasionally going to lead to an incorrect

122
00:07:32.091 --> 00:07:35.240
attribution's people losing accounts?
And things like that.
So

123
00:07:38.030 --> 00:07:42.320
<v 0>yeah,
they're,
they're in a weird conundrum right now or they don't,
they,
they're,</v>

124
00:07:42.321 --> 00:07:45.710
they're trying to keep everything safe and they want to encourage people to

125
00:07:45.711 --> 00:07:50.330
communicate on the platforms.
So they want to keep people from harassing folks.

126
00:07:50.780 --> 00:07:53.300
But because of that,
they've also,

127
00:07:53.360 --> 00:07:56.060
they've got these algorithms and they,
they,

128
00:07:56.270 --> 00:08:01.270
they tend to miss very often like this whole learn to code fiasco where people

129
00:08:01.851 --> 00:08:04.550
are getting banned for life for saying learn to code,

130
00:08:04.850 --> 00:08:07.190
which is about as preposterous as it gets.

131
00:08:07.191 --> 00:08:10.910
I think the learn to code fiasco is going to be the tipping point where a lot of

132
00:08:10.911 --> 00:08:13.970
people in the,
in the future,
when they look back on,

133
00:08:14.180 --> 00:08:18.470
when did the heavy handedness become overreach,
learn to code.

134
00:08:18.710 --> 00:08:22.670
Because I mean,
Jesus Christ,
I mean that if you can't say learn to code.
I mean,

135
00:08:22.671 --> 00:08:26.750
I look at my mentions,
I mean on any given day,
especially like yesterday,
I had,

136
00:08:26.751 --> 00:08:31.700
um,
uh,
uh,
vaccine proponent.
Yeah,
yeah.
Peter's Great.

137
00:08:31.720 --> 00:08:35.930
And,
and you know,
and it seemed like what was really disturbing to me was like,

138
00:08:35.960 --> 00:08:39.050
the vast majority of the comments were about vaccines.

139
00:08:39.051 --> 00:08:44.051
And so few about these unchecked diseases that are running rampant in poor

140
00:08:44.541 --> 00:08:47.630
communities,
which was the most disturbing aspect of the conversation to me,

141
00:08:47.900 --> 00:08:52.820
that there's diseases that rob you of your intellectual capacity that are

142
00:08:52.821 --> 00:08:56.160
extremely common,
that as many as 10% of people in these poor neighborhoods

143
00:08:56.160 --> 00:09:00.630
<v 1>have almost no discussion.
It was all just insults and,</v>

144
00:09:00.631 --> 00:09:03.920
and you know,
you fucking chill and this and that.
You know,

145
00:09:03.980 --> 00:09:07.410
it's going to be interesting but they're going to be a disaster.

146
00:09:09.900 --> 00:09:11.280
Well,
let me,
let me,

147
00:09:13.440 --> 00:09:16.650
I think that one of the challenges for the platforms,

148
00:09:17.430 --> 00:09:20.000
there's a lot of things start out like learn to code.
I remember,
yeah,

149
00:09:20.050 --> 00:09:24.930
I watched that play out.
Covington Catholic was another thing that,
I mean,
God,

150
00:09:25.450 --> 00:09:27.390
um,
I learned to code.

151
00:09:27.391 --> 00:09:31.220
There was some of the people who are trolling and just saying learn to code and

152
00:09:31.250 --> 00:09:33.750
you know,
whatever,
you don't have a right to not be offended.

153
00:09:33.930 --> 00:09:35.240
But then there was the,
um,

154
00:09:35.970 --> 00:09:39.480
the other accounts that kind of took it that step further and began to throw in

155
00:09:39.481 --> 00:09:42.210
like the ovens and the other stuff like that.
It would learn to code.
Right.

156
00:09:43.380 --> 00:09:44.910
And that's one of the challenges with the platform,

157
00:09:44.911 --> 00:09:49.911
which is if you're trying to assess the,

158
00:09:51.600 --> 00:09:56.230
um,
just the content itself,
like if you start doing keyword bands,

159
00:09:57.060 --> 00:09:59.160
you're going to catch a lot of shit that you don't want to catch.

160
00:09:59.640 --> 00:10:03.740
But the flip side is if you,
you know,
the,
the,

161
00:10:03.750 --> 00:10:08.470
this is the challenge of moderating at scale,
which is,
um,
where,
you know,
what,

162
00:10:08.471 --> 00:10:10.260
what side do you come down on?

163
00:10:10.261 --> 00:10:14.760
Do come down on saying like 75% of people with Hashtag learn to code or just,

164
00:10:14.820 --> 00:10:19.710
you know,
I'm not doing anything incredibly offensive.

165
00:10:19.711 --> 00:10:21.330
And then the 25% who are,

166
00:10:21.570 --> 00:10:25.830
they really changed the tone of the overall campaign and the Hashtag for the

167
00:10:25.831 --> 00:10:28.320
entire community.
And that's where you see Twitter,
I think,

168
00:10:28.321 --> 00:10:32.580
come in with the more heavy handed and just shut it down kind of thing.

169
00:10:33.180 --> 00:10:38.160
Um,
I don't,
I don't know that there's an easy answer.
I think that we are,

170
00:10:38.910 --> 00:10:42.190
you know,
even today,
what was the latest kerfuffle?
Elizabeth Warren got a,

171
00:10:42.191 --> 00:10:45.030
an ad taken down on Facebook and then there was a whole conversation about,

172
00:10:45.031 --> 00:10:47.720
was Facebook censoring Elizabeth Warren.

173
00:10:47.721 --> 00:10:52.350
And I personally didn't think that it read like censorship,
but it was,
um,

174
00:10:52.590 --> 00:10:57.160
it was an ad about funny enough her a platform to break up Facebook.
Whoa.

175
00:10:59.310 --> 00:11:03.990
This is kind of,
it seems like it sort of read more like a cell phone.

176
00:11:03.991 --> 00:11:07.200
Like she had a picture of Facebook's logo in the,
um,

177
00:11:07.680 --> 00:11:11.460
in the image and there that violates the ads terms of service.

178
00:11:11.940 --> 00:11:16.290
And the reason behind that is actually because Facebook doesn't want people

179
00:11:16.291 --> 00:11:19.100
putting up ads that have the Facebook logo in it because that's how you scam

180
00:11:19.101 --> 00:11:21.940
people.
Right.
That's a great way to,
to rip people off.

181
00:11:21.941 --> 00:11:25.190
And so probably just like an automated,
um,
you know,

182
00:11:25.200 --> 00:11:26.670
an automated take down like an automated,

183
00:11:26.730 --> 00:11:28.930
they get halts the ad you have to go and make some changes,

184
00:11:28.960 --> 00:11:31.130
then you can proceed it back out again.
Uh,

185
00:11:31.140 --> 00:11:32.910
but it just happens at a time when there's like,

186
00:11:32.911 --> 00:11:36.000
so little assumption of good faith and so little assumption of

187
00:11:37.650 --> 00:11:42.480
such extreme,
uh,
anger and polarization and um,

188
00:11:42.570 --> 00:11:46.650
you know,
assumption that the platforms are censoring with,

189
00:11:46.680 --> 00:11:51.180
with every little kind of moderation snafu that it,
it makes it,

190
00:11:51.300 --> 00:11:52.133
I think,

191
00:11:53.110 --> 00:11:57.030
I don't know how we have the conversation in a way that's healthy and looks

192
00:11:57.031 --> 00:12:01.330
towards solutions as opposed to the left screaming that it censored the right

193
00:12:01.331 --> 00:12:05.380
screaming that it's censored.
The platforms,
um,
you know,

194
00:12:05.381 --> 00:12:08.170
trying to get around how do we both moderate and not moderate,

195
00:12:08.171 --> 00:12:12.130
which is a tough position to be in.
Um,
I think,

196
00:12:13.750 --> 00:12:14.583
I don't have any

197
00:12:14.600 --> 00:12:15.830
<v 0>good,
no one does,</v>

198
00:12:15.831 --> 00:12:20.480
no good answers to the issue and Vigia discussed that pretty much in depth what

199
00:12:20.481 --> 00:12:23.780
she was saying.
This is about moderating and scale.

200
00:12:23.781 --> 00:12:28.781
When you're talking about millions and millions and millions of posts and a

201
00:12:29.720 --> 00:12:31.220
couple thousand people working for the organization.

202
00:12:31.640 --> 00:12:34.640
And then algorithms and computer learning that's trying to keep up.

203
00:12:34.641 --> 00:12:36.740
And that's where things like learn to code.

204
00:12:36.741 --> 00:12:40.970
And people were so outraged and pissed off because when they do get banned,

205
00:12:41.090 --> 00:12:42.380
they feel like they've been targeted.

206
00:12:42.390 --> 00:12:47.150
Whether you really just ran into some code and then it's really hard to get

207
00:12:47.151 --> 00:12:49.340
someone to pay attention to your appeal.
Right.

208
00:12:49.370 --> 00:12:53.330
Because there's not enough people that are looking at these appeals and there's

209
00:12:53.510 --> 00:12:57.530
probably millions of appeals every day.
It's almost impossible.

210
00:12:58.060 --> 00:13:01.480
<v 1>Yeah.
And there's,
you know,
depending on which,
um,
which side you're on,</v>

211
00:13:01.481 --> 00:13:03.230
you're also here like,
um,

212
00:13:03.640 --> 00:13:06.550
this person is harassing me and I'm demanding moderation and nobody's doing

213
00:13:06.551 --> 00:13:10.660
anything about it.
Yes.
Uh,
so it's,
it's definitely I think,

214
00:13:11.380 --> 00:13:15.730
gotten worse.
Um,
it's,

215
00:13:15.970 --> 00:13:20.200
it's interesting to look back at 2016 and wonder how much of the,

216
00:13:20.950 --> 00:13:21.783
um,

217
00:13:22.510 --> 00:13:27.160
where we are now is in part because not a whole lot happened in 2016 and 2016 it

218
00:13:27.161 --> 00:13:30.700
was 2015 in particular,
very light,
like almost no moderation.

219
00:13:30.701 --> 00:13:34.520
Just kind of let it all hang out there.
And I look at it,
um,

220
00:13:35.740 --> 00:13:36.281
I look at it now,

221
00:13:36.281 --> 00:13:39.040
particularly as it evolved into this conversation about free speech,

222
00:13:39.041 --> 00:13:41.280
public squares,
um,

223
00:13:42.310 --> 00:13:47.310
and what the new kind of infrastructure for speech and what rights we should

224
00:13:48.341 --> 00:13:53.290
expect on it.
It's a really tough,

225
00:13:53.710 --> 00:13:58.500
um,
you know,
I think some of it is,

226
00:13:58.560 --> 00:14:00.580
is almost like the people who are,

227
00:14:01.300 --> 00:14:05.020
who hear the words free speech and they just assume that it's people asking for

228
00:14:05.021 --> 00:14:08.810
carp launch,
right.
To harass and saying,
you know,
um,

229
00:14:08.860 --> 00:14:10.480
how do we balance that?
You do.

230
00:14:10.490 --> 00:14:13.200
I think Jack and Vagina was saying this on your show.
Um,

231
00:14:13.810 --> 00:14:16.350
how do we maximize the number of people who are involved,

232
00:14:16.370 --> 00:14:21.370
make sure that all voices do get heard without being unnecessarily heavy handed

233
00:14:21.971 --> 00:14:26.971
and moderating a thought or content and instead Monterrey behavior and,

234
00:14:28.700 --> 00:14:30.430
um,
and said moderate,
um,

235
00:14:31.060 --> 00:14:35.560
particular types of signatures of things that are inauthentic or things that are

236
00:14:35.561 --> 00:14:39.460
coordinated and looking at this beginning gets to this information to,

237
00:14:39.461 --> 00:14:43.180
rather than trying to police disinformation by looking at content,

238
00:14:43.810 --> 00:14:47.650
really looking instead at actions and behavior and account authenticity and

239
00:14:47.651 --> 00:14:49.780
dissemination patterns.
Cause a lot of the worst.

