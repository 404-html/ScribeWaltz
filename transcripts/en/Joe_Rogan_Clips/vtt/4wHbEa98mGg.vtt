WEBVTT

1
00:00:00.210 --> 00:00:03.360
That's important to sort of be nervous about it in that way,

2
00:00:03.780 --> 00:00:08.780
but it's not conducive to what do we do about it and the people that know what

3
00:00:09.721 --> 00:00:14.370
to do about it are the people trying to build this technology of building this

4
00:00:14.371 --> 00:00:17.940
future one step at a time.
I know what to do about it because like let's,

5
00:00:17.941 --> 00:00:20.490
let's put it in terms of a Elon Musk,
right?

6
00:00:20.640 --> 00:00:24.420
Like Ilan mosque is terrified of artificial intelligence cause he thinks by the

7
00:00:24.421 --> 00:00:27.060
time it becomes sentient and it'll be too late,

8
00:00:27.230 --> 00:00:29.460
it will be smarter than us and we'll,

9
00:00:29.461 --> 00:00:32.850
we'll have essentially created our successors.
Yes.

10
00:00:33.810 --> 00:00:37.680
And let me quote Joe Rogan and say that's just one guy.
Yeah.

11
00:00:38.040 --> 00:00:40.990
Well Sam Harris thinks the same thing.
Yes.
And there's a lot,
lot,

12
00:00:41.000 --> 00:00:42.990
a few people that think that Sam Harris,

13
00:00:42.991 --> 00:00:45.270
I think is one of the smartest people I know.

14
00:00:45.330 --> 00:00:50.310
And Elon Musk intelligence aside is one of the most impactful people I know.

15
00:00:50.530 --> 00:00:54.980
And he's actually building these cars.
And in the narrow AI sense,

16
00:00:55.070 --> 00:00:59.580
if he's built these
autopilot system that we've been studying,

17
00:01:00.870 --> 00:01:03.930
the way that system works is incredible.

18
00:01:04.020 --> 00:01:06.270
It was very surprising to me on many levels.

19
00:01:06.300 --> 00:01:11.300
It's an incredible demonstration of what AI can do in a positive way in the

20
00:01:11.401 --> 00:01:13.390
world.
So I don't know,

21
00:01:13.560 --> 00:01:16.440
but I people can disagree.

22
00:01:17.280 --> 00:01:22.280
I'm not sure the functional value of his fear about the possibility of this

23
00:01:24.150 --> 00:01:25.410
philosophies.
Correct.

24
00:01:25.650 --> 00:01:29.460
There's functional value and hitting the brakes before this takes place.

25
00:01:29.700 --> 00:01:34.700
The two just just to be a person who's standing on top of the rocks with a light

26
00:01:35.400 --> 00:01:38.940
to warn the boats,
Hey,
there's a rock here.
Like are you,

27
00:01:38.960 --> 00:01:42.390
you pay attention to where we're going because there's perils ahead.

28
00:01:42.720 --> 00:01:46.350
I think that's what he saying that I don't think there's anything wrong with

29
00:01:46.351 --> 00:01:47.400
saying that and I think there's,

30
00:01:47.660 --> 00:01:51.600
there's plenty of room for people saying what he's saying and people saying what

31
00:01:51.601 --> 00:01:55.980
you're saying.
I think what would hurt us is if we tried to silence either voice.

32
00:01:56.190 --> 00:02:01.190
I think what we need in terms of our understanding of this future is many,

33
00:02:03.871 --> 00:02:04.860
many,
many,
many,

34
00:02:04.861 --> 00:02:08.970
many of these conversations where you're dealing with the,

35
00:02:09.960 --> 00:02:14.960
the current state of technology versus a bunch of creative interpretations of

36
00:02:15.421 --> 00:02:20.310
where this could go and have discussions about where it should go or what could

37
00:02:20.311 --> 00:02:25.311
be the possible pitfalls of any current or future actions.

38
00:02:26.250 --> 00:02:28.800
I don't think there's anything wrong with this.
So when you say like,

39
00:02:29.580 --> 00:02:32.730
what's the benefit of thinking in a negative way?
Well,

40
00:02:32.731 --> 00:02:37.260
it's to prevent our demise.
So totally,

41
00:02:37.560 --> 00:02:42.560
I agree on a percent negativity or worry about the existential threat is really

42
00:02:44.011 --> 00:02:47.670
important to have as part of the conversation.
But there's this level,

43
00:02:47.671 --> 00:02:48.361
there's this line,

44
00:02:48.361 --> 00:02:52.800
it's hard to put into words is it is a line that you cross when that worry

45
00:02:52.801 --> 00:02:55.010
becomes hyperbole.
Yeah.
And,

46
00:02:55.011 --> 00:02:58.770
and then there's something about human psyche where it becomes paralyzing for

47
00:02:58.771 --> 00:02:59.710
some,
right

48
00:03:00.100 --> 00:03:05.020
<v 1>now when I have beers and my friends than Non-i folks,
we actually go,</v>

49
00:03:05.080 --> 00:03:07.930
we crossed that line all day and have fun with it.

50
00:03:08.350 --> 00:03:10.780
I talked should get you drunk right now.
Maybe

51
00:03:12.430 --> 00:03:17.190
regret every moment of it.
This,
I talked to Steve Pinker,
uh,
enlightenment.

52
00:03:17.200 --> 00:03:21.970
Now this book kind of highlights that,

53
00:03:23.590 --> 00:03:25.710
that kind of,
um,

54
00:03:25.890 --> 00:03:30.890
that he's totally doesn't find that appealing because that's crossing all realms

55
00:03:31.061 --> 00:03:34.120
of rationality and reason.
What'd you say that appealing?

56
00:03:34.121 --> 00:03:37.660
What do you mean a crossing the line into what will happen in 50 years?

57
00:03:37.690 --> 00:03:40.030
What could happen?
What could happen?
He doesn't find that appealing.

58
00:03:40.410 --> 00:03:44.410
It doesn't find it appealing because he's studied and I'm not sure I d I agree

59
00:03:44.411 --> 00:03:46.780
with him,
uh,
to,
to the degree that he takes it.

60
00:03:48.610 --> 00:03:51.910
He finds that there's no evidence.

61
00:03:51.911 --> 00:03:56.911
He wants there all our discussions to be grounded in evidence and data.

62
00:03:57.790 --> 00:03:58.623
And he,

63
00:03:59.020 --> 00:04:02.830
he highlights the fact that there's something about human psyche that desires

64
00:04:02.860 --> 00:04:07.750
this negativity.
That it,
once there's,

65
00:04:07.930 --> 00:04:12.190
there's something undeniable where we want to create an engineer,

66
00:04:12.191 --> 00:04:15.190
the gods that overpower us and destroy us.

67
00:04:15.860 --> 00:04:20.240
<v 0>We want to,
or we worry about it.
There's stuff we want to,</v>

68
00:04:21.280 --> 00:04:24.460
<v 1>we,
uh,
let me rephrase that.
We want to worry about it.</v>

69
00:04:24.461 --> 00:04:26.530
There's something about the psyche that,
but yeah,

70
00:04:26.590 --> 00:04:29.650
<v 0>that because you can't take the genie and put it back in the bottle.</v>

71
00:04:29.680 --> 00:04:34.480
That's right.
Yeah,
I mean when you say there's no reason to think this way,

72
00:04:35.170 --> 00:04:39.310
but if you do have cars that are semi autonomous now,

73
00:04:39.311 --> 00:04:43.000
and if you do have computers that can beat human beings who are world go

74
00:04:43.001 --> 00:04:47.110
champions.
And if you do have computers,
computers that can beat people at chess,

75
00:04:47.111 --> 00:04:50.150
and you do have people that are consistently working on artificial intelligence

76
00:04:50.151 --> 00:04:55.151
and you do have Boston dynamics who are getting these robots to do all sorts of

77
00:04:55.481 --> 00:04:57.220
spectacular physical stunts.

78
00:04:57.221 --> 00:05:00.580
And then you think about the possible future convergence of all these

79
00:05:00.581 --> 00:05:01.420
technologies.

80
00:05:01.720 --> 00:05:05.800
And then you think about the possibility of this exponential increase in

81
00:05:05.801 --> 00:05:10.060
technology that allows them to be sentient,
like within a decade,
two decades,

82
00:05:10.061 --> 00:05:13.450
three decades.
What,
what more evidence do you need?
Your,

83
00:05:13.451 --> 00:05:18.250
you're seeing all the building blocks of a potential successor being laid out in

84
00:05:18.251 --> 00:05:23.251
front of you and you're seeing what we do with every single aspect of

85
00:05:23.381 --> 00:05:27.160
technology.
We constantly and consistently improve and innovate,
right?

86
00:05:27.161 --> 00:05:29.860
With everything,
whether it's computers or cars or anything.

87
00:05:30.040 --> 00:05:33.970
Everything today is better than everything.
That was 20 years ago.

88
00:05:34.360 --> 00:05:36.880
So if you looked at artificial intelligence,

89
00:05:36.881 --> 00:05:40.620
which does agree Dee does exist to a certain extent,
and you,

90
00:05:40.650 --> 00:05:45.550
you look at what it could potentially be 30,
40,
50 years from now,

91
00:05:45.551 --> 00:05:50.530
whatever it is,
why wouldn't you look at all these data points and say,
hey,

92
00:05:50.890 --> 00:05:54.280
this could go bad.
I mean it could go great,

93
00:05:54.610 --> 00:05:55.990
but it could also go bad.

94
00:05:58.100 --> 00:06:01.580
<v 1>I do not want to be mistaken as the person who's not the champion or the</v>

95
00:06:01.581 --> 00:06:04.820
impossible.
I agree with you completely.
It think it's impossible.

96
00:06:06.020 --> 00:06:08.750
It's impossible at all.
I think it's inevitable.

97
00:06:10.550 --> 00:06:13.220
I don't,
I think it is inevitable.
Yes,

98
00:06:14.960 --> 00:06:17.630
it's the Sam Harris argument.
If,

99
00:06:17.720 --> 00:06:21.110
if super intelligence is nothing more than information processing,

100
00:06:22.650 --> 00:06:27.350
uh,
it's same as the argument of the simulation that we're living in a simulation

101
00:06:27.980 --> 00:06:30.260
that's very difficult to argue against the fact that we're living in a

102
00:06:30.261 --> 00:06:34.880
simulation.
The question is when and what the world would look like,

103
00:06:35.100 --> 00:06:39.860
right?
So it's like I said,
a race and,
and it's difficult.

104
00:06:40.700 --> 00:06:43.880
You have to balance those two minds.
I agree with you.
Totally.

105
00:06:43.881 --> 00:06:48.770
You and I disagree with my fellow robotics folks who don't want to think about

106
00:06:48.771 --> 00:06:49.780
it at all.
Of course,
they don't

107
00:06:49.810 --> 00:06:51.190
<v 0>the one to buy new houses.</v>

108
00:06:51.390 --> 00:06:53.710
If they've got a lot of money invested in this adventure,

109
00:06:53.890 --> 00:06:56.200
they want to keep the party rolling.
They don't want to pull the brakes.

110
00:06:56.260 --> 00:06:58.780
Everybody pulled the cords out of the walls.
We've got to stop.

111
00:06:59.170 --> 00:07:01.930
No one's going to do that.
No one's,
no one's going to come along and say,
Hey,

112
00:07:01.931 --> 00:07:02.440
we've,

113
00:07:02.440 --> 00:07:05.200
we've run all this data through a computer and we found that if we just keep

114
00:07:05.201 --> 00:07:06.850
going the way we're going in 30 years from now,

115
00:07:06.851 --> 00:07:10.810
we will have a successor that will decide that human beings are outdated and

116
00:07:10.811 --> 00:07:15.811
inefficient and dangerous to the the actual world that we live in and we're

117
00:07:15.881 --> 00:07:16.960
going to start wiping them out.

118
00:07:17.930 --> 00:07:19.380
<v 1>But that's not exactly,</v>

119
00:07:19.880 --> 00:07:24.680
<v 0>that's exactly right.
Now.
It doesn't exist right now,
but if that did happen,</v>

120
00:07:25.040 --> 00:07:30.040
if someone did come to the UN and had this multistage presentation with data

121
00:07:32.001 --> 00:07:34.880
that showed that if we continue on the path we're going,

122
00:07:34.881 --> 00:07:39.200
we have seven years before artificial intelligence decides to eliminate human

123
00:07:39.201 --> 00:07:42.800
beings based on these data points.
What do you,
what do they do?

124
00:07:42.801 --> 00:07:46.010
What did the Boston dynamics people do well,
building a house in Cambridge.

125
00:07:46.011 --> 00:07:49.520
What are you talking about man?
I'm not going anywhere.
Come on.

126
00:07:49.521 --> 00:07:52.260
I just bought a new Tesla.
I need to finances thing.
Hey,

127
00:07:52.261 --> 00:07:54.860
you got got credit card bills.
I got student loans.
I'm still paying off.

128
00:07:55.190 --> 00:07:58.550
Like what?
How do you stop people from doing what they do for a living?

129
00:07:58.551 --> 00:08:01.090
How do you say that?
Hey you,

130
00:08:01.130 --> 00:08:05.030
I know that you would like to look at the future with rose colored glasses on,

131
00:08:05.180 --> 00:08:09.410
but there's a real potential pitfall that could be the extermination of the

132
00:08:09.411 --> 00:08:10.340
human species.

133
00:08:10.650 --> 00:08:15.080
<v 1>Right,
and obviously I'm going way far with this.
Yeah,
I like it.</v>

134
00:08:16.310 --> 00:08:21.310
I think every one of us trying to build these systems are similar in sound to

135
00:08:22.731 --> 00:08:27.731
the way you were talking about the touch of death in that my dream and the dream

136
00:08:28.971 --> 00:08:33.971
of manual roboticist is to create intelligence systems that will improve our

137
00:08:34.131 --> 00:08:38.660
lives and working really hard at it,

138
00:08:38.690 --> 00:08:40.250
not for our house and Cambridge,

139
00:08:40.820 --> 00:08:45.320
not $4 billion for selling a startup paycheck.

140
00:08:45.950 --> 00:08:47.720
<v 0>We love this stuff.
Some of you.</v>

141
00:08:47.980 --> 00:08:52.090
So obviously the motivations are different for every single human being.

142
00:08:52.091 --> 00:08:53.510
It's involved in every endeavor.

143
00:08:53.600 --> 00:08:54.100
<v 1>So,</v>

144
00:08:54.100 --> 00:08:58.740
and we're trying really hard to build these systems and is really hard.

145
00:08:58.830 --> 00:09:02.070
So whenever the,
the question is,
well,

146
00:09:02.160 --> 00:09:05.430
this is going to look at historically is going to take off,

147
00:09:05.880 --> 00:09:07.740
it can potentially take off any moment.

148
00:09:08.160 --> 00:09:13.160
It's very difficult to really be cognizant as an engineer about how it takes off

149
00:09:15.541 --> 00:09:19.380
because you're trying to make it take off in a positive direction and you're

150
00:09:19.381 --> 00:09:23.340
failing.
Everybody's failing.
It's been really hard.

151
00:09:24.210 --> 00:09:28.170
And so you have to acknowledge that there,

152
00:09:28.171 --> 00:09:29.640
that overnight,

153
00:09:30.060 --> 00:09:34.710
some Eli Musk type character might come along and you know,

154
00:09:34.740 --> 00:09:38.250
people with this boring company,
uh,
or with space x,

155
00:09:38.251 --> 00:09:42.270
people didn't think anybody but NASA could do what Musk is doing and he's doing

156
00:09:42.271 --> 00:09:46.920
it.
It's hard to think about that too much.
You have to do that.

157
00:09:47.220 --> 00:09:52.220
But the reality is we're trying to create these super intelligent beings.

158
00:09:53.270 --> 00:09:53.631
<v 0>Sure.</v>

159
00:09:53.631 --> 00:09:57.650
But isn't the reality also that we have done things in the past because we were

160
00:09:57.651 --> 00:10:01.820
trying to do it and then we realized that these have horrific consequences for

161
00:10:01.821 --> 00:10:06.350
the human race.
Like Oppenheimer in the Manhattan project.
You know,
when he said,

162
00:10:06.620 --> 00:10:10.200
I am death destroyer of worlds,
when,

163
00:10:10.201 --> 00:10:11.990
when he was quoting the Bhagavad Gita,

164
00:10:12.290 --> 00:10:16.310
when he's detonating the first nuclear bomb and realizing what he's done,

165
00:10:16.730 --> 00:10:20.090
just because something's possible to do doesn't necessarily mean it's a good

166
00:10:20.091 --> 00:10:21.590
idea for human beings to do it.
Now,

167
00:10:21.770 --> 00:10:26.630
we haven't destroyed the world with Oppenheimers discovery and through the work

168
00:10:26.631 --> 00:10:27.530
of the Manhattan project,

169
00:10:27.531 --> 00:10:30.970
we've managed to somehow or another keep the lid on this shit for the last

170
00:10:30.980 --> 00:10:33.620
incredible.
It's crazy.
Right?
You know,

171
00:10:33.621 --> 00:10:38.180
I mean for the last 70 years,
how has it been?
Seven days?
Sounds right.

172
00:10:38.181 --> 00:10:41.000
Ten thousand twenty thousand nukes all over the world right now.

173
00:10:41.300 --> 00:10:46.290
Crazy mean we literally could kill everything on the planet and somehow we dot

174
00:10:46.400 --> 00:10:51.110
somehow somehow in some amazing way we have not.
But that doesn't mean we,

175
00:10:51.170 --> 00:10:56.170
I mean this a very short amount of time in relation to the actual lifespan of

176
00:10:57.441 --> 00:11:02.360
the earth itself and certainly in terms of the time human history has been
around

177
00:11:02.920 --> 00:11:04.990
<v 1>and nuclear weapons.</v>

178
00:11:06.340 --> 00:11:09.070
Global warming is another one.
Sure.

179
00:11:09.180 --> 00:11:11.310
<v 0>That's a side effect of our actions,
right?
This is,</v>

180
00:11:11.311 --> 00:11:16.311
we're talking about a direct effect of human ingenuity and innovation.

181
00:11:16.410 --> 00:11:21.020
The nuclear bomb,
it's a direct effect.
We tried to make it,
we made it there.

182
00:11:21.040 --> 00:11:25.170
Ghost global warming's an accidental consequence of human civilization.

183
00:11:26.270 --> 00:11:30.410
<v 1>So you can't,
I don't think is possible</v>

184
00:11:30.410 --> 00:11:32.360
<v 0>to not be build a nuclear bomb.</v>

185
00:11:33.290 --> 00:11:36.920
You don't think it's possible to not build it in terms of it because people are

186
00:11:36.921 --> 00:11:38.510
tribal,
they speak different languages,

187
00:11:38.511 --> 00:11:40.830
they have different desires and needs and they were,

188
00:11:40.831 --> 00:11:44.000
and more so if all these engineers were working towards it,

189
00:11:44.330 --> 00:11:46.460
it was not possible to not build it.

190
00:11:46.740 --> 00:11:48.450
<v 1>Yep.
And the,
like I said,</v>

191
00:11:48.451 --> 00:11:53.130
there's something about us chimps in a large collective where we,
uh,

192
00:11:53.530 --> 00:11:54.190
are born

193
00:11:54.190 --> 00:11:56.570
<v 0>and push forward towards progress of technology.</v>

194
00:11:56.590 --> 00:11:59.380
You cannot stop the progress of technology.

195
00:11:59.620 --> 00:12:02.560
So the goal is to how to develop,

196
00:12:02.561 --> 00:12:05.590
how to guide that development into a positive direction.

197
00:12:05.620 --> 00:12:10.620
But surely if we do understand that this has taken place and we did drop these

198
00:12:11.890 --> 00:12:16.890
enormous bombs on Hiroshima and Nagasaki and killed untold amounts of innocent

199
00:12:18.491 --> 00:12:20.980
people with these destinations,

200
00:12:21.400 --> 00:12:25.900
that it's not necessarily always a good thing to pursue technology.

201
00:12:26.620 --> 00:12:31.610
It nobody is.
So then you see what I'm saying?
Hundred percent.

202
00:12:31.750 --> 00:12:35.140
I agree with you.
Totally.
I'm more playing devil's advocate than anything,

203
00:12:35.380 --> 00:12:35.921
but what I'm,

204
00:12:35.921 --> 00:12:39.220
what I'm saying is you guys are looking at these things like we're just trying

205
00:12:39.221 --> 00:12:44.020
to make these things happen and what I think people like Elon Musk and Sam

206
00:12:44.020 --> 00:12:47.380
Harris and a bunch of others that are gravely concerned about the potential for

207
00:12:47.381 --> 00:12:52.030
AI are saying is that's,
I understand what you're doing,

208
00:12:52.270 --> 00:12:54.730
but you've got to understand the other side of it.

209
00:12:54.970 --> 00:12:57.850
You've got to understand that there are people out there that are terrified that

210
00:12:57.851 --> 00:12:59.020
if you do extrapolate,

211
00:12:59.021 --> 00:13:04.021
if you do take this are relentless thirst for innovation and keep going with it,

212
00:13:05.650 --> 00:13:08.710
would you look at what we can do?
So what human beings can do?

213
00:13:08.711 --> 00:13:12.020
So far in our crude manner of two,

214
00:13:12.160 --> 00:13:15.670
2018 we'll all of the amazing things they've been able to accomplish,

215
00:13:16.150 --> 00:13:20.260
it's entirely possible that we might be creating our successors.

216
00:13:20.290 --> 00:13:23.980
This is not outside the realm of possibility and all of our biological

217
00:13:23.981 --> 00:13:26.030
limitations might be we,

218
00:13:26.120 --> 00:13:30.970
we might figure out a better way and this better way might be some sort of an

219
00:13:31.000 --> 00:13:33.310
artificial creature.
Yup.

220
00:13:33.490 --> 00:13:37.150
Ai began with our dream to forge the gods,
and that's,

221
00:13:37.780 --> 00:13:41.260
I would like,
I think that it's impossible to stop.

