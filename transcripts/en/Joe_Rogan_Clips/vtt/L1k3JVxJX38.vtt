WEBVTT

1
00:00:00.120 --> 00:00:02.580
Now I want to talk to you about two parts of what you just said.

2
00:00:02.581 --> 00:00:07.581
One being the possibility that one day we can upload our mind or make copies of

3
00:00:07.891 --> 00:00:08.480
our mind.

4
00:00:08.480 --> 00:00:11.770
<v 1>You're,
you're up for it
here.</v>

5
00:00:12.560 --> 00:00:15.170
My Little Joe Rogan them from my phone.

6
00:00:15.210 --> 00:00:20.030
You can just call me dude like you the organic version.
But the,

7
00:00:21.240 --> 00:00:25.110
<v 0>do you think that that's a real possibilities inside of our lifetime that we can</v>

8
00:00:25.111 --> 00:00:29.280
map out the human mind to the point where we can essentially recreate it.

9
00:00:29.760 --> 00:00:34.020
But if you do recreate it without all the biological words and the human reward

10
00:00:34.021 --> 00:00:36.200
systems that are built in the,
what the fuck are we,

11
00:00:36.270 --> 00:00:40.030
<v 1>I mean,
well that's a different question,
right?
I mean,
I think,
what is your mind?</v>

12
00:00:41.020 --> 00:00:45.340
Well,
I,
I think that there's two things that are needed for,
let's say,

13
00:00:45.550 --> 00:00:50.290
let's say human body uploading to simplify things,
body uploading.

14
00:00:50.320 --> 00:00:51.760
There are two things that are needed.

15
00:00:51.790 --> 00:00:56.530
One thing is a better computing infrastructure than we have.

16
00:00:56.531 --> 00:00:59.950
No.
Okay.
To host the uploaded body and,

17
00:00:59.951 --> 00:01:04.951
and the other thing is a better scanning technology because right now we don't

18
00:01:05.081 --> 00:01:09.850
have a way to scan the molecular structure of your body without like freezing

19
00:01:09.851 --> 00:01:11.530
you,
slicing you and scanning you,

20
00:01:11.531 --> 00:01:14.530
which you probably don't want done at this point in time.
Right?

21
00:01:15.010 --> 00:01:20.010
So assuming both those are solved and you could then recreate in some computer

22
00:01:20.681 --> 00:01:25.450
simulation,
you know,
a,
an accurate simulacrum of,

23
00:01:25.451 --> 00:01:27.580
of,
of,
of what you are,
right,

24
00:01:28.470 --> 00:01:32.860
<v 0>where I'm getting this,
where I'm getting at an accurate simulate crumb is gay.</v>

25
00:01:32.910 --> 00:01:35.880
That's getting weird because the biological variability of human beings,

26
00:01:35.881 --> 00:01:37.260
we vary day to day

27
00:01:37.520 --> 00:01:40.950
<v 1>very dependent upon it and we're swimming leg room would also vary day to day.</v>

28
00:01:40.951 --> 00:01:41.784
So with Dva,

29
00:01:42.040 --> 00:01:42.320
<v 0>Graham and,</v>

30
00:01:42.320 --> 00:01:47.320
and to have flaws because while we vary dependent upon how much sleep we get,

31
00:01:47.481 --> 00:01:49.360
whether or not we're feeling sick,
whether we're alone.

32
00:01:49.550 --> 00:01:51.620
<v 1>So it is all these,
if you're upload,</v>

33
00:01:51.621 --> 00:01:56.621
we're an accurate copy of then the simulation hosting your upload would need to

34
00:01:56.811 --> 00:02:01.811
have an accurate simulation of the laws of bio physics and chemistry that allow

35
00:02:03.591 --> 00:02:06.710
your body to,
you know,
evolve from one second to the neck.

36
00:02:06.740 --> 00:02:10.550
My concern is you upload would change second by second,

37
00:02:10.551 --> 00:02:14.140
just like just like you do and then we'll diverge from you.
Right.
So I mean,

38
00:02:14.150 --> 00:02:16.460
after an hour it'll be a little different.

39
00:02:16.461 --> 00:02:20.850
After a year it might have gone in,
in a quite different direction for you.

40
00:02:21.670 --> 00:02:25.630
<v 0>Oh,
some Super God monk living on the top of a mountain somewhere in a year.</v>

41
00:02:25.940 --> 00:02:26.970
If he keeps giving the problem

42
00:02:27.290 --> 00:02:30.320
<v 1>whenever.
It depends on what virtual world it's living in.
True.
I mean,</v>

43
00:02:30.321 --> 00:02:32.120
if it's living in the,
in the virtual world,

44
00:02:32.290 --> 00:02:33.490
<v 0>well,
there'll be a virtual world.</v>

45
00:02:33.520 --> 00:02:37.530
If you're not talking about the potential of downloading this again in sort of
a,

46
00:02:37.600 --> 00:02:38.433
into a biologic,

47
00:02:38.470 --> 00:02:41.230
<v 1>there's a lot of possibilities,
right?
Yeah.
I mean you,
you,
you could,</v>

48
00:02:41.260 --> 00:02:45.970
you could upload into a Joe Rogan living in the virtual world and then just

49
00:02:45.971 --> 00:02:47.740
create your own fantasy universe.

50
00:02:47.980 --> 00:02:52.980
Or you or you could three d print and alternate synthetic body,

51
00:02:53.530 --> 00:02:54.620
right?
I mean once you,

52
00:02:54.820 --> 00:02:59.820
once you have the ability to manipulate molecules at at will,

53
00:03:00.310 --> 00:03:00.740
the,

54
00:03:00.740 --> 00:03:04.690
the scope of possibilities becomes much greater now than we're used to thinking.

55
00:03:05.350 --> 00:03:09.010
My question is,
do,
do we replicate flaws?

56
00:03:09.070 --> 00:03:12.390
Do we replicate depression?
Of course.
But we got,

57
00:03:12.450 --> 00:03:14.170
we do that when we want to cure depression.

58
00:03:14.380 --> 00:03:18.270
So if we don't cure depression when we start,
here's the interesting thing.
Okay.

59
00:03:18.400 --> 00:03:21.920
Once,
once we have you in a digital form,
right?

60
00:03:22.420 --> 00:03:26.890
Then it's very programmable.
So then they stopped the dopamine and Serotonin,

61
00:03:26.891 --> 00:03:30.400
then you can change what you want and then you have a whole different set of

62
00:03:30.401 --> 00:03:34.180
issues.
Right?
Yeah.
Cause once you've changed,
I mean,

63
00:03:34.240 --> 00:03:39.240
suppose you make a fork of yourself and then you manipulate it in a certain way

64
00:03:39.550 --> 00:03:42.980
and you're,
then after a few hours you're like,
well I don't,

65
00:03:43.100 --> 00:03:46.690
I don't much like this,
uh,
new Joe here.

66
00:03:46.930 --> 00:03:50.710
Maybe we should roll back that change.
But the new Joe was like,
well,

67
00:03:51.220 --> 00:03:53.740
I like myself very well.
Thank you.
All right.

68
00:03:53.980 --> 00:03:58.980
So then there there's a lot of issues that that will come up once we can write,

69
00:04:00.250 --> 00:04:03.300
modify,
and reprogram ourselves.

70
00:04:04.120 --> 00:04:07.650
The ramifications of these decisions are almost insurmountable,

71
00:04:07.651 --> 00:04:12.050
like once once the ball gets rolling,
well,
the ramifications of the,

72
00:04:12.230 --> 00:04:17.050
the ramifications of these decisions are going to be very interesting to
explore.

73
00:04:17.110 --> 00:04:21.490
Yes,
you're super positive,
Ben.
Super positive,
but you're optimistic.

74
00:04:21.520 --> 00:04:24.760
Many bad things will happen.
Many good things will happen.
That's a very,

75
00:04:24.761 --> 00:04:28.210
that's a very easy prediction to make.
Okay,
I see what you're saying.
Yeah.

76
00:04:28.211 --> 00:04:32.020
I've just a wondering they think,
think about like world travel,
right?
Let go.

77
00:04:32.021 --> 00:04:33.640
Like hundreds of years ago,

78
00:04:33.641 --> 00:04:38.110
most people didn't travel more than a very short distance from their home and

79
00:04:38.111 --> 00:04:42.130
you could say,
well,
okay,
what if,
what if people could travel all over the world,

80
00:04:42.131 --> 00:04:45.340
right?
Like what horrible things could happen?
They would lose their culture.

81
00:04:45.670 --> 00:04:48.760
Like they might go marry someone from,
from a random tribe,

82
00:04:49.210 --> 00:04:52.150
you can get killed and in the Arctic region or something,

83
00:04:52.360 --> 00:04:55.240
a lot of bad things can happen when you travel far from your home.

84
00:04:55.360 --> 00:04:57.100
A lot of good things can happen.

85
00:04:57.490 --> 00:05:02.490
And the ultimately the ramifications were not foreseen by people 500 years ago.

86
00:05:02.620 --> 00:05:05.830
Yeah.
I mean,
we're going into a lot of new domains.

87
00:05:06.220 --> 00:05:11.170
We can't see the details of the pluses and minuses that are going to unfold.

88
00:05:11.810 --> 00:05:16.330
Uh,
W it would behoove us to simply become comfortable with radical uncertainty

89
00:05:16.630 --> 00:05:19.510
because otherwise we're going to confront it anyway and we're just going to be

90
00:05:19.511 --> 00:05:24.130
nervous.
So it's just inevitable.
It's almost inevitable.
I mean,
uh,

91
00:05:24.210 --> 00:05:27.670
of course.
Sorry.
Any natural,
I mean,
yeah,
I mean,

92
00:05:27.671 --> 00:05:31.570
of course Trump could start a nuclear war and then we're reselling to ground

93
00:05:31.571 --> 00:05:35.860
zero where we get hit by an asteroid,
right?
Yeah.
I mean,

94
00:05:36.100 --> 00:05:38.590
so barring a catastrophic outcome,

95
00:05:39.040 --> 00:05:44.040
I believe a technological singularity is essentially inevitable.

96
00:05:45.130 --> 00:05:49.960
There's a radical uncertainty attached to this.
On the other hand,

97
00:05:50.890 --> 00:05:53.590
you know,
in as much as we humans can know anything,

98
00:05:54.160 --> 00:05:59.160
it would seem common sensically there's the ability to buy us this in a positive

99
00:05:59.751 --> 00:06:02.690
rather than the negative direction.

100
00:06:03.110 --> 00:06:07.940
We should be spending more of our attention on doing that rather than,

101
00:06:07.941 --> 00:06:12.740
for instance,
advertising spying in making chocolate and your chocolates.

102
00:06:12.741 --> 00:06:16.580
And all the other things that,
I mean it's prevalent,
it's everywhere,

103
00:06:16.610 --> 00:06:19.790
but I mean how many people are actually at the helm of that as opposed to how

104
00:06:19.791 --> 00:06:24.710
many people are working on various aspects of technology all across the planet?

105
00:06:24.800 --> 00:06:26.750
It's a small group and in comparison,

106
00:06:26.751 --> 00:06:30.650
working on explicitly bringing about the singularity is a small group.

107
00:06:31.490 --> 00:06:36.260
On the other hand,
supporting technologies is a very large group.

108
00:06:36.261 --> 00:06:41.090
So think about like Gpu is,
where did they come from?
Accelerating gaming,

109
00:06:41.091 --> 00:06:45.980
right?
Lo and behold,
they're amazingly useful for training neural net models,

110
00:06:45.981 --> 00:06:49.280
which is the one among many important types of Ai,
right?

111
00:06:49.281 --> 00:06:53.900
So a large amount of the planet's resources are now getting spent on

112
00:06:53.901 --> 00:06:58.901
technologies that are indirectly supporting these singular attarian technology.

113
00:06:59.251 --> 00:07:00.620
So as another example,

114
00:07:00.621 --> 00:07:05.090
like microarray or is it let you measure the expression level of genes,

115
00:07:05.300 --> 00:07:08.750
how,
how much each gene is doing in your body at each point in time.

116
00:07:09.050 --> 00:07:12.580
These were originally developed,
you know,
as an outgrowth of,

117
00:07:12.581 --> 00:07:16.880
of printing technology.
Then instead of squirting ink after metrics figured out,

118
00:07:16.881 --> 00:07:19.970
you could squirt DNA.
Right?
So,
I mean,

119
00:07:19.971 --> 00:07:24.971
the amount of technology specifically oriented toward the singularity doesn't

120
00:07:25.551 --> 00:07:28.580
have to be large because the overall,
you know,

121
00:07:28.640 --> 00:07:33.170
spectrum of supporting technologies can be subverted in that direction.

122
00:07:33.740 --> 00:07:37.970
Do you,
do you have any concerns at all about a virtual world?
I mean,

123
00:07:38.330 --> 00:07:42.560
we may be in one right now.
That's true,
but as far as I know,

124
00:07:42.561 --> 00:07:46.380
my problem is I want to find that program are and get them to make more,
more,

125
00:07:46.450 --> 00:07:48.280
more attractive people.
You know,

126
00:07:49.050 --> 00:07:52.250
that I thought I would say that that's part of the reason why it attracted

127
00:07:52.251 --> 00:07:55.230
people are so interesting I that they're unique and rare.
Yeah.
All right.

128
00:07:55.260 --> 00:08:00.200
Our problems with calling everything beautiful.
Yeah.
Beautiful.

129
00:08:00.201 --> 00:08:03.020
I was like,
well,
you just have to get realistic.

130
00:08:03.050 --> 00:08:06.280
If I get in the right frame of mind that can find anything beautiful and you

131
00:08:06.290 --> 00:08:10.490
could find it unique and interesting.
Oh,
I can find anything below I guess.

132
00:08:10.491 --> 00:08:15.110
But in terms of like,
yeah,
I guess it's subjective,
right?
Really it is.

133
00:08:15.111 --> 00:08:18.230
We're talking about beauty,
right?
Huh?
Yeah.
Now,

134
00:08:18.260 --> 00:08:21.250
but VR existential angst just on the,

135
00:08:21.590 --> 00:08:25.220
when people sit and think about the pointlessness of our own existence,

136
00:08:25.460 --> 00:08:29.400
like we are these finite beings that are clinging to a balls.

137
00:08:29.401 --> 00:08:32.960
It spins a thousand miles an hour hurling through infinity and what's the point?

138
00:08:33.170 --> 00:08:35.720
I think there's a lot of that that goes around already.
We've,

139
00:08:35.721 --> 00:08:40.721
we create an artificial environment that we can literally somehow another

140
00:08:40.970 --> 00:08:45.970
download a version of us and it exists and this block chain created or or

141
00:08:51.170 --> 00:08:56.170
powered weird fucking simulation world.

142
00:08:57.570 --> 00:09:01.890
What would be,
I mean what would it be?
The point of that I really believe,

143
00:09:02.430 --> 00:09:03.263
which

144
00:09:04.710 --> 00:09:08.190
is a bit personal and maybe different than many of my colleagues.

145
00:09:08.690 --> 00:09:13.690
What I really believe is that these advancing technologies are going to lead us

146
00:09:15.901 --> 00:09:20.901
to unlock many different states of consciousness and experience.

147
00:09:21.240 --> 00:09:25.580
Then then most people are,
are,

148
00:09:25.581 --> 00:09:30.040
are currently aware of are,
I mean you,
you,

149
00:09:30.041 --> 00:09:34.410
you say we're,
we're just an insignificant species on the,
on a,
you know,

150
00:09:34.411 --> 00:09:35.610
a speck of rock hurling.

151
00:09:35.640 --> 00:09:40.020
In the other space there's people that have existential angst because they

152
00:09:40.021 --> 00:09:41.890
wonder about what the purpose was.

153
00:09:42.420 --> 00:09:47.420
I ended that I tend to feel like we understand almost nothing about who and what

154
00:09:51.661 --> 00:09:55.110
we are and our knowledge about the universe's far small,

155
00:09:55.111 --> 00:09:56.790
extremely minuscule.

156
00:09:56.791 --> 00:10:00.930
I mean if anything I look at things from more of a a Buddhist or

157
00:10:00.931 --> 00:10:02.210
phenomenological way,

158
00:10:02.220 --> 00:10:07.220
like there's sense perceptions and then out of those sense perceptions,

159
00:10:07.770 --> 00:10:12.770
models arise and accumulate including a model of the self and the model of the

160
00:10:14.431 --> 00:10:17.700
body and the model of the physical world out there.

161
00:10:18.060 --> 00:10:20.850
And by the time you get to planets and stars and blockchains,

162
00:10:20.851 --> 00:10:25.560
you're building like hypothetical models on top of hypothetical models.

163
00:10:25.620 --> 00:10:27.630
And then,
you know,
we're,

164
00:10:27.900 --> 00:10:32.900
we're by building intelligent machines and mindup loading machines and virtual

165
00:10:33.931 --> 00:10:37.890
realities,
we're going to radically transform,

166
00:10:38.580 --> 00:10:38.971
you know,

167
00:10:38.971 --> 00:10:43.440
our whole state of consciousness or understanding of what mind and matter are.

168
00:10:43.830 --> 00:10:48.830
Our experience of our own selves or even whether it's self exists.

169
00:10:49.201 --> 00:10:54.201
And I think ultimately the state of consciousness of a human being like a

170
00:10:56.010 --> 00:11:01.010
hundred years from now after a technological singularity is going to bear very

171
00:11:01.141 --> 00:11:05.730
little resemblance to the states of consciousness we have now.

172
00:11:05.731 --> 00:11:06.610
We're just going to,

173
00:11:06.810 --> 00:11:11.280
we're going to see a much wider universe in any of us.
Now.

174
00:11:12.180 --> 00:11:15.630
Imagine,
imagine to exist now.

175
00:11:16.320 --> 00:11:18.500
This is my own personal view of things.
The,

176
00:11:18.510 --> 00:11:23.100
you don't have to agree with that to think the technological singularity will be

177
00:11:23.640 --> 00:11:25.010
valuable,
but that is how I look.

178
00:11:25.070 --> 00:11:29.640
Know like Ray Kurzweil and I agree there's going to be a technological

179
00:11:29.641 --> 00:11:31.860
singularity within decades at most.

180
00:11:32.490 --> 00:11:35.280
And Ray and I agree that,
you know,

181
00:11:35.370 --> 00:11:40.070
if we buy US technology development appropriately,
we can very likely,

182
00:11:40.190 --> 00:11:40.500
you know,

183
00:11:40.500 --> 00:11:45.500
guide this to be a world of abundance and benefit for humans as well as ais.

184
00:11:46.670 --> 00:11:51.670
But Ray is a bit more of a down to Earth empiricist then than I am lucky.

185
00:11:53.230 --> 00:11:56.310
He thinks we understand more about the universe right now.

186
00:11:56.311 --> 00:11:57.780
I've been that than I do.

187
00:11:57.820 --> 00:12:02.020
So I mean there's a wide spectrum of views that are,

188
00:12:02.021 --> 00:12:03.940
are rational and sensible,
the hub.

189
00:12:03.941 --> 00:12:07.900
But my own view is we understand really,

190
00:12:07.901 --> 00:12:11.030
really little of what we are and what,

191
00:12:11.260 --> 00:12:13.120
what this world is.

192
00:12:13.121 --> 00:12:18.121
And this is part of my own personal quest for wanting to upgrade my brain and

193
00:12:19.691 --> 00:12:24.010
wanting to create artificial intelligence is I've always been driven above our

194
00:12:24.011 --> 00:12:26.740
last borrowing,
the understand everything I can about the world.

195
00:12:26.740 --> 00:12:31.060
So I mean I've studied every kind of science and engineering and social science

196
00:12:31.061 --> 00:12:32.590
and read every kind of literature.

197
00:12:32.591 --> 00:12:37.591
But in the end of the scope of human understanding is clearly very small,

198
00:12:38.171 --> 00:12:41.590
although at least we're smart enough to understand how little we understand,

199
00:12:41.591 --> 00:12:45.670
which I think might,
my dog doesn't understand how the understands,
right?
So,

200
00:12:45.880 --> 00:12:50.230
and even like my 10 month old son,
he understands a little,
he understands,

201
00:12:50.410 --> 00:12:54.370
which is interesting,
right?
Because he's,
because he's,
because he's also a human,

202
00:12:54.380 --> 00:12:57.160
right?
So I think,
I mean,

203
00:12:57.460 --> 00:13:01.750
everything we think and believe now is going to seem absolutely absurd to us

204
00:13:02.060 --> 00:13:03.330
after there's a singular idea.

205
00:13:03.331 --> 00:13:08.110
We're just going to look back and laugh in a warm hearted way and all the

206
00:13:08.111 --> 00:13:12.730
incredibly silly things we were thinking and doing back when we were trapped in

207
00:13:12.731 --> 00:13:16.930
our,
in our,
you know,
our,
our primitive biological brains and bodies.

208
00:13:16.990 --> 00:13:21.990
A stunning the ta in your opinion or your assessment is somewhere less than a

209
00:13:22.051 --> 00:13:24.430
thought a hundred years away from now.
Yeah.

210
00:13:24.440 --> 00:13:28.360
It requires exponential thinking.
Right?
Because if you,

211
00:13:28.660 --> 00:13:30.220
that's hard to wrap your head around.
Right?

212
00:13:31.520 --> 00:13:33.700
It's immediate for me to wrap my head around,

213
00:13:34.240 --> 00:13:35.860
but for a lot of people that you explain it to,

214
00:13:35.861 --> 00:13:39.400
I'm sure that that's a little bit of a roadblock.
It is.
It is.

215
00:13:39.401 --> 00:13:42.910
It took me sometime to get my parents dropped their heads around it because they

216
00:13:42.911 --> 00:13:45.940
didn't,
they're not,
they're not technologists.
I mean,

217
00:13:45.941 --> 00:13:50.941
I find if you get people to pay attention and sort of lead them through all the

218
00:13:52.061 --> 00:13:53.410
supporting evidence,

219
00:13:53.980 --> 00:13:58.980
most people can comprehend these ideas reasonably well to computers for 19 six

220
00:14:00.491 --> 00:14:04.690
it's just hard to grind.
It's hard to grab people's attention.
Then mobile phones,

221
00:14:04.691 --> 00:14:05.710
it made a big difference.

222
00:14:05.711 --> 00:14:10.000
Like I spent a lot of time in Africa and all this Ababa in Ethiopia where we

223
00:14:10.001 --> 00:14:15.001
have a large AI development office and you know the fact that mobile phones and

224
00:14:15.651 --> 00:14:19.990
then smart phones and rolled out so quickly even in rural Africa and that if

225
00:14:19.991 --> 00:14:21.700
it's such a transformative impact,

226
00:14:21.701 --> 00:14:25.750
I mean this is a metaphor that lets people understand the speed with what's

227
00:14:26.080 --> 00:14:27.760
exponential change can happen.

228
00:14:28.090 --> 00:14:31.720
When you talk about yourself and you talk about consciousness and how you

229
00:14:31.721 --> 00:14:35.180
interface with the world,
how do you see this?
I mean when you,

230
00:14:35.181 --> 00:14:37.390
when you say that we might be living in a simulation,

231
00:14:37.630 --> 00:14:40.240
do you actually entertain that?
Oh yeah you do.

232
00:14:40.750 --> 00:14:45.750
I mean I think the word simulation is probably wrong but yet the idea of an

233
00:14:46.590 --> 00:14:47.620
empirical,

234
00:14:47.890 --> 00:14:52.890
you know materialist physical world is almost certainly wrong also.

235
00:14:54.020 --> 00:14:55.040
So I'm in so,

236
00:14:56.090 --> 00:15:01.090
well again if you go back to a phenomenal view,

237
00:15:01.580 --> 00:15:06.580
I mean you could look at the mind is primary and you know your mind is building

238
00:15:08.630 --> 00:15:11.930
the world as as a model,
as a simple explanation of it,

239
00:15:11.931 --> 00:15:16.640
of its perceptions.
On the other hand then what is the mind?

240
00:15:16.641 --> 00:15:21.500
The self is also a model that get,
that gets built out of its perceptions,

241
00:15:21.740 --> 00:15:26.740
but then if I accept the new mind has some fundamental existence also based on

242
00:15:28.191 --> 00:15:30.740
the sort of are you feeling the ear like a mind,

243
00:15:30.741 --> 00:15:35.741
there are minds are working together to build each other and to build this world

244
00:15:37.220 --> 00:15:41.060
and there's a,
there's,
there's a whole different way of,

245
00:15:41.061 --> 00:15:46.061
of thinking about reality in terms of first and second person experience rather

246
00:15:46.191 --> 00:15:51.191
than these empiricist views like this is a computer simulation or something.

247
00:15:51.470 --> 00:15:51.711
Right,

248
00:15:51.711 --> 00:15:55.730
but you still greeted as a physical reality that we exist in or do you not,

249
00:15:55.731 --> 00:15:57.890
what does that word mean?
That's a weird word,
right?

250
00:15:58.160 --> 00:16:02.060
It is women who are looking as if reality.
If you look in,

251
00:16:02.080 --> 00:16:05.240
in modern physics,
even in quantum mechanics,

252
00:16:05.600 --> 00:16:09.350
there's something called the relational interpretation of quantum mechanics,

253
00:16:09.830 --> 00:16:14.480
which says that there's no sense in thinking about an observed entity,

254
00:16:14.660 --> 00:16:17.840
you should only think about an observed come observe her pair.

255
00:16:18.350 --> 00:16:23.350
Like there's no sense to think about some thing except from the perspective of,

256
00:16:23.550 --> 00:16:25.580
of some observer.
So that's,

257
00:16:25.940 --> 00:16:30.940
that's even true within our best current theory of modern physics as,

258
00:16:31.491 --> 00:16:35.450
as induced from empirical,
empirical observations.

259
00:16:35.480 --> 00:16:38.690
But in a pragmatic sense,
you know,
if you take a plane and fly to China,

260
00:16:38.691 --> 00:16:43.040
you actually land in China,
I guess you'd guess.

261
00:16:43.550 --> 00:16:47.520
Don't you live there?
I live in Hong Kong.
Yeah,
well cost.

262
00:16:47.760 --> 00:16:49.460
I mean I,

263
00:16:50.450 --> 00:16:53.440
I have an unusual state of consciousness I'm in.
I'm,

264
00:16:53.460 --> 00:16:55.040
that's what I'm trying to get at.
Oh,
if you,

265
00:16:55.041 --> 00:17:00.041
if you think about it like how do you know that you're not a brain floating in a

266
00:17:01.041 --> 00:17:02.990
vat somewhere,

267
00:17:03.200 --> 00:17:08.200
which is being fed illusions by certain evil scientist and two seconds from now

268
00:17:09.700 --> 00:17:12.770
he's going to pull this simulated world disappears and you realize you're just a

269
00:17:12.771 --> 00:17:15.320
brain in a Vat.
Again,
you don't know that.

270
00:17:15.350 --> 00:17:19.340
You write on your own personal experiences of falling in love with a woman and

271
00:17:19.341 --> 00:17:22.640
moving to another,
but these may all be put into my parent by the evil scientist.

272
00:17:22.850 --> 00:17:27.050
How do we know?
But they're,
they're very consistent.
Are they not the,

273
00:17:27.140 --> 00:17:31.730
the possibly illusory and implanted memories are very consistent,
I guess.

274
00:17:31.970 --> 00:17:35.750
I guess my own state of mind is,

275
00:17:35.751 --> 00:17:40.751
I'm always sort of acutely aware that this simulation might all disappear at

276
00:17:43.220 --> 00:17:44.630
any,
at any one moment.

277
00:17:45.020 --> 00:17:49.430
I'm so acutely aware of this consciously on everyday basis.
Pretty much.
Really?

278
00:17:49.690 --> 00:17:53.280
Yeah.
Really?
Why is that?
That doesn't seem to make sense.
I mean it's,
it's pretty,

279
00:17:53.760 --> 00:17:57.740
it's pretty rock solid.
It's here every day.
I mean,
so,

280
00:17:57.750 --> 00:18:02.640
so you're a possibly imploded memories lead you to believe?
Yes.
Possibly.

281
00:18:02.650 --> 00:18:05.970
Implanted memories need to believe that this life is incredibly consistent.

282
00:18:06.110 --> 00:18:09.420
Oh yeah.
Yeah.
This is incredibly consistent.

283
00:18:09.421 --> 00:18:12.650
This is you UMES problem of,
of induction,
right?

284
00:18:12.651 --> 00:18:16.890
From philosophy class and it's not,
that's not solved.

285
00:18:16.930 --> 00:18:21.490
And with you in a conceptual sense,
I get,
I just feel this philosophy,
you,

286
00:18:21.580 --> 00:18:24.870
but you,
you embody it,
right?
This is something you carry with you all the time.

287
00:18:24.890 --> 00:18:27.930
Yeah.
On the other hand,
I'm in,

288
00:18:28.590 --> 00:18:33.590
I'm still carrying out many actions with longterm planning in mind.

289
00:18:35.050 --> 00:18:35.580
I've been,

290
00:18:35.580 --> 00:18:40.580
I've been working on designing AI for 30 years and my beat designing it inside

291
00:18:42.361 --> 00:18:43.770
of him,
I might be,

292
00:18:43.771 --> 00:18:48.660
and I'm working on building the same AI system,

293
00:18:48.661 --> 00:18:48.871
you know,

294
00:18:48.871 --> 00:18:53.871
since we started open cog in 2008 but that's using code from 2001 the,

295
00:18:55.690 --> 00:18:59.490
I was building with my colleagues even even earlier.
So,
I mean,
I think

296
00:19:01.050 --> 00:19:05.820
longterm planning is very natural to me,
but,
but nevertheless,

297
00:19:06.020 --> 00:19:09.690
I don't,
I don't wanna make any assumptions about what sort of,

298
00:19:10.650 --> 00:19:15.120
what sort of simulation or a reality that we're living in.

299
00:19:15.620 --> 00:19:18.720
And I think everyone's going to hit a lot of surprises.

300
00:19:18.910 --> 00:19:21.480
One once the simulant singularity,
cause you know,
we,

301
00:19:21.750 --> 00:19:25.200
we may find out that this hat is a messenger from after the singularity.

302
00:19:25.201 --> 00:19:29.430
So it traveled back through time to implant into my brain,

303
00:19:29.431 --> 00:19:33.480
the idea of how to create AI and thus this bring it into existence.

