1
00:00:00,060 --> 00:00:04,350
Well what I'm, what I'm trying to get at
was like, okay, like when things come up,

2
00:00:04,351 --> 00:00:08,370
like say if you find out that
there's people from Isis,

3
00:00:08,520 --> 00:00:11,700
they're using Twitter and they are
using Twitter and posting things.

4
00:00:11,701 --> 00:00:15,270
Like what is the conversation like how do,
what do we do about this?

5
00:00:15,271 --> 00:00:19,200
Do we let them leave this up?
Do we we recognize this as free speech?

6
00:00:19,440 --> 00:00:24,440
Do we only take it down if they're
calling for murder or hate speech?

7
00:00:24,670 --> 00:00:26,280
Like what,
how do you handle that?

8
00:00:27,040 --> 00:00:28,810
Well, it evolves. I mean,

9
00:00:28,811 --> 00:00:33,640
because like we first saw isis when the
world saw isis and we needed to change

10
00:00:33,641 --> 00:00:34,810
our policy to deal with it.

11
00:00:35,810 --> 00:00:37,700
What was the,
what was the initial reaction to it?

12
00:00:37,701 --> 00:00:39,470
So once you realized
that people from Isis,

13
00:00:39,471 --> 00:00:43,130
we're making Twitter accounts and they
were trying to recruit people and doing

14
00:00:43,131 --> 00:00:45,160
all these things, what was
the, what was the thought?

15
00:00:45,840 --> 00:00:48,010
Oh, it was a question. Like what,
what are we gonna do about this?

16
00:00:48,130 --> 00:00:51,360
We haven't experienced this before.
We need to, nobody has, I mean,

17
00:00:51,380 --> 00:00:54,730
you're [inaudible]
pioneers. Yeah. But there,

18
00:00:55,030 --> 00:00:59,140
there are people who have experienced in
different forms and different mediums.

19
00:00:59,200 --> 00:01:01,420
So we reached out to our
government partners, for instance,

20
00:01:01,421 --> 00:01:03,210
our law enforcement partners.
Uh,

21
00:01:03,220 --> 00:01:06,730
we reached out to our peer companies to
ask if they're seeing the same things

22
00:01:06,731 --> 00:01:11,650
that we're seeing. Um, we have a
punch of civil societies that we, uh,

23
00:01:11,740 --> 00:01:13,810
talk to, uh, to get
their take on it as well.

24
00:01:13,811 --> 00:01:18,670
And we try to balance that across,
um, you know, various spectrums,

25
00:01:18,671 --> 00:01:21,550
whether it be, uh, you know, uh,

26
00:01:21,580 --> 00:01:26,440
more organizations that are more focused
on preventing online harassment all the

27
00:01:26,441 --> 00:01:31,441
way to the Aclu and the eff who are
protecting the first amendment online.

28
00:01:32,230 --> 00:01:35,170
So we, we try to get as many
perspectives as possible,

29
00:01:35,650 --> 00:01:38,200
take that and then make
some informed decisions,

30
00:01:38,201 --> 00:01:41,170
but also realize that we're probably
gonna make some mistakes along the way and

31
00:01:42,010 --> 00:01:46,610
all we can do to correct some of
us just be open about where we are.

32
00:01:46,610 --> 00:01:51,070
And that's probably where we failed the
most in the past is we just haven't been

33
00:01:51,071 --> 00:01:54,270
open about our thinking process.
Um,

34
00:01:54,310 --> 00:01:56,290
what led to particular decisions,

35
00:01:56,291 --> 00:01:59,680
how our terms of service evolve.

36
00:01:59,780 --> 00:02:04,300
A terms of service as an area
in our industry is just a,

37
00:02:04,720 --> 00:02:07,810
it's a mess. No one reads
them. You, you know,

38
00:02:07,811 --> 00:02:12,811
you sign up for these services and you
quickly hit accept and we expect people

39
00:02:14,291 --> 00:02:17,770
to read these rules of the road,

40
00:02:18,520 --> 00:02:22,070
but they haven't read them. And have
you ever read them? I have read,

41
00:02:23,780 --> 00:02:27,130
read Facebook's.
I haven't read Facebook's Facebook.

42
00:02:27,670 --> 00:02:32,380
I'm not on Facebook for Facebook. Right.
I'm just kidding. What about Instagram?

43
00:02:32,400 --> 00:02:34,870
Do you ever read theirs? I was, uh,

44
00:02:34,990 --> 00:02:39,160
I was in the first 10 users
of Instagram. Really? Uh,

45
00:02:39,220 --> 00:02:40,960
Kevin was a intern.

46
00:02:40,990 --> 00:02:45,070
Kevin Systrom was an
intern at Odeo and um,

47
00:02:45,400 --> 00:02:48,670
I was one of the first investors
of Instagram and love the service.

48
00:02:48,670 --> 00:02:52,060
I don't think I've ever read their terms
of service. Yeah, that's what I'm saying.

49
00:02:52,061 --> 00:02:55,540
Even you, even me. But I read ours
and you, one of the things I noticed

50
00:02:57,100 --> 00:02:59,140
right away is, you know, you read our, uh,

51
00:02:59,440 --> 00:03:03,820
you read our terms of service and one of
the first things that we put at the top

52
00:03:03,821 --> 00:03:06,700
of the page was copyright.

53
00:03:07,060 --> 00:03:10,300
And intellectual property protections.

54
00:03:11,110 --> 00:03:12,550
You go down and you scroll down,

55
00:03:12,551 --> 00:03:17,551
you see everything about violent
threats and abuse and harassment and uh,

56
00:03:18,460 --> 00:03:19,120
safety.

57
00:03:19,120 --> 00:03:24,120
And it's not that the company
intended for that to be the order,

58
00:03:24,251 --> 00:03:26,560
it just,
we just added things going on.

59
00:03:26,561 --> 00:03:31,330
But even a read of that puts
forth our point of view.

60
00:03:31,450 --> 00:03:36,130
Like what we're, we're actually putting
copyright infringement above the safety,

61
00:03:36,131 --> 00:03:37,540
the physical safety of someone.

62
00:03:38,050 --> 00:03:41,680
So we need to relook at some of these
things and how they've evolved and how

63
00:03:41,681 --> 00:03:44,880
they reacted and visit above
just because it's listed second.

64
00:03:44,920 --> 00:03:49,840
I mean they're essentially all in the
same one sheet there on the one sheet when

65
00:03:49,841 --> 00:03:54,070
you bring it up, when you
discuss it first. Is that
really critical? They're all,

66
00:03:54,100 --> 00:03:57,100
they're all part of the terms of service.
Yeah. But I think the ordering matters.

67
00:03:57,220 --> 00:03:59,560
Like what,
what do we consider to be most important?

68
00:04:00,260 --> 00:04:03,220
And then we have to consider
physical safety to the,

69
00:04:03,430 --> 00:04:07,870
to be the one thing that we protect
the most. And so I to go threats,

70
00:04:07,990 --> 00:04:09,010
physical threats,

71
00:04:09,460 --> 00:04:14,460
doxxing anything that impinges
on someone's physical safety.

72
00:04:14,950 --> 00:04:19,950
This is an area where I don't think a
technology and services like ours have a

73
00:04:20,950 --> 00:04:22,060
focused on enough.

74
00:04:22,061 --> 00:04:26,260
We haven't focused on the off platform
ramifications of what happens online.

75
00:04:26,500 --> 00:04:30,160
So what do you do? Like, like
here's a good, for instance,

76
00:04:30,650 --> 00:04:32,150
this situation with this,

77
00:04:32,151 --> 00:04:36,500
a young kid who had the Maggot hat on
and the native American gentleman who was

78
00:04:36,501 --> 00:04:40,580
in front of him banging the drum and then
people are calling for this kid's name.

79
00:04:40,820 --> 00:04:43,460
They want his name, they want us
to address including Kathy Griffin.

80
00:04:43,850 --> 00:04:45,830
Like how do you,
how do you handle something like that?

81
00:04:46,810 --> 00:04:51,010
Well, cause that's essentially
request for doxing and that's,

82
00:04:51,040 --> 00:04:54,820
that is a new vector that we haven't
seen in mass. I this, these are,

83
00:04:54,850 --> 00:04:58,570
these are the cases that bring up entirely
new thing. So we have to study it.

84
00:04:58,571 --> 00:05:03,160
We have to see how we reacted,
what happened with a network.

85
00:05:03,790 --> 00:05:06,850
Um,
but this goes back to the incentives.

86
00:05:06,880 --> 00:05:11,880
Like we are incentivizing this very
quick reaction and it's taking away from

87
00:05:12,850 --> 00:05:13,510
some more,

88
00:05:13,510 --> 00:05:18,010
some more of the like considered work
that we need to do to really diagnose

89
00:05:18,011 --> 00:05:21,280
what's happening in the moment.
And it was,

90
00:05:21,370 --> 00:05:26,370
it's such an interesting case study to
see how that evolved over just 48 hours.

91
00:05:27,900 --> 00:05:32,260
And that's one of the most fascinating
new cycles or stories in the news cycle

92
00:05:32,261 --> 00:05:35,050
and quite a while because it's nuanced.
There's,

93
00:05:35,051 --> 00:05:36,910
there's many different levels to it.
Yeah.

94
00:05:37,080 --> 00:05:39,720
And a lot of like really
knee jerk reaction.

95
00:05:39,750 --> 00:05:44,730
It's totally, and, but we
helped that had to help it.

96
00:05:45,180 --> 00:05:48,510
Well, it's just, that's how some of
the dynamics of the service work.

97
00:05:48,690 --> 00:05:50,540
But is the nose of this
things we can dynamics

98
00:05:50,540 --> 00:05:53,480
of the service or is it the way
people choose to use the service?

99
00:05:53,690 --> 00:05:58,010
Like if you're a thoughtful person,
you wouldn't just like for instance,

100
00:05:58,040 --> 00:06:01,220
the original image that was distributed,
uh,

101
00:06:01,400 --> 00:06:05,210
came from an account that's now
banned. Right. And so it was,

102
00:06:05,300 --> 00:06:09,920
it was discovered that that
account was a troll account. What,

103
00:06:09,950 --> 00:06:13,970
how does that happen and what,
what was the thought process behind that?

104
00:06:14,060 --> 00:06:18,980
Because the image that they posted was a
legitimate image. It really did happen.

105
00:06:19,260 --> 00:06:22,310
It wasn't, it was a part of
an actual occurring event.

106
00:06:22,640 --> 00:06:26,030
So why did you abandon the person
or the troll account that put it up?

107
00:06:26,750 --> 00:06:28,280
I Dunno about this particular case,

108
00:06:28,281 --> 00:06:33,230
but it's likely that it was found.
There's,

109
00:06:33,260 --> 00:06:35,330
there's a lot of what
you see on the surface,

110
00:06:35,600 --> 00:06:38,570
the Twitter and some of the actions
that we take on the surface.

111
00:06:39,200 --> 00:06:43,220
But where we spend a lot of our
enforcement is actually what's happening

112
00:06:43,221 --> 00:06:46,910
underneath. So in many cases we have, um,

113
00:06:47,060 --> 00:06:52,060
trolls or people like the case that you
mentioned whose sole purpose is just to

114
00:06:55,250 --> 00:06:59,360
harass or abuse or spread,
uh, particular information.

115
00:06:59,810 --> 00:07:04,610
And oftentimes these accounts might be
connected or they start one account that

116
00:07:04,611 --> 00:07:08,900
gets banned, they start another
account. But we can actually see, um,

117
00:07:09,350 --> 00:07:12,890
this through a network Lens and we can
actually see some of those behaviors.

118
00:07:12,891 --> 00:07:16,730
So that might have been one of
the reasons I'm not sure in,

119
00:07:16,820 --> 00:07:20,240
in that particular case.
But, you know, the,

120
00:07:20,580 --> 00:07:24,070
how do you know, do you know because of
Ip addresses? Do you know, because of the,

121
00:07:24,071 --> 00:07:24,500
the

122
00:07:24,500 --> 00:07:26,050
Alrighty of things.
Like it could be,

123
00:07:26,051 --> 00:07:30,100
it could be trying to use the same phone
number, same email address, Ip addresses,

124
00:07:30,550 --> 00:07:33,730
um, device ids, all these
things that we can use to,

125
00:07:33,790 --> 00:07:35,710
to judge what's happening with them,
the context.

126
00:07:35,711 --> 00:07:40,711
So we do have a lot of occurrences of
suspending or temporarily suspending

127
00:07:41,351 --> 00:07:46,090
accounts because of activities
across accounts. Um, and, and that,

128
00:07:46,420 --> 00:07:50,790
that happens a ton. But what I mean,
what I mean in the, in that we're,

129
00:07:50,791 --> 00:07:53,770
we're helping this right now. It's
like some of the incentives, like just,

130
00:07:53,800 --> 00:07:58,800
just imagine seeing that unfold and
when you see someone with one take,

131
00:07:59,710 --> 00:08:04,540
it kind of emboldens something to follow
along and then this mob kind of rules.

132
00:08:05,170 --> 00:08:10,150
So there has to be a way
for us to incentivize, uh,

133
00:08:10,540 --> 00:08:14,890
a lot more, more considered and more
nuanced introspection of what's going on.

134
00:08:14,920 --> 00:08:16,330
Yeah.
Give everybody mushrooms.

135
00:08:16,630 --> 00:08:19,750
Probably the only way to know how are
you going to get people to be more

136
00:08:19,760 --> 00:08:20,760
considerate? I mean, what,

137
00:08:21,130 --> 00:08:26,070
what do you mean writing constantly
engineering social behavior, right? Yup.

138
00:08:26,240 --> 00:08:27,230
Provided more context.

139
00:08:27,910 --> 00:08:32,850
One more context. I could an
example. Let's, let's take
Brexit for example. Okay. So,

140
00:08:32,851 --> 00:08:37,440
um, if I followed a bunch
of accounts that like,

141
00:08:37,770 --> 00:08:42,030
like Boris Johnson who was constantly
giving me information about reasons to

142
00:08:42,031 --> 00:08:46,800
leave, I would probably
only see that perspective.

143
00:08:46,830 --> 00:08:49,290
Nigel Farrage yeah. And those, and those,

144
00:08:49,500 --> 00:08:54,500
a lot of folks just will not follow
accounts that have a completely different

145
00:08:54,631 --> 00:08:58,440
perspective, a different
influence. A number of people do.

146
00:08:58,770 --> 00:09:02,610
Hopefully journalists do,
but most people won't do that work.

147
00:09:02,730 --> 00:09:07,650
So this is the only tool we give
people follow an account. If, however,

148
00:09:08,010 --> 00:09:12,120
during that time you followed the Hashtag
you followed the Hashtag vote leave

149
00:09:12,600 --> 00:09:17,600
95% of the conversation and the tweets
you see are all reasons to leave.

150
00:09:18,660 --> 00:09:23,190
But there's a small percentage that shows
a different perspective and that shows

151
00:09:23,191 --> 00:09:26,280
a different um,
a different reasoning.

152
00:09:27,270 --> 00:09:31,320
We don't make it easy for anyone
to do that and that is a loss.

153
00:09:31,320 --> 00:09:36,030
Anyone to follow perspective,
follow the Hashtag follow topic,

154
00:09:36,031 --> 00:09:39,210
follow, follow an interest.
And because of that, um,

155
00:09:39,330 --> 00:09:44,330
we help build an echo
chamber and something that
doesn't really challenge any

156
00:09:46,020 --> 00:09:50,430
perspective. And not to say that
we should force it upon people,

157
00:09:50,640 --> 00:09:53,350
but we don't even make it easy for
people to do in the first place though.

158
00:09:53,351 --> 00:09:57,090
The way you do that today, as you go
to this, the explore tab, you look,

159
00:09:57,120 --> 00:10:00,960
you search for a Hashtag or you tap
into a Hashtag and you can see all the

160
00:10:00,961 --> 00:10:05,520
conversation. But that's, that's work.
And most people just won't do the work.

161
00:10:05,521 --> 00:10:07,380
They'll stay in their timeline,
they'll see what they need to see.

162
00:10:07,381 --> 00:10:11,310
And I can certainly imagine,
you know why?

163
00:10:11,670 --> 00:10:14,970
If I'm just following a bunch of people
who have the exact same take on this,

164
00:10:14,971 --> 00:10:19,971
it just continues to embolden and
Bowman bolden and they see nothing of a

165
00:10:20,461 --> 00:10:23,250
different perspective on the exact same,

166
00:10:24,180 --> 00:10:25,500
the exact same situation.

