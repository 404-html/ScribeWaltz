1
00:00:01,230 --> 00:00:03,870
Seattle, you must be, I mean,
cause you're here, right?

2
00:00:03,871 --> 00:00:05,820
You feel tremor because they must
say they happen all the time. Right.

3
00:00:05,821 --> 00:00:09,100
I've only been a few times. One
time I felt them before. The,

4
00:00:09,101 --> 00:00:11,640
the biggest ones I ever felt
was when I first moved here.

5
00:00:11,940 --> 00:00:15,340
I first moved here in 94 and it was right
after the Northridge earthquake and uh,

6
00:00:15,380 --> 00:00:17,190
I was in my house and I felt,
uh,

7
00:00:17,820 --> 00:00:21,150
I guess it was like a 5.5
where the whole thing just,

8
00:00:23,150 --> 00:00:23,941
it was weird,

9
00:00:23,941 --> 00:00:28,140
but it gave me the impression that
the house that I was living in or the

10
00:00:28,141 --> 00:00:30,900
apartment that I was in was
made out of the same stuff,

11
00:00:31,290 --> 00:00:33,510
like a box that a
refrigerator would come in.

12
00:00:33,540 --> 00:00:36,720
You know how like you can
move it around inside.

13
00:00:36,780 --> 00:00:41,190
The whole thing was just
moving like easily left and
right. I was like, Holy Shit.

14
00:00:41,190 --> 00:00:43,770
That was my, I've only had a few times
that I've, you know, felt on, I'm like,

15
00:00:44,130 --> 00:00:46,110
it's like it's, you know, at
first I thought it was a truck.

16
00:00:46,111 --> 00:00:49,410
I thought a truck was rolling,
you know? And I'm like, no man,

17
00:00:49,411 --> 00:00:52,380
that was an earthquake. And the
one that I experienced was nothing.

18
00:00:52,530 --> 00:00:53,920
I mean baby one,

19
00:00:53,980 --> 00:00:58,560
the ones that they're getting right now
somewhere in the world, I mean there's,

20
00:00:58,561 --> 00:01:01,770
there's always something that's
like a five or six. Yeah.

21
00:01:02,220 --> 00:01:04,800
So this goes back to your point about the,
uh, you know, a lot of planets, right?

22
00:01:04,801 --> 00:01:09,800
We'll be like way more plate
tectonically active, volatile than ours.

23
00:01:09,801 --> 00:01:10,321
So,
you know,

24
00:01:10,321 --> 00:01:13,530
I mean a lot to me and I
think good planets are going
to be like really hard to

25
00:01:13,531 --> 00:01:18,360
find. Yeah. So, you know,
it may be, that may be the
solution. The other solution is,

26
00:01:18,361 --> 00:01:20,720
you know, I mean, the way
you guys found out about me,

27
00:01:20,721 --> 00:01:24,450
so I did that article about the previous
civilization, you know, like what is it,

28
00:01:24,451 --> 00:01:27,950
how do you know whether or not there was
a previous civilization? And you know,

29
00:01:28,560 --> 00:01:31,350
one solution could be that,
look, they were here, you know,

30
00:01:31,351 --> 00:01:33,780
that somebody did arrive and
spent some time on earth.

31
00:01:33,990 --> 00:01:36,870
But if it was like half a billion
years ago and they only last, right,

32
00:01:36,871 --> 00:01:40,150
every civilization has a finite
lifetime. They only lasted for, you know,

33
00:01:40,230 --> 00:01:42,420
even a few hundred thousand years,
they wouldn't leave.

34
00:01:42,421 --> 00:01:44,880
There'd be no record left to, it'd
be nothing. There'd be nothing.

35
00:01:44,881 --> 00:01:48,000
What do you think that's possible that
something's ever visited here and because

36
00:01:48,001 --> 00:01:49,840
that is the big question and that's the,

37
00:01:50,380 --> 00:01:54,630
the thing that gets the UFO dorks
more jazzed up than anything is the

38
00:01:54,631 --> 00:01:58,110
possibility that we'd been visited. Yeah.
Not In the, I'm, I'm definitely not you.

39
00:01:58,150 --> 00:02:03,030
Ufos. There's, there's two
arguments against you. Fo's
uh, one is as, as you know,

40
00:02:03,031 --> 00:02:06,090
a friend of mine, Jason Wright says, he's
like, look as astronomers, man, you know,

41
00:02:06,091 --> 00:02:09,270
we're finding all those aspects. We're
finding a little chunks of rock moving.

42
00:02:09,271 --> 00:02:13,200
You know, nobody looks at the sky harder
than we do. Right, right. And you know,

43
00:02:13,201 --> 00:02:15,810
if anything, unless you want to
go to the conspiracy theories,

44
00:02:16,110 --> 00:02:18,360
we would have found something.
My other argument is like,

45
00:02:18,361 --> 00:02:20,640
what's with the headlights? You know what
I mean? People are just like, Oh man,

46
00:02:20,641 --> 00:02:22,900
I saw lights in the sky and then they
moved around and then they disappeared.

47
00:02:23,010 --> 00:02:25,260
You know, but they don't really
want to be seen. I'm like,

48
00:02:25,290 --> 00:02:28,090
why did they have headlights on? Like
why just turn off the fricking roommates?

49
00:02:28,140 --> 00:02:30,000
It just, why would they have
lights in the first place?

50
00:02:31,130 --> 00:02:33,690
You came from another civilization and
you can't get around without your high

51
00:02:33,691 --> 00:02:34,100
beams.

52
00:02:34,100 --> 00:02:38,490
You know what the fuck are you seeing
what those lights and you're not seeing

53
00:02:38,491 --> 00:02:39,324
shit.

54
00:02:39,500 --> 00:02:42,990
When was the last time you saw planes
have lights or their planes don't run into

55
00:02:42,991 --> 00:02:45,900
them. They don't have lights so they could
see where they're going. Yeah. So what,

56
00:02:45,901 --> 00:02:47,220
so why do they have, right, exactly.

57
00:02:47,221 --> 00:02:48,930
The planes have lights so
other people can see them.

58
00:02:48,931 --> 00:02:50,760
So they're here but they're mysterious.

59
00:02:50,761 --> 00:02:53,280
They don't want to be found in turn
off the fricking light. So they, yeah,

60
00:02:53,281 --> 00:02:55,590
they wouldn't have any lines.
Yeah, they always have lights.

61
00:02:56,650 --> 00:02:58,770
People are always saying like a hub and
I saw license guy and then they moved

62
00:02:58,771 --> 00:03:02,940
back and forth. And the problem
with human memory memories, right?

63
00:03:02,950 --> 00:03:07,950
Horrible and memory of events that are
stunning or shocking or a nerving or you

64
00:03:08,351 --> 00:03:11,500
think you saw something, right. They've
done all that research. Even in trials,

65
00:03:11,501 --> 00:03:15,610
right. Where people see things currently
Ufo is, you know, I'm just kinda like,

66
00:03:16,150 --> 00:03:19,810
do you think it's possible that there
could be life on other planets and it's

67
00:03:19,811 --> 00:03:23,290
possible there could be intelligent life
on other planets and we send probes to

68
00:03:23,291 --> 00:03:27,280
Mars? Why wouldn't I think that
if they do send something here,

69
00:03:27,281 --> 00:03:30,270
they're going to send something without
biological life inside of it. Yeah. Yeah.

70
00:03:30,280 --> 00:03:30,851
Well this is,
you know,

71
00:03:30,851 --> 00:03:32,920
it's a really interesting question is
like you were talking about like what will

72
00:03:32,921 --> 00:03:36,280
we evolve into in, you know,
a million years, uh, you know,

73
00:03:36,281 --> 00:03:39,670
it may be possible that biology
is a short period of intelligence.

74
00:03:40,470 --> 00:03:45,400
You build machines and the
machines become, you know,
download yourself into them.

75
00:03:45,401 --> 00:03:47,350
I mean, that's a real
possibility that like, you know,

76
00:03:47,351 --> 00:03:50,610
silicon makes a lot
more sense than wetware.

77
00:03:50,800 --> 00:03:54,730
While the problem is we think
of artificial intelligence
as artificial, right?

78
00:03:54,731 --> 00:03:58,750
It's definitely real, right? Yeah, no,
that's exactly, it's not, it's not fake.

79
00:03:58,780 --> 00:04:01,900
It's right there. Right. You know,
it's like there's artificial milk,

80
00:04:02,290 --> 00:04:05,890
you know what I mean? But it's, it's
a liquid. It's an actual liquid.

81
00:04:06,010 --> 00:04:10,570
There's not an artificial life,
silicon created life.

82
00:04:10,710 --> 00:04:12,070
That's again,
the idea that the biosphere,

83
00:04:12,071 --> 00:04:16,420
this may be that what the biosphere
solution to spreading itself to getting it

84
00:04:16,421 --> 00:04:20,220
maybe that like, yeah, silicon that
that's kind of a normal, I mean I'm,

85
00:04:20,470 --> 00:04:25,240
I am super skeptical about like the whole
trans human thing about we're going to

86
00:04:25,241 --> 00:04:27,270
download ourselves into intelligent,
you know, it took, are you,

87
00:04:27,271 --> 00:04:30,940
have you ever gone to one of those
conventions dorks now there's some pretty

88
00:04:30,941 --> 00:04:34,720
serious dork in, well, there's
geniuses in those dorks too.

89
00:04:34,721 --> 00:04:36,970
It's really interesting.
It's like they're fully,

90
00:04:36,971 --> 00:04:39,820
wholly committed to this prospect
of downloading themselves.

91
00:04:39,821 --> 00:04:42,610
You've talked to Kurt's well before.
Uh, no, but I've read his stuff.

92
00:04:42,680 --> 00:04:47,450
I got a chance to interview him a few
years back for [inaudible] smart beyond

93
00:04:47,460 --> 00:04:50,830
smart. But God damn is he committed
to it. And then when you get to it,

94
00:04:50,831 --> 00:04:55,090
you realize that what he actually
wants to do is recreate his father.

95
00:04:56,110 --> 00:05:00,910
Yeah. He's, his father died when he
was young and he wants to be able to,

96
00:05:00,911 --> 00:05:05,320
through memories and photographs
and what he knows about his father,

97
00:05:05,530 --> 00:05:10,330
literally recreate his father and be
able to have a conversation with them.

98
00:05:10,660 --> 00:05:11,650
He was talking about this,

99
00:05:12,210 --> 00:05:15,730
but what he's doing is he's going way,

100
00:05:15,731 --> 00:05:19,960
way into the future into the possibilities
of, he's not seeing like, well,

101
00:05:20,170 --> 00:05:22,840
five years from now we're going
to be able to send, you know,

102
00:05:23,110 --> 00:05:25,930
gigabyte pictures through the mail.
Now he's saying,

103
00:05:26,200 --> 00:05:31,200
let's think of if you
keep going exponentially
electronics and technology and

104
00:05:31,721 --> 00:05:33,670
innovation keeps continually accelerating.

105
00:05:33,940 --> 00:05:38,500
We could potentially get to the
point where it's the impossible.

106
00:05:38,680 --> 00:05:39,070
Well,

107
00:05:39,070 --> 00:05:43,600
you'd be able to recreate human beings
based on your knowledge of them and in

108
00:05:43,601 --> 00:05:45,940
the end there'll be able to come
up with some sort of a program.

109
00:05:45,941 --> 00:05:48,460
Will he'll be able to have a
conversation with his father. Yeah. See,

110
00:05:48,461 --> 00:05:50,260
I think that misunder I
mean, I think it's, you know,

111
00:05:50,261 --> 00:05:53,470
it's true about like the possibility
of a singularity in technological

112
00:05:53,471 --> 00:05:57,530
development, but you know, it's not going
to be his father. That's the sad thing,

113
00:05:57,560 --> 00:05:59,590
you know, I think we seriously, you know,

114
00:05:59,591 --> 00:06:02,570
I've done some work on this as an
interest of mine about mind. What is mind,

115
00:06:02,600 --> 00:06:05,450
you know, what's the relationship
between mind and matter? And, um,

116
00:06:05,480 --> 00:06:07,700
I think we have like a
serious misunderstood,

117
00:06:07,701 --> 00:06:11,630
we don't understand consciousness very
well at all. So the idea that like,

118
00:06:11,660 --> 00:06:14,180
oh yeah, it's just got to be
trivial to download your, you know,

119
00:06:14,181 --> 00:06:17,060
your consciousness into a computer.
I think it's pretty, you know,

120
00:06:17,061 --> 00:06:21,170
there's like a shit load of assumptions
in there about like what mind is and the

121
00:06:21,171 --> 00:06:25,280
relationship between like your
neurons and, you know, awareness.

122
00:06:25,340 --> 00:06:27,620
So that's why I think those guys,
it's a little bit of a religion,

123
00:06:27,740 --> 00:06:30,890
you know what I mean? There's a little
bit of religion and, and you know,

124
00:06:30,891 --> 00:06:35,570
so it's like 2045 is the new benchmark.
For whatever reason.

125
00:06:35,571 --> 00:06:37,520
That's the new number.
They get to keep going back.

126
00:06:37,521 --> 00:06:40,430
It's gonna be like every
other, every other rapture. Ah,

127
00:06:40,431 --> 00:06:42,970
we thought it was 2045 we really got 2065,
you know,

128
00:06:42,990 --> 00:06:45,020
we went to this 2045 conference.

129
00:06:45,021 --> 00:06:48,230
Me and my friend Ran Dunkin and we got
to talk to some of these guys and it's

130
00:06:48,650 --> 00:06:49,640
really fascinating.

131
00:06:49,910 --> 00:06:54,410
And one of them had created some
robot that was supposed to be him,

132
00:06:54,950 --> 00:06:57,800
but it didn't work well. So they
never revealed that at the conference.

133
00:06:58,010 --> 00:07:01,100
Too many bugs in it.
It's intriguing.

134
00:07:01,101 --> 00:07:03,770
It's like I want them to
keep going cause I mean when,

135
00:07:03,771 --> 00:07:05,210
when do we get going to get to x?

136
00:07:05,211 --> 00:07:08,810
Mokena when are we going to talk about
good movies? So did you like that?

137
00:07:08,811 --> 00:07:10,850
One of my favorite movies ever.
So smart.

138
00:07:10,851 --> 00:07:14,630
So like just like simple stop beating me
over the head with it and cut the shit

139
00:07:14,631 --> 00:07:18,590
most moving moments in that
movie. Yeah, the entire,

140
00:07:18,650 --> 00:07:21,830
I think that's one of my
all time top 20 movies,

141
00:07:21,831 --> 00:07:25,610
which she stabs him just emotionless.
It was amazing.

142
00:07:25,820 --> 00:07:30,820
Amazing alert. Did you
like a annihilation? Yes.

143
00:07:32,270 --> 00:07:34,100
I didn't like it as much,
but I liked it.

144
00:07:34,101 --> 00:07:38,030
It was very weird and the ending seemed
like some producers put their urges in

145
00:07:38,031 --> 00:07:41,990
the soup and just like back to Pittsburgh,
what's happening here?

146
00:07:41,991 --> 00:07:46,940
Who made this part? But I enjoyed
it. Yeah. So I think like, you know,

147
00:07:46,941 --> 00:07:50,490
I mean the danger with anything when
it comes to AI and said, so what are,

148
00:07:50,491 --> 00:07:53,360
you know, here's an interesting thing
about Ai. Like we're, we are getting,

149
00:07:53,390 --> 00:07:55,410
making amazing strides with AI now.

150
00:07:55,480 --> 00:07:58,220
Now artificial intelligence is
different from artificial consciousness,

151
00:07:58,221 --> 00:08:01,820
you know what I mean? So like,
but um, but the AI that, you know,

152
00:08:01,821 --> 00:08:04,820
what we're getting out of it is nothing
like us, right? So the, you know,

153
00:08:04,821 --> 00:08:06,590
back in the day where people were like,
oh,

154
00:08:06,591 --> 00:08:09,530
we're going to like model the human
brain and that's how we'll do it. Well,

155
00:08:09,531 --> 00:08:11,780
like make programs that are, and
what they've learned, it's like,

156
00:08:11,810 --> 00:08:15,050
oh that doesn't really work that well.
So now this is the whole big data thing.

157
00:08:15,051 --> 00:08:18,650
Like, you know, network
theory and big data and deep
learning. We're like, you know,

158
00:08:18,680 --> 00:08:21,010
they're using statistical,
you know, the power of,

159
00:08:21,020 --> 00:08:25,580
of having huge amounts of computing
and statistical reasoning so that like,

160
00:08:25,581 --> 00:08:29,270
you know, get the computer. Like, you
know, it'll find the picture of the cat,

161
00:08:29,450 --> 00:08:32,480
but you have no idea why it found
the pit didn't reason. Like, Oh yeah,

162
00:08:32,481 --> 00:08:34,970
that was where it looks like
cat and I liked cuts, you
know, it was just like, oh,

163
00:08:35,000 --> 00:08:37,910
these kinds of lines go with that kind
of thing and it has no idea what it's

164
00:08:37,911 --> 00:08:42,200
doing, but it'll act intelligently.
And so that's, you know,

165
00:08:42,201 --> 00:08:45,950
I mean that's kind of freaky deaky in
a lot of ways too. I think people are,

166
00:08:46,040 --> 00:08:49,400
I think it's smart to think
about the dangers of AI,

167
00:08:49,401 --> 00:08:53,450
not necessarily what additional
features to us. Yeah.

168
00:08:53,660 --> 00:08:55,160
Not necessarily the dangerous.

169
00:08:55,380 --> 00:08:59,790
Just the dangers to this thing that we
are this weird monkey thing that wants to

170
00:08:59,791 --> 00:09:02,820
keep being a monkey thing.
Well you mean in the sense of like,

171
00:09:02,821 --> 00:09:06,330
right that it's going to replace us?
Yes. Yeah. Yeah. I think it's inevitable.

172
00:09:06,750 --> 00:09:07,351
I mean,
do you,

173
00:09:07,351 --> 00:09:10,680
don't you think that if you go back and
interview the single celled organism

174
00:09:10,681 --> 00:09:12,360
before branched off into multicell,

175
00:09:12,420 --> 00:09:15,870
that'd be kind of a boring fuck
this multi-celled bullshit.

176
00:09:15,900 --> 00:09:19,790
I don't want to stay a single cell
at the bottom of the assholes,

177
00:09:19,870 --> 00:09:24,420
man with their rock and roll
music metal cars. Fuck that.

178
00:09:24,510 --> 00:09:29,280
I don't want a car, man. I liked staying
down here. There's a real thought to that,

179
00:09:29,281 --> 00:09:33,240
that we are the wetware that
is the problem. No, no, listen,

180
00:09:33,241 --> 00:09:37,320
I think it's fully conceivable. Like I'm
not going to like, it would never happen,

181
00:09:37,530 --> 00:09:40,050
but as a, a guy knows, a
philosopher says, you know,

182
00:09:40,051 --> 00:09:43,500
there's a certain way in which everything
we do with AI right now, it's not like,

183
00:09:43,650 --> 00:09:48,300
you know, Watson is playing chess. It's
like we're using Watson to play chess.

184
00:09:48,330 --> 00:09:52,800
It's our tool. You know, and I think the
PR, the fear is the tool gets out of hand.

185
00:09:52,801 --> 00:09:55,830
Not that it like develops a thing
where like I hid humans. You know, dad,

186
00:09:55,831 --> 00:09:58,260
I hate you, but more
that it's like, you know,

187
00:09:58,290 --> 00:10:00,690
these things which are
not actually thinking.

188
00:10:00,720 --> 00:10:03,300
They act intelligent but they're
not thinking it can have a huge,

189
00:10:03,301 --> 00:10:06,240
like it'll have a really negative
impact. You know, it'll, it'll, you know,

190
00:10:06,241 --> 00:10:07,440
what does it,
there's that example of like,

191
00:10:07,441 --> 00:10:10,370
if you design something that is
an AI system to make paper clips,

192
00:10:10,500 --> 00:10:13,740
it ends up consuming the entire planet
making paperclips because that's what you

193
00:10:13,741 --> 00:10:17,000
told it to do. Right. You never told it
to make a sustainable amount of. Right,

194
00:10:17,160 --> 00:10:21,060
exactly. So I'm, I'm more worried about
that than I am of like Skynet, you know,

195
00:10:21,061 --> 00:10:22,170
coming over and like,
you know,

196
00:10:22,171 --> 00:10:25,110
deciding that it's going to take a drop
all the bombs on us cause you need to

197
00:10:25,111 --> 00:10:25,510
get rid of it.

