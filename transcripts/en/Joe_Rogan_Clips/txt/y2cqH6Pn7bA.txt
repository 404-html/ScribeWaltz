Speaker 1:          00:00          Don't feel bad. I get you one. Hold on a second. Um, who's, who's seen all this stuff? Is this stuff mean? Uh, it has, obviously Facebook has a check this out. I'm sure Twitter's aware. What has the reaction been and is there any sort of a concerted effort to mitigate some of some of the impact of these, these sites of

Speaker 2:          00:21          yeah, lots of it actually. So I think, um, in, in 2017 was when we started, like we being independent researchers, I guess people on the outside of the company's academics, um, began to find the content, you know, really began to investigative journalists would identify the name of a page and then me and people like me would go and we would scour the Internet looking for evidence of what was on that page. So I found a bunch of the stuff on Pinterest, for example, wrote about it. Um, guy by the name of Jonathan Albright, uh, found a crowd tangle data cache. And with that we got the names of a bunch more pages, bunch more posts, and we had some really interesting stuff to work with originally. The platforms were very resistant to the idea that this had happened. And so as a result of that, they were, um, in, you know, there was a, the first thing that Zach said in 2016 when Trump gets elected Twitter, it goes crazy that night with people who work at Twitter saying, oh my God, where are we responsible for this?

Speaker 2:          01:34          Which is very silicon valley thing to say. Um, but what I think they meant by that was their platform had been implicated as hosting Russian bots and fake news and harassment mobs in a number of other things. And there was always the sense that it didn't have an impact on it didn't matter. And so this was the first time that they started to ask the question, did it matter? And then Zach made that statement. Um, fake news is a very small percentage of, you know, whatever on Facebook, the amount of information on Facebook and the idea that it could have swung on election was ludicrous. So you have the platforms kind of, uh, the leaders at the platforms digging in and saying it's inconceivable that, that, that this, you know, could have happened. And as the research and the discovery begins to take place over the next nine months or so, you, you, you get to when the tech hearings happen.

Speaker 2:          02:26          So I worked with, um, guy by the name of Tristan Harris. He's the one who introduced me to Sam. Um, and he, he and I started going to DC with a third fellow, Roger Mcnamee and saying, Hey, there's so much, there's this body of evidence that's coming out here and we need to have a hearing. We need to have congress ask the tech companies to account for what happened, to tell the American people what happened. Because what we're seeing here as outside researchers, what investigative journalists are writing, the things that we're finding just don't line up with the statements that, that nothing happened. And was, this was all no big deal. And so we start asking for these hearings and actually, um, myself and a couple of others then began asking them in the course of these hearings, can you get them to give you the data because the platform's hadn't given the data.

Speaker 2:          03:22          So it was that lobbying by concerned citizens and journalists and researchers saying, we have to have some accountability here. We have to have the platforms account for what happened. They have to tell people. Um, because it had become such a politically divisive issue, did it even happen? And we felt like having them actually sit there in front of Congress and account for it would be the first step towards, um, towards moving forward in a way, but, but also towards, um, changing the minds of the public and making them realize that what happened on social platforms matters. And it was, it was really interesting to, to be part of that as it, as it played out. Um, because one of the things that Senator Blumenthal, one of the, one of the senators said was actually said, um, Facebook and Twitter have to notify people who engage with this content.

Speaker 2:          04:15          And so there, there was this idea that, um, if you are engaging with propagandas content, you should have the right to know. And so they started to push messages. Twitter sent out these emails to all these people saying you engaged with this Russian troll. And, um, Facebook created a little field, a little little page that told people if they had liked or followed a troll page. So it was really trying to get at making the platform's accountable with they it outside the platform through email. Huh. Twitter is interesting because I would never read an email. The Twitter sends me, this has just gotta be nonsense. I didn't get one. So I maybe, I guess I just got lucky, but, um, I might've had a multiple day back and forth and some Russian, but that was, I think one of the first steps towards saying like, how do we make the platforms accountable?

Speaker 2:          05:10          Because the idea of that platform should be accountable, was not a, a thing that everybody agreed on in 2015 when we were having this conversation about isis. And that's where there's the, the through line here, which is, um, and it does connect into some of the speech issues too, which is what kind of monitoring and moderation do you want the platforms to do. And when we were having this conversation about Isis, there was a, um, not insignificant collection of voices that were really concerned that if we moderated isis trolls on Twitter and then not the beheading videos, there were sort of universal agreement that the beheading videos should come down. But if we took out what were called the Isis fanboys, which were like 30, 40,000 accounts at their peak, um, that we would, yeah, there's a document called the isis Twitter census for anyone who wants to actually see the research done on understanding the Twitter network in 2015 there was a sense that like one man's terrorist is another man's freedom fighter.

Speaker 2:          06:10          And if we took down Isis fanboys, um, where we stifling their freedom of speech, freedom of expression and like goodness, what would come next. And that when you, when you look at that, um, that fundamental swing that has happened now in 2018, 20, 19 or those, there's that same narrative because originally no moderation was taking place and then now there's a feeling that it's kind of swung too far in the other direction. But the original conversations were really, how do we make Twitter take responsibility for this? And legally they aren't responsible for it. All right? They are legally indemnified against the, they're not responsible for any of the content on their platforms. None of the platforms are. There's a law called communications decency act section two 30, and that says that they're not responsible. They have the right to moderate but not the obligation to moderate because they are indemnified from responsibility. So the question becomes, now that we know that these platforms are used for these kinds of harms and they are used for this kind of interference, um, where is that balance? What do we want them are responsible for monitoring and moderating? And how do we, um, how do we recognize that that is occasionally going to lead to an incorrect attribution's people losing accounts? And things like that. So

Speaker 1:          07:38          yeah, they're, they're in a weird conundrum right now or they don't, they, they're, they're trying to keep everything safe and they want to encourage people to communicate on the platforms. So they want to keep people from harassing folks. But because of that, they've also, they've got these algorithms and they, they, they tend to miss very often like this whole learn to code fiasco where people are getting banned for life for saying learn to code, which is about as preposterous as it gets. I think the learn to code fiasco is going to be the tipping point where a lot of people in the, in the future, when they look back on, when did the heavy handedness become overreach, learn to code. Because I mean, Jesus Christ, I mean that if you can't say learn to code. I mean, I look at my mentions, I mean on any given day, especially like yesterday, I had, um, uh, uh, vaccine proponent. Yeah, yeah. Peter's Great. And, and you know, and it seemed like what was really disturbing to me was like, the vast majority of the comments were about vaccines. And so few about these unchecked diseases that are running rampant in poor communities, which was the most disturbing aspect of the conversation to me, that there's diseases that rob you of your intellectual capacity that are extremely common, that as many as 10% of people in these poor neighborhoods

Speaker 2:          08:56          have almost no discussion. It was all just insults and, and you know, you fucking chill and this and that. You know, it's going to be interesting but they're going to be a disaster. Well, let me, let me, I think that one of the challenges for the platforms, there's a lot of things start out like learn to code. I remember, yeah, I watched that play out. Covington Catholic was another thing that, I mean, God, um, I learned to code. There was some of the people who are trolling and just saying learn to code and you know, whatever, you don't have a right to not be offended. But then there was the, um, the other accounts that kind of took it that step further and began to throw in like the ovens and the other stuff like that. It would learn to code. Right. And that's one of the challenges with the platform, which is if you're trying to assess the, um, just the content itself, like if you start doing keyword bands, you're going to catch a lot of shit that you don't want to catch.

Speaker 2:          09:59          But the flip side is if you, you know, the, the, this is the challenge of moderating at scale, which is, um, where, you know, what, what side do you come down on? Do come down on saying like 75% of people with Hashtag learn to code or just, you know, I'm not doing anything incredibly offensive. And then the 25% who are, they really changed the tone of the overall campaign and the Hashtag for the entire community. And that's where you see Twitter, I think, come in with the more heavy handed and just shut it down kind of thing. Um, I don't, I don't know that there's an easy answer. I think that we are, you know, even today, what was the latest kerfuffle? Elizabeth Warren got a, an ad taken down on Facebook and then there was a whole conversation about, was Facebook censoring Elizabeth Warren. And I personally didn't think that it read like censorship, but it was, um, it was an ad about funny enough her a platform to break up Facebook. Whoa.

Speaker 2:          10:59          This is kind of, it seems like it sort of read more like a cell phone. Like she had a picture of Facebook's logo in the, um, in the image and there that violates the ads terms of service. And the reason behind that is actually because Facebook doesn't want people putting up ads that have the Facebook logo in it because that's how you scam people. Right. That's a great way to, to rip people off. And so probably just like an automated, um, you know, an automated take down like an automated, they get halts the ad you have to go and make some changes, then you can proceed it back out again. Uh, but it just happens at a time when there's like, so little assumption of good faith and so little assumption of such extreme, uh, anger and polarization and um, you know, assumption that the platforms are censoring with, with every little kind of moderation snafu that it, it makes it, I think, I don't know how we have the conversation in a way that's healthy and looks towards solutions as opposed to the left screaming that it censored the right screaming that it's censored.

Speaker 2:          12:02          The platforms, um, you know, trying to get around how do we both moderate and not moderate, which is a tough position to be in. Um, I think, I don't have any

Speaker 1:          12:14          good, no one does, no good answers to the issue and Vigia discussed that pretty much in depth what she was saying. This is about moderating and scale. When you're talking about millions and millions and millions of posts and a couple thousand people working for the organization. And then algorithms and computer learning that's trying to keep up. And that's where things like learn to code. And people were so outraged and pissed off because when they do get banned, they feel like they've been targeted. Whether you really just ran into some code and then it's really hard to get someone to pay attention to your appeal. Right. Because there's not enough people that are looking at these appeals and there's probably millions of appeals every day. It's almost impossible.

Speaker 2:          12:58          Yeah. And there's, you know, depending on which, um, which side you're on, you're also here like, um, this person is harassing me and I'm demanding moderation and nobody's doing anything about it. Yes. Uh, so it's, it's definitely I think, gotten worse. Um, it's, it's interesting to look back at 2016 and wonder how much of the, um, where we are now is in part because not a whole lot happened in 2016 and 2016 it was 2015 in particular, very light, like almost no moderation. Just kind of let it all hang out there. And I look at it, um, I look at it now, particularly as it evolved into this conversation about free speech, public squares, um, and what the new kind of infrastructure for speech and what rights we should expect on it. It's a really tough, um, you know, I think some of it is, is almost like the people who are, who hear the words free speech and they just assume that it's people asking for carp launch, right.

Speaker 2:          14:05          To harass and saying, you know, um, how do we balance that? You do. I think Jack and Vagina was saying this on your show. Um, how do we maximize the number of people who are involved, make sure that all voices do get heard without being unnecessarily heavy handed and moderating a thought or content and instead Monterrey behavior and, um, and said moderate, um, particular types of signatures of things that are inauthentic or things that are coordinated and looking at this beginning gets to this information to, rather than trying to police disinformation by looking at content, really looking instead at actions and behavior and account authenticity and dissemination patterns. Cause a lot of the worst.